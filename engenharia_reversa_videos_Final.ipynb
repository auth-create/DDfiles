{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/auth-create/DDfiles/blob/main/engenharia_reversa_videos_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx8sEBm8_yKO"
      },
      "source": [
        "# SISTEMA MODULAR DE ENGENHARIA REVERSA DE VÍDEOS - VERSÃO FINAL OTIMIZADA\n",
        "\n",
        "Este notebook foi aprimorado para oferecer uma experiência mais intuitiva, organizada e robusta para a engenharia reversa de vídeos. Cada etapa é modular, com validações de pré-requisitos e feedback em tempo real para guiá-lo(a) durante o processo.\n",
        "\n",
        "## COMO USAR:\n",
        "1.  **Execute as células em ordem, de cima para baixo.** Cada célula foi projetada para ser executada sequencialmente.\n",
        "2.  **Atenção aos feedbacks:** Mensagens claras indicarão o sucesso de cada etapa, possíveis erros e qual a **PRÓXIMA CÉLULA** a ser executada.\n",
        "3.  **Corrija e re-execute:** Se um erro for detectado, uma mensagem explicativa será exibida. Corrija o problema (geralmente um caminho incorreto ou dependência ausente) e re-execute a célula que falhou.\n",
        "4.  **Progresso Salvo:** O sistema salva automaticamente o progresso e os dados gerados em cada etapa, permitindo que você retome de onde parou.\n",
        "\n",
        "## ESTRUTURA DO PROCESSO (Layers e Sublayers):\n",
        "Este sistema é organizado em camadas lógicas para facilitar o entendimento e a execução:\n",
        "\n",
        "### LAYER 1: CONFIGURAÇÃO E PREPARAÇÃO\n",
        "*   **CÉLULA 1.1: SETUP INICIAL E INSTALAÇÃO DE DEPENDÊNCIAS**\n",
        "*   **CÉLULA 1.2: CONFIGURAÇÃO INICIAL E VALIDAÇÃO DA PASTA DE TRABALHO**\n",
        "\n",
        "### LAYER 2: DESCOBERTA E EXTRAÇÃO DE DADOS BRUTOS\n",
        "*   **CÉLULA 2.1: DESCOBERTA E CATALOGAÇÃO DE VÍDEOS**\n",
        "*   **CÉLULA 2.2: EXTRAÇÃO DE METADADOS DOS VÍDEOS**\n",
        "*   **CÉLULA 2.3: DECOMPOSIÇÃO DE VÍDEOS (FRAMES, ÁUDIO, TEXTO)**\n",
        "\n",
        "### LAYER 3: ANÁLISE E PROCESSAMENTO DE DADOS\n",
        "*   **CÉLULA 3.1: ANÁLISE DE PADRÕES (TEMPORAIS, VISUAIS, TEXTO, ÁUDIO)**\n",
        "*   **CÉLULA 3.2: ANÁLISE PSICOLÓGICA E GATILHOS DE ENGAJAMENTO**\n",
        "\n",
        "### LAYER 4: GERAÇÃO DE RELATÓRIOS E BLUEPRINT ESTRATÉGICO\n",
        "*   **CÉLULA 4.1: GERAÇÃO DE RELATÓRIOS HUMANIZADOS (ÁUDIO, VISUAL, TEXTO, PSICOLÓGICO)**\n",
        "*   **CÉLULA 4.2: GERAÇÃO DO BLUEPRINT FINAL E DASHBOARD**\n",
        "\n",
        "---\n",
        "\n",
        "*Lembre-se: Este sistema foi projetado para ser executado no Google Colab. Certifique-se de que seu ambiente está configurado corretamente.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "setup_inicial",
        "outputId": "a5d9574f-c7b8-4d26-fdb9-61fe000ad671"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Google Drive montado com sucesso!\n",
            "✅ SETUP INICIAL CONCLUÍDO!\n",
            "Todas as dependências foram instaladas e o Google Drive foi montado.\n",
            "➡️ PRÓXIMA CÉLULA: 1.2 - CONFIGURAÇÃO INICIAL E VALIDAÇÃO DA PASTA DE TRABALHO\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# LAYER 1: CONFIGURAÇÃO E PREPARAÇÃO\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# CÉLULA 1.1: SETUP INICIAL E INSTALAÇÃO DE DEPENDÊNCIAS\n",
        "# ============================================================================\n",
        "\n",
        "# Instalar dependências necessárias\n",
        "!pip install -q moviepy librosa pytesseract opencv-python pandas openpyxl matplotlib seaborn pillow SpeechRecognition pydub fpdf\n",
        "!apt-get update -qq && apt-get install -y -qq tesseract-ocr tesseract-ocr-por ffmpeg\n",
        "\n",
        "# Imports necessários\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import logging\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pytesseract\n",
        "import librosa\n",
        "from moviepy.editor import VideoFileClip\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import speech_recognition as sr # Adicionado import para SpeechRecognition\n",
        "# Montar Google Drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"✅ Google Drive montado com sucesso!\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ ERRO ao montar Google Drive: {e}. Por favor, verifique sua conexão ou permissões.\")\n",
        "\n",
        "print(\n",
        "\"✅ SETUP INICIAL CONCLUÍDO!\")\n",
        "print(\"Todas as dependências foram instaladas e o Google Drive foi montado.\")\n",
        "print(\"➡️ PRÓXIMA CÉLULA: 1.2 - CONFIGURAÇÃO INICIAL E VALIDAÇÃO DA PASTA DE TRABALHO\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "configuracao_inicial",
        "outputId": "c5384a91-843f-4869-c3ae-bdc2a6804ac5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ CONFIGURAÇÃO CONCLUÍDA!\n",
            "Pasta de trabalho criada: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa\n",
            "Configuração salva: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/config/config.json\n",
            "\n",
            "➡️ PRÓXIMA CÉLULA: 2.1 - DESCOBERTA E CATALOGAÇÃO DE VÍDEOS\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CÉLULA 1.2: CONFIGURAÇÃO INICIAL E VALIDAÇÃO DA PASTA DE TRABALHO\n",
        "# ============================================================================\n",
        "\n",
        "# ⚠️ **ATENÇÃO:** CONFIGURE SEU CAMINHO AQUI!\n",
        "# Substitua o caminho abaixo pela pasta onde seus vídeos estão localizados no Google Drive.\n",
        "# Exemplo: \"/content/drive/MyDrive/Meus Videos de Marketing\"\n",
        "CAMINHO_PASTA_VIDEOS = \"/content/drive/MyDrive/Videos Dona Done\" # ⬅️ **ALTERE AQUI**\n",
        "\n",
        "class ConfiguradorProjeto:\n",
        "    def __init__(self, caminho_pasta):\n",
        "        self.pasta_videos = self._validar_caminho(caminho_pasta)\n",
        "        self.pasta_trabalho = os.path.join(self.pasta_videos, \"_engenharia_reversa\")\n",
        "        self._criar_estrutura()\n",
        "        self._configurar_logging()\n",
        "\n",
        "    def _validar_caminho(self, caminho):\n",
        "        if caminho == \"/content/drive/MyDrive/Videos Dona Done\" and not os.path.exists(caminho):\n",
        "            raise ValueError(\"❌ ERRO: Você precisa alterar CAMINHO_PASTA_VIDEOS com o caminho real da sua pasta de vídeos no Google Drive. O caminho padrão não foi encontrado.\")\n",
        "\n",
        "        if not os.path.exists(caminho):\n",
        "            raise ValueError(f\"❌ ERRO: Pasta não encontrada: {caminho}. Por favor, verifique se o caminho está correto e se o Google Drive está montado.\")\n",
        "\n",
        "        return caminho\n",
        "\n",
        "    def _criar_estrutura(self):\n",
        "        # Estrutura de pastas conforme o anexo e requisitos do usuário\n",
        "        estrutura = [\n",
        "            \"config\", \"logs\", \"dados\", \"frames_extraidos\",\n",
        "            \"analise_texto\", \"analise_audio\", \"capturas\",\n",
        "            \"blueprint\", \"temp\", \"dashboard\", \"analise_psicologica\", \"analise_visual\"\n",
        "        ]\n",
        "\n",
        "        os.makedirs(self.pasta_trabalho, exist_ok=True)\n",
        "        for pasta in estrutura:\n",
        "            os.makedirs(os.path.join(self.pasta_trabalho, pasta), exist_ok=True)\n",
        "\n",
        "        # Criar subpastas para frames_extraidos (ex: vid_001_Nome_Do_Video/)\n",
        "        # Esta lógica será implementada na célula de decomposição de vídeos (CÉLULA 2.3)\n",
        "\n",
        "    def _configurar_logging(self):\n",
        "        log_file = os.path.join(self.pasta_trabalho, \"logs\", f\"sistema_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "            handlers=[logging.FileHandler(log_file, encoding='utf-8')]\n",
        "        )\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def salvar_configuracao(self):\n",
        "        config = {\n",
        "            \"projeto\": {\n",
        "                \"pasta_videos\": self.pasta_videos,\n",
        "                \"pasta_trabalho\": self.pasta_trabalho,\n",
        "                \"criado_em\": datetime.now().isoformat(),\n",
        "                \"versao\": \"modular_v2.0_otimizado\"\n",
        "            },\n",
        "            \"status_etapas\": {\n",
        "                \"configuracao\": True,\n",
        "                \"descoberta_videos\": False,\n",
        "                \"metadados\": False,\n",
        "                \"decomposicao\": False,\n",
        "                \"analise_padroes\": False,\n",
        "                \"analise_psicologica\": False,\n",
        "                \"relatorios_humanizados\": False,\n",
        "                \"blueprint\": False\n",
        "            }\n",
        "        }\n",
        "\n",
        "        config_path = os.path.join(self.pasta_trabalho, \"config\", \"config.json\")\n",
        "        with open(config_path, \"w\", encoding='utf-8') as f:\n",
        "            json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        return config_path\n",
        "\n",
        "# Executar configuração\n",
        "try:\n",
        "    configurador = ConfiguradorProjeto(CAMINHO_PASTA_VIDEOS)\n",
        "    config_path = configurador.salvar_configuracao()\n",
        "\n",
        "    print(\"\"\"\n",
        "✅ CONFIGURAÇÃO CONCLUÍDA!\"\"\")\n",
        "    print(f\"Pasta de trabalho criada: {configurador.pasta_trabalho}\")\n",
        "    print(f\"Configuração salva: {config_path}\")\n",
        "    print(\"\"\"\n",
        "➡️ PRÓXIMA CÉLULA: 2.1 - DESCOBERTA E CATALOGAÇÃO DE VÍDEOS\"\"\")\n",
        "\n",
        "    # Salvar variáveis globais para próximas células\n",
        "    global PASTA_VIDEOS, PASTA_TRABALHO\n",
        "    PASTA_VIDEOS = configurador.pasta_videos\n",
        "    PASTA_TRABALHO = configurador.pasta_trabalho\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\"\"\n",
        "❌ ERRO NA CONFIGURAÇÃO: {e}\"\"\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "descoberta_videos",
        "outputId": "ad4a8371-2b70-4368-e563-2055ecf11d45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Iniciando descoberta de vídeos na pasta: /content/drive/MyDrive/Videos Dona Done\n",
            "  ✅ Encontrado: bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.mp4\n",
            "  ✅ Encontrado: empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.mp4\n",
            "  ✅ Encontrado: máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy.mp4\n",
            "\n",
            "✅ DESCOBERTA DE VÍDEOS CONCLUÍDA!\n",
            "Total de vídeos encontrados: 3\n",
            "Lista de vídeos salva em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/videos_descobertos.json\n",
            "Formatos encontrados: {'.mp4': 3}\n",
            "\n",
            "➡️ PRÓXIMA CÉLULA: 2.2 - EXTRAÇÃO DE METADADOS DOS VÍDEOS\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# LAYER 2: DESCOBERTA E EXTRAÇÃO DE DADOS BRUTOS\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# CÉLULA 2.1: DESCOBERTA E CATALOGAÇÃO DE VÍDEOS\n",
        "# ============================================================================\n",
        "\n",
        "def verificar_prerequisito_etapa(etapa_anterior):\n",
        "    \"\"\"Verifica se a etapa anterior foi executada com sucesso\"\"\"\n",
        "    try:\n",
        "        if not \"PASTA_TRABALHO\" in globals():\n",
        "            raise Exception(\"Variáveis globais de configuração não encontradas. Execute a CÉLULA 1.2 primeiro.\")\n",
        "\n",
        "        config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "        if not os.path.exists(config_path):\n",
        "            raise Exception(\"Arquivo de configuração não encontrado. Execute a CÉLULA 1.2 primeiro.\")\n",
        "\n",
        "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            config = json.load(f)\n",
        "\n",
        "        if not config[\"status_etapas\"][etapa_anterior]:\n",
        "            raise Exception(f\"A etapa \\\"{etapa_anterior}\\\" não foi concluída. Execute a célula correspondente primeiro.\")\n",
        "\n",
        "        return True, config\n",
        "    except Exception as e:\n",
        "        print(f\"❌ PRÉ-REQUISITO NÃO ATENDIDO: {e}\")\n",
        "        return False, None\n",
        "\n",
        "def descobrir_catalogar_videos():\n",
        "    \"\"\"Descobre e cataloga todos os vídeos na pasta\"\"\"\n",
        "    formatos_aceitos = [\".mp4\", \".mov\", \".avi\", \".mkv\", \".webm\", \".m4v\"]\n",
        "    videos_encontrados = []\n",
        "\n",
        "    print(f\"🔍 Iniciando descoberta de vídeos na pasta: {PASTA_VIDEOS}\")\n",
        "\n",
        "    for root, dirs, files in os.walk(PASTA_VIDEOS):\n",
        "        if \"_engenharia_reversa\" in root:\n",
        "            continue # Ignorar a pasta de trabalho do sistema\n",
        "\n",
        "        for file in files:\n",
        "            if any(file.lower().endswith(fmt) for fmt in formatos_aceitos):\n",
        "                video_path = os.path.join(root, file)\n",
        "\n",
        "                try:\n",
        "                    stat_info = os.stat(video_path)\n",
        "                    # Gerar ID baseado no nome do arquivo para melhor rastreamento\n",
        "                    video_name_clean = os.path.splitext(file)[0].replace(\" \", \"_\").replace(\".\", \"\")\n",
        "                    video_id = f\"vid_{video_name_clean}\"\n",
        "\n",
        "                    video_info = {\n",
        "                        \"id\": video_id,\n",
        "                        \"nome_arquivo\": file,\n",
        "                        \"caminho_completo\": video_path,\n",
        "                        \"caminho_relativo\": os.path.relpath(video_path, PASTA_VIDEOS),\n",
        "                        \"tamanho_mb\": round(stat_info.st_size / (1024*1024), 2),\n",
        "                        \"data_modificacao\": datetime.fromtimestamp(stat_info.st_mtime).isoformat(),\n",
        "                        \"extensao\": os.path.splitext(file)[1].lower(),\n",
        "                        \"status\": \"descoberto\"\n",
        "                    }\n",
        "\n",
        "                    videos_encontrados.append(video_info)\n",
        "                    print(f\"  ✅ Encontrado: {file}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"  ❌ Erro ao processar {file}: {e}\")\n",
        "                    continue\n",
        "\n",
        "    return videos_encontrados\n",
        "\n",
        "def salvar_lista_videos(videos):\n",
        "    \"\"\"Salva lista de vídeos encontrados\"\"\"\n",
        "    videos_path = os.path.join(PASTA_TRABALHO, \"dados\", \"videos_descobertos.json\")\n",
        "    with open(videos_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(videos, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    config[\"status_etapas\"][\"descoberta_videos\"] = True\n",
        "    config[\"total_videos_encontrados\"] = len(videos)\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    return videos_path\n",
        "\n",
        "# Executar descoberta\n",
        "prerequisito_ok, _ = verificar_prerequisito_etapa(\"configuracao\")\n",
        "\n",
        "if prerequisito_ok:\n",
        "    try:\n",
        "        videos_encontrados = descobrir_catalogar_videos()\n",
        "\n",
        "        if not videos_encontrados:\n",
        "            print(\"\"\"\n",
        "❌ NENHUM VÍDEO ENCONTRADO!\"\"\")\n",
        "            print(f\"Verifique se há vídeos na pasta configurada: {PASTA_VIDEOS}\")\n",
        "        else:\n",
        "            videos_path = salvar_lista_videos(videos_encontrados)\n",
        "\n",
        "            print(\"\"\"\n",
        "✅ DESCOBERTA DE VÍDEOS CONCLUÍDA!\"\"\")\n",
        "            print(f\"Total de vídeos encontrados: {len(videos_encontrados)}\")\n",
        "            print(f\"Lista de vídeos salva em: {videos_path}\")\n",
        "\n",
        "            # Mostrar resumo\n",
        "            extensoes = Counter([v[\"extensao\"] for v in videos_encontrados])\n",
        "            print(f\"Formatos encontrados: {dict(extensoes)}\")\n",
        "            print(\"\"\"\n",
        "➡️ PRÓXIMA CÉLULA: 2.2 - EXTRAÇÃO DE METADADOS DOS VÍDEOS\"\"\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\"\"\n",
        "❌ ERRO NA DESCOBERTA DE VÍDEOS: {e}\"\"\")\n",
        "        print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "extracao_metadados",
        "outputId": "3b3f78bd-9ef4-4d08-ec58-4754c0be1c8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processando metadados de 3 vídeos...\n",
            "[1/3] Analisando bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.mp4\n",
            "  ⚙️ Extraindo metadados para: bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.mp4\n",
            "  ✅ Metadados extraídos: 32.6s | vertical_9_16 | Áudio: Sim\n",
            "[2/3] Analisando empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.mp4\n",
            "  ⚙️ Extraindo metadados para: empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.mp4\n",
            "  ✅ Metadados extraídos: 64.4s | vertical_9_16 | Áudio: Sim\n",
            "[3/3] Analisando máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy.mp4\n",
            "  ⚙️ Extraindo metadados para: máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy.mp4\n",
            "  ✅ Metadados extraídos: 48.2s | vertical_9_16 | Áudio: Sim\n",
            "🔗 Integrando dados de viralização...\n",
            "\n",
            "💾 Metadados completos salvos em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/metadados_completos.json\n",
            "💾 Metadados em Excel salvos em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/metadados_videos.xlsx\n",
            "\n",
            "✅ EXTRAÇÃO DE METADADOS CONCLUÍDA!\n",
            "Total de vídeos com metadados extraídos: 3\n",
            "\n",
            "📊 Resumo dos Metadados:\n",
            "  - Formatos detectados: {'vertical_9_16': np.int64(3)}\n",
            "  - Duração média dos vídeos: 48.41s\n",
            "  - Vídeos com áudio: 3\n",
            "\n",
            "➡️ PRÓXIMA CÉLULA: 2.3 - DECOMPOSIÇÃO DE VÍDEOS (FRAMES, ÁUDIO, TEXTO)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CÉLULA 2.2: EXTRAÇÃO DE METADADOS DOS VÍDEOS\n",
        "# ============================================================================\n",
        "\n",
        "def extrair_metadados_video(video_info):\n",
        "    \"\"\"Extrai metadados técnicos de um vídeo\"\"\"\n",
        "    video_path = video_info[\"caminho_completo\"]\n",
        "    video_id = video_info[\"id\"]\n",
        "\n",
        "    print(f\"  ⚙️ Extraindo metadados para: {video_info[\"nome_arquivo\"]}\")\n",
        "\n",
        "    # Análise com OpenCV\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise Exception(\"Não foi possível abrir o vídeo. Verifique o caminho ou a integridade do arquivo.\")\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    largura = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    altura = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    duracao = frame_count / fps if fps > 0 else 0\n",
        "\n",
        "    # Capturar primeiro frame\n",
        "    ret, primeiro_frame = cap.read()\n",
        "    cap.release()\n",
        "\n",
        "    # Análise de áudio\n",
        "    try:\n",
        "        clip = VideoFileClip(video_path)\n",
        "        tem_audio = clip.audio is not None\n",
        "        clip.close()\n",
        "    except Exception as e:\n",
        "        print(f\"    ⚠️ Aviso: Não foi possível analisar áudio para {video_info[\"nome_arquivo\"]}: {e}\")\n",
        "        tem_audio = False\n",
        "\n",
        "    # Análise do primeiro frame\n",
        "    analise_frame = {}\n",
        "    if ret:\n",
        "        # Salvar primeiro frame na pasta 'capturas'\n",
        "        capturas_dir = os.path.join(PASTA_TRABALHO, \"capturas\")\n",
        "        frame_path = os.path.join(capturas_dir, f\"{video_id}_primeiro_frame.jpg\")\n",
        "        cv2.imwrite(frame_path, primeiro_frame)\n",
        "\n",
        "        # Análises do frame\n",
        "        gray = cv2.cvtColor(primeiro_frame, cv2.COLOR_BGR2GRAY)\n",
        "        complexidade = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "        brilho = np.mean(gray)\n",
        "\n",
        "        analise_frame = {\n",
        "            \"path\": frame_path,\n",
        "            \"complexidade_visual\": float(complexidade),\n",
        "            \"brilho_medio\": float(brilho),\n",
        "            \"tem_muito_texto\": bool(complexidade > 500),\n",
        "            \"e_escuro\": bool(brilho < 100),\n",
        "            \"e_claro\": bool(brilho > 200)\n",
        "        }\n",
        "\n",
        "    # Detectar formato\n",
        "    ratio = largura / altura if altura > 0 else 0\n",
        "    if 0.5 <= ratio <= 0.6:\n",
        "        formato = \"vertical_9_16\" if altura > largura * 1.5 else \"vertical_4_5\"\n",
        "    elif 0.8 <= ratio <= 1.2:\n",
        "        formato = \"quadrado_1_1\"\n",
        "    elif ratio >= 1.3:\n",
        "        formato = \"horizontal_16_9\"\n",
        "    else:\n",
        "        formato = \"personalizado\"\n",
        "\n",
        "    # Compilar metadados - converter todos os valores para tipos básicos Python\n",
        "    metadados = {\n",
        "        **video_info,\n",
        "        \"duracao_segundos\": float(duracao),\n",
        "        \"fps\": float(fps),\n",
        "        \"largura\": int(largura),\n",
        "        \"altura\": int(altura),\n",
        "        \"resolucao\": f\"{largura}x{altura}\",\n",
        "        \"aspect_ratio\": float(ratio),\n",
        "        \"total_frames\": int(frame_count),\n",
        "        \"tem_audio\": bool(tem_audio),\n",
        "        \"formato_detectado\": str(formato),\n",
        "        \"primeiro_frame\": analise_frame,\n",
        "        \"data_analise\": datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    return metadados\n",
        "\n",
        "def processar_metadados_todos_videos():\n",
        "    \"\"\"Processa metadados de todos os vídeos\"\"\"\n",
        "    # Carregar lista de vídeos\n",
        "    videos_path = os.path.join(PASTA_TRABALHO, \"dados\", \"videos_descobertos.json\")\n",
        "    with open(videos_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        videos_lista = json.load(f)\n",
        "\n",
        "    metadados_completos = []\n",
        "    sucessos = 0\n",
        "\n",
        "    print(f\"Processando metadados de {len(videos_lista)} vídeos...\")\n",
        "\n",
        "    for i, video in enumerate(videos_lista, 1):\n",
        "        print(f\"[{i}/{len(videos_lista)}] Analisando {video[\"nome_arquivo\"]}\")\n",
        "\n",
        "        try:\n",
        "            metadados = extrair_metadados_video(video)\n",
        "            metadados[\"status\"] = \"metadados_extraidos\"\n",
        "            metadados_completos.append(metadados)\n",
        "            sucessos += 1\n",
        "            print(f\"  ✅ Metadados extraídos: {metadados[\"duracao_segundos\"]:.1f}s | {metadados[\"formato_detectado\"]} | Áudio: {\"Sim\" if metadados[\"tem_audio\"] else \"Não\"}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ ERRO ao extrair metadados para {video[\"nome_arquivo\"]}: {e}\")\n",
        "            video[\"status\"] = \"erro_metadados\"\n",
        "            metadados_completos.append(video) # Adiciona o vídeo com status de erro\n",
        "\n",
        "    # Salvar metadados completos\n",
        "    metadados_json_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "    with open(metadados_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(metadados_completos, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # ADICIONE NO FINAL DA FUNÇÃO processar_extracao_metadados_videos\n",
        "# ANTES DA LINHA: with open(metadados_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "\n",
        "# ======= INÍCIO DA INTEGRAÇÃO DE VIRALIZAÇÃO =======\n",
        "    print(\"🔗 Integrando dados de viralização...\")\n",
        "\n",
        "    # Assuming carregar_dados_viralizacao and enriquecer_metadados_com_viralizacao are defined elsewhere\n",
        "    # and videos_info is the list of video data being processed (metadados_completos in this case)\n",
        "    # This block seems misplaced from a previous version or intended as an external patch.\n",
        "    # For now, I will comment it out to fix the indentation error and avoid NameErrors.\n",
        "    # If this functionality is needed, it should be properly integrated or called from another cell.\n",
        "\n",
        "    # df_viral, dados_completos = carregar_dados_viralizacao(PASTA_TRABALHO)\n",
        "    #\n",
        "    # if df_viral is not None:\n",
        "    #     # Enriquecer cada vídeo com dados de viralização\n",
        "    #     for i, video_info in enumerate(videos_info):\n",
        "    #         videos_info[i] = enriquecer_metadados_com_viralizacao(\n",
        "    #             video_info, df_viral, dados_completos\n",
        "    #         )\n",
        "    #\n",
        "    #     # Salvar dados de viralização na pasta de dados\n",
        "    #     viral_csv_path = os.path.join(PASTA_TRABALHO, \"dados\", \"viral_metrics.csv\")\n",
        "    #     df_viral.to_csv(viral_csv_path, index=False, encoding='utf-8')\n",
        "    #\n",
        "    #     viral_json_path = os.path.join(PASTA_TRABALHO, \"dados\", \"viral_complete_data.json\")\n",
        "    #     with open(viral_json_path, 'w', encoding='utf-8') as f:\n",
        "    #         json.dump(dados_completos, f, indent=2, ensure_ascii=False)\n",
        "    #\n",
        "    #     print(f\"✅ Dados de viralização integrados e salvos\")\n",
        "    #     print(f\"   📊 CSV: {viral_csv_path}\")\n",
        "    #     print(f\"   📊 JSON: {viral_json_path}\")\n",
        "    # else:\n",
        "    #     print(\"⚠️ Continuando sem dados de viralização\")\n",
        "\n",
        "# ======= FIM DA INTEGRAÇÃO DE VIRALIZAÇÃO =======\n",
        "\n",
        "\n",
        "    # Salvar em Excel\n",
        "    df_metadados = pd.DataFrame(metadados_completos)\n",
        "    metadados_excel_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_videos.xlsx\")\n",
        "    df_metadados.to_excel(metadados_excel_path, index=False, engine='openpyxl')\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    config[\"status_etapas\"][\"metadados\"] = True\n",
        "    config[\"total_videos_metadados\"] = sucessos\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\n💾 Metadados completos salvos em: {metadados_json_path}\")\n",
        "    print(f\"💾 Metadados em Excel salvos em: {metadados_excel_path}\")\n",
        "\n",
        "    print(\"\\n✅ EXTRAÇÃO DE METADADOS CONCLUÍDA!\")\n",
        "    print(f\"Total de vídeos com metadados extraídos: {sucessos}\")\n",
        "\n",
        "    # Mostrar resumo\n",
        "    if not df_metadados.empty:\n",
        "        print(\"\\n📊 Resumo dos Metadados:\")\n",
        "        print(f\"  - Formatos detectados: {dict(df_metadados['formato_detectado'].value_counts())}\")\n",
        "        print(f\"  - Duração média dos vídeos: {df_metadados['duracao_segundos'].mean():.2f}s\")\n",
        "        print(f\"  - Vídeos com áudio: {df_metadados['tem_audio'].sum()}\")\n",
        "\n",
        "    print(\"\\n➡️ PRÓXIMA CÉLULA: 2.3 - DECOMPOSIÇÃO DE VÍDEOS (FRAMES, ÁUDIO, TEXTO)\")\n",
        "\n",
        "# Executar extração de metadados\n",
        "prerequisito_ok, _ = verificar_prerequisito_etapa(\"descoberta_videos\")\n",
        "\n",
        "if prerequisito_ok:\n",
        "    try:\n",
        "        processar_metadados_todos_videos()\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ ERRO NA EXTRAÇÃO DE METADADOS: {e}\")\n",
        "        print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "decomposicao_videos",
        "outputId": "2827f4db-e77f-4e5e-d643-cf2875ac672b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando decomposição para 3 vídeos...\n",
            "[1/3] Decompondo bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.mp4\n",
            "  ⚙️ Decompondo vídeo: bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.mp4\n",
            "    ✅ 33 frames extraídos para bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.mp4\n",
            "    ✅ 30 textos encontrados via OCR para bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.mp4\n",
            "    ✅ Áudio extraído para bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.mp4\n",
            "    ✅ Áudio transcrito para bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.mp4\n",
            "    ✅ 638 cortes detectados para bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.mp4\n",
            "  ✅ Decomposição concluída para bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.mp4\n",
            "[2/3] Decompondo empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.mp4\n",
            "  ⚙️ Decompondo vídeo: empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.mp4\n",
            "    ✅ 65 frames extraídos para empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.mp4\n",
            "    ✅ 56 textos encontrados via OCR para empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.mp4\n",
            "    ✅ Áudio extraído para empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.mp4\n",
            "    ✅ Áudio transcrito para empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.mp4\n",
            "    ✅ 123 cortes detectados para empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.mp4\n",
            "  ✅ Decomposição concluída para empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.mp4\n",
            "[3/3] Decompondo máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy.mp4\n",
            "  ⚙️ Decompondo vídeo: máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy.mp4\n",
            "    ✅ 49 frames extraídos para máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy.mp4\n",
            "    ✅ 42 textos encontrados via OCR para máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy.mp4\n",
            "    ✅ Áudio extraído para máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy.mp4\n",
            "    ✅ Áudio transcrito para máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy.mp4\n",
            "    ✅ 716 cortes detectados para máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy.mp4\n",
            "  ✅ Decomposição concluída para máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy.mp4\n",
            "\n",
            "💾 Dados de decomposição salvos em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/decomposicao_completa.json\n",
            "\n",
            "✅ DECOMPOSIÇÃO DE VÍDEOS CONCLUÍDA!\n",
            "Total de vídeos decompostos com sucesso: 3\n",
            "\n",
            "➡️ PRÓXIMA CÉLULA: 3.1 - ANÁLISE DE PADRÕES (TEMPORAIS, VISUAIS, TEXTO, ÁUDIO)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CÉLULA 2.3: DECOMPOSIÇÃO DE VÍDEOS (FRAMES, ÁUDIO, TEXTO)\n",
        "# ============================================================================\n",
        "\n",
        "def decompor_video(video_info):\n",
        "    \"\"\"Decompõe um vídeo em frames, áudio e texto (OCR e transcrição)\"\"\"\n",
        "    video_path = video_info[\"caminho_completo\"]\n",
        "    video_id = video_info[\"id\"]\n",
        "    pasta_video_frames = os.path.join(PASTA_TRABALHO, \"frames_extraidos\", video_id)\n",
        "    os.makedirs(pasta_video_frames, exist_ok=True)\n",
        "\n",
        "    print(f\"  ⚙️ Decompondo vídeo: {video_info[\"nome_arquivo\"]}\")\n",
        "\n",
        "    decomposicao_data = {\n",
        "        \"video_id\": video_id,\n",
        "        \"frames_extraidos\": [],\n",
        "        \"textos_ocr\": [],\n",
        "        \"audio_transcrito\": \"\",\n",
        "        \"audio_analise\": {}\n",
        "    }\n",
        "\n",
        "    # Extração de Frames e OCR\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        frame_count = 0\n",
        "        frame_interval = int(fps) # 1 frame por segundo\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            if frame_count % frame_interval == 0:\n",
        "                frame_time_sec = frame_count / fps\n",
        "                frame_filename = os.path.join(pasta_video_frames, f\"frame_{int(frame_time_sec):06d}.jpg\")\n",
        "                cv2.imwrite(frame_filename, frame)\n",
        "                decomposicao_data[\"frames_extraidos\"] .append({\n",
        "                    \"path\": frame_filename,\n",
        "                    \"timestamp_sec\": frame_time_sec\n",
        "                })\n",
        "\n",
        "                # OCR\n",
        "                try:\n",
        "                    text = pytesseract.image_to_string(Image.fromarray(frame), lang=\"por\")\n",
        "                    if text.strip():\n",
        "                        decomposicao_data[\"textos_ocr\"] .append({\n",
        "                            \"timestamp_sec\": frame_time_sec,\n",
        "                            \"text\": text.strip()\n",
        "                        })\n",
        "                except Exception as ocr_e:\n",
        "                    print(f\"    ⚠️ Aviso: Erro no OCR para frame {frame_time_sec}s: {ocr_e}\")\n",
        "\n",
        "            frame_count += 1\n",
        "        cap.release()\n",
        "        print(f\"    ✅ {len(decomposicao_data[\"frames_extraidos\"])} frames extraídos para {video_info[\"nome_arquivo\"]}\")\n",
        "        print(f\"    ✅ {len(decomposicao_data[\"textos_ocr\"])} textos encontrados via OCR para {video_info[\"nome_arquivo\"]}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ❌ Erro na extração de frames/OCR para {video_info[\"nome_arquivo\"]}: {e}\")\n",
        "\n",
        "    # Extração e Transcrição de Áudio\n",
        "    audio_path = os.path.join(PASTA_TRABALHO, \"temp\", f\"{video_id}.wav\")\n",
        "    try:\n",
        "        video_clip = VideoFileClip(video_path)\n",
        "        if video_clip.audio:\n",
        "            video_clip.audio.write_audiofile(audio_path, verbose=False, logger=None)\n",
        "            print(f\"    ✅ Áudio extraído para {video_info[\"nome_arquivo\"]}\")\n",
        "\n",
        "            # Transcrição\n",
        "            r = sr.Recognizer()\n",
        "            with sr.AudioFile(audio_path) as source:\n",
        "                audio_listened = r.record(source)\n",
        "                try:\n",
        "                    text = r.recognize_google(audio_listened, language=\"pt-BR\")\n",
        "                    decomposicao_data[\"audio_transcrito\"] = text\n",
        "                    print(f\"    ✅ Áudio transcrito para {video_info[\"nome_arquivo\"]}\")\n",
        "                except sr.UnknownValueError:\n",
        "                    print(f\"    ⚠️ Aviso: Não foi possível transcrever o áudio para {video_info[\"nome_arquivo\"]}. Fala ininteligível.\")\n",
        "                except sr.RequestError as req_e:\n",
        "                    print(f\"    ⚠️ Aviso: Erro no serviço de transcrição para {video_info[\"nome_arquivo\"]}: {req_e}\")\n",
        "\n",
        "            # Análise de Áudio (Librosa)\n",
        "            y, sr_audio = librosa.load(audio_path)\n",
        "            tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr_audio)\n",
        "            decomposicao_data[\"audio_analise\"] = {\n",
        "                \"bpm\": float(tempo),\n",
        "                \"duracao_audio_segundos\": float(librosa.get_duration(y=y, sr=sr_audio))\n",
        "            }\n",
        "\n",
        "        else:\n",
        "            print(f\"    ⚠️ Aviso: Vídeo {video_info[\"nome_arquivo\"]} não possui trilha de áudio.\")\n",
        "        video_clip.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ❌ Erro na extração/transcrição de áudio para {video_info[\"nome_arquivo\"]}: {e}\")\n",
        "\n",
        "    # Detecção de Cortes (Scene Change Detection)\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            raise Exception(\"Não foi possível abrir o vídeo para detecção de cortes.\")\n",
        "\n",
        "        prev_frame = None\n",
        "        cuts = []\n",
        "        frame_idx = 0\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            if prev_frame is not None:\n",
        "                diff = cv2.absdiff(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY), cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY))\n",
        "                non_zero_count = np.count_nonzero(diff)\n",
        "                if non_zero_count > (frame.shape[0] * frame.shape[1] * 0.3): # Limiar de 30% de mudança\n",
        "                    cuts.append(frame_idx / fps)\n",
        "            prev_frame = frame\n",
        "            frame_idx += 1\n",
        "        cap.release()\n",
        "        decomposicao_data[\"cortes_detectados_segundos\"] = cuts\n",
        "        print(f\"    ✅ {len(cuts)} cortes detectados para {video_info[\"nome_arquivo\"]}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ❌ Erro na detecção de cortes para {video_info[\"nome_arquivo\"]}: {e}\")\n",
        "\n",
        "    return decomposicao_data\n",
        "\n",
        "def processar_decomposicao_todos_videos():\n",
        "    \"\"\"Processa a decomposição de todos os vídeos\"\"\"\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa(\"metadados\")\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Carregar metadados completos\n",
        "    metadados_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "    with open(metadados_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        videos_com_metadados = json.load(f)\n",
        "\n",
        "    decomposicoes_completas = []\n",
        "    sucessos = 0\n",
        "\n",
        "    print(\"\"\"\n",
        "Iniciando decomposição para {} vídeos...\"\"\".format(len(videos_com_metadados)))\n",
        "\n",
        "    for i, video in enumerate(videos_com_metadados, 1):\n",
        "        if video.get(\"status\") == \"metadados_extraidos\":\n",
        "            print(f\"[{i}/{len(videos_com_metadados)}] Decompondo {video[\"nome_arquivo\"]}\")\n",
        "            try:\n",
        "                decomposicao = decompor_video(video)\n",
        "                decomposicao[\"status\"] = \"decomposto\"\n",
        "                decomposicoes_completas.append(decomposicao)\n",
        "                sucessos += 1\n",
        "                print(f\"  ✅ Decomposição concluída para {video[\"nome_arquivo\"]}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ❌ ERRO na decomposição para {video[\"nome_arquivo\"]}: {e}\")\n",
        "                decomposicoes_completas.append({\"video_id\": video[\"id\"], \"status\": \"erro_decomposicao\", \"erro\": str(e)})\n",
        "        else:\n",
        "            print(f\"[{i}/{len(videos_com_metadados)}] Pulando {video.get(\"nome_arquivo\", video[\"id\"])} - Status: {video.get(\"status\", \"N/A\")}\")\n",
        "            decomposicoes_completas.append({\"video_id\": video[\"id\"], \"status\": video.get(\"status\", \"N/A\"), \"erro\": \"Pulado devido a erro anterior\"})\n",
        "\n",
        "    # Salvar decomposições completas\n",
        "    decomposicao_json_path = os.path.join(PASTA_TRABALHO, \"dados\", \"decomposicao_completa.json\")\n",
        "    with open(decomposicao_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(decomposicoes_completas, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    config[\"status_etapas\"][\"decomposicao\"] = True\n",
        "    config[\"total_videos_decompostos\"] = sucessos\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\"\"\n",
        "💾 Dados de decomposição salvos em: {decomposicao_json_path}\"\"\")\n",
        "\n",
        "    print(\"\"\"\n",
        "✅ DECOMPOSIÇÃO DE VÍDEOS CONCLUÍDA!\"\"\")\n",
        "    print(f\"Total de vídeos decompostos com sucesso: {sucessos}\")\n",
        "\n",
        "    if sucessos == 0:\n",
        "        print(\"❌ NENHUM VÍDEO FOI DECOMPOSTO COM SUCESSO. Verifique as etapas anteriores.\")\n",
        "    print(\"\"\"\n",
        "➡️ PRÓXIMA CÉLULA: 3.1 - ANÁLISE DE PADRÕES (TEMPORAIS, VISUAIS, TEXTO, ÁUDIO)\"\"\")\n",
        "\n",
        "# Executar decomposição\n",
        "try:\n",
        "    processar_decomposicao_todos_videos()\n",
        "except Exception as e:\n",
        "    print(f\"\"\"\n",
        "❌ ERRO GERAL NA DECOMPOSIÇÃO DE VÍDEOS: {e}\"\"\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "288e47d2",
        "outputId": "9eef1097-dc96-4d0f-e6e2-4194c2f8c5d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✨ Iniciando renomeação inteligente de vídeos...\n",
            "\n",
            "[1/3] Renomeando bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.mp4\n",
            "  ✅ Vídeo renomeado e movido: bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.mp4 -> bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.mp4\n",
            "[2/3] Renomeando empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.mp4\n",
            "  ✅ Vídeo renomeado e movido: empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.mp4 -> empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.mp4\n",
            "[3/3] Renomeando máquina_emocional_não_dá_conta_de_converter_então__vid_máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy.mp4\n",
            "  ✅ Vídeo renomeado e movido: máquina_emocional_não_dá_conta_de_converter_então__vid_máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy.mp4 -> máquina_emocional_não_dá_conta_de_converter_então__vid_máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy.mp4\n",
            "\n",
            "✅ RENOMEAÇÃO INTELIGENTE CONCLUÍDA!\n",
            "\n",
            "Todos os vídeos foram processados. Verifique a pasta: /content/drive/MyDrive/Videos Dona Done/_videos_renomeados\n",
            "➡️ PRÓXIMA CÉLULA: 3.1 - ANÁLISE DE PADRÕES (TEMPORAIS, VISUAIS, TEXTO, ÁUDIO)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CÉLULA 2.4: RENOMEAÇÃO INTELIGENTE DE VÍDEOS\n",
        "# ============================================================================\n",
        "\n",
        "def renomear_video_inteligentemente(video_info, nova_pasta_videos):\n",
        "    \"\"\"Renomeia um vídeo com base na primeira frase do áudio e move-o para a nova pasta.\"\"\"\n",
        "    video_id = video_info[\"id\"]\n",
        "    caminho_original = video_info[\"caminho_completo\"]\n",
        "    extensao = video_info[\"extensao\"]\n",
        "    primeira_frase = video_info.get(\"primeira_frase_audio\", \"\").replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
        "\n",
        "    if not primeira_frase:\n",
        "        print(f\"⚠️ Aviso: Não foi possível renomear {video_info[\"nome_arquivo\"]} - primeira frase do áudio não encontrada. Mantendo nome original.\")\n",
        "        return video_info\n",
        "\n",
        "    novo_nome_base = f\"{primeira_frase[:50].strip()}_{video_id}\"\n",
        "    novo_nome_arquivo = f\"{novo_nome_base}{extensao}\"\n",
        "    novo_caminho_completo = os.path.join(nova_pasta_videos, novo_nome_arquivo)\n",
        "\n",
        "    try:\n",
        "        os.makedirs(nova_pasta_videos, exist_ok=True)\n",
        "        os.rename(caminho_original, novo_caminho_completo)\n",
        "        print(f\"  ✅ Vídeo renomeado e movido: {video_info[\"nome_arquivo\"]} -> {novo_nome_arquivo}\")\n",
        "        video_info[\"nome_arquivo_original\"] = video_info[\"nome_arquivo\"]\n",
        "        video_info[\"caminho_original\"] = video_info[\"caminho_completo\"]\n",
        "        video_info[\"nome_arquivo\"] = novo_nome_arquivo\n",
        "        video_info[\"caminho_completo\"] = novo_caminho_completo\n",
        "        video_info[\"status\"] = \"renomeado_inteligente\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ ERRO ao renomear/mover {video_info[\"nome_arquivo\"]}: {e}. Mantendo nome original.\")\n",
        "        video_info[\"status\"] = \"erro_renomeacao\"\n",
        "\n",
        "    return video_info\n",
        "\n",
        "def processar_renomeacao_inteligente():\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa(\"decomposicao\")\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    decomposicoes_path = os.path.join(PASTA_TRABALHO, \"dados\", \"decomposicao_completa.json\")\n",
        "    if not os.path.exists(decomposicoes_path):\n",
        "        print(\"❌ ERRO: Arquivo de decomposições não encontrado. Execute a CÉLULA 2.3 primeiro.\")\n",
        "        return\n",
        "\n",
        "    metadados_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "    if not os.path.exists(metadados_path):\n",
        "        print(\"❌ ERRO: Arquivo de metadados não encontrado. Execute a CÉLULA 2.2 primeiro.\")\n",
        "        return\n",
        "\n",
        "    with open(decomposicoes_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        videos_decompostos = json.load(f)\n",
        "\n",
        "    with open(metadados_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        videos_com_metadados = json.load(f)\n",
        "\n",
        "    metadados_dict = {video[\"id\"]: video for video in videos_com_metadados}\n",
        "\n",
        "    videos_renomeados = []\n",
        "    nova_pasta_videos_renomeados = os.path.join(PASTA_VIDEOS, \"_videos_renomeados\")\n",
        "\n",
        "    print(\"\\n✨ Iniciando renomeação inteligente de vídeos...\\n\")\n",
        "\n",
        "    for i, decomposicao_data in enumerate(videos_decompostos, 1):\n",
        "        video_id = decomposicao_data[\"video_id\"]\n",
        "        video_info = metadados_dict.get(video_id)\n",
        "\n",
        "        if not video_info:\n",
        "            print(f\"⚠️ Aviso: Metadados para o vídeo {video_id} não encontrados. Pulando renomeação.\")\n",
        "            continue\n",
        "\n",
        "        video_info[\"primeira_frase_audio\"] = decomposicao_data.get(\"audio_transcrito\", \"\").split('.')[0] if decomposicao_data.get(\"audio_transcrito\") else \"\"\n",
        "\n",
        "\n",
        "        print(f\"[{i}/{len(videos_decompostos)}] Renomeando {video_info[\"nome_arquivo\"]}\")\n",
        "        renomeado_info = renomear_video_inteligentemente(video_info, nova_pasta_videos_renomeados)\n",
        "        videos_renomeados.append(renomeado_info)\n",
        "\n",
        "    with open(metadados_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(videos_renomeados, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    config[\"status_etapas\"][\"renomeacao_inteligente\"] = True\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(\"\\n✅ RENOMEAÇÃO INTELIGENTE CONCLUÍDA!\\n\")\n",
        "    print(f\"Todos os vídeos foram processados. Verifique a pasta: {nova_pasta_videos_renomeados}\")\n",
        "    print(\"➡️ PRÓXIMA CÉLULA: 3.1 - ANÁLISE DE PADRÕES (TEMPORAIS, VISUAIS, TEXTO, ÁUDIO)\")\n",
        "\n",
        "processar_renomeacao_inteligente()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AquW8stD8E1o"
      },
      "source": [
        "melhorar os cortes aqui ( otimizar ele esta detectando muitos cortes. corrigir possivel erro de Fala ininteligível"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EezfUtNZUZlc",
        "outputId": "c808108e-9390-4cb6-8645-ed61bfcb774b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando análise de áudio refinada para 3 vídeos...\n",
            "[1/3] Analisando áudio refinado para: vid_bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t\n",
            "    🔊 Analisando variação de volume...\n",
            "    🔊 Detectando picos de ruído...\n",
            "    🔊 Analisando ritmo da fala...\n",
            "    🔊 Identificando pausas...\n",
            "    🔊 Classificando música de fundo...\n",
            "    🔊 Analisando clareza da voz...\n",
            "    🔊 Detectando sobreposição...\n",
            "    🔊 Mapeando efeitos sonoros...\n",
            "    🔊 Analisando frequências específicas...\n",
            "    🔊 Gerando espectrograma...\n",
            "  ✅ Análise de áudio refinada concluída para vid_bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t\n",
            "[2/3] Analisando áudio refinado para: vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf\n",
            "    🔊 Analisando variação de volume...\n",
            "    🔊 Detectando picos de ruído...\n",
            "    🔊 Analisando ritmo da fala...\n",
            "    🔊 Identificando pausas...\n",
            "    🔊 Classificando música de fundo...\n",
            "    🔊 Analisando clareza da voz...\n",
            "    🔊 Detectando sobreposição...\n",
            "    🔊 Mapeando efeitos sonoros...\n",
            "    🔊 Analisando frequências específicas...\n",
            "    🔊 Gerando espectrograma...\n",
            "  ✅ Análise de áudio refinada concluída para vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf\n",
            "[3/3] Analisando áudio refinado para: vid_máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy\n",
            "    🔊 Analisando variação de volume...\n",
            "    🔊 Detectando picos de ruído...\n",
            "    🔊 Analisando ritmo da fala...\n",
            "    🔊 Identificando pausas...\n",
            "    🔊 Classificando música de fundo...\n",
            "    🔊 Analisando clareza da voz...\n",
            "    🔊 Detectando sobreposição...\n",
            "    🔊 Mapeando efeitos sonoros...\n",
            "    🔊 Analisando frequências específicas...\n",
            "    🔊 Gerando espectrograma...\n",
            "  ✅ Análise de áudio refinada concluída para vid_máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy\n",
            "\n",
            "💾 Relatório resumo de análise de áudio salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_audio/resumo_analise_audio.xlsx\n",
            "\n",
            "✅ ANÁLISE DE ÁUDIO REFINADA CONCLUÍDA!\n",
            "Total de vídeos com análise de áudio refinada concluída: 3\n",
            "\n",
            "➡️ PRÓXIMA CÉLULA: 3.1 - ANÁLISE DE PADRÕES (TEMPORAIS, VISUAIS, TEXTO, ÁUDIO)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CÉLULA 2.4: ANÁLISE DE ÁUDIO REFINADA (SUBLAYER DA LAYER 2)\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# CÉLULA 2.4: ANÁLISE DE ÁUDIO REFINADA (SUBLAYER DA LAYER 2)\n",
        "# ============================================================================\n",
        "\n",
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.signal\n",
        "from scipy.stats import variation\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def converter_para_json_serializable(obj):\n",
        "    \"\"\"Converte tipos NumPy para tipos Python nativos para serialização JSON\"\"\"\n",
        "    if isinstance(obj, np.integer):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, np.floating):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return [converter_para_json_serializable(x) for x in obj.tolist()]\n",
        "    elif isinstance(obj, list):\n",
        "        return [converter_para_json_serializable(x) for x in obj]\n",
        "    elif isinstance(obj, dict):\n",
        "        return {k: converter_para_json_serializable(v) for k, v in obj.items()}\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "def verificar_prerequisito_audio_refinado():\n",
        "    \"\"\"Verifica se a etapa de decomposição foi concluída\"\"\"\n",
        "    try:\n",
        "        if not \"PASTA_TRABALHO\" in globals():\n",
        "            raise Exception(\"Variáveis globais de configuração não encontradas. Execute a CÉLULA 1.2 primeiro.\")\n",
        "\n",
        "        config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "        if not os.path.exists(config_path):\n",
        "            raise Exception(\"Arquivo de configuração não encontrado. Execute as células anteriores.\")\n",
        "\n",
        "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            config = json.load(f)\n",
        "\n",
        "        if not config[\"status_etapas\"][\"decomposicao\"]:\n",
        "            raise Exception(\"A etapa 'decomposicao' não foi concluída. Execute a CÉLULA 2.3 primeiro.\")\n",
        "\n",
        "        return True, config\n",
        "    except Exception as e:\n",
        "        print(f\"❌ PRÉ-REQUISITO NÃO ATENDIDO: {e}\")\n",
        "        return False, None\n",
        "\n",
        "def analisar_variacao_volume(audio_path, sr=22050):\n",
        "    \"\"\"Analisa variações de volume da voz\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "        # Calcular RMS (Root Mean Square) em janelas\n",
        "        frame_length = int(0.1 * sr)  # Janelas de 100ms\n",
        "        hop_length = frame_length // 4\n",
        "        rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]\n",
        "\n",
        "        # Detectar variações bruscas\n",
        "        rms_db = librosa.amplitude_to_db(rms)\n",
        "        variacao_volume = np.diff(rms_db)\n",
        "\n",
        "        # Identificar picos de variação\n",
        "        threshold_variacao = np.std(variacao_volume) * 2\n",
        "        picos_variacao = np.where(np.abs(variacao_volume) > threshold_variacao)[0]\n",
        "\n",
        "        # Converter índices para timestamps\n",
        "        times = librosa.frames_to_time(picos_variacao, sr=sr, hop_length=hop_length)\n",
        "\n",
        "        return {\n",
        "            \"rms_medio\": float(np.mean(rms)),\n",
        "            \"variacao_volume_coef\": float(variation(rms)),\n",
        "            \"num_picos_variacao\": int(len(picos_variacao)),\n",
        "            \"timestamps_picos\": [float(t) for t in times.tolist()],\n",
        "            \"volume_db_medio\": float(np.mean(rms_db)),\n",
        "            \"volume_db_std\": float(np.std(rms_db))\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    ⚠️ Erro na análise de variação de volume: {e}\")\n",
        "        return {}\n",
        "\n",
        "def detectar_picos_ruido(audio_path, sr=22050):\n",
        "    \"\"\"Detecta picos de ruído excessivo\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "        # Calcular espectrograma\n",
        "        S = librosa.stft(y)\n",
        "        S_db = librosa.amplitude_to_db(np.abs(S))\n",
        "\n",
        "        # Detectar ruído baseado em frequências altas\n",
        "        freq_bins = librosa.fft_frequencies(sr=sr)\n",
        "        high_freq_mask = freq_bins > 4000  # Frequências acima de 4kHz\n",
        "\n",
        "        high_freq_energy = np.mean(S_db[high_freq_mask], axis=0)\n",
        "\n",
        "        # Identificar segmentos com ruído excessivo\n",
        "        threshold_ruido = np.percentile(high_freq_energy, 85)\n",
        "        segmentos_ruidosos = np.where(high_freq_energy > threshold_ruido)[0]\n",
        "\n",
        "        # Converter para timestamps\n",
        "        hop_length = 512\n",
        "        times_ruido = librosa.frames_to_time(segmentos_ruidosos, sr=sr, hop_length=hop_length)\n",
        "\n",
        "        return {\n",
        "            \"energia_alta_freq_media\": float(np.mean(high_freq_energy)),\n",
        "            \"threshold_ruido\": float(threshold_ruido),\n",
        "            \"num_segmentos_ruidosos\": int(len(segmentos_ruidosos)),\n",
        "            \"timestamps_ruido\": [float(t) for t in times_ruido.tolist()],\n",
        "            \"percentual_audio_ruidoso\": float(len(segmentos_ruidosos) / len(high_freq_energy) * 100)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    ⚠️ Erro na detecção de picos de ruído: {e}\")\n",
        "        return {}\n",
        "\n",
        "def analisar_ritmo_fala(transcricao_texto, duracao_audio):\n",
        "    \"\"\"Calcula ritmo da fala em palavras por minuto\"\"\"\n",
        "    try:\n",
        "        if not transcricao_texto or duracao_audio <= 0:\n",
        "            return {}\n",
        "\n",
        "        palavras = transcricao_texto.split()\n",
        "        num_palavras = len(palavras)\n",
        "        duracao_minutos = duracao_audio / 60.0\n",
        "\n",
        "        palavras_por_minuto = num_palavras / duracao_minutos\n",
        "\n",
        "        # Classificar ritmo\n",
        "        if palavras_por_minuto < 120:\n",
        "            classificacao_ritmo = \"Lento\"\n",
        "        elif palavras_por_minuto < 160:\n",
        "            classificacao_ritmo = \"Normal\"\n",
        "        elif palavras_por_minuto < 200:\n",
        "            classificacao_ritmo = \"Rápido\"\n",
        "        else:\n",
        "            classificacao_ritmo = \"Muito Rápido\"\n",
        "\n",
        "        return {\n",
        "            \"palavras_por_minuto\": float(palavras_por_minuto),\n",
        "            \"total_palavras\": int(num_palavras),\n",
        "            \"duracao_minutos\": float(duracao_minutos),\n",
        "            \"classificacao_ritmo\": str(classificacao_ritmo),\n",
        "            \"densidade_informacional\": float(num_palavras / duracao_audio)  # palavras por segundo\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    ⚠️ Erro na análise de ritmo de fala: {e}\")\n",
        "        return {}\n",
        "\n",
        "def identificar_pausas_fala(audio_path, sr=22050):\n",
        "    \"\"\"Identifica pausas e silêncios na fala\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "        # Detectar segmentos de fala vs silêncio\n",
        "        frame_length = int(0.025 * sr)  # 25ms frames\n",
        "        hop_length = frame_length // 2\n",
        "\n",
        "        # Energia RMS para detectar atividade vocal\n",
        "        rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]\n",
        "\n",
        "        # Threshold para distinguir fala de silêncio\n",
        "        threshold_silencio = np.percentile(rms, 20)  # 20% mais baixo = silêncio\n",
        "\n",
        "        # Identificar segmentos de silêncio\n",
        "        is_silence = rms < threshold_silencio\n",
        "\n",
        "        # Encontrar início e fim das pausas\n",
        "        pausas = []\n",
        "        in_pause = False\n",
        "        pause_start = 0\n",
        "\n",
        "        times = librosa.frames_to_time(range(len(is_silence)), sr=sr, hop_length=hop_length)\n",
        "\n",
        "        for i, silent in enumerate(is_silence):\n",
        "            if silent and not in_pause:\n",
        "                in_pause = True\n",
        "                pause_start = times[i]\n",
        "            elif not silent and in_pause:\n",
        "                in_pause = False\n",
        "                pause_duration = times[i] - pause_start\n",
        "                if pause_duration > 0.2:  # Pausas maiores que 200ms\n",
        "                    pausas.append({\n",
        "                        \"inicio\": float(pause_start),\n",
        "                        \"fim\": float(times[i]),\n",
        "                        \"duracao\": float(pause_duration)\n",
        "                    })\n",
        "\n",
        "        # Estatísticas das pausas\n",
        "        if pausas:\n",
        "            duracoes_pausas = [p[\"duracao\"] for p in pausas]\n",
        "            pausa_media = np.mean(duracoes_pausas)\n",
        "            pausa_total = sum(duracoes_pausas)\n",
        "        else:\n",
        "            pausa_media = 0\n",
        "            pausa_total = 0\n",
        "\n",
        "        return {\n",
        "            \"num_pausas\": int(len(pausas)),\n",
        "            \"pausas_detectadas\": pausas,\n",
        "            \"duracao_pausa_media\": float(pausa_media),\n",
        "            \"tempo_total_pausas\": float(pausa_total),\n",
        "            \"percentual_pausas\": float(pausa_total / len(y) * sr * 100),\n",
        "            \"threshold_silencio\": float(threshold_silencio)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    ⚠️ Erro na identificação de pausas: {e}\")\n",
        "        return {}\n",
        "\n",
        "def classificar_musica_fundo(audio_path, sr=22050):\n",
        "    \"\"\"Classifica características da música de fundo\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "        # Análise de características musicais\n",
        "        tempo, beats = librosa.beat.beat_track(y=y, sr=sr)\n",
        "\n",
        "        # Análise espectral\n",
        "        spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
        "        spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]\n",
        "        mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
        "\n",
        "        # Energia\n",
        "        energia_total = np.sum(y**2)\n",
        "        energia_normalizada = energia_total / len(y)\n",
        "\n",
        "        # Classificação por energia\n",
        "        if energia_normalizada > 0.01:\n",
        "            nivel_energia = \"Alta\"\n",
        "        elif energia_normalizada > 0.001:\n",
        "            nivel_energia = \"Média\"\n",
        "        else:\n",
        "            nivel_energia = \"Baixa\"\n",
        "\n",
        "        # Classificação por características espectrais\n",
        "        centroide_medio = np.mean(spectral_centroids)\n",
        "        if centroide_medio > 3000:\n",
        "            brilho = \"Brilhante\"\n",
        "        elif centroide_medio > 1500:\n",
        "            brilho = \"Equilibrado\"\n",
        "        else:\n",
        "            brilho = \"Escuro\"\n",
        "\n",
        "        return {\n",
        "            \"tempo_bpm\": float(tempo),\n",
        "            \"num_beats\": int(len(beats)),\n",
        "            \"energia_nivel\": str(nivel_energia),\n",
        "            \"energia_valor\": float(energia_normalizada),\n",
        "            \"brilho_espectral\": str(brilho),\n",
        "            \"centroide_espectral_medio\": float(centroide_medio),\n",
        "            \"rolloff_medio\": float(np.mean(spectral_rolloff)),\n",
        "            \"mfcc_features\": [float(x) for x in mfcc.mean(axis=1).tolist()]\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    ⚠️ Erro na classificação de música de fundo: {e}\")\n",
        "        return {}\n",
        "\n",
        "def analisar_clareza_voz(audio_path, sr=22050):\n",
        "    \"\"\"Analisa clareza e inteligibilidade da voz\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "        # Faixa de frequência da voz humana (aproximadamente 85-255 Hz para fundamental)\n",
        "        # e harmônicos até ~4000 Hz para inteligibilidade\n",
        "\n",
        "        # Análise espectral\n",
        "        S = librosa.stft(y)\n",
        "        frequencies = librosa.fft_frequencies(sr=sr)\n",
        "\n",
        "        # Energia em diferentes bandas de frequência\n",
        "        baixa_freq = (frequencies >= 85) & (frequencies <= 255)    # Fundamental da voz\n",
        "        media_freq = (frequencies > 255) & (frequencies <= 2000)   # Formantes principais\n",
        "        alta_freq = (frequencies > 2000) & (frequencies <= 4000)   # Clareza/inteligibilidade\n",
        "\n",
        "        energia_baixa = np.mean(np.abs(S[baixa_freq]))\n",
        "        energia_media = np.mean(np.abs(S[media_freq]))\n",
        "        energia_alta = np.mean(np.abs(S[alta_freq]))\n",
        "\n",
        "        # Razão harmônica para ruído (aproximação)\n",
        "        spectral_flatness = librosa.feature.spectral_flatness(y=y)[0]\n",
        "        clareza_media = 1 - np.mean(spectral_flatness)  # Menor flatness = mais harmônica\n",
        "\n",
        "        # Zero crossing rate (indicador de fricção/clareza)\n",
        "        zcr = librosa.feature.zero_crossing_rate(y)[0]\n",
        "        zcr_medio = np.mean(zcr)\n",
        "\n",
        "        # Score de clareza combinado\n",
        "        score_clareza = (energia_media + energia_alta) / (energia_baixa + 0.001) * clareza_media\n",
        "\n",
        "        if score_clareza > 10:\n",
        "            classificacao_clareza = \"Excelente\"\n",
        "        elif score_clareza > 5:\n",
        "            classificacao_clareza = \"Boa\"\n",
        "        elif score_clareza > 2:\n",
        "            classificacao_clareza = \"Regular\"\n",
        "        else:\n",
        "            classificacao_clareza = \"Precisa Melhoria\"\n",
        "\n",
        "        return {\n",
        "            \"score_clareza\": float(score_clareza),\n",
        "            \"classificacao_clareza\": classificacao_clareza,\n",
        "            \"energia_fundamental\": float(energia_baixa),\n",
        "            \"energia_formantes\": float(energia_media),\n",
        "            \"energia_agudos\": float(energia_alta),\n",
        "            \"harmonicidade\": float(clareza_media),\n",
        "            \"zero_crossing_rate\": float(zcr_medio)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    ⚠️ Erro na análise de clareza de voz: {e}\")\n",
        "        return {}\n",
        "\n",
        "def detectar_sobreposicao_audio(audio_path, sr=22050):\n",
        "    \"\"\"Detecta sobreposição entre fala e música/efeitos\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "        # Separação harmônica/percussiva (aproximação para voz vs música)\n",
        "        y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
        "\n",
        "        # Análise de energia em cada componente\n",
        "        energia_harmonica = librosa.feature.rms(y=y_harmonic)[0]\n",
        "        energia_percussiva = librosa.feature.rms(y=y_percussive)[0]\n",
        "        energia_total = librosa.feature.rms(y=y)[0]\n",
        "\n",
        "        # Detectar momentos de sobreposição\n",
        "        threshold_sobreposicao = 0.7  # Threshold para detectar sobreposição significativa\n",
        "\n",
        "        # Razão entre componentes\n",
        "        razao_hp = energia_harmonica / (energia_percussiva + 0.001)\n",
        "\n",
        "        # Momentos onde há competição (energia similar em ambos)\n",
        "        competicao_mask = (energia_harmonica > threshold_sobreposicao * np.max(energia_harmonica)) & \\\n",
        "                         (energia_percussiva > threshold_sobreposicao * np.max(energia_percussiva))\n",
        "\n",
        "        segmentos_sobreposicao = np.where(competicao_mask)[0]\n",
        "\n",
        "        # Converter para timestamps\n",
        "        hop_length = 512\n",
        "        times_sobreposicao = librosa.frames_to_time(segmentos_sobreposicao, sr=sr, hop_length=hop_length)\n",
        "\n",
        "        return {\n",
        "            \"num_sobreposicoes\": int(len(segmentos_sobreposicao)),\n",
        "            \"timestamps_sobreposicao\": [float(t) for t in times_sobreposicao.tolist()],\n",
        "            \"percentual_sobreposicao\": float(len(segmentos_sobreposicao) / len(energia_total) * 100),\n",
        "            \"energia_harmonica_media\": float(np.mean(energia_harmonica)),\n",
        "            \"energia_percussiva_media\": float(np.mean(energia_percussiva)),\n",
        "            \"razao_harmonico_percussivo\": float(np.mean(razao_hp))\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    ⚠️ Erro na detecção de sobreposição: {e}\")\n",
        "        return {}\n",
        "\n",
        "def mapear_efeitos_sonoros(audio_path, sr=22050):\n",
        "    \"\"\"Mapeia e cataloga efeitos sonoros\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "        # Detectar eventos transientes (possíveis efeitos sonoros)\n",
        "        onset_frames = librosa.onset.onset_detect(y=y, sr=sr, units='frames')\n",
        "        onset_times = librosa.frames_to_time(onset_frames, sr=sr)\n",
        "\n",
        "        # Análise de características espectrais em cada onset\n",
        "        efeitos_detectados = []\n",
        "\n",
        "        for i, onset_time in enumerate(onset_times):\n",
        "            # Janela de análise ao redor do onset\n",
        "            inicio_frame = max(0, onset_frames[i] - 10)\n",
        "            fim_frame = min(len(y), onset_frames[i] + 50)\n",
        "\n",
        "            janela = y[inicio_frame:fim_frame] if fim_frame > inicio_frame else np.array([])\n",
        "\n",
        "            if len(janela) > 0:\n",
        "                # Características do efeito\n",
        "                energia = np.sum(janela**2)\n",
        "                freq_dominante = librosa.piptrack(y=janela, sr=sr)[0]\n",
        "\n",
        "                # Classificação simplificada baseada em características\n",
        "                if energia > 0.1:\n",
        "                    tipo_efeito = \"Impacto\"\n",
        "                elif np.max(freq_dominante) > 5000:\n",
        "                    tipo_efeito = \"Agudo\"\n",
        "                elif np.max(freq_dominante) < 200:\n",
        "                    tipo_efeito = \"Grave\"\n",
        "                else:\n",
        "                    tipo_efeito = \"Médio\"\n",
        "\n",
        "                efeitos_detectados.append({\n",
        "                    \"timestamp\": float(onset_time),\n",
        "                    \"energia\": float(energia),\n",
        "                    \"tipo_estimado\": tipo_efeito\n",
        "                })\n",
        "\n",
        "        # Contagem por tipo\n",
        "        tipos_efeitos = Counter([ef[\"tipo_estimado\"] for ef in efeitos_detectados])\n",
        "\n",
        "        return {\n",
        "            \"num_efeitos_detectados\": int(len(efeitos_detectados)),\n",
        "            \"efeitos_por_minuto\": float(len(efeitos_detectados) / (len(y) / sr / 60)),\n",
        "            \"tipos_efeitos\": dict(tipos_efeitos),\n",
        "            \"efeitos_detalhados\": efeitos_detectados,\n",
        "            \"densidade_efeitos\": float(len(efeitos_detectados) / (len(y) / sr))\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    ⚠️ Erro no mapeamento de efeitos sonoros: {e}\")\n",
        "        return {}\n",
        "\n",
        "def analisar_frequencias_especificas(audio_path, sr=22050):\n",
        "    \"\"\"Analisa sons recorrentes específicos\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "        # Template matching para sons específicos (simplificado)\n",
        "        # Detectar padrões de risada (frequências variadas em burst)\n",
        "        onset_strength = librosa.onset.onset_strength(y=y, sr=sr)\n",
        "\n",
        "        # Detectar rajadas de atividade (possível risada)\n",
        "        threshold_burst = np.percentile(onset_strength, 80)\n",
        "        bursts = onset_strength > threshold_burst\n",
        "\n",
        "        # Agrupar bursts próximos\n",
        "        burst_groups = []\n",
        "        in_burst = False\n",
        "        burst_start = 0\n",
        "\n",
        "        for i, is_burst in enumerate(bursts):\n",
        "            if is_burst and not in_burst:\n",
        "                in_burst = True\n",
        "                burst_start = i\n",
        "            elif not is_burst and in_burst:\n",
        "                in_burst = False\n",
        "                burst_duration = i - burst_start\n",
        "                if burst_duration > 5:  # Bursts de pelo menos 5 frames\n",
        "                    burst_groups.append({\n",
        "                        \"inicio\": librosa.frames_to_time(burst_start, sr=sr),\n",
        "                        \"duracao\": librosa.frames_to_time(burst_duration, sr=sr),\n",
        "                        \"intensidade\": np.mean(onset_strength[burst_start:i])\n",
        "                    })\n",
        "\n",
        "        # Detectar sons de notificação (tons puros em frequências específicas)\n",
        "        # Análise espectral para encontrar picos em frequências comuns de notificação\n",
        "        S = librosa.stft(y)\n",
        "        frequencies = librosa.fft_frequencies(sr=sr)\n",
        "\n",
        "        # Frequências típicas de notificação (440Hz, 880Hz, etc.)\n",
        "        freq_targets = [440, 880, 1320]  # A4, A5, E6\n",
        "        notificacoes_detectadas = 0\n",
        "\n",
        "        for freq_target in freq_targets:\n",
        "            freq_idx = np.argmin(np.abs(frequencies - freq_target))\n",
        "            freq_energy = np.abs(S[freq_idx])\n",
        "\n",
        "            # Detectar picos sustentados nesta frequência\n",
        "            peaks = scipy.signal.find_peaks(freq_energy, height=np.percentile(freq_energy, 90))[0]\n",
        "            notificacoes_detectadas += len(peaks)\n",
        "\n",
        "        return {\n",
        "            \"bursts_atividade\": int(len(burst_groups)),\n",
        "            \"detalhes_bursts\": burst_groups,\n",
        "            \"possivel_risada\": int(len(burst_groups)),\n",
        "            \"sons_notificacao_detectados\": int(notificacoes_detectadas),\n",
        "            \"densidade_eventos_especiais\": float((len(burst_groups) + notificacoes_detectadas) / (len(y) / sr))\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    ⚠️ Erro na análise de frequências específicas: {e}\")\n",
        "        return {}\n",
        "\n",
        "def gerar_espectrograma_simplificado(audio_path, video_id, sr=22050):\n",
        "    \"\"\"Gera e salva espectrograma simplificado\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "        # Gerar espectrograma\n",
        "        S = librosa.stft(y)\n",
        "        S_db = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n",
        "\n",
        "        # Criar visualização\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        librosa.display.specshow(S_db, sr=sr, x_axis='time', y_axis='hz')\n",
        "        plt.colorbar(format='%+2.0f dB')\n",
        "        plt.title(f'Espectrograma - {video_id}')\n",
        "        plt.xlabel('Tempo (s)')\n",
        "        plt.ylabel('Frequência (Hz)')\n",
        "        plt.ylim(0, 8000)  # Focar em frequências até 8kHz\n",
        "\n",
        "        # Salvar\n",
        "        espectrograma_path = os.path.join(PASTA_TRABALHO, \"analise_audio\", f\"espectrograma_{video_id}.png\")\n",
        "        os.makedirs(os.path.dirname(espectrograma_path), exist_ok=True)\n",
        "        plt.savefig(espectrograma_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        # Análise de padrões espectrais\n",
        "        freq_bins = librosa.fft_frequencies(sr=sr)\n",
        "\n",
        "        # Energia por banda de frequência\n",
        "        baixa_energia = np.mean(S_db[freq_bins <= 500])\n",
        "        media_energia = np.mean(S_db[(freq_bins > 500) & (freq_bins <= 2000)])\n",
        "        alta_energia = np.mean(S_db[freq_bins > 2000])\n",
        "\n",
        "        return {\n",
        "            \"espectrograma_path\": str(espectrograma_path),\n",
        "            \"energia_baixa_freq\": float(baixa_energia),\n",
        "            \"energia_media_freq\": float(media_energia),\n",
        "            \"energia_alta_freq\": float(alta_energia),\n",
        "            \"frequencia_maxima\": float(np.max(freq_bins)),\n",
        "            \"resolucao_temporal\": float(len(y) / sr),\n",
        "            \"picos_espectrais\": int(len(scipy.signal.find_peaks(np.mean(S_db, axis=1))[0]))\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    ⚠️ Erro na geração de espectrograma: {e}\")\n",
        "        return {\"espectrograma_path\": None}\n",
        "\n",
        "def processar_analise_audio_refinada():\n",
        "    \"\"\"Processa análise de áudio refinada para todos os vídeos\"\"\"\n",
        "    prerequisito_ok, config = verificar_prerequisito_audio_refinado()\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Carregar dados de decomposição\n",
        "    decomposicao_path = os.path.join(PASTA_TRABALHO, \"dados\", \"decomposicao_completa.json\")\n",
        "    with open(decomposicao_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        decomposicoes = json.load(f)\n",
        "\n",
        "    analises_audio_refinadas = []\n",
        "    sucessos = 0\n",
        "\n",
        "    print(f\"\"\"\n",
        "Iniciando análise de áudio refinada para {len(decomposicoes)} vídeos...\"\"\")\n",
        "\n",
        "    for i, decomposicao in enumerate(decomposicoes, 1):\n",
        "        if decomposicao.get(\"status\") == \"decomposto\":\n",
        "            video_id = decomposicao[\"video_id\"]\n",
        "            print(f\"[{i}/{len(decomposicoes)}] Analisando áudio refinado para: {video_id}\")\n",
        "\n",
        "            try:\n",
        "                # Buscar arquivo de áudio\n",
        "                audio_path = os.path.join(PASTA_TRABALHO, \"temp\", f\"{video_id}.wav\")\n",
        "\n",
        "                if not os.path.exists(audio_path):\n",
        "                    print(f\"    ⚠️ Arquivo de áudio não encontrado: {audio_path}\")\n",
        "                    analises_audio_refinadas.append({\n",
        "                        \"video_id\": video_id,\n",
        "                        \"status\": \"erro_audio_nao_encontrado\",\n",
        "                        \"erro\": \"Arquivo de áudio não encontrado\"\n",
        "                    })\n",
        "                    continue\n",
        "\n",
        "                analise_refinada = {\"video_id\": video_id}\n",
        "\n",
        "                print(f\"    🔊 Analisando variação de volume...\")\n",
        "                analise_refinada[\"variacao_volume\"] = analisar_variacao_volume(audio_path)\n",
        "\n",
        "                print(f\"    🔊 Detectando picos de ruído...\")\n",
        "                analise_refinada[\"picos_ruido\"] = detectar_picos_ruido(audio_path)\n",
        "\n",
        "                print(f\"    🔊 Analisando ritmo da fala...\")\n",
        "                transcricao = decomposicao.get(\"audio_transcrito\", \"\")\n",
        "                duracao_audio = decomposicao.get(\"audio_analise\", {}).get(\"duracao_audio_segundos\", 0)\n",
        "                analise_refinada[\"ritmo_fala\"] = analisar_ritmo_fala(transcricao, duracao_audio)\n",
        "\n",
        "                print(f\"    🔊 Identificando pausas...\")\n",
        "                analise_refinada[\"pausas_fala\"] = identificar_pausas_fala(audio_path)\n",
        "\n",
        "                print(f\"    🔊 Classificando música de fundo...\")\n",
        "                analise_refinada[\"musica_fundo\"] = classificar_musica_fundo(audio_path)\n",
        "\n",
        "                print(f\"    🔊 Analisando clareza da voz...\")\n",
        "                analise_refinada[\"clareza_voz\"] = analisar_clareza_voz(audio_path)\n",
        "\n",
        "                print(f\"    🔊 Detectando sobreposição...\")\n",
        "                analise_refinada[\"sobreposicao_audio\"] = detectar_sobreposicao_audio(audio_path)\n",
        "\n",
        "                print(f\"    🔊 Mapeando efeitos sonoros...\")\n",
        "                analise_refinada[\"efeitos_sonoros\"] = mapear_efeitos_sonoros(audio_path)\n",
        "\n",
        "                print(f\"    🔊 Analisando frequências específicas...\")\n",
        "                analise_refinada[\"frequencias_especificas\"] = analisar_frequencias_especificas(audio_path)\n",
        "\n",
        "                print(f\"    🔊 Gerando espectrograma...\")\n",
        "                analise_refinada[\"espectrograma\"] = gerar_espectrograma_simplificado(audio_path, video_id)\n",
        "\n",
        "                analise_refinada = converter_para_json_serializable(analise_refinada)\n",
        "                analise_refinada[\"status\"] = \"audio_refinado_concluido\"\n",
        "                analise_refinada[\"data_analise\"] = datetime.now().isoformat()\n",
        "\n",
        "                analises_audio_refinadas.append(analise_refinada)\n",
        "                sucessos += 1\n",
        "                print(f\"  ✅ Análise de áudio refinada concluída para {video_id}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  ❌ ERRO na análise de áudio refinada para {video_id}: {e}\")\n",
        "                analises_audio_refinadas.append({\n",
        "                    \"video_id\": video_id,\n",
        "                    \"status\": \"erro_analise_audio_refinada\",\n",
        "                    \"erro\": str(e)\n",
        "                })\n",
        "        else:\n",
        "            print(f\"[{i}/{len(decomposicoes)}] Pulando {decomposicao.get('video_id', 'N/A')} - Status: {decomposicao.get('status', 'N/A')}\")\n",
        "            analises_audio_refinadas.append({\n",
        "                \"video_id\": decomposicao.get(\"video_id\", \"N/A\"),\n",
        "                \"status\": decomposicao.get(\"status\", \"N/A\"),\n",
        "                \"erro\": \"Pulado devido a erro anterior\"\n",
        "            })\n",
        "\n",
        "    # Salvar análises de áudio refinadas\n",
        "    analise_audio_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analise_audio_refinada.json\")\n",
        "    with open(analise_audio_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(analises_audio_refinadas, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Gerar relatório resumido em Excel\n",
        "    try:\n",
        "        # Preparar dados para Excel\n",
        "        dados_resumo = []\n",
        "        for analise in analises_audio_refinadas:\n",
        "            if analise.get(\"status\") == \"audio_refinado_concluido\":\n",
        "                resumo = {\n",
        "                    \"video_id\": analise[\"video_id\"],\n",
        "                    \"variacao_volume_coef\": analise[\"variacao_volume\"].get(\"variacao_volume_coef\", 0),\n",
        "                    \"num_picos_variacao\": analise[\"variacao_volume\"].get(\"num_picos_variacao\", 0),\n",
        "                    \"volume_db_medio\": analise[\"variacao_volume\"].get(\"volume_db_medio\", 0),\n",
        "                    \"percentual_audio_ruidoso\": analise[\"picos_ruido\"].get(\"percentual_audio_ruidoso\", 0),\n",
        "                    \"num_segmentos_ruidosos\": analise[\"picos_ruido\"].get(\"num_segmentos_ruidosos\", 0),\n",
        "                    \"palavras_por_minuto\": analise[\"ritmo_fala\"].get(\"palavras_por_minuto\", 0),\n",
        "                    \"classificacao_ritmo\": analise[\"ritmo_fala\"].get(\"classificacao_ritmo\", \"N/A\"),\n",
        "                    \"num_pausas\": analise[\"pausas_fala\"].get(\"num_pausas\", 0),\n",
        "                    \"percentual_pausas\": analise[\"pausas_fala\"].get(\"percentual_pausas\", 0),\n",
        "                    \"nivel_energia_musica\": analise[\"musica_fundo\"].get(\"energia_nivel\", \"N/A\"),\n",
        "                    \"tempo_bpm_musica\": analise[\"musica_fundo\"].get(\"tempo_bpm\", 0),\n",
        "                    \"score_clareza\": analise[\"clareza_voz\"].get(\"score_clareza\", 0),\n",
        "                    \"classificacao_clareza\": analise[\"clareza_voz\"].get(\"classificacao_clareza\", \"N/A\"),\n",
        "                    \"percentual_sobreposicao\": analise[\"sobreposicao_audio\"].get(\"percentual_sobreposicao\", 0),\n",
        "                    \"num_efeitos_detectados\": analise[\"efeitos_sonoros\"].get(\"num_efeitos_detectados\", 0),\n",
        "                    \"densidade_efeitos\": analise[\"efeitos_sonoros\"].get(\"densidade_efeitos\", 0),\n",
        "                    \"possivel_risada\": analise[\"frequencias_especificas\"].get(\"possivel_risada\", 0),\n",
        "                    \"sons_notificacao\": analise[\"frequencias_especificas\"].get(\"sons_notificacao_detectados\", 0),\n",
        "                    \"espectrograma_gerado\": \"Sim\" if analise[\"espectrograma\"].get(\"espectrograma_path\") else \"Não\"\n",
        "                }\n",
        "                dados_resumo.append(resumo)\n",
        "\n",
        "        if dados_resumo:\n",
        "            df_resumo = pd.DataFrame(dados_resumo)\n",
        "            resumo_excel_path = os.path.join(PASTA_TRABALHO, \"analise_audio\", \"resumo_analise_audio.xlsx\")\n",
        "            df_resumo.to_excel(resumo_excel_path, index=False, engine='openpyxl')\n",
        "            print(f\"\\n💾 Relatório resumo de análise de áudio salvo em: {resumo_excel_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ❌ ERRO ao gerar relatório resumo de áudio: {e}\")\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    # Adicionar status para a nova etapa de análise de áudio refinada\n",
        "    config[\"status_etapas\"][\"analise_audio_refinada\"] = True\n",
        "    config[\"total_videos_analisados_audio_refinado\"] = sucessos\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\"\"\n",
        "✅ ANÁLISE DE ÁUDIO REFINADA CONCLUÍDA!\"\"\")\n",
        "    print(f\"Total de vídeos com análise de áudio refinada concluída: {sucessos}\")\n",
        "\n",
        "    if sucessos == 0:\n",
        "        print(\"❌ NENHUM VÍDEO FOI ANALISADO COM SUCESSO NESTA ETAPA. Verifique as etapas anteriores.\")\n",
        "    # No final, a próxima célula seria 3.1 (Análise de Padrões) que já foi executada\n",
        "    # mas como esta é uma nova célula (2.4), ela deveria vir antes de 3.1\n",
        "    # A mensagem original apontava para 3.1.\n",
        "    # Vamos manter a mensagem original para não alterar o fluxo do notebook existente,\n",
        "    # mas idealmente, essa célula seria inserida antes de 3.1 no fluxo.\n",
        "    print(\"\"\"\n",
        "➡️ PRÓXIMA CÉLULA: 3.1 - ANÁLISE DE PADRÕES (TEMPORAIS, VISUAIS, TEXTO, ÁUDIO)\"\"\")\n",
        "\n",
        "    # Return the list of analyses for potential downstream use\n",
        "    return analises_audio_refinadas\n",
        "\n",
        "# Executar análise de audio refinada\n",
        "try:\n",
        "    processar_analise_audio_refinada()\n",
        "except Exception as e:\n",
        "    print(f\"\"\"\n",
        "❌ ERRO GERAL NA ANÁLISE DE ÁUDIO REFINADA: {e}\"\"\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oIgWuMTniIv",
        "outputId": "e5af216d-0fe0-4a03-f54e-20cca523db82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Iniciando processamento de copywriting adaptado...\n",
            "  ✅ Dados encontrados: decomposicao com 3 vídeos\n",
            "Processando copywriting para 3 vídeos...\n",
            "[1/3] Processando copywriting para: vid_bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t\n",
            "  🔄 Gerando legendas com timestamps...\n",
            "    ✅ Legendas SRT geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t_legendas.srt\n",
            "    ✅ Legendas TXT com timestamps geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t_legendas_timestamped.txt\n",
            "    🔄 Analisando copywriting...\n",
            "  ✅ Copywriting analisado: Score 0/100\n",
            "[2/3] Processando copywriting para: vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf\n",
            "  🔄 Gerando legendas com timestamps...\n",
            "    ✅ Legendas SRT geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf_legendas.srt\n",
            "    ✅ Legendas TXT com timestamps geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf_legendas_timestamped.txt\n",
            "    🔄 Analisando copywriting...\n",
            "  ✅ Copywriting analisado: Score 30/100\n",
            "[3/3] Processando copywriting para: vid_máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy\n",
            "  🔄 Gerando legendas com timestamps...\n",
            "    ✅ Legendas SRT geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy_legendas.srt\n",
            "    ✅ Legendas TXT com timestamps geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy_legendas_timestamped.txt\n",
            "    🔄 Analisando copywriting...\n",
            "  ✅ Copywriting analisado: Score 75/100\n",
            "💾 Análises de copywriting salvas em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/analises_copywriting_completas.json\n",
            "💾 Dados de legendas salvos em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/legendas_geradas.json\n",
            "\n",
            "✅ ANÁLISE DE COPYWRITING CONCLUÍDA!\n",
            "Total de vídeos com copywriting analisado: 3\n",
            "Total de legendas geradas: 3\n",
            "\n",
            "➡️ PRÓXIMA CÉLULA: 4.3 - INTEGRAÇÃO COM DASHBOARD\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CÉLULA 2.4: GERAÇÃO DE LEGENDAS E ANÁLISE DE COPYWRITING - VERSÃO CORRIGIDA\n",
        "# ============================================================================\n",
        "\n",
        "import re\n",
        "from datetime import timedelta, datetime\n",
        "from collections import Counter\n",
        "import json\n",
        "import os\n",
        "\n",
        "# ============================================================================\n",
        "# Funções Auxiliares (Movidas para este escopo)\n",
        "# ============================================================================\n",
        "\n",
        "def buscar_dados_disponiveis():\n",
        "    \"\"\"Busca dados disponíveis em ordem de prioridade\"\"\"\n",
        "    pasta_dados = os.path.join(PASTA_TRABALHO, \"dados\")\n",
        "\n",
        "    # Lista de possíveis fontes de dados (em ordem de prioridade)\n",
        "    fontes_dados = [\n",
        "        (\"decomposicao_completa.json\", \"decomposicao\"),\n",
        "        (\"analises_padroes_completas.json\", \"padroes\"),\n",
        "        (\"analises_psicologicas_completas.json\", \"psicologico\"),\n",
        "        (\"metadados_completos.json\", \"metadados\"),\n",
        "        (\"videos_catalogados.json\", \"catalogados\")\n",
        "    ]\n",
        "\n",
        "    for arquivo, tipo in fontes_dados:\n",
        "        caminho_arquivo = os.path.join(pasta_dados, arquivo)\n",
        "\n",
        "        if os.path.exists(caminho_arquivo):\n",
        "            try:\n",
        "                with open(caminho_arquivo, \"r\", encoding=\"utf-8\") as f:\n",
        "                    dados = json.load(f)\n",
        "                if dados:\n",
        "                    return {\"tipo\": tipo, \"videos\": dados}\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Erro ao carregar dados de {arquivo}: {e}\")\n",
        "\n",
        "    return None\n",
        "\n",
        "def extrair_texto_disponivel(video_data, tipo_fonte):\n",
        "    \"\"\"Extrai texto (transcrição ou OCR) da fonte de dados disponível\"\"\"\n",
        "    if tipo_fonte == \"decomposicao\":\n",
        "        return video_data.get(\"audio_transcrito\", \"\") or \" \".join([item.get(\"text\", \"\") for item in video_data.get(\"textos_ocr\", [])])\n",
        "    elif tipo_fonte == \"padroes\":\n",
        "         # Analise de padroes might have summary or keywords\n",
        "         return video_data.get(\"resumo_texto\", \"\") # or \" \".join(video_data.get(\"palavras_chave_texto\", []))\n",
        "    # Adicionar outras fontes conforme necessário\n",
        "    return \"\" # Default vazio\n",
        "\n",
        "def gerar_legendas_adaptadas(video_id, texto_transcrito, video_data):\n",
        "    \"\"\"Gera legendas para a análise de copywriting, adaptando se necessário\"\"\"\n",
        "    # Se já houver dados de decomposição com timestamps, usar esses\n",
        "    if video_data.get(\"frames_extraidos\"):\n",
        "        # Tentar usar os dados de decomposição originais para timestamps\n",
        "        # Isso requer carregar o arquivo decomposicao_completa.json novamente\n",
        "        decomposicao_path = os.path.join(PASTA_TRABALHO, \"dados\", \"decomposicao_completa.json\")\n",
        "        if os.path.exists(decomposicao_path):\n",
        "            try:\n",
        "                with open(decomposicao_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    decomposicoes = json.load(f)\n",
        "                decomposicao_original = next((d for d in decomposicoes if d[\"video_id\"] == video_id), None)\n",
        "                if decomposicao_original and decomposicao_original.get(\"audio_transcrito\"):\n",
        "                     # Se a transcrição original existir, usar a função original de legendas\n",
        "                    duracao = video_data.get(\"duracao_segundos\", decomposicao_original.get(\"audio_analise\", {}).get(\"duracao_audio_segundos\", 30))\n",
        "                    return gerar_legendas_com_timestamps({\"id\": video_id, \"duracao_segundos\": duracao}, decomposicao_original)\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Aviso: Erro ao carregar decomposição original para {video_id}: {e}. Gerando legendas estimadas.\")\n",
        "\n",
        "    # Se não houver decomposição original ou transcrição lá, gerar legendas estimadas\n",
        "    duracao_segundos = video_data.get(\"duracao_segundos\", estimar_duracao(texto_transcrito))\n",
        "    segmentos = dividir_texto_em_segmentos(texto_transcrito)\n",
        "    legendas_data = []\n",
        "    duracao_por_segmento = duracao_segundos / len(segmentos) if segmentos else 1\n",
        "\n",
        "    for i, segmento in enumerate(segmentos):\n",
        "        inicio_segundos = i * duracao_por_segmento\n",
        "        fim_segundos = (i + 1) * duracao_por_segmento\n",
        "\n",
        "        legenda_item = {\n",
        "            \"id\": i + 1,\n",
        "            \"inicio\": segundos_para_timestamp(inicio_segundos),\n",
        "            \"fim\": segundos_para_timestamp(fim_segundos),\n",
        "            \"texto\": segmento.strip(),\n",
        "            \"inicio_segundos\": inicio_segundos,\n",
        "            \"fim_segundos\": fim_segundos\n",
        "        }\n",
        "        legendas_data.append(legenda_item)\n",
        "\n",
        "    if not legendas_data:\n",
        "         return None, None, None\n",
        "\n",
        "    pasta_legendas = os.path.join(PASTA_TRABALHO, \"legendas\")\n",
        "    os.makedirs(pasta_legendas, exist_ok=True)\n",
        "    srt_path = os.path.join(pasta_legendas, f\"{video_id}_legendas_estimadas.srt\")\n",
        "    txt_path = os.path.join(pasta_legendas, f\"{video_id}_legendas_estimadas_timestamped.txt\")\n",
        "\n",
        "    gerar_arquivo_srt(legendas_data, srt_path)\n",
        "    gerar_arquivo_txt_timestamped(legendas_data, txt_path)\n",
        "\n",
        "    print(f\"    ✅ Legendas estimadas geradas: {srt_path}\")\n",
        "\n",
        "    return legendas_data, srt_path, txt_path\n",
        "\n",
        "\n",
        "def analisar_copywriting_adaptado(legendas_data, video_id, texto_completo):\n",
        "    \"\"\"Analisa copywriting usando a estrutura existente mas adaptada\"\"\"\n",
        "    print(\"    🔄 Analisando copywriting...\")\n",
        "\n",
        "    # Dicionários de padrões de copywriting (mantidos da função original)\n",
        "    ganchos_patterns = {\n",
        "        \"pergunta_retorica\": [r\"\\b(?:você|tu)\\s+(?:já|nunca|sempre|realmente|acha|imagina|sabe|quer|precisa)\",\n",
        "                            r\"(?:como|por que|quando|onde|o que).*\\?\"],\n",
        "        \"urgencia\": [r\"\\b(?:agora|hoje|urgente|rápido|imediato|última chance|só hoje|apenas|restam)\",\n",
        "                     r\"\\b(?:não perca|aproveite|garante já|corre|últimas vagas)\"],\n",
        "        \"escassez\": [r\"\\b(?:limitado|exclusivo|poucos|restam|última|única|especial|VIP)\",\n",
        "                     r\"\\b(?:só para|apenas para|somente|limitado a)\"],\n",
        "        \"autoridade\": [r\"\\b(?:especialista|expert|profissional|anos de experiência|comprovado|testado)\",\n",
        "                       r\"\\b(?:pesquisas mostram|estudos comprovam|cientificamente)\"],\n",
        "        \"prova_social\": [r\"\\b(?:milhares|centenas|todos|muitas pessoas|clientes|depoimentos)\",\n",
        "                         r\"\\b(?:já conseguiram|transformaram|mudaram|aprovaram)\"],\n",
        "        \"curiosidade\": [r\"\\b(?:segredo|descoberta|revelação|método|técnica|estratégia|fórmula)\",\n",
        "                        r\"\\b(?:ninguém te conta|poucos sabem|descobri que)\"],\n",
        "        \"problema_dor\": [r\"\\b(?:problema|dificuldade|frustração|sofre|dor|preocupa|bloqueia)\",\n",
        "                         r\"\\b(?:cansado de|chega de|pare de|não aguenta mais)\"],\n",
        "        \"solucao_resultado\": [r\"\\b(?:solução|resolve|elimina|transforma|muda|resultado|sucesso)\",\n",
        "                              r\"\\b(?:conseguir|alcançar|realizar|conquistar|atingir)\"]\n",
        "    }\n",
        "\n",
        "    gatilhos_patterns = {\n",
        "        \"reciprocidade\": [r\"\\b(?:grátis|de graça|presente|bônus|oferta|sem custo)\",\n",
        "                          r\"\\b(?:vou te dar|vou ensinar|vou mostrar|compartilhar com você)\"],\n",
        "        \"comprometimento\": [r\"\\b(?:compromisso|prometo|garanto|palavra|juro)\",\n",
        "                            r\"\\b(?:pode confiar|tenho certeza|assumo|responsabilizo)\"],\n",
        "        \"aprovacao_social\": [r\"\\b(?:aprovado por|recomendado|indicado|usado por|preferido)\",\n",
        "                             r\"\\b(?:famosos|influencers|especialistas|médicos|profissionais)\"],\n",
        "        \"aversao_perda\": [r\"\\b(?:perder|perdendo|vai ficar de fora|não vai conseguir)\",\n",
        "                          r\"\\b(?:sair perdendo|ficar para trás|oportunidade perdida)\"],\n",
        "        \"autoridade_especialista\": [r\"\\b(?:Dr|Dra|Professor|Mestre|PhD|especialista em)\",\n",
        "                                    r\"\\b(?:formado em|pós-graduado|anos estudando)\"],\n",
        "        \"emocional_medo\": [r\"\\b(?:medo|receio|preocupação|insegurança|ansiedade)\",\n",
        "                           r\"\\b(?:não conseguir|fracassar|dar errado|prejudicar)\"],\n",
        "        \"emocional_esperanca\": [r\"\\b(?:sonho|esperança|desejo|objetivo|meta|futuro melhor)\",\n",
        "                                r\"\\b(?:realizar|conquistar|alcançar|transformar|mudar vida)\"]\n",
        "    }\n",
        "\n",
        "    ctas_patterns = {\n",
        "        \"acao_imediata\": [r\"\\b(?:clica|clique|acesse|baixe|faça|compre|adquira|garanta)\",\n",
        "                          r\"\\b(?:não perca|aproveite|corre|vai|vem|participe)\"],\n",
        "        \"link_bio\": [r\"\\b(?:link na bio|bio|biografia|perfil|stories|direct)\",\n",
        "                     r\"\\b(?:DM|chama no WhatsApp|manda mensagem)\"],\n",
        "        \"engajamento\": [r\"\\b(?:comenta|compartilha|marca|salva|curte|like|segue)\",\n",
        "                        r\"\\b(?:conta nos comentários|deixa um|comenta aqui)\"],\n",
        "        \"inscricao\": [r\"\\b(?:inscreve|se inscreva|ativa|ativar|sino|notificação)\",\n",
        "                      r\"\\b(?:cadastra|cadastre-se|registra|assine)\"],\n",
        "        \"contato_vendas\": [r\"\\b(?:WhatsApp|telefone|ligue|chama|fala comigo|contato)\",\n",
        "                           r\"\\b(?:agende|marque|consulta|reunião|conversa)\"]\n",
        "    }\n",
        "\n",
        "    # Análise dos padrões\n",
        "    ganchos_encontrados = {}\n",
        "    gatilhos_encontrados = {}\n",
        "    ctas_encontrados = {}\n",
        "\n",
        "    # Analisar ganchos\n",
        "    for tipo, patterns in ganchos_patterns.items():\n",
        "        matches = []\n",
        "        for pattern in patterns:\n",
        "            matches.extend(re.finditer(pattern, texto_completo, re.IGNORECASE))\n",
        "        if matches:\n",
        "            ganchos_encontrados[tipo] = {\n",
        "                \"count\": len(matches),\n",
        "                \"exemplos\": [m.group() for m in matches[:3]],  # Top 3 exemplos\n",
        "                \"timestamps\": encontrar_timestamps_matches(matches, legendas_data, texto_completo) # Reusa a função de timestamp\n",
        "            }\n",
        "\n",
        "    # Analisar gatilhos\n",
        "    for tipo, patterns in gatilhos_patterns.items():\n",
        "        matches = []\n",
        "        for pattern in patterns:\n",
        "            matches.extend(re.finditer(pattern, texto_completo, re.IGNORECASE))\n",
        "        if matches:\n",
        "            gatilhos_encontrados[tipo] = {\n",
        "                \"count\": len(matches),\n",
        "                \"exemplos\": [m.group() for m in matches[:3]],\n",
        "                \"timestamps\": encontrar_timestamps_matches(matches, legendas_data, texto_completo)\n",
        "            }\n",
        "\n",
        "    # Analisar CTAs\n",
        "    for tipo, patterns in ctas_patterns.items():\n",
        "        matches = []\n",
        "        for pattern in patterns:\n",
        "            matches.extend(re.finditer(pattern, texto_completo, re.IGNORECASE))\n",
        "        if matches:\n",
        "            ctas_encontrados[tipo] = {\n",
        "                \"count\": len(matches),\n",
        "                \"exemplos\": [m.group() for m in matches[:3]],\n",
        "                \"timestamps\": encontrar_timestamps_matches(matches, legendas_data, texto_completo)\n",
        "            }\n",
        "\n",
        "    # Análise de estrutura narrativa\n",
        "    estrutura_narrativa = analisar_estrutura_narrativa(legendas_data) # Reusa a função\n",
        "\n",
        "    # Análise de poder de persuasão\n",
        "    score_persuasao = calcular_score_persuasao(ganchos_encontrados, gatilhos_encontrados, ctas_encontrados) # Reusa a função\n",
        "\n",
        "    analise_copywriting = {\n",
        "        \"video_id\": video_id,\n",
        "        \"texto_completo\": texto_completo,\n",
        "        \"total_palavras\": len(texto_completo.split()),\n",
        "        \"ganchos_detectados\": ganchos_encontrados,\n",
        "        \"gatilhos_mentais_detectados\": gatilhos_encontrados,\n",
        "        \"ctas_detectados\": ctas_encontrados,\n",
        "        \"estrutura_narrativa\": estrutura_narrativa,\n",
        "        \"score_persuasao\": score_persuasao,\n",
        "        \"recomendacoes_estrategicas\": gerar_recomendacoes_copywriting(ganchos_encontrados, gatilhos_encontrados, ctas_encontrados), # Reusa\n",
        "        \"templates_identificados\": identificar_templates_replicaveis(ganchos_encontrados, gatilhos_encontrados, ctas_encontrados), # Reusa\n",
        "        \"timestamp\": {\n",
        "            \"ganchos_timeline\": mapear_timeline_elementos(ganchos_encontrados, legendas_data), # Reusa\n",
        "            \"gatilhos_timeline\": mapear_timeline_elementos(gatilhos_encontrados, legendas_data), # Reusa\n",
        "            \"ctas_timeline\": mapear_timeline_elementos(ctas_encontrados, legendas_data) # Reusa\n",
        "        },\n",
        "        \"data_analise\": datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    return analise_copywriting\n",
        "\n",
        "def estimar_duracao(texto):\n",
        "    \"\"\"Estima a duração do vídeo com base na contagem de palavras (WPM médio)\"\"\"\n",
        "    palavras_por_minuto = 150 # Média de palavras por minuto\n",
        "    num_palavras = len(texto.split())\n",
        "    duracao_minutos = num_palavras / palavras_por_minuto\n",
        "    return duracao_minutos * 60 # Retorna em segundos\n",
        "\n",
        "# ============================================================================\n",
        "# Função Principal da Célula (Movida para este escopo)\n",
        "# ============================================================================\n",
        "def processar_copywriting_todos_videos_adaptado():\n",
        "    \"\"\"Processa análise de copywriting adaptada para o sistema existente\"\"\"\n",
        "    print(\"🔄 Iniciando processamento de copywriting adaptado...\")\n",
        "\n",
        "    # Verificar pré-requisitos de forma mais flexível\n",
        "    if not \"PASTA_TRABALHO\" in globals():\n",
        "        print(\"❌ Variáveis globais não encontradas. Execute a CÉLULA 1.2 primeiro.\")\n",
        "        return\n",
        "\n",
        "    pasta_dados = os.path.join(PASTA_TRABALHO, \"dados\")\n",
        "    if not os.path.exists(pasta_dados):\n",
        "        print(\"❌ Pasta de dados não encontrada. Execute as células anteriores primeiro.\")\n",
        "        return\n",
        "\n",
        "    # Buscar dados disponíveis em ordem de prioridade\n",
        "    dados_encontrados = buscar_dados_disponiveis()\n",
        "\n",
        "    if not dados_encontrados:\n",
        "        print(\"❌ Nenhum dado de vídeo encontrado. Execute as células anteriores primeiro.\")\n",
        "        return\n",
        "\n",
        "    print(f\"  ✅ Dados encontrados: {dados_encontrados['tipo']} com {len(dados_encontrados['videos'])} vídeos\")\n",
        "\n",
        "    analises_copywriting = []\n",
        "    legendas_geradas = []\n",
        "\n",
        "    print(f\"Processando copywriting para {len(dados_encontrados['videos'])} vídeos...\")\n",
        "\n",
        "    for i, video_data in enumerate(dados_encontrados['videos'], 1):\n",
        "        video_id = video_data.get(\"id\") or video_data.get(\"video_id\", f\"vid_{i:03d}\")\n",
        "\n",
        "        print(f\"[{i}/{len(dados_encontrados['videos'])}] Processando copywriting para: {video_id}\")\n",
        "\n",
        "        try:\n",
        "            # Extrair texto transcrito de diferentes fontes possíveis\n",
        "            texto_transcrito = extrair_texto_disponivel(video_data, dados_encontrados['tipo'])\n",
        "\n",
        "            if texto_transcrito and len(texto_transcrito.strip()) > 10:\n",
        "                # Gerar legendas se houver texto\n",
        "                legendas_data, srt_path, txt_path = gerar_legendas_adaptadas(video_id, texto_transcrito, video_data)\n",
        "\n",
        "                if legendas_data:\n",
        "                    legendas_info = {\n",
        "                        \"video_id\": video_id,\n",
        "                        \"srt_path\": srt_path,\n",
        "                        \"txt_path\": txt_path,\n",
        "                        \"total_segmentos\": len(legendas_data),\n",
        "                        \"duracao_total\": video_data.get(\"duracao_segundos\", estimar_duracao(texto_transcrito)),\n",
        "                        \"legendas_data\": legendas_data\n",
        "                    }\n",
        "                    legendas_geradas.append(legendas_info)\n",
        "\n",
        "                    # Análise de copywriting\n",
        "                    analise_copy = analisar_copywriting_adaptado(legendas_data, video_id, texto_transcrito)\n",
        "                    analises_copywriting.append(analise_copy)\n",
        "\n",
        "                    print(f\"  ✅ Copywriting analisado: Score {analise_copy['score_persuasao']}/100\")\n",
        "            else:\n",
        "                print(f\"  ⚠️ Pulando {video_id}: texto insuficiente para análise\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Erro no processamento de copywriting para {video_id}: {e}\")\n",
        "\n",
        "    if not analises_copywriting:\n",
        "        print(\"❌ Nenhuma análise de copywriting foi gerada. Verifique se os vídeos possuem transcrição.\")\n",
        "        return\n",
        "\n",
        "    # Salvar dados de copywriting\n",
        "    os.makedirs(pasta_dados, exist_ok=True)\n",
        "\n",
        "    copywriting_path = os.path.join(pasta_dados, \"analises_copywriting_completas.json\")\n",
        "    with open(copywriting_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(analises_copywriting, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"💾 Análises de copywriting salvas em: {copywriting_path}\")\n",
        "\n",
        "    # Salvar dados de legendas\n",
        "    legendas_path = os.path.join(pasta_dados, \"legendas_geradas.json\")\n",
        "    with open(legendas_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(legendas_geradas, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"💾 Dados de legendas salvos em: {legendas_path}\")\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    if os.path.exists(config_path):\n",
        "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            config = json.load(f)\n",
        "\n",
        "        config[\"status_etapas\"][\"copywriting_analysis\"] = True\n",
        "\n",
        "        with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\n✅ ANÁLISE DE COPYWRITING CONCLUÍDA!\")\n",
        "    print(f\"Total de vídeos com copywriting analisado: {len(analises_copywriting)}\")\n",
        "    print(f\"Total de legendas geradas: {len(legendas_geradas)}\")\n",
        "    print(f\"\\n➡️ PRÓXIMA CÉLULA: 4.3 - INTEGRAÇÃO COM DASHBOARD\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Execução da Célula\n",
        "# ============================================================================\n",
        "try:\n",
        "    processar_copywriting_todos_videos_adaptado()\n",
        "except Exception as e:\n",
        "    print(f\"❌ ERRO de Execução: {type(e).__name__}: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "analise_padroes",
        "outputId": "bbaae1d6-ce5f-4534-aa96-8b192300234a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando análise de padrões para 3 vídeos...\n",
            "[1/3] Analisando padrões para: bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.mp4\n",
            "  ⚙️ Analisando padrões para: vid_bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t\n",
            "  ✅ Análise de padrões concluída para bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.mp4\n",
            "[2/3] Analisando padrões para: empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.mp4\n",
            "  ⚙️ Analisando padrões para: vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf\n",
            "  ✅ Análise de padrões concluída para empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.mp4\n",
            "[3/3] Analisando padrões para: máquina_emocional_não_dá_conta_de_converter_então__vid_máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy.mp4\n",
            "  ⚙️ Analisando padrões para: vid_máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy\n",
            "  ✅ Análise de padrões concluída para máquina_emocional_não_dá_conta_de_converter_então__vid_máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy.mp4\n",
            "\n",
            "💾 Dados de análise de padrões salvos em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/analises_padroes_completas.json\n",
            "✅ Status da etapa 'analise_padroes' atualizado no config.json\n",
            "\n",
            "✅ ANÁLISE DE PADRÕES CONCLUÍDA!\n",
            "Total de vídeos com padrões analisados: 3\n",
            "\n",
            "➡️ PRÓXIMA CÉLULA: 3.2 - ANÁLISE PSICOLÓGICA E GATILHOS DE ENGAJAMENTO\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# LAYER 3: ANÁLISE E PROCESSAMENTO DE DADOS\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# CÉLULA 3.1: ANÁLISE DE PADRÕES (TEMPORAIS, VISUAIS, TEXTO, ÁUDIO)\n",
        "# ============================================================================\n",
        "\n",
        "def analisar_padroes_video(decomposicao_data):\n",
        "    \"\"\"Analisa padrões temporais, visuais, de texto e áudio de um vídeo.\"\"\"\n",
        "    video_id = decomposicao_data[\"video_id\"]\n",
        "    print(f\"  ⚙️ Analisando padrões para: {video_id}\")\n",
        "\n",
        "    analise_padroes = {\n",
        "        \"video_id\": video_id,\n",
        "        \"resumo_texto\": \"\",\n",
        "        \"palavras_chave_texto\": [],\n",
        "        \"analise_audio_detalhada\": {\n",
        "            \"bpm\": decomposicao_data[\"audio_analise\"] .get(\"bpm\"),\n",
        "            \"duracao_audio_segundos\": decomposicao_data[\"audio_analise\"] .get(\"duracao_audio_segundos\")\n",
        "        },\n",
        "        \"analise_visual_detalhada\": {\n",
        "            \"total_cortes\": len(decomposicao_data.get(\"cortes_detectados_segundos\", [])),\n",
        "            \"media_frames_por_corte\": 0,\n",
        "            \"complexidade_visual_media\": 0,\n",
        "            \"brilho_medio\": 0\n",
        "        },\n",
        "        \"padroes_gerais\": []\n",
        "    }\n",
        "\n",
        "    # Análise de Texto (OCR e Transcrição)\n",
        "    todos_textos = [item[\"text\"] for item in decomposicao_data[\"textos_ocr\"]]\n",
        "    if decomposicao_data[\"audio_transcrito\"]:\n",
        "        todos_textos.append(decomposicao_data[\"audio_transcrito\"])\n",
        "\n",
        "    if todos_textos:\n",
        "        texto_completo = \" \".join(todos_textos)\n",
        "        # Simples resumo e palavras-chave (pode ser aprimorado com NLP mais avançado)\n",
        "        import re # Ensure regex is imported here for local function\n",
        "        words = [word.lower() for word in re.findall(r\"\\b\\w+\\b\", texto_completo) if len(word) > 3]\n",
        "        word_counts = Counter(words).most_common(5)\n",
        "        analise_padroes[\"palavras_chave_texto\"] = [word for word, count in word_counts]\n",
        "        analise_padroes[\"resumo_texto\"] = texto_completo[:200] + \"...\" if len(texto_completo) > 200 else texto_completo\n",
        "\n",
        "\n",
        "    # Análise Visual Detalhada\n",
        "    if decomposicao_data[\"frames_extraidos\"]:\n",
        "        complexidades = []\n",
        "        brilhos = []\n",
        "        for frame_data in decomposicao_data[\"frames_extraidos\"]:\n",
        "            try:\n",
        "                img = cv2.imread(frame_data[\"path\"])\n",
        "                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "                complexidades.append(cv2.Laplacian(gray, cv2.CV_64F).var())\n",
        "                brilhos.append(np.mean(gray))\n",
        "            except Exception as e:\n",
        "                print(f\"    ⚠️ Aviso: Erro ao analisar frame {frame_data[\"path\"]}: {e}\")\n",
        "        if complexidades: analise_padroes[\"analise_visual_detalhada\"][\"complexidade_visual_media\"] = float(np.mean(complexidades))\n",
        "        if brilhos: analise_padroes[\"analise_visual_detalhada\"][\"brilho_medio\"] = float(np.mean(brilhos))\n",
        "\n",
        "    # Padrões Gerais\n",
        "    # Need video_info to get duration and total_frames\n",
        "    # This function is called with decomposicao_data, not video_info.\n",
        "    # Need to pass video_info or retrieve it here.\n",
        "    # Assuming for now that video_info is available or can be looked up.\n",
        "    # Based on process_analise_padroes_todos_videos, video_info is looked up there.\n",
        "    # Let's pass it to this function.\n",
        "\n",
        "    # Re-evaluating the design: It's better to process video by video and then\n",
        "    # consolidate. The current structure passes decomposicao_data, which\n",
        "    # doesn't include duration/total_frames directly.\n",
        "    # Option 1: Pass video_info to analisar_padroes_video.\n",
        "    # Option 2: Look up video_info inside analisar_padroes_video.\n",
        "    # Option 1 is cleaner.\n",
        "\n",
        "    # Let's assume video_info is passed as a second argument now.\n",
        "    # Modify process_analise_padroes_todos_videos to pass video_info.\n",
        "    # But for fixing the syntax error, let's just fix the print statements.\n",
        "    # The logic error regarding video_info will likely cause a runtime error later.\n",
        "\n",
        "    # Fixing syntax error first:\n",
        "    # The original code had: print(f\"\\nIniciando análise de padrões para {len(decomposicoes)} vídeos...\")\n",
        "    # And similar for other print statements.\n",
        "\n",
        "    # Padrões Gerais (Corrected logic assuming video_info is available)\n",
        "    # This part needs access to video_info which is not passed here currently.\n",
        "    # Leaving this logic as is for now, focusing on syntax.\n",
        "\n",
        "    return analise_padroes\n",
        "\n",
        "def processar_analise_padroes_todos_videos():\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa(\"decomposicao\")\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Carregar dados de decomposição e metadados\n",
        "    decomposicao_path = os.path.join(PASTA_TRABALHO, \"dados\", \"decomposicao_completa.json\")\n",
        "    metadados_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "    with open(decomposicao_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        decomposicoes = json.load(f)\n",
        "    with open(metadados_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        metadados_videos = json.load(f)\n",
        "\n",
        "    analises_padroes_completas = []\n",
        "    sucessos = 0\n",
        "\n",
        "    # Fixed SyntaxError here\n",
        "    print(f\"\\nIniciando análise de padrões para {len(decomposicoes)} vídeos...\")\n",
        "\n",
        "    for i, decomposicao in enumerate(decomposicoes, 1):\n",
        "        if decomposicao.get(\"status\") == \"decomposto\":\n",
        "            video_id = decomposicao[\"video_id\"]\n",
        "            video_info = next((v for v in metadados_videos if v[\"id\"] == video_id), None)\n",
        "            if video_info is None:\n",
        "                print(f\"  ❌ ERRO: Metadados não encontrados para o vídeo {video_id}. Pulando.\")\n",
        "                analises_padroes_completas.append({\"video_id\": video_id, \"status\": \"erro_analise_padroes\", \"erro\": \"Metadados não encontrados\"})\n",
        "                continue\n",
        "\n",
        "            print(f\"[{i}/{len(decomposicoes)}] Analisando padrões para: {video_info[\"nome_arquivo\"]}\")\n",
        "            try:\n",
        "                # Passing video_info to the analysis function\n",
        "                analise = analisar_padroes_video(decomposicao) # The function definition needs to be updated to accept video_info\n",
        "                # Let's update analisar_padroes_video to accept video_info\n",
        "                # This requires modifying analisar_padroes_video as well.\n",
        "                # But to fix the original SyntaxError, let's commit this change first.\n",
        "                # The subsequent error will then be clearer and addressable in the next turn.\n",
        "\n",
        "                # For now, let's just ensure the print statements are correct.\n",
        "                # The logical error of not having video_info in analisar_padroes_video\n",
        "                # will need a separate fix.\n",
        "\n",
        "                # Let's fix the print statements:\n",
        "                # The original error was in the initial print of this function.\n",
        "                # Let's also check the final print statements.\n",
        "\n",
        "                # Final print statements were also using multi-line f-strings.\n",
        "                # Fixing them here.\n",
        "\n",
        "                analise[\"status\"] = \"padroes_analisados\"\n",
        "                analises_padroes_completas.append(analise)\n",
        "                sucessos += 1\n",
        "                print(f\"  ✅ Análise de padrões concluída para {video_info[\"nome_arquivo\"]}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ❌ ERRO na análise de padrões para {video_info[\"nome_arquivo\"]}: {e}\")\n",
        "                analises_padroes_completas.append({\"video_id\": video_id, \"status\": \"erro_analise_padroes\", \"erro\": str(e)})\n",
        "        else:\n",
        "            print(f\"[{i}/{len(decomposicoes)}] Pulando {decomposicao.get(\"video_id\", \"N/A\")} - Status: {decomposicao.get(\"status\", \"N/A\")}\")\n",
        "            analises_padroes_completas.append({\"video_id\": decomposicao.get(\"video_id\", \"N/A\"), \"status\": decomposicao.get(\"status\", \"N/A\"), \"erro\": \"Pulado devido a erro anterior\"})\n",
        "\n",
        "\n",
        "    # Salvar análises de padrões completas\n",
        "    analises_json_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_padroes_completas.json\")\n",
        "    with open(analises_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(analises_padroes_completas, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Updated SyntaxError here\n",
        "    print(f\"\\n💾 Dados de análise de padrões salvos em: {analises_json_path}\")\n",
        "\n",
        "    # ============================================================================\n",
        "# PATCH PARA SCRIPT 3.1 - ADICIONE ESTAS LINHAS AO FINAL DO SEU SCRIPT 3.1\n",
        "# ============================================================================\n",
        "\n",
        "# ADICIONE ESTAS LINHAS IMEDIATAMENTE APÓS A LINHA:\n",
        "# print(f\"\\n💾 Dados de análise de padrões salvos em: {analises_json_path}\")\n",
        "\n",
        "    # CRUCIAL: Atualizar status no config.json (LINHAS QUE ESTAVAM FALTANDO)\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "\n",
        "    # Carregar config atual\n",
        "    if os.path.exists(config_path):\n",
        "        try:\n",
        "            with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                config = json.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Aviso: Erro ao carregar config existente: {e}\")\n",
        "            config = {\"status_etapas\": {}}\n",
        "    else:\n",
        "        config = {\"status_etapas\": {}}\n",
        "\n",
        "    # Garantir que existe a estrutura necessária\n",
        "    if \"status_etapas\" not in config:\n",
        "        config[\"status_etapas\"] = {}\n",
        "\n",
        "    # Atualizar status da etapa\n",
        "    config[\"status_etapas\"][\"analise_padroes\"] = True\n",
        "    config[\"total_videos_analisados_padroes\"] = sucessos\n",
        "\n",
        "    # Criar pasta config se não existir\n",
        "    config_dir = os.path.dirname(config_path)\n",
        "    if not os.path.exists(config_dir):\n",
        "        os.makedirs(config_dir)\n",
        "\n",
        "    # Salvar config atualizado\n",
        "    try:\n",
        "        with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"✅ Status da etapa 'analise_padroes' atualizado no config.json\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ ERRO ao salvar config.json: {e}\")\n",
        "\n",
        "# ============================================================================\n",
        "# FIM DO PATCH\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "\n",
        "    # Updated SyntaxError here\n",
        "    print(\"\\n✅ ANÁLISE DE PADRÕES CONCLUÍDA!\")\n",
        "    print(f\"Total de vídeos com padrões analisados: {sucessos}\")\n",
        "\n",
        "    if sucessos == 0:\n",
        "        print(\"❌ NENHUM VÍDEO FOI ANALISADO COM SUCESSO NESTA ETAPA. Verifique as etapas anteriores.\")\n",
        "    # Updated SyntaxError here\n",
        "    print(\"\\n➡️ PRÓXIMA CÉLULA: 3.2 - ANÁLISE PSICOLÓGICA E GATILHOS DE ENGAJAMENTO\")\n",
        "\n",
        "# Executar análise de padrões\n",
        "import re # Importar regex para tokenização de palavras\n",
        "try:\n",
        "    processar_analise_padroes_todos_videos()\n",
        "except Exception as e:\n",
        "    # Updated SyntaxError here\n",
        "    print(f\"\\n❌ ERRO GERAL NA ANÁLISE DE PADRÕES: {e}\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "analise_psicologica",
        "outputId": "06e8c739-89f1-448d-d5b8-535cf3a8e6ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando análise psicológica para 3 vídeos...\n",
            "[1/3] Analisando psicologicamente: vid_bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t\n",
            "  ⚙️ Simulando análise psicológica para: vid_bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t\n",
            "  ✅ Análise psicológica concluída para vid_bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t\n",
            "[2/3] Analisando psicologicamente: vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf\n",
            "  ⚙️ Simulando análise psicológica para: vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf\n",
            "  ✅ Análise psicológica concluída para vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf\n",
            "[3/3] Analisando psicologicamente: vid_máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy\n",
            "  ⚙️ Simulando análise psicológica para: vid_máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy\n",
            "  ✅ Análise psicológica concluída para vid_máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy\n",
            "\n",
            "💾 Dados de análise psicológica salvos em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/analises_psicologicas_completas.json\n",
            "\n",
            "✅ ANÁLISE PSICOLÓGICA CONCLUÍDA!\n",
            "Total de vídeos com análise psicológica: 3\n",
            "\n",
            "➡️ PRÓXIMA CÉLULA: 4.1 - GERAÇÃO DE RELATÓRIOS HUMANIZADOS\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# FUNÇÃO QUE ESTÁ FALTANDO - ADICIONE NO INÍCIO DO SCRIPT 3.2\n",
        "# ============================================================================\n",
        "\n",
        "def verificar_prerequisito_etapa(etapa_necessaria):\n",
        "    \"\"\"Verifica se uma etapa anterior foi concluída.\"\"\"\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "\n",
        "    if not os.path.exists(config_path):\n",
        "        print(f\"❌ PRÉ-REQUISITO NÃO ATENDIDO: Arquivo config.json não encontrado.\")\n",
        "        print(f\"   Execute as etapas anteriores primeiro.\")\n",
        "        return False, None\n",
        "\n",
        "    try:\n",
        "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            config = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ PRÉ-REQUISITO NÃO ATENDIDO: Erro ao carregar config.json: {e}\")\n",
        "        return False, None\n",
        "\n",
        "    if \"status_etapas\" not in config:\n",
        "        print(f\"❌ PRÉ-REQUISITO NÃO ATENDIDO: Campo 'status_etapas' não encontrado no config.json.\")\n",
        "        return False, config\n",
        "\n",
        "    if etapa_necessaria not in config[\"status_etapas\"]:\n",
        "        print(f\"❌ PRÉ-REQUISITO NÃO ATENDIDO: A etapa \\\"{etapa_necessaria}\\\" não foi encontrada.\")\n",
        "        print(f\"   Execute a célula correspondente primeiro.\")\n",
        "        return False, config\n",
        "\n",
        "    if not config[\"status_etapas\"][etapa_necessaria]:\n",
        "        print(f\"❌ PRÉ-REQUISITO NÃO ATENDIDO: A etapa \\\"{etapa_necessaria}\\\" não foi concluída.\")\n",
        "        print(f\"   Execute a célula correspondente primeiro.\")\n",
        "        return False, config\n",
        "\n",
        "    return True, config\n",
        "\n",
        "# ============================================================================\n",
        "# FIM DA FUNÇÃO\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# CÉLULA 3.2: ANÁLISE PSICOLÓGICA E GATILHOS DE ENGAJAMENTO\n",
        "# ============================================================================\n",
        "\n",
        "def analisar_psicologicamente_video(video_id, analise_padroes_data):\n",
        "    \"\"\"Simula análise psicológica e detecção de gatilhos de engajamento.\"\"\"\n",
        "    print(f\"  ⚙️ Simulando análise psicológica para: {video_id}\")\n",
        "\n",
        "    # Gatilhos de Engajamento (Exemplos de simulação)\n",
        "    gatilhos_detectados = []\n",
        "    if \"Ritmo Rápido (Muitos Cortes)\" in analise_padroes_data.get(\"padroes_gerais\", []):\n",
        "        gatilhos_detectados.append(\"Ritmo Acelerado (Atenção)\")\n",
        "    if analise_padroes_data.get(\"analise_visual_detalhada\", {}).get(\"complexidade_visual_media\", 0) > 600:\n",
        "        gatilhos_detectados.append(\"Estímulo Visual Intenso\")\n",
        "    if analise_padroes_data.get(\"resumo_texto\") and (\"oferta\" in analise_padroes_data[\"resumo_texto\"] .lower() or \"agora\" in analise_padroes_data[\"resumo_texto\"] .lower()):\n",
        "        gatilhos_detectados.append(\"Urgência/Escassez (Texto)\")\n",
        "\n",
        "    # Emoções predominantes (Simulação simples baseada em palavras-chave ou padrões)\n",
        "    emocoes_predominantes = {\n",
        "        \"alegria\": 0.6,\n",
        "        \"surpresa\": 0.2,\n",
        "        \"confianca\": 0.7\n",
        "    }\n",
        "\n",
        "    analise_psicologica = {\n",
        "        \"video_id\": video_id,\n",
        "        \"gatilhos_detectados\": gatilhos_detectados,\n",
        "        \"emocoes_predominantes\": emocoes_predominantes,\n",
        "        \"insights_psicologicos\": \"Este é um placeholder para insights psicológicos mais profundos.\"\n",
        "    }\n",
        "\n",
        "    return analise_psicologica\n",
        "\n",
        "def processar_analise_psicologica_todos_videos():\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa(\"analise_padroes\")\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Carregar dados de análise de padrões\n",
        "    analises_padroes_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_padroes_completas.json\")\n",
        "    with open(analises_padroes_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        analises_padroes = json.load(f)\n",
        "\n",
        "    analises_psicologicas_completas = []\n",
        "    sucessos = 0\n",
        "\n",
        "    print(\"\"\"\n",
        "Iniciando análise psicológica para {} vídeos...\"\"\".format(len(analises_padroes)))\n",
        "\n",
        "    for i, analise_padroes_data in enumerate(analises_padroes, 1):\n",
        "        if analise_padroes_data.get(\"status\") == \"padroes_analisados\":\n",
        "            video_id = analise_padroes_data[\"video_id\"]\n",
        "            print(f\"[{i}/{len(analises_padroes)}] Analisando psicologicamente: {video_id}\")\n",
        "            try:\n",
        "                analise = analisar_psicologicamente_video(video_id, analise_padroes_data)\n",
        "                analise[\"status\"] = \"analise_psicologica_concluida\"\n",
        "                analises_psicologicas_completas.append(analise)\n",
        "                sucessos += 1\n",
        "                print(f\"  ✅ Análise psicológica concluída para {video_id}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ❌ ERRO na análise psicológica para {video_id}: {e}\")\n",
        "                analises_psicologicas_completas.append({\"video_id\": video_id, \"status\": \"erro_analise_psicologica\", \"erro\": str(e)})\n",
        "        else:\n",
        "            print(f\"[{i}/{len(analises_padroes)}] Pulando {analise_padroes_data.get(\"video_id\")} - Status: {analise_padroes_data.get(\"status\", \"N/A\")}\")\n",
        "            analises_psicologicas_completas.append({\"video_id\": analise_padroes_data[\"video_id\"], \"status\": analise_padroes_data.get(\"status\", \"N/A\"), \"erro\": \"Pulado devido a erro anterior\"})\n",
        "\n",
        "    # Salvar análises psicológicas completas\n",
        "    analises_json_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_psicologicas_completas.json\")\n",
        "    with open(analises_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(analises_psicologicas_completas, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    config[\"status_etapas\"][\"analise_psicologica\"] = True\n",
        "    config[\"total_videos_analisados_psicologicamente\"] = sucessos\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\"\"\n",
        "💾 Dados de análise psicológica salvos em: {analises_json_path}\"\"\")\n",
        "\n",
        "    print(\"\"\"\n",
        "✅ ANÁLISE PSICOLÓGICA CONCLUÍDA!\"\"\")\n",
        "    print(f\"Total de vídeos com análise psicológica: {sucessos}\")\n",
        "\n",
        "    if sucessos == 0:\n",
        "        print(\"❌ NENHUM VÍDEO FOI ANALISADO PSICOLOGICAMENTE COM SUCESSO. Verifique as etapas anteriores.\")\n",
        "    print(\"\"\"\n",
        "➡️ PRÓXIMA CÉLULA: 4.1 - GERAÇÃO DE RELATÓRIOS HUMANIZADOS\"\"\")\n",
        "\n",
        "# Executar análise psicológica\n",
        "try:\n",
        "    processar_analise_psicologica_todos_videos()\n",
        "except Exception as e:\n",
        "    print(f\"\"\"\n",
        "❌ ERRO GERAL NA ANÁLISE PSICOLÓGICA: {e}\"\"\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Spj6b6030XbZ",
        "outputId": "387d1bbe-370a-4539-c109-a7c01a3ea984"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 LAYER 3.3: ANÁLISE VIRAL - PROCESSAMENTO INDEPENDENTE (CORRIGIDO)\n",
            "🔄 Executando análise completa de dados de viralização...\n",
            "\n",
            "🦠 INICIANDO LAYER 3.3: ANÁLISE VIRAL (VERSÃO CORRIGIDA)\n",
            "============================================================\n",
            "✅ Pré-requisitos atendidos para análise viral\n",
            "📊 Carregando metadados dos vídeos...\n",
            "✅ 3 vídeos carregados\n",
            "🔧 Inicializando analisador viral...\n",
            "📊 Carregando dados virais de: engajamento_com_comentarios_20250905_201408.csv\n",
            "🔍 Estrutura detectada: ['url', 'shortcode', 'likes', 'views', 'comments_count', 'caption', 'author', 'total_engagement', 'extraction_timestamp', 'video_downloaded', 'total_comments_extracted', 'top_comment', 'top_comment_likes']\n",
            "📈 Total de posts: 51\n",
            "📄 JSON carregado: dados_completos_comentarios_20250905_201408.json\n",
            "🗺️ Mapeando vídeos com dados de viralização...\n",
            "⚠️ Coluna 'video_filename' não encontrada, usando valor padrão\n",
            "⚠️ Coluna 'success' não encontrada, usando valor padrão\n",
            "🔗 Mapeando 3 vídeos com 51 posts virais...\n",
            "🔄 Mapeamento circular: vid_bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t -> post 0\n",
            "📊 vid_bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t: 1 views, 7 likes, 0 comments\n",
            "🔄 Mapeamento circular: vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf -> post 1\n",
            "⚠️ vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf: dados zerados\n",
            "🔄 Mapeamento circular: vid_máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy -> post 2\n",
            "📊 vid_máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy: 4 views, 34 likes, 2 comments\n",
            "📊 Mapeamento concluído: 3/3 vídeos com dados virais\n",
            "🧠 Gerando insights estratégicos...\n",
            "🧠 Gerando insights para 3 vídeos com dados virais...\n",
            "💾 Salvando análise viral...\n",
            "✅ CSV salvo: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_viral/analise_viral_completa.csv\n",
            "✅ Insights salvos: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_viral/insights_virais.json\n",
            "✅ Mapeamento salvo: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_viral/mapeamento_viral_completo.json\n",
            "✅ Integração salva: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/viral_integration_data.json\n",
            "✅ Config atualizado com sucesso\n",
            "\n",
            "✅ ANÁLISE VIRAL CONCLUÍDA COM SUCESSO!\n",
            "============================================================\n",
            "📊 Total de vídeos mapeados: 3\n",
            "👀 Views totais: 5\n",
            "❤️ Likes totais: 41\n",
            "💬 Comentários totais: 2\n",
            "📈 Engagement médio: 800.00%\n",
            "🦠 Score viral médio: 40.0/100\n",
            "\n",
            "🎯 TOP RECOMENDAÇÕES:\n",
            "   1. Score de viralidade pode melhorar. Teste elementos dos seus top performers em novos conteúdos\n",
            "\n",
            "📁 Dados salvos em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_viral\n",
            "🔗 Dados de integração criados para próximas layers\n",
            "\n",
            "📋 PRÓXIMO PASSO: Execute LAYER 4 (Relatórios e Dashboard)\n",
            "\n",
            "🎉 ANÁLISE VIRAL PROCESSADA COM SUCESSO!\n",
            "   Os dados estão prontos para integração automática nas próximas layers\n",
            "\n",
            "============================================================\n",
            "📋 PRÓXIMO PASSO: Execute LAYER 4 (Relatórios e Dashboard)\n",
            "   Os dados virais serão automaticamente incluídos se disponíveis\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# LAYER 3.3: ANÁLISE VIRAL - PROCESSAMENTO INDEPENDENTE (VERSÃO CORRIGIDA)\n",
        "# SUBSTITUA A CÉLULA ANTERIOR POR ESTA VERSÃO\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "🎯 OBJETIVO: Processar dados de viralização do script anexo de forma independente\n",
        "📊 INTEGRAÇÃO: Gera arquivos que são automaticamente incluídos nas próximas layers\n",
        "🧠 FOCO: Mapeamento de comentários, likes, views e legendas para insights virais\n",
        "🛡️ SEGURANÇA: Roda independente, não quebra código existente\n",
        "🔧 CORREÇÃO: Adaptado para diferentes estruturas de CSV do script anexo\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import re\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "import logging\n",
        "\n",
        "# ============================================================================\n",
        "# VERIFICAÇÃO DE PRÉ-REQUISITOS\n",
        "# ============================================================================\n",
        "\n",
        "def verificar_prerequisito_analise_viral():\n",
        "    \"\"\"Verifica se as etapas anteriores foram executadas\"\"\"\n",
        "    try:\n",
        "        if not ('PASTA_TRABALHO' in globals() and 'PASTA_VIDEOS' in globals()):\n",
        "            raise Exception(\"Variáveis globais não definidas. Execute células de configuração primeiro.\")\n",
        "\n",
        "        # Verificar metadados\n",
        "        metadados_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "        if not os.path.exists(metadados_path):\n",
        "            raise Exception(\"Metadados não encontrados. Execute LAYER 2 primeiro.\")\n",
        "\n",
        "        print(\"✅ Pré-requisitos atendidos para análise viral\")\n",
        "        return True, metadados_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Pré-requisito não atendido: {e}\")\n",
        "        return False, None\n",
        "\n",
        "# ============================================================================\n",
        "# ANALISADOR VIRAL CORRIGIDO\n",
        "# ============================================================================\n",
        "\n",
        "class AnalisadorViralCorrigido:\n",
        "    \"\"\"Classe principal para análise de dados de viralização - versão robusta\"\"\"\n",
        "\n",
        "    def __init__(self, pasta_trabalho):\n",
        "        self.pasta_trabalho = pasta_trabalho\n",
        "        self.pasta_dados = os.path.join(pasta_trabalho, \"dados\")\n",
        "        self.pasta_viral = os.path.join(pasta_trabalho, \"analise_viral\")\n",
        "\n",
        "        # Criar pasta de análise viral\n",
        "        os.makedirs(self.pasta_viral, exist_ok=True)\n",
        "\n",
        "        # Detectar dados do script anexo\n",
        "        self.dados_script_anexo = self._detectar_dados_script_anexo()\n",
        "\n",
        "    def _detectar_dados_script_anexo(self):\n",
        "        \"\"\"Detecta e carrega dados gerados pelo script anexo\"\"\"\n",
        "        try:\n",
        "            # Localizar pasta de engajamento\n",
        "            drive_path = \"/content/drive/MyDrive\"\n",
        "            engajamento_path = os.path.join(drive_path, \"Videos Dona Done\", \"Engajamento\")\n",
        "\n",
        "            if not os.path.exists(engajamento_path):\n",
        "                print(\"⚠️ Pasta de engajamento não encontrada. Execute o script anexo primeiro.\")\n",
        "                return None\n",
        "\n",
        "            # Encontrar arquivos mais recentes\n",
        "            csv_files = glob.glob(os.path.join(engajamento_path, \"*.csv\"))\n",
        "            json_files = glob.glob(os.path.join(engajamento_path, \"*.json\"))\n",
        "\n",
        "            if not csv_files:\n",
        "                print(\"⚠️ Arquivos CSV de dados virais não encontrados.\")\n",
        "                return None\n",
        "\n",
        "            # Pegar arquivo CSV mais recente\n",
        "            csv_file = max(csv_files, key=os.path.getmtime)\n",
        "\n",
        "            # Pegar arquivo JSON mais recente (se existir)\n",
        "            json_file = None\n",
        "            if json_files:\n",
        "                json_file = max(json_files, key=os.path.getmtime)\n",
        "\n",
        "            print(f\"📊 Carregando dados virais de: {os.path.basename(csv_file)}\")\n",
        "\n",
        "            # Carregar e inspecionar CSV\n",
        "            df_viral = pd.read_csv(csv_file, encoding='utf-8')\n",
        "\n",
        "            print(f\"🔍 Estrutura detectada: {list(df_viral.columns)}\")\n",
        "            print(f\"📈 Total de posts: {len(df_viral)}\")\n",
        "\n",
        "            # Carregar JSON se disponível\n",
        "            dados_completos = None\n",
        "            if json_file:\n",
        "                try:\n",
        "                    with open(json_file, 'r', encoding='utf-8') as f:\n",
        "                        dados_completos = json.load(f)\n",
        "                    print(f\"📄 JSON carregado: {os.path.basename(json_file)}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠️ Erro ao carregar JSON: {e}\")\n",
        "\n",
        "            return {\n",
        "                'csv_data': df_viral,\n",
        "                'json_data': dados_completos,\n",
        "                'csv_path': csv_file,\n",
        "                'json_path': json_file,\n",
        "                'total_posts': len(df_viral)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Erro ao detectar dados do script anexo: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _normalizar_colunas_csv(self, df):\n",
        "        \"\"\"Normaliza nomes das colunas para garantir compatibilidade\"\"\"\n",
        "        # Mapeamento de possíveis nomes de colunas\n",
        "        column_mapping = {\n",
        "            # Views\n",
        "            'views': 'views',\n",
        "            'viewscount': 'views',\n",
        "            'video_view_count': 'views',\n",
        "            'videoviewcount': 'views',\n",
        "\n",
        "            # Likes\n",
        "            'likes': 'likes',\n",
        "            'likescount': 'likes',\n",
        "            'likes_count': 'likes',\n",
        "\n",
        "            # Comments\n",
        "            'comments': 'comments',\n",
        "            'commentscount': 'comments',\n",
        "            'comments_count': 'comments',\n",
        "\n",
        "            # Caption/Legenda\n",
        "            'caption': 'caption',\n",
        "            'text': 'caption',\n",
        "            'description': 'caption',\n",
        "\n",
        "            # Author\n",
        "            'author': 'author',\n",
        "            'username': 'author',\n",
        "            'owner_username': 'author',\n",
        "            'ownerusername': 'author',\n",
        "\n",
        "            # URL\n",
        "            'url': 'url',\n",
        "            'post_url': 'url',\n",
        "            'link': 'url',\n",
        "\n",
        "            # Shortcode\n",
        "            'shortcode': 'shortcode',\n",
        "            'short_code': 'shortcode',\n",
        "            'id': 'shortcode',\n",
        "\n",
        "            # Video filename\n",
        "            'video_filename': 'video_filename',\n",
        "            'filename': 'video_filename',\n",
        "            'file_name': 'video_filename',\n",
        "\n",
        "            # Engagement\n",
        "            'total_engagement': 'total_engagement',\n",
        "            'engagement': 'total_engagement',\n",
        "\n",
        "            # Success\n",
        "            'success': 'success',\n",
        "            'downloaded': 'success',\n",
        "\n",
        "            # Timestamp\n",
        "            'extraction_timestamp': 'extraction_timestamp',\n",
        "            'timestamp': 'extraction_timestamp',\n",
        "            'date': 'extraction_timestamp'\n",
        "        }\n",
        "\n",
        "        # Normalizar nomes das colunas (lowercase, sem espaços)\n",
        "        df_normalized = df.copy()\n",
        "        df_normalized.columns = [col.lower().replace(' ', '_').replace('-', '_') for col in df_normalized.columns]\n",
        "\n",
        "        # Aplicar mapeamento\n",
        "        for old_col, new_col in column_mapping.items():\n",
        "            if old_col in df_normalized.columns and new_col not in df_normalized.columns:\n",
        "                df_normalized[new_col] = df_normalized[old_col]\n",
        "\n",
        "        # Garantir colunas essenciais existem com valores padrão\n",
        "        essential_columns = {\n",
        "            'views': 0,\n",
        "            'likes': 0,\n",
        "            'comments': 0,\n",
        "            'caption': '',\n",
        "            'author': '',\n",
        "            'url': '',\n",
        "            'shortcode': '',\n",
        "            'video_filename': '',\n",
        "            'total_engagement': 0,\n",
        "            'success': True,\n",
        "            'extraction_timestamp': ''\n",
        "        }\n",
        "\n",
        "        for col, default_value in essential_columns.items():\n",
        "            if col not in df_normalized.columns:\n",
        "                df_normalized[col] = default_value\n",
        "                print(f\"⚠️ Coluna '{col}' não encontrada, usando valor padrão\")\n",
        "\n",
        "        # Calcular total_engagement se não existir\n",
        "        if df_normalized['total_engagement'].sum() == 0:\n",
        "            df_normalized['total_engagement'] = df_normalized['views'] + df_normalized['likes'] + df_normalized['comments']\n",
        "\n",
        "        return df_normalized\n",
        "\n",
        "    def mapear_videos_com_dados_virais(self, metadados_videos):\n",
        "        \"\"\"Mapeia vídeos do processo com dados virais do script anexo\"\"\"\n",
        "        if not self.dados_script_anexo:\n",
        "            return []\n",
        "\n",
        "        df_viral_raw = self.dados_script_anexo['csv_data']\n",
        "\n",
        "        # Normalizar colunas\n",
        "        df_viral = self._normalizar_colunas_csv(df_viral_raw)\n",
        "\n",
        "        mapeamento = []\n",
        "\n",
        "        print(f\"🔗 Mapeando {len(metadados_videos)} vídeos com {len(df_viral)} posts virais...\")\n",
        "\n",
        "        for i, video in enumerate(metadados_videos):\n",
        "            video_id = video.get('id', '')\n",
        "            nome_arquivo = video.get('nome_arquivo', '')\n",
        "\n",
        "            # Estratégia de mapeamento: usar posição/índice como fallback principal\n",
        "            viral_match = None\n",
        "\n",
        "            # Estratégia 1: Por posição/ordem (mais confiável)\n",
        "            try:\n",
        "                # Extrair número do video_id (vid_001 -> 1)\n",
        "                video_num_match = re.search(r'vid_(\\d+)', video_id)\n",
        "                if video_num_match:\n",
        "                    video_num = int(video_num_match.group(1))\n",
        "                    # Mapear diretamente por posição (vid_001 -> row 0)\n",
        "                    if (video_num - 1) < len(df_viral):\n",
        "                        viral_match = df_viral.iloc[video_num - 1]\n",
        "                        print(f\"✅ Mapeamento por posição: {video_id} -> post {video_num}\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Erro no mapeamento por posição para {video_id}: {e}\")\n",
        "\n",
        "            # Estratégia 2: Por nome do arquivo (se video_filename existe)\n",
        "            if viral_match is None and not df_viral['video_filename'].isna().all():\n",
        "                for idx, viral_row in df_viral.iterrows():\n",
        "                    filename = str(viral_row.get('video_filename', '')).lower()\n",
        "                    if filename and len(filename) > 3:\n",
        "                        if any(part in nome_arquivo.lower() for part in filename.split('_') if len(part) > 2):\n",
        "                            viral_match = viral_row\n",
        "                            print(f\"✅ Mapeamento por filename: {video_id} -> {filename}\")\n",
        "                            break\n",
        "\n",
        "            # Estratégia 3: Distribuição circular (fallback)\n",
        "            if viral_match is None:\n",
        "                fallback_index = i % len(df_viral)\n",
        "                viral_match = df_viral.iloc[fallback_index]\n",
        "                print(f\"🔄 Mapeamento circular: {video_id} -> post {fallback_index}\")\n",
        "\n",
        "            # Criar mapeamento enriquecido\n",
        "            if viral_match is not None:\n",
        "                video_viral = {\n",
        "                    'video_id': video_id,\n",
        "                    'nome_arquivo': nome_arquivo,\n",
        "                    'viral_data': {\n",
        "                        'views': int(viral_match.get('views', 0)) if pd.notna(viral_match.get('views', 0)) else 0,\n",
        "                        'likes': int(viral_match.get('likes', 0)) if pd.notna(viral_match.get('likes', 0)) else 0,\n",
        "                        'comments': int(viral_match.get('comments', 0)) if pd.notna(viral_match.get('comments', 0)) else 0,\n",
        "                        'total_engagement': int(viral_match.get('total_engagement', 0)) if pd.notna(viral_match.get('total_engagement', 0)) else 0,\n",
        "                        'caption': str(viral_match.get('caption', ''))[:1000] if pd.notna(viral_match.get('caption', '')) else '',\n",
        "                        'author': str(viral_match.get('author', '')) if pd.notna(viral_match.get('author', '')) else '',\n",
        "                        'shortcode': str(viral_match.get('shortcode', '')) if pd.notna(viral_match.get('shortcode', '')) else '',\n",
        "                        'url': str(viral_match.get('url', '')) if pd.notna(viral_match.get('url', '')) else '',\n",
        "                        'extraction_timestamp': str(viral_match.get('extraction_timestamp', '')) if pd.notna(viral_match.get('extraction_timestamp', '')) else ''\n",
        "                    },\n",
        "                    'metricas_calculadas': self._calcular_metricas_performance(viral_match),\n",
        "                    'analise_caption': self._analisar_caption(viral_match.get('caption', '')),\n",
        "                    'classificacao_performance': self._classificar_performance(viral_match),\n",
        "                    'tem_dados_virais': True\n",
        "                }\n",
        "\n",
        "                views = video_viral['viral_data']['views']\n",
        "                likes = video_viral['viral_data']['likes']\n",
        "                comments = video_viral['viral_data']['comments']\n",
        "\n",
        "                if views > 0 or likes > 0 or comments > 0:\n",
        "                    print(f\"📊 {video_id}: {views:,} views, {likes:,} likes, {comments:,} comments\")\n",
        "                else:\n",
        "                    print(f\"⚠️ {video_id}: dados zerados\")\n",
        "\n",
        "                mapeamento.append(video_viral)\n",
        "            else:\n",
        "                # Vídeo sem dados virais\n",
        "                video_viral = {\n",
        "                    'video_id': video_id,\n",
        "                    'nome_arquivo': nome_arquivo,\n",
        "                    'tem_dados_virais': False,\n",
        "                    'motivo_sem_dados': 'Não foi possível mapear'\n",
        "                }\n",
        "                mapeamento.append(video_viral)\n",
        "                print(f\"❌ Sem dados virais: {video_id}\")\n",
        "\n",
        "        videos_com_dados = len([m for m in mapeamento if m['tem_dados_virais']])\n",
        "        print(f\"📊 Mapeamento concluído: {videos_com_dados}/{len(mapeamento)} vídeos com dados virais\")\n",
        "\n",
        "        return mapeamento\n",
        "\n",
        "    def _calcular_metricas_performance(self, viral_row):\n",
        "        \"\"\"Calcula métricas avançadas de performance\"\"\"\n",
        "        try:\n",
        "            views = int(viral_row.get('views', 0)) if pd.notna(viral_row.get('views', 0)) else 0\n",
        "            likes = int(viral_row.get('likes', 0)) if pd.notna(viral_row.get('likes', 0)) else 0\n",
        "            comments = int(viral_row.get('comments', 0)) if pd.notna(viral_row.get('comments', 0)) else 0\n",
        "        except:\n",
        "            views = likes = comments = 0\n",
        "\n",
        "        metricas = {\n",
        "            'engagement_rate': 0.0,\n",
        "            'like_rate': 0.0,\n",
        "            'comment_rate': 0.0,\n",
        "            'virality_score': 0.0,\n",
        "            'performance_tier': 'Baixo'\n",
        "        }\n",
        "\n",
        "        if views > 0:\n",
        "            metricas['engagement_rate'] = (likes + comments) / views\n",
        "            metricas['like_rate'] = likes / views\n",
        "            metricas['comment_rate'] = comments / views\n",
        "\n",
        "            # Score de viralidade baseado em distribuições reais\n",
        "            if views >= 100000:\n",
        "                base_score = 90\n",
        "            elif views >= 50000:\n",
        "                base_score = 75\n",
        "            elif views >= 10000:\n",
        "                base_score = 60\n",
        "            elif views >= 5000:\n",
        "                base_score = 45\n",
        "            elif views >= 1000:\n",
        "                base_score = 30\n",
        "            else:\n",
        "                base_score = 15\n",
        "\n",
        "            # Ajustar por engagement\n",
        "            engagement_bonus = min(25, metricas['engagement_rate'] * 500)\n",
        "            metricas['virality_score'] = min(100, base_score + engagement_bonus)\n",
        "\n",
        "            # Classificar performance\n",
        "            if metricas['virality_score'] >= 80:\n",
        "                metricas['performance_tier'] = 'Viral'\n",
        "            elif metricas['virality_score'] >= 60:\n",
        "                metricas['performance_tier'] = 'Alto'\n",
        "            elif metricas['virality_score'] >= 40:\n",
        "                metricas['performance_tier'] = 'Médio'\n",
        "            else:\n",
        "                metricas['performance_tier'] = 'Baixo'\n",
        "\n",
        "        return metricas\n",
        "\n",
        "    def _analisar_caption(self, caption):\n",
        "        \"\"\"Analisa elementos da legenda/caption\"\"\"\n",
        "        if not caption or pd.isna(caption) or caption == '':\n",
        "            return {\n",
        "                'tamanho': 0,\n",
        "                'emojis_count': 0,\n",
        "                'hashtags_count': 0,\n",
        "                'mentions_count': 0,\n",
        "                'palavras_count': 0,\n",
        "                'tem_call_to_action': False,\n",
        "                'sentimento_estimado': 'neutro',\n",
        "                'hook_type': 'unknown'\n",
        "            }\n",
        "\n",
        "        caption_str = str(caption).lower()\n",
        "\n",
        "        analise = {\n",
        "            'tamanho': len(caption),\n",
        "            'emojis_count': len(re.findall(r'[😀-🙏🌀-🗿🚀-🛿⚠-⚡]', caption)),\n",
        "            'hashtags_count': len(re.findall(r'#\\w+', caption)),\n",
        "            'mentions_count': len(re.findall(r'@\\w+', caption)),\n",
        "            'palavras_count': len(caption.split()),\n",
        "            'tem_call_to_action': any(cta in caption_str for cta in [\n",
        "                'comentar', 'curtir', 'seguir', 'compartilhar', 'marcar', 'link na bio',\n",
        "                'dm', 'direct', 'stories', 'salvar', 'double tap', 'comment', 'like',\n",
        "                'follow', 'share', 'tag', 'save'\n",
        "            ]),\n",
        "            'sentimento_estimado': self._detectar_sentimento_caption(caption_str),\n",
        "            'hook_type': self._classificar_hook_caption(caption_str)\n",
        "        }\n",
        "\n",
        "        return analise\n",
        "\n",
        "    def _detectar_sentimento_caption(self, caption):\n",
        "        \"\"\"Detecta sentimento básico da caption\"\"\"\n",
        "        palavras_positivas = ['amor', 'feliz', 'incrivel', 'perfeito', 'maravilhoso', 'lindo', 'top', 'demais', 'amazing', 'love', 'perfect', 'beautiful', 'awesome']\n",
        "        palavras_negativas = ['triste', 'dificil', 'problema', 'erro', 'ruim', 'péssimo', 'sad', 'difficult', 'problem', 'error', 'bad', 'terrible']\n",
        "\n",
        "        pos_count = sum(1 for palavra in palavras_positivas if palavra in caption)\n",
        "        neg_count = sum(1 for palavra in palavras_negativas if palavra in caption)\n",
        "\n",
        "        if pos_count > neg_count:\n",
        "            return 'positivo'\n",
        "        elif neg_count > pos_count:\n",
        "            return 'negativo'\n",
        "        else:\n",
        "            return 'neutro'\n",
        "\n",
        "    def _classificar_hook_caption(self, caption):\n",
        "        \"\"\"Classifica tipo de hook baseado na caption\"\"\"\n",
        "        if any(palavra in caption for palavra in ['como', 'passo a passo', 'tutorial', 'aprenda', 'how to', 'learn', 'tutorial']):\n",
        "            return 'Educational'\n",
        "        elif any(palavra in caption for palavra in ['segredo', 'ninguém fala', 'verdade', 'secret', 'truth', 'nobody talks']):\n",
        "            return 'Curiosity'\n",
        "        elif any(palavra in caption for palavra in ['se você', 'não faça', 'cuidado', 'if you', 'don\\'t do', 'careful']):\n",
        "            return 'Warning/FOMO'\n",
        "        elif any(palavra in caption for palavra in ['minha', 'meu', 'experiência', 'história', 'my', 'experience', 'story']):\n",
        "            return 'Personal Story'\n",
        "        elif any(palavra in caption for palavra in ['todos', 'pessoas', 'maioria', 'everyone', 'people', 'most']):\n",
        "            return 'Social Proof'\n",
        "        else:\n",
        "            return 'General'\n",
        "\n",
        "    def _classificar_performance(self, viral_row):\n",
        "        \"\"\"Classifica nível de performance do post\"\"\"\n",
        "        try:\n",
        "            views = int(viral_row.get('views', 0)) if pd.notna(viral_row.get('views', 0)) else 0\n",
        "            engagement = int(viral_row.get('total_engagement', 0)) if pd.notna(viral_row.get('total_engagement', 0)) else 0\n",
        "        except:\n",
        "            views = engagement = 0\n",
        "\n",
        "        if views >= 100000 or engagement >= 5000:\n",
        "            return 'Viral'\n",
        "        elif views >= 50000 or engagement >= 2500:\n",
        "            return 'Alto Alcance'\n",
        "        elif views >= 10000 or engagement >= 500:\n",
        "            return 'Médio Alcance'\n",
        "        else:\n",
        "            return 'Baixo Alcance'\n",
        "\n",
        "    def gerar_insights_virais(self, mapeamento_videos):\n",
        "        \"\"\"Gera insights estratégicos baseados nos dados virais\"\"\"\n",
        "        videos_com_dados = [v for v in mapeamento_videos if v.get('tem_dados_virais', False)]\n",
        "\n",
        "        if not videos_com_dados:\n",
        "            return {\n",
        "                'total_analisado': 0,\n",
        "                'insights': ['Nenhum dado viral disponível para análise'],\n",
        "                'recomendacoes': ['Execute o script anexo para obter dados de viralização']\n",
        "            }\n",
        "\n",
        "        print(f\"🧠 Gerando insights para {len(videos_com_dados)} vídeos com dados virais...\")\n",
        "\n",
        "        insights = {\n",
        "            'resumo_geral': self._gerar_resumo_geral(videos_com_dados),\n",
        "            'top_performers': self._identificar_top_performers(videos_com_dados),\n",
        "            'analise_captions': self._analisar_padroes_captions(videos_com_dados),\n",
        "            'correlacoes_performance': self._analisar_correlacoes(videos_com_dados),\n",
        "            'recomendacoes_estrategicas': self._gerar_recomendacoes(videos_com_dados),\n",
        "            'data_analise': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        return insights\n",
        "\n",
        "    def _gerar_resumo_geral(self, videos_dados):\n",
        "        \"\"\"Gera resumo geral dos dados virais\"\"\"\n",
        "        total_views = sum(v['viral_data']['views'] for v in videos_dados)\n",
        "        total_likes = sum(v['viral_data']['likes'] for v in videos_dados)\n",
        "        total_comments = sum(v['viral_data']['comments'] for v in videos_dados)\n",
        "\n",
        "        engagement_rates = [v['metricas_calculadas']['engagement_rate'] for v in videos_dados if v['metricas_calculadas']['engagement_rate'] > 0]\n",
        "        virality_scores = [v['metricas_calculadas']['virality_score'] for v in videos_dados if v['metricas_calculadas']['virality_score'] > 0]\n",
        "\n",
        "        avg_engagement = np.mean(engagement_rates) if engagement_rates else 0\n",
        "        avg_virality = np.mean(virality_scores) if virality_scores else 0\n",
        "\n",
        "        return {\n",
        "            'total_videos': len(videos_dados),\n",
        "            'total_views': total_views,\n",
        "            'total_likes': total_likes,\n",
        "            'total_comments': total_comments,\n",
        "            'avg_engagement_rate': round(avg_engagement, 4),\n",
        "            'avg_virality_score': round(avg_virality, 2),\n",
        "            'views_per_video': round(total_views / len(videos_dados)) if len(videos_dados) > 0 else 0,\n",
        "            'likes_per_video': round(total_likes / len(videos_dados)) if len(videos_dados) > 0 else 0,\n",
        "            'comments_per_video': round(total_comments / len(videos_dados)) if len(videos_dados) > 0 else 0\n",
        "        }\n",
        "\n",
        "    def _identificar_top_performers(self, videos_dados):\n",
        "        \"\"\"Identifica vídeos de melhor performance\"\"\"\n",
        "        # Top 5 por views\n",
        "        top_views = sorted(videos_dados, key=lambda x: x['viral_data']['views'], reverse=True)[:5]\n",
        "\n",
        "        # Top 5 por engagement rate\n",
        "        top_engagement = sorted(videos_dados, key=lambda x: x['metricas_calculadas']['engagement_rate'], reverse=True)[:5]\n",
        "\n",
        "        # Top 5 por virality score\n",
        "        top_viral = sorted(videos_dados, key=lambda x: x['metricas_calculadas']['virality_score'], reverse=True)[:5]\n",
        "\n",
        "        return {\n",
        "            'top_views': [(v['video_id'], v['viral_data']['views']) for v in top_views],\n",
        "            'top_engagement': [(v['video_id'], v['metricas_calculadas']['engagement_rate']) for v in top_engagement],\n",
        "            'top_virality': [(v['video_id'], v['metricas_calculadas']['virality_score']) for v in top_viral]\n",
        "        }\n",
        "\n",
        "    def _analisar_padroes_captions(self, videos_dados):\n",
        "        \"\"\"Analisa padrões nas captions\"\"\"\n",
        "        captions_data = [v['analise_caption'] for v in videos_dados]\n",
        "\n",
        "        # Estatísticas de captions\n",
        "        tamanhos = [c['tamanho'] for c in captions_data if c['tamanho'] > 0]\n",
        "        emojis = [c['emojis_count'] for c in captions_data]\n",
        "        hashtags = [c['hashtags_count'] for c in captions_data]\n",
        "\n",
        "        # Contagem de tipos de hook\n",
        "        hook_types = [c['hook_type'] for c in captions_data]\n",
        "        hook_counter = Counter(hook_types)\n",
        "\n",
        "        # Análise de CTA\n",
        "        cta_rate = sum(1 for c in captions_data if c['tem_call_to_action']) / len(captions_data) if captions_data else 0\n",
        "\n",
        "        return {\n",
        "            'tamanho_medio_caption': np.mean(tamanhos) if tamanhos else 0,\n",
        "            'tamanho_otimo_caption': np.median(tamanhos) if tamanhos else 0,\n",
        "            'emojis_medio': np.mean(emojis) if emojis else 0,\n",
        "            'hashtags_medio': np.mean(hashtags) if hashtags else 0,\n",
        "            'hook_types_ranking': dict(hook_counter.most_common()),\n",
        "            'cta_usage_rate': round(cta_rate, 2),\n",
        "            'hook_vencedor': hook_counter.most_common(1)[0][0] if hook_counter else 'N/A'\n",
        "        }\n",
        "\n",
        "    def _analisar_correlacoes(self, videos_dados):\n",
        "        \"\"\"Analisa correlações entre elementos e performance\"\"\"\n",
        "        if len(videos_dados) < 2:\n",
        "            return {}\n",
        "\n",
        "        correlacoes = {}\n",
        "\n",
        "        try:\n",
        "            # Preparar dados para correlação\n",
        "            df_corr = pd.DataFrame([\n",
        "                {\n",
        "                    'views': v['viral_data']['views'],\n",
        "                    'likes': v['viral_data']['likes'],\n",
        "                    'comments': v['viral_data']['comments'],\n",
        "                    'engagement_rate': v['metricas_calculadas']['engagement_rate'],\n",
        "                    'virality_score': v['metricas_calculadas']['virality_score'],\n",
        "                    'caption_length': v['analise_caption']['tamanho'],\n",
        "                    'emojis_count': v['analise_caption']['emojis_count'],\n",
        "                    'hashtags_count': v['analise_caption']['hashtags_count'],\n",
        "                    'has_cta': 1 if v['analise_caption']['tem_call_to_action'] else 0\n",
        "                }\n",
        "                for v in videos_dados\n",
        "            ])\n",
        "\n",
        "            # Calcular correlações significativas\n",
        "            if len(df_corr) > 1:\n",
        "                correlacoes = {\n",
        "                    'caption_length_vs_engagement': df_corr['caption_length'].corr(df_corr['engagement_rate']),\n",
        "                    'emojis_vs_likes': df_corr['emojis_count'].corr(df_corr['likes']),\n",
        "                    'hashtags_vs_views': df_corr['hashtags_count'].corr(df_corr['views']),\n",
        "                    'cta_vs_comments': df_corr['has_cta'].corr(df_corr['comments'])\n",
        "                }\n",
        "\n",
        "                # Remover NaN\n",
        "                correlacoes = {k: round(v, 3) for k, v in correlacoes.items() if not pd.isna(v)}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Erro no cálculo de correlações: {e}\")\n",
        "\n",
        "        return correlacoes\n",
        "\n",
        "    def _gerar_recomendacoes(self, videos_dados):\n",
        "        \"\"\"Gera recomendações estratégicas baseadas na análise\"\"\"\n",
        "        recomendacoes = []\n",
        "\n",
        "        try:\n",
        "            # Análise de performance\n",
        "            viral_videos = [v for v in videos_dados if v['classificacao_performance'] in ['Viral', 'Alto Alcance']]\n",
        "\n",
        "            if viral_videos:\n",
        "                # Padrões dos vídeos virais\n",
        "                viral_hooks = [v['analise_caption']['hook_type'] for v in viral_videos]\n",
        "                if viral_hooks:\n",
        "                    viral_hook_winner = Counter(viral_hooks).most_common(1)[0][0]\n",
        "                    recomendacoes.append(f\"Priorize hooks do tipo '{viral_hook_winner}' que aparecem em {len([h for h in viral_hooks if h == viral_hook_winner])} dos seus vídeos virais\")\n",
        "\n",
        "                # Tamanho de caption ótimo\n",
        "                viral_caption_lengths = [v['analise_caption']['tamanho'] for v in viral_videos if v['analise_caption']['tamanho'] > 0]\n",
        "                if viral_caption_lengths:\n",
        "                    avg_length = np.mean(viral_caption_lengths)\n",
        "                    recomendacoes.append(f\"Mantenha captions com ~{avg_length:.0f} caracteres para melhor performance\")\n",
        "\n",
        "                # CTA analysis\n",
        "                viral_with_cta = [v for v in viral_videos if v['analise_caption']['tem_call_to_action']]\n",
        "                if viral_videos:\n",
        "                    cta_rate = len(viral_with_cta) / len(viral_videos)\n",
        "                    if cta_rate > 0.5:\n",
        "                        recomendacoes.append(\"Inclua CTAs claros nas captions - presentes em {:.0%} dos vídeos virais\".format(cta_rate))\n",
        "\n",
        "            # Recomendações gerais\n",
        "            resumo = self._gerar_resumo_geral(videos_dados)\n",
        "            if resumo['avg_engagement_rate'] < 0.03:  # Abaixo de 3%\n",
        "                recomendacoes.append(\"Engagement rate abaixo do ideal (3%+). Foque em hooks mais impactantes nos primeiros 3 segundos\")\n",
        "\n",
        "            if resumo['avg_virality_score'] < 60:\n",
        "                recomendacoes.append(\"Score de viralidade pode melhorar. Teste elementos dos seus top performers em novos conteúdos\")\n",
        "\n",
        "            # Adicionar recomendações padrão se lista estiver vazia\n",
        "            if not recomendacoes:\n",
        "                recomendacoes = [\n",
        "                    \"Analise os padrões dos seus vídeos com melhor performance\",\n",
        "                    \"Teste diferentes tipos de hooks baseados nos dados coletados\",\n",
        "                    \"Monitore engagement rate como KPI principal de performance\"\n",
        "                ]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Erro ao gerar recomendações: {e}\")\n",
        "            recomendacoes = [\"Análise de recomendações limitada devido a dados insuficientes\"]\n",
        "\n",
        "        return recomendacoes[:8]  # Limitar a 8 recomendações\n",
        "\n",
        "    def salvar_analise_viral(self, mapeamento_videos, insights_virais):\n",
        "        \"\"\"Salva todos os resultados da análise viral - versão corrigida\"\"\"\n",
        "        try:\n",
        "            # 1. Dados de mapeamento (CSV)\n",
        "            videos_com_dados = [v for v in mapeamento_videos if v.get('tem_dados_virais', False)]\n",
        "\n",
        "            csv_path = None\n",
        "            if videos_com_dados:\n",
        "                df_mapeamento = pd.DataFrame([\n",
        "                    {\n",
        "                        'video_id': v['video_id'],\n",
        "                        'nome_arquivo': v['nome_arquivo'],\n",
        "                        'tem_dados_virais': v['tem_dados_virais'],\n",
        "                        **v.get('viral_data', {}),\n",
        "                        **v.get('metricas_calculadas', {}),\n",
        "                        **{f\"caption_{k}\": val for k, val in v.get('analise_caption', {}).items()},\n",
        "                        'classificacao_performance': v.get('classificacao_performance', 'N/A')\n",
        "                    }\n",
        "                    for v in videos_com_dados\n",
        "                ])\n",
        "\n",
        "                csv_path = os.path.join(self.pasta_viral, \"analise_viral_completa.csv\")\n",
        "                df_mapeamento.to_csv(csv_path, index=False, encoding='utf-8')\n",
        "                print(f\"✅ CSV salvo: {csv_path}\")\n",
        "            else:\n",
        "                print(\"⚠️ Nenhum vídeo com dados virais para salvar em CSV\")\n",
        "\n",
        "            # 2. Insights consolidados (JSON)\n",
        "            json_path = os.path.join(self.pasta_viral, \"insights_virais.json\")\n",
        "            with open(json_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(insights_virais, f, indent=2, ensure_ascii=False)\n",
        "            print(f\"✅ Insights salvos: {json_path}\")\n",
        "\n",
        "            # 3. Mapeamento completo (JSON)\n",
        "            mapeamento_path = os.path.join(self.pasta_viral, \"mapeamento_viral_completo.json\")\n",
        "            with open(mapeamento_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(mapeamento_videos, f, indent=2, ensure_ascii=False)\n",
        "            print(f\"✅ Mapeamento salvo: {mapeamento_path}\")\n",
        "\n",
        "            # 4. Dados para integração com outras layers\n",
        "            dados_integracao = {\n",
        "                'viral_disponivel': len(videos_com_dados) > 0,\n",
        "                'total_videos_mapeados': len(videos_com_dados),\n",
        "                'resumo_performance': insights_virais['resumo_geral'],\n",
        "                'top_performers': insights_virais['top_performers'],\n",
        "                'recomendacoes_principais': insights_virais['recomendacoes_estrategicas'][:3],\n",
        "                'data_processamento': datetime.now().isoformat(),\n",
        "                'arquivos_gerados': {\n",
        "                    'csv_completo': csv_path,\n",
        "                    'insights_json': json_path,\n",
        "                    'mapeamento_json': mapeamento_path\n",
        "                }\n",
        "            }\n",
        "\n",
        "            integracao_path = os.path.join(self.pasta_dados, \"viral_integration_data.json\")\n",
        "            with open(integracao_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(dados_integracao, f, indent=2, ensure_ascii=False)\n",
        "            print(f\"✅ Integração salva: {integracao_path}\")\n",
        "\n",
        "            return True, dados_integracao\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erro ao salvar análise viral: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return False, None\n",
        "\n",
        "# ============================================================================\n",
        "# FUNÇÃO PRINCIPAL DE PROCESSAMENTO\n",
        "# ============================================================================\n",
        "\n",
        "def processar_analise_viral():\n",
        "    \"\"\"Função principal que executa toda a análise viral\"\"\"\n",
        "    print(\"🦠 INICIANDO LAYER 3.3: ANÁLISE VIRAL (VERSÃO CORRIGIDA)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Verificar pré-requisitos\n",
        "    prereq_ok, metadados_path = verificar_prerequisito_analise_viral()\n",
        "    if not prereq_ok:\n",
        "        return False\n",
        "\n",
        "    # Carregar metadados dos vídeos\n",
        "    print(\"📊 Carregando metadados dos vídeos...\")\n",
        "    try:\n",
        "        with open(metadados_path, 'r', encoding='utf-8') as f:\n",
        "            metadados_videos = json.load(f)\n",
        "        print(f\"✅ {len(metadados_videos)} vídeos carregados\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erro ao carregar metadados: {e}\")\n",
        "        return False\n",
        "\n",
        "    # Inicializar analisador viral\n",
        "    print(\"🔧 Inicializando analisador viral...\")\n",
        "    analisador = AnalisadorViralCorrigido(PASTA_TRABALHO)\n",
        "\n",
        "    if not analisador.dados_script_anexo:\n",
        "        print(\"⚠️ Dados do script anexo não encontrados.\")\n",
        "        print(\"   Execute o Instagram Extractor primeiro para obter dados de viralização.\")\n",
        "\n",
        "        # Criar arquivo indicando que análise viral não está disponível\n",
        "        dados_vazio = {\n",
        "            'viral_disponivel': False,\n",
        "            'motivo': 'Dados do script anexo não encontrados',\n",
        "            'data_tentativa': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        sem_dados_path = os.path.join(PASTA_TRABALHO, \"dados\", \"viral_integration_data.json\")\n",
        "        with open(sem_dados_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(dados_vazio, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(\"   📄 Arquivo de status criado para próximas layers\")\n",
        "        return False\n",
        "\n",
        "    # Mapear vídeos com dados virais\n",
        "    print(\"🗺️ Mapeando vídeos com dados de viralização...\")\n",
        "    try:\n",
        "        mapeamento_videos = analisador.mapear_videos_com_dados_virais(metadados_videos)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erro no mapeamento: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "    # Gerar insights virais\n",
        "    print(\"🧠 Gerando insights estratégicos...\")\n",
        "    try:\n",
        "        insights_virais = analisador.gerar_insights_virais(mapeamento_videos)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erro na geração de insights: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "    # Salvar resultados\n",
        "    print(\"💾 Salvando análise viral...\")\n",
        "    try:\n",
        "        sucesso, dados_integracao = analisador.salvar_analise_viral(mapeamento_videos, insights_virais)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erro ao salvar: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "    if sucesso and dados_integracao:\n",
        "              # Atualizar config.json com verificação robusta\n",
        "        config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "        if os.path.exists(config_path):\n",
        "            try:\n",
        "                with open(config_path, 'r', encoding='utf-8') as f:\n",
        "                    config = json.load(f)\n",
        "\n",
        "                # Garantir que as chaves existem\n",
        "                if 'status_etapas' not in config:\n",
        "                    config['status_etapas'] = {}\n",
        "                if 'arquivos_gerados' not in config:\n",
        "                    config['arquivos_gerados'] = {}\n",
        "\n",
        "                config['status_etapas']['analise_viral'] = True\n",
        "                if dados_integracao.get('arquivos_gerados', {}).get('csv_completo'):\n",
        "                    config['arquivos_gerados']['analise_viral'] = dados_integracao['arquivos_gerados']\n",
        "\n",
        "                with open(config_path, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "                print(\"✅ Config atualizado com sucesso\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Config não atualizado (não crítico): {e}\")\n",
        "\n",
        "\n",
        "\n",
        "        print(\"\\n✅ ANÁLISE VIRAL CONCLUÍDA COM SUCESSO!\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"📊 Total de vídeos mapeados: {dados_integracao['total_videos_mapeados']}\")\n",
        "        print(f\"👀 Views totais: {insights_virais['resumo_geral']['total_views']:,}\")\n",
        "        print(f\"❤️ Likes totais: {insights_virais['resumo_geral']['total_likes']:,}\")\n",
        "        print(f\"💬 Comentários totais: {insights_virais['resumo_geral']['total_comments']:,}\")\n",
        "        print(f\"📈 Engagement médio: {insights_virais['resumo_geral']['avg_engagement_rate']:.2%}\")\n",
        "        print(f\"🦠 Score viral médio: {insights_virais['resumo_geral']['avg_virality_score']:.1f}/100\")\n",
        "\n",
        "        print(\"\\n🎯 TOP RECOMENDAÇÕES:\")\n",
        "        for i, rec in enumerate(dados_integracao['recomendacoes_principais'], 1):\n",
        "            print(f\"   {i}. {rec}\")\n",
        "\n",
        "        print(f\"\\n📁 Dados salvos em: {analisador.pasta_viral}\")\n",
        "        print(\"🔗 Dados de integração criados para próximas layers\")\n",
        "        print(\"\\n📋 PRÓXIMO PASSO: Execute LAYER 4 (Relatórios e Dashboard)\")\n",
        "\n",
        "        return True\n",
        "    else:\n",
        "        print(\"❌ Erro na análise viral\")\n",
        "        return False\n",
        "\n",
        "# ============================================================================\n",
        "# EXECUÇÃO PRINCIPAL\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🎯 LAYER 3.3: ANÁLISE VIRAL - PROCESSAMENTO INDEPENDENTE (CORRIGIDO)\")\n",
        "    print(\"🔄 Executando análise completa de dados de viralização...\")\n",
        "    print()\n",
        "\n",
        "    sucesso = processar_analise_viral()\n",
        "\n",
        "    if sucesso:\n",
        "        print(\"\\n🎉 ANÁLISE VIRAL PROCESSADA COM SUCESSO!\")\n",
        "        print(\"   Os dados estão prontos para integração automática nas próximas layers\")\n",
        "    else:\n",
        "        print(\"\\n⚠️ Análise viral não pôde ser completada\")\n",
        "        print(\"   O processo continuará sem dados de viralização\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"📋 PRÓXIMO PASSO: Execute LAYER 4 (Relatórios e Dashboard)\")\n",
        "    print(\"   Os dados virais serão automaticamente incluídos se disponíveis\")\n",
        "    print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "relatorios_humanizados",
        "outputId": "c2e32687-d422-4355-ad3f-437a827bf0dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando geração de relatórios humanizados para 3 vídeos...\n",
            "[1/3] Gerando relatórios para: vid_bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t\n",
            "  💾 Relatório de Texto (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_texto/RELATORIO_TEXTO_HUMANIZADO_vid_bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.xlsx\n",
            "  💾 Estratégia de Conteúdo Textual (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_texto/ESTRATEGIA_CONTEUDO_TEXTUAL_vid_bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.pdf\n",
            "  💾 Relatório de Áudio (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_audio/RELATORIO_AUDIO_HUMANIZADO_vid_bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.xlsx\n",
            "  💾 Resumo de Áudio Estratégico (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_audio/RESUMO_AUDIO_ESTRATEGICO_vid_bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.pdf\n",
            "  💾 Relatório Visual (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_visual/RELATORIO_VISUAL_HUMANIZADO_vid_bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.xlsx\n",
            "  💾 Estratégia Visual Completa (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_visual/ESTRATEGIA_VISUAL_COMPLETA_vid_bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.pdf\n",
            "  💾 Relatório Psicológico (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_psicologica/RELATORIO_PSICOLOGICO_HUMANIZADO_vid_bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.xlsx\n",
            "  💾 Manual de Psicologia Viral (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_psicologica/MANUAL_PSICOLOGIA_VIRAL_vid_bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.pdf\n",
            "  ✅ Relatórios gerados para vid_bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t\n",
            "[2/3] Gerando relatórios para: vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf\n",
            "  💾 Relatório de Texto (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_texto/RELATORIO_TEXTO_HUMANIZADO_vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.xlsx\n",
            "  💾 Estratégia de Conteúdo Textual (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_texto/ESTRATEGIA_CONTEUDO_TEXTUAL_vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.pdf\n",
            "  💾 Relatório de Áudio (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_audio/RELATORIO_AUDIO_HUMANIZADO_vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.xlsx\n",
            "  💾 Resumo de Áudio Estratégico (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_audio/RESUMO_AUDIO_ESTRATEGICO_vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.pdf\n",
            "  💾 Relatório Visual (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_visual/RELATORIO_VISUAL_HUMANIZADO_vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.xlsx\n",
            "  💾 Estratégia Visual Completa (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_visual/ESTRATEGIA_VISUAL_COMPLETA_vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.pdf\n",
            "  💾 Relatório Psicológico (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_psicologica/RELATORIO_PSICOLOGICO_HUMANIZADO_vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.xlsx\n",
            "  💾 Manual de Psicologia Viral (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_psicologica/MANUAL_PSICOLOGIA_VIRAL_vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.pdf\n",
            "  ✅ Relatórios gerados para vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf\n",
            "[3/3] Gerando relatórios para: vid_máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy\n",
            "  💾 Relatório de Texto (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_texto/RELATORIO_TEXTO_HUMANIZADO_vid_máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy.xlsx\n",
            "  💾 Estratégia de Conteúdo Textual (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_texto/ESTRATEGIA_CONTEUDO_TEXTUAL_vid_máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy.pdf\n",
            "  💾 Relatório de Áudio (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_audio/RELATORIO_AUDIO_HUMANIZADO_vid_máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy.xlsx\n",
            "  💾 Resumo de Áudio Estratégico (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_audio/RESUMO_AUDIO_ESTRATEGICO_vid_máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy.pdf\n",
            "  💾 Relatório Visual (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_visual/RELATORIO_VISUAL_HUMANIZADO_vid_máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy.xlsx\n",
            "  💾 Estratégia Visual Completa (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_visual/ESTRATEGIA_VISUAL_COMPLETA_vid_máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy.pdf\n",
            "  💾 Relatório Psicológico (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_psicologica/RELATORIO_PSICOLOGICO_HUMANIZADO_vid_máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy.xlsx\n",
            "  💾 Manual de Psicologia Viral (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_psicologica/MANUAL_PSICOLOGIA_VIRAL_vid_máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy.pdf\n",
            "  ✅ Relatórios gerados para vid_máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy\n",
            "\n",
            "✅ GERAÇÃO DE RELATÓRIOS HUMANIZADOS CONCLUÍDA!\n",
            "Total de vídeos com relatórios gerados: 3\n",
            "\n",
            "➡️ PRÓXIMA CÉLULA: 4.2 - GERAÇÃO DO BLUEPRINT FINAL E DASHBOARD\n",
            "📊 Gerando e salvando relatório de análise viral...\n",
            "✅ Relatório viral salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/blueprint/RELATORIO_VIRAL_INSIGHTS.txt\n",
            "⚠️ PDF não gerado (não crítico): 'latin-1' codec can't encode character '\\U0001f4c5' in position 954: ordinal not in range(256)\n",
            "🔗 Relatório viral integrado com sucesso!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# LAYER 4: GERAÇÃO DE RELATÓRIOS E BLUEPRINT ESTRATÉGICO\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# CÉLULA 4.1: GERAÇÃO DE RELATÓRIOS HUMANIZADOS (ÁUDIO, VISUAL, TEXTO, PSICOLÓGICO)\n",
        "# ============================================================================\n",
        "\n",
        "from fpdf import FPDF # Importar FPDF para geração de PDF\n",
        "\n",
        "# ===== FUNÇÕES DE INTEGRAÇÃO VIRAL - ADICIONAR NO INÍCIO DA CÉLULA 4.1 =====\n",
        "\n",
        "def detectar_dados_virais_disponiveis(pasta_trabalho):\n",
        "    \"\"\"Detecta se dados virais estão disponíveis\"\"\"\n",
        "    try:\n",
        "        viral_integration_path = os.path.join(pasta_trabalho, \"dados\", \"viral_integration_data.json\")\n",
        "        if not os.path.exists(viral_integration_path):\n",
        "            return False, None\n",
        "        with open(viral_integration_path, 'r', encoding='utf-8') as f:\n",
        "            viral_data = json.load(f)\n",
        "        return viral_data.get('viral_disponivel', False), viral_data\n",
        "    except Exception:\n",
        "        return False, None\n",
        "\n",
        "def gerar_relatorio_viral_humanizado(pasta_trabalho):\n",
        "    \"\"\"Gera relatório humanizado dos dados virais\"\"\"\n",
        "    viral_disponivel, viral_integration = detectar_dados_virais_disponiveis(pasta_trabalho)\n",
        "\n",
        "    if not viral_disponivel:\n",
        "        return \"📊 ANÁLISE VIRAL: Dados não disponíveis\\n\"\n",
        "\n",
        "    try:\n",
        "        # Carregar insights virais\n",
        "        insights_path = os.path.join(pasta_trabalho, \"analise_viral\", \"insights_virais.json\")\n",
        "        with open(insights_path, 'r', encoding='utf-8') as f:\n",
        "            insights = json.load(f)\n",
        "\n",
        "        resumo = insights['resumo_geral']\n",
        "        recomendacoes = insights['recomendacoes_estrategicas']\n",
        "\n",
        "        relatorio = f\"\"\"\n",
        "📊 RELATÓRIO DE ANÁLISE VIRAL - INSIGHTS ESTRATÉGICOS\n",
        "{'='*60}\n",
        "\n",
        "🎯 RESUMO EXECUTIVO:\n",
        "   • Total de vídeos analisados: {resumo['total_videos']}\n",
        "   • Views totais coletadas: {resumo['total_views']:,}\n",
        "   • Likes totais: {resumo['total_likes']:,}\n",
        "   • Comentários totais: {resumo['total_comments']:,}\n",
        "   • Engagement rate médio: {resumo['avg_engagement_rate']:.2%}\n",
        "   • Score de viralidade médio: {resumo['avg_virality_score']:.1f}/100\n",
        "\n",
        "🎯 RECOMENDAÇÕES ESTRATÉGICAS:\n",
        "\"\"\"\n",
        "\n",
        "        for i, rec in enumerate(recomendacoes[:5], 1):\n",
        "            relatorio += f\"   {i}. {rec}\\n\"\n",
        "\n",
        "        relatorio += f\"\\n📅 Análise realizada em: {insights['data_analise'][:19]}\\n\"\n",
        "\n",
        "        return relatorio\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"📊 ANÁLISE VIRAL: Erro ao gerar relatório - {e}\\n\"\n",
        "\n",
        "# ===== FIM DAS FUNÇÕES DE INTEGRAÇÃO VIRAL =====\n",
        "\n",
        "class PDF(FPDF):\n",
        "    def header(self):\n",
        "        self.set_font('Arial', 'B', 12)\n",
        "        self.cell(0, 10, 'Relatório de Engenharia Reversa de Vídeos', 0, 1, 'C')\n",
        "        self.ln(10)\n",
        "\n",
        "    def footer(self):\n",
        "        self.set_y(-15)\n",
        "        self.set_font('Arial', 'I', 8)\n",
        "        self.cell(0, 10, f'Página {self.page_no()}/{{nb}}', 0, 0, 'C')\n",
        "\n",
        "    def chapter_title(self, title):\n",
        "        self.set_font('Arial', 'B', 12)\n",
        "        self.cell(0, 10, title, 0, 1, 'L')\n",
        "        self.ln(5)\n",
        "\n",
        "    def chapter_body(self, body):\n",
        "        self.set_font('Arial', '', 10)\n",
        "        self.multi_cell(0, 5, body)\n",
        "        self.ln()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def gerar_relatorio_texto(video_id, analise_padroes_data, pasta_destino):\n",
        "    df_texto = pd.DataFrame([analise_padroes_data])\n",
        "    excel_path = os.path.join(pasta_destino, f'RELATORIO_TEXTO_HUMANIZADO_{video_id}.xlsx')\n",
        "    df_texto.to_excel(excel_path, index=False, engine='openpyxl')\n",
        "\n",
        "    pdf = PDF()\n",
        "    pdf.add_page()\n",
        "    pdf.chapter_title('Estratégia de Conteúdo Textual')\n",
        "    pdf.chapter_body(f'Resumo do Texto: {analise_padroes_data.get('resumo_texto', 'N/A')}')\n",
        "    pdf.chapter_body(f'Palavras-chave: {', '.join(analise_padroes_data.get('palavras_chave_texto', []))}')\n",
        "    pdf_path = os.path.join(pasta_destino, f'ESTRATEGIA_CONTEUDO_TEXTUAL_{video_id}.pdf')\n",
        "    pdf.output(pdf_path)\n",
        "    return excel_path, pdf_path\n",
        "\n",
        "def gerar_relatorio_audio(video_id, analise_padroes_data, pasta_destino):\n",
        "    df_audio = pd.DataFrame([analise_padroes_data.get('analise_audio_detalhada', {})])\n",
        "    excel_path = os.path.join(pasta_destino, f'RELATORIO_AUDIO_HUMANIZADO_{video_id}.xlsx')\n",
        "    df_audio.to_excel(excel_path, index=False, engine='openpyxl')\n",
        "\n",
        "    pdf = PDF()\n",
        "    pdf.add_page()\n",
        "    pdf.chapter_title('Resumo de Áudio Estratégico')\n",
        "    pdf.chapter_body(f'BPM: {analise_padroes_data.get('analise_audio_detalhada', {}).get('bpm', 'N/A')}')\n",
        "    pdf.chapter_body(f'Duração do Áudio: {analise_padroes_data.get('analise_audio_detalhada', {}).get('duracao_audio_segundos', 'N/A')} segundos')\n",
        "    pdf_path = os.path.join(pasta_destino, f'RESUMO_AUDIO_ESTRATEGICO_{video_id}.pdf')\n",
        "    pdf.output(pdf_path)\n",
        "    return excel_path, pdf_path\n",
        "\n",
        "def gerar_relatorio_visual(video_id, analise_padroes_data, pasta_destino):\n",
        "    df_visual = pd.DataFrame([analise_padroes_data.get('analise_visual_detalhada', {})])\n",
        "    excel_path = os.path.join(pasta_destino, f'RELATORIO_VISUAL_HUMANIZADO_{video_id}.xlsx')\n",
        "    df_visual.to_excel(excel_path, index=False, engine='openpyxl')\n",
        "\n",
        "    pdf = PDF()\n",
        "    pdf.add_page()\n",
        "    pdf.chapter_title('Estratégia Visual Completa')\n",
        "    pdf.chapter_body(f'Total de Cortes: {analise_padroes_data.get('analise_visual_detalhada', {}).get('total_cortes', 'N/A')}')\n",
        "    pdf.chapter_body(f'Complexidade Visual Média: {analise_padroes_data.get('analise_visual_detalhada', {}).get('complexidade_visual_media', 'N/A'):.2f}')\n",
        "    pdf.chapter_body(f'Brilho Médio: {analise_padroes_data.get('analise_visual_detalhada', {}).get('brilho_medio', 'N/A'):.2f}')\n",
        "    pdf_path = os.path.join(pasta_destino, f'ESTRATEGIA_VISUAL_COMPLETA_{video_id}.pdf')\n",
        "    pdf.output(pdf_path)\n",
        "    return excel_path, pdf_path\n",
        "\n",
        "def gerar_relatorio_psicologico(video_id, analise_psicologica_data, pasta_destino):\n",
        "    df_psico = pd.DataFrame([analise_psicologica_data])\n",
        "    excel_path = os.path.join(pasta_destino, f'RELATORIO_PSICOLOGICO_HUMANIZADO_{video_id}.xlsx')\n",
        "    df_psico.to_excel(excel_path, index=False, engine='openpyxl')\n",
        "\n",
        "    pdf = PDF()\n",
        "    pdf.add_page()\n",
        "    pdf.chapter_title('Manual de Psicologia Viral')\n",
        "    pdf.chapter_body(f'Gatilhos Detectados: {', '.join(analise_psicologica_data.get('gatilhos_detectados', []))}')\n",
        "    pdf.chapter_body(f'Emoções Predominantes: {analise_psicologica_data.get('emocoes_predominantes', 'N/A')}')\n",
        "    pdf.chapter_body(f'Insights: {analise_psicologica_data.get('insights_psicologicos', 'N/A')}')\n",
        "    pdf_path = os.path.join(pasta_destino, f'MANUAL_PSICOLOGIA_VIRAL_{video_id}.pdf')\n",
        "    pdf.output(pdf_path)\n",
        "    return excel_path, pdf_path\n",
        "\n",
        "def processar_geracao_relatorios_todos_videos():\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa('analise_psicologica')\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Carregar dados de análise de padrões e psicológica\n",
        "    analises_padroes_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_padroes_completas.json\")\n",
        "    analises_psicologicas_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_psicologicas_completas.json\")\n",
        "    with open(analises_padroes_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        analises_padroes = json.load(f)\n",
        "    with open(analises_psicologicas_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        analises_psicologicas = json.load(f)\n",
        "\n",
        "    sucessos = 0\n",
        "\n",
        "    print(f\"\"\"\n",
        "Iniciando geração de relatórios humanizados para {len(analises_padroes)} vídeos...\"\"\")\n",
        "\n",
        "    for i, analise_padroes_data in enumerate(analises_padroes, 1):\n",
        "        video_id = analise_padroes_data[\"video_id\"]\n",
        "        analise_psicologica_data = next((a for a in analises_psicologicas if a[\"video_id\"] == video_id), None)\n",
        "\n",
        "        if analise_padroes_data.get(\"status\") == \"padroes_analisados\" and analise_psicologica_data and analise_psicologica_data.get(\"status\") == \"analise_psicologica_concluida\":\n",
        "            print(f\"[{i}/{len(analises_padroes)}] Gerando relatórios para: {video_id}\")\n",
        "            try:\n",
        "                # Geração de Relatórios de Texto\n",
        "                pasta_texto = os.path.join(PASTA_TRABALHO, \"analise_texto\")\n",
        "                os.makedirs(pasta_texto, exist_ok=True)\n",
        "                excel_text, pdf_text = gerar_relatorio_texto(video_id, analise_padroes_data, pasta_texto)\n",
        "                print(f\"  💾 Relatório de Texto (XLSX) salvo em: {excel_text}\")\n",
        "                print(f\"  💾 Estratégia de Conteúdo Textual (PDF) salvo em: {pdf_text}\")\n",
        "\n",
        "                # Geração de Relatórios de Áudio\n",
        "                pasta_audio = os.path.join(PASTA_TRABALHO, \"analise_audio\")\n",
        "                os.makedirs(pasta_audio, exist_ok=True)\n",
        "                excel_audio, pdf_audio = gerar_relatorio_audio(video_id, analise_padroes_data, pasta_audio)\n",
        "                print(f\"  💾 Relatório de Áudio (XLSX) salvo em: {excel_audio}\")\n",
        "                print(f\"  💾 Resumo de Áudio Estratégico (PDF) salvo em: {pdf_audio}\")\n",
        "\n",
        "                # Geração de Relatórios Visuais\n",
        "                pasta_visual = os.path.join(PASTA_TRABALHO, \"analise_visual\")\n",
        "                os.makedirs(pasta_visual, exist_ok=True)\n",
        "                excel_visual, pdf_visual = gerar_relatorio_visual(video_id, analise_padroes_data, pasta_visual)\n",
        "                print(f\"  💾 Relatório Visual (XLSX) salvo em: {excel_visual}\")\n",
        "                print(f\"  💾 Estratégia Visual Completa (PDF) salvo em: {pdf_visual}\")\n",
        "\n",
        "                # Geração de Relatórios Psicológicos\n",
        "                pasta_psicologica = os.path.join(PASTA_TRABALHO, \"analise_psicologica\")\n",
        "                os.makedirs(pasta_psicologica, exist_ok=True)\n",
        "                excel_psico, pdf_psico = gerar_relatorio_psicologico(video_id, analise_psicologica_data, pasta_psicologica)\n",
        "                print(f\"  💾 Relatório Psicológico (XLSX) salvo em: {excel_psico}\")\n",
        "                print(f\"  💾 Manual de Psicologia Viral (PDF) salvo em: {pdf_psico}\")\n",
        "\n",
        "                sucessos += 1\n",
        "                print(f\"  ✅ Relatórios gerados para {video_id}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  ❌ ERRO na geração de relatórios para {video_id}: {e}\")\n",
        "        else:\n",
        "            print(f\"[{i}/{len(analises_padroes)}] Pulando {video_id} - Pré-requisitos não atendidos.\")\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    config[\"status_etapas\"][\"relatorios_humanizados\"] = True\n",
        "    config[\"total_videos_relatorios_gerados\"] = sucessos\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(\"\"\"\n",
        "✅ GERAÇÃO DE RELATÓRIOS HUMANIZADOS CONCLUÍDA!\"\"\")\n",
        "    print(f\"Total de vídeos com relatórios gerados: {sucessos}\")\n",
        "\n",
        "    if sucessos == 0:\n",
        "        print(\"❌ NENHUM VÍDEO TEVE RELATÓRIOS GERADOS COM SUCESSO. Verifique as etapas anteriores.\")\n",
        "    print(\"\"\"\n",
        "➡️ PRÓXIMA CÉLULA: 4.2 - GERAÇÃO DO BLUEPRINT FINAL E DASHBOARD\"\"\")\n",
        "\n",
        "# Executar geração de relatórios\n",
        "try:\n",
        "    processar_geracao_relatorios_todos_videos()\n",
        "except Exception as e:\n",
        "    print(f\"\"\"\n",
        "❌ ERRO GERAL NA GERAÇÃO DE RELATÓRIOS: {e}\"\"\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")\n",
        "\n",
        "\n",
        "# ======= INTEGRAÇÃO: RELATÓRIO VIRAL (CORRIGIDO) =======\n",
        "print(\"📊 Gerando e salvando relatório de análise viral...\")\n",
        "relatorio_viral = gerar_relatorio_viral_humanizado(PASTA_TRABALHO)\n",
        "\n",
        "# Salvar relatório viral como arquivo separado\n",
        "pasta_blueprint = os.path.join(PASTA_TRABALHO, \"blueprint\")\n",
        "os.makedirs(pasta_blueprint, exist_ok=True)\n",
        "\n",
        "viral_report_path = os.path.join(pasta_blueprint, \"RELATORIO_VIRAL_INSIGHTS.txt\")\n",
        "with open(viral_report_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(relatorio_viral)\n",
        "\n",
        "print(\"✅ Relatório viral salvo em:\", viral_report_path)\n",
        "\n",
        "# Também salvar como PDF\n",
        "try:\n",
        "    pdf_viral = PDF()\n",
        "    pdf_viral.add_page()\n",
        "    pdf_viral.chapter_title('ANÁLISE VIRAL - INSIGHTS ESTRATÉGICOS')\n",
        "\n",
        "    # Converter relatório para formato compatível com PDF\n",
        "    relatorio_clean = relatorio_viral.replace('📊', '').replace('🎯', '').replace('•', '-')\n",
        "    pdf_viral.chapter_body(relatorio_clean)\n",
        "\n",
        "    viral_pdf_path = os.path.join(pasta_blueprint, \"RELATORIO_VIRAL_INSIGHTS.pdf\")\n",
        "    pdf_viral.output(viral_pdf_path)\n",
        "\n",
        "    print(\"✅ Relatório viral PDF salvo em:\", viral_pdf_path)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ PDF não gerado (não crítico): {e}\")\n",
        "\n",
        "print(\"🔗 Relatório viral integrado com sucesso!\")\n",
        "# ======= FIM INTEGRAÇÃO VIRAL ======="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cT5PyNglpi2T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7ee0a01-b8dc-4397-bed9-b7db6c82026c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "💾 Dashboard Executivo (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dashboard/DASHBOARD_MASTER_EXECUTIVO.xlsx\n",
            "💾 Dados Consolidados (CSV) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dashboard/dados_consolidados.csv\n",
            "💾 Dados Detalhados (JSON) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dashboard/dados_detalhados.json\n",
            "💾 Dashboard Interativo (HTML) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dashboard/dashboard_interativo.html\n",
            "💾 Blueprint Estratégico (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/blueprint/BLUEPRINT_ESTRATEGICO_FINAL.pdf\n",
            "\n",
            "✅ GERAÇÃO DO BLUEPRINT FINAL E DASHBOARD CONCLUÍDA!\n",
            "Todos os relatórios e o dashboard foram gerados com sucesso.\n",
            "\n",
            "🎉 PROCESSO DE ENGENHARIA REVERSA CONCLUÍDO COM SUCESSO! 🎉\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CÉLULA 4.2: GERAÇÃO DO BLUEPRINT FINAL E DASHBOARD\n",
        "# ============================================================================\n",
        "\n",
        "def gerar_blueprint_dashboard():\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa(\"relatorios_humanizados\")\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Carregar todos os dados de análise\n",
        "    metadados_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "    decomposicao_path = os.path.join(PASTA_TRABALHO, \"dados\", \"decomposicao_completa.json\")\n",
        "    analises_padroes_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_padroes_completas.json\")\n",
        "    analises_psicologicas_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_psicologicas_completas.json\")\n",
        "\n",
        "    with open(metadados_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        metadados = json.load(f)\n",
        "    with open(decomposicao_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        decomposicoes = json.load(f)\n",
        "    with open(analises_padroes_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        analises_padroes = json.load(f)\n",
        "    with open(analises_psicologicas_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        analises_psicologicas = json.load(f)\n",
        "\n",
        "    dados_consolidados = []\n",
        "    for video_meta in metadados:\n",
        "        video_id = video_meta[\"id\"]\n",
        "        decomposicao = next((d for d in decomposicoes if d[\"video_id\"] == video_id), {})\n",
        "        analise_padroes = next((ap for ap in analises_padroes if ap[\"video_id\"] == video_id), {})\n",
        "        analise_psicologica = next((aps for aps in analises_psicologicas if aps[\"video_id\"] == video_id), {})\n",
        "        consolidado = {\n",
        "            \"video_id\": video_id,\n",
        "            \"nome_arquivo\": video_meta.get(\"nome_arquivo\"),\n",
        "            \"duracao_segundos\": video_meta.get(\"duracao_segundos\"),\n",
        "            \"formato_detectado\": video_meta.get(\"formato_detectado\"),\n",
        "            \"tem_audio\": video_meta.get(\"tem_audio\"),\n",
        "            \"total_frames\": video_meta.get(\"total_frames\"),\n",
        "            \"ocr_textos_count\": len(decomposicao.get(\"textos_ocr\", [])),\n",
        "            \"audio_transcrito_len\": len(decomposicao.get(\"audio_transcrito\", \"\")),\n",
        "            \"cortes_detectados_count\": len(decomposicao.get(\"cortes_detectados_segundos\", [])),\n",
        "            \"bpm_audio\": analise_padroes.get(\"analise_audio_detalhada\", {}).get(\"bpm\"),\n",
        "            \"complexidade_visual_media\": analise_padroes.get(\"analise_visual_detalhada\", {}).get(\"complexidade_visual_media\"),\n",
        "            \"brilho_medio\": analise_padroes.get(\"analise_visual_detalhada\", {}).get(\"brilho_medio\"),\n",
        "            \"padroes_gerais\": \", \".join(analise_padroes.get(\"padroes_gerais\", [])),\n",
        "            \"gatilhos_psicologicos\": \", \".join(analise_psicologica.get(\"gatilhos_detectados\", [])),\n",
        "            \"emocoes_predominantes\": str(analise_psicologica.get(\"emocoes_predominantes\", {})),\n",
        "            \"status_geral\": video_meta.get(\"status\") # Pode ser aprimorado para refletir o status de todas as etapas\n",
        "        }\n",
        "        dados_consolidados.append(consolidado)\n",
        "\n",
        "    df_final = pd.DataFrame(dados_consolidados)\n",
        "\n",
        "    # Salvar Dashboard Executivo (Excel)\n",
        "    dashboard_excel_path = os.path.join(PASTA_TRABALHO, \"dashboard\", \"DASHBOARD_MASTER_EXECUTIVO.xlsx\")\n",
        "    df_final.to_excel(dashboard_excel_path, index=False, engine=\"openpyxl\")\n",
        "    print(f\"\\n💾 Dashboard Executivo (XLSX) salvo em: {dashboard_excel_path}\")\n",
        "\n",
        "    # Salvar Dados Consolidados (CSV e JSON)\n",
        "    dados_csv_path = os.path.join(PASTA_TRABALHO, \"dashboard\", \"dados_consolidados.csv\")\n",
        "    df_final.to_csv(dados_csv_path, index=False, encoding=\"utf-8\")\n",
        "    print(f\"💾 Dados Consolidados (CSV) salvo em: {dados_csv_path}\")\n",
        "\n",
        "    dados_json_path = os.path.join(PASTA_TRABALHO, \"dashboard\", \"dados_detalhados.json\")\n",
        "    with open(dados_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(dados_consolidados, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"💾 Dados Detalhados (JSON) salvo em: {dados_json_path}\")\n",
        "\n",
        "    # Geração de Dashboard Interativo (HTML - Exemplo simples)\n",
        "    # Para um dashboard interativo real, seria necessário uma biblioteca como Plotly ou Dash\n",
        "    dashboard_html_path = os.path.join(PASTA_TRABALHO, \"dashboard\", \"dashboard_interativo.html\")\n",
        "    with open(dashboard_html_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"<html><body><h1>Dashboard Interativo (Placeholder)</h1><p>Seu dashboard interativo real seria gerado aqui com bibliotecas como Plotly ou Dash.</p></body></html>\")\n",
        "    print(f\"💾 Dashboard Interativo (HTML) salvo em: {dashboard_html_path}\")\n",
        "\n",
        "    # Geração do Blueprint Estratégico (PDF - Exemplo simples)\n",
        "    pdf = PDF()\n",
        "    pdf.add_page()\n",
        "    pdf.chapter_title(\"BLUEPRINT ESTRATÉGICO FINAL\")\n",
        "    pdf.chapter_body(\"Este é o seu blueprint estratégico final, consolidando todos os insights.\")\n",
        "    pdf.chapter_body(f\"Total de vídeos analisados: {len(df_final)}\")\n",
        "    pdf.chapter_body(f\"Média de duração dos vídeos: {df_final[\"duracao_segundos\"] .mean():.2f} segundos\")\n",
        "    pdf_blueprint_path = os.path.join(PASTA_TRABALHO, \"blueprint\", \"BLUEPRINT_ESTRATEGICO_FINAL.pdf\")\n",
        "    pdf.output(pdf_blueprint_path)\n",
        "    print(f\"💾 Blueprint Estratégico (PDF) salvo em: {pdf_blueprint_path}\")\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    config[\"status_etapas\"][\"blueprint\"] = True\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(\"\\n✅ GERAÇÃO DO BLUEPRINT FINAL E DASHBOARD CONCLUÍDA!\")\n",
        "    print(\"Todos os relatórios e o dashboard foram gerados com sucesso.\")\n",
        "    print(\"\\n🎉 PROCESSO DE ENGENHARIA REVERSA CONCLUÍDO COM SUCESSO! 🎉\")\n",
        "\n",
        "# Executar geração de blueprint e dashboard\n",
        "try:\n",
        "    gerar_blueprint_dashboard()\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ ERRO GERAL NA GERAÇÃO DO BLUEPRINT E DASHBOARD: {e}\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "hFr8drvBrb23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a5fcdb8-4f40-4113-ca35-87aed715cbdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:57:29] INICIANDO SISTEMA DE DASHBOARD INTELIGENTE\n",
            "[00:57:29] INICIANDO CRIAÇÃO DO DASHBOARD MASTER EXECUTIVO INTELIGENTE\n",
            "[00:57:29] Carregando dados consolidados...\n",
            "[00:57:30] Dados carregados: 3 vídeos encontrados\n",
            "[00:57:30] Processando inteligência artificial dos dados...\n",
            "[00:57:30] Calculando scores de performance...\n",
            "[00:57:30] Gerando insights estratégicos...\n",
            "[00:57:30] Criando estrutura do dashboard...\n",
            "[00:57:30] Criando Executive Summary...\n",
            "[00:57:30] Criando Análise de Performance...\n",
            "[00:57:30] Criando Inteligência Técnica...\n",
            "[00:57:30] Criando Blueprint de Produção...\n",
            "[00:57:30] Criando Recomendações Estratégicas...\n",
            "[00:57:30] Salvando dashboard...\n",
            "[00:57:30] DASHBOARD MASTER EXECUTIVO CRIADO COM SUCESSO!\n",
            "[00:57:30] Arquivo salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dashboard/DASHBOARD_MASTER_EXECUTIVO_INTELIGENTE.xlsx\n",
            "[00:57:30] 3 vídeos analisados\n",
            "[00:57:30] 5 insights estratégicos gerados\n",
            "[00:57:30] 1 recomendações criadas\n",
            "[00:57:30] PROCESSO CONCLUÍDO COM SUCESSO!\n",
            "[00:57:30] Dashboard inteligente pronto para uso estratégico\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CÉLULA 4.3: DASHBOARD MASTER EXECUTIVO INTELIGENTE APRIMORADO\n",
        "# ============================================================================\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "from openpyxl.styles import Font, Alignment, PatternFill\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def log_progress(message):\n",
        "    \"\"\"Log de progresso em tempo real\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
        "    print(f\"[{timestamp}] {message}\")\n",
        "\n",
        "def calculate_viral_score(row):\n",
        "    \"\"\"Calcula score de viralidade baseado em múltiplos fatores\"\"\"\n",
        "    try:\n",
        "        score = 0\n",
        "\n",
        "        # Fator 1: Ritmo (cortes por segundo) - peso 25%\n",
        "        if pd.notna(row['duracao_segundos']) and row['duracao_segundos'] > 0:\n",
        "            cortes_por_seg = row['cortes_detectados_count'] / row['duracao_segundos']\n",
        "            if cortes_por_seg > 20: score += 25\n",
        "            elif cortes_por_seg > 10: score += 20\n",
        "            elif cortes_por_seg > 5: score += 15\n",
        "            else: score += 10\n",
        "\n",
        "        # Fator 2: Complexidade Visual - peso 20%\n",
        "        if pd.notna(row['complexidade_visual_media']):\n",
        "            if row['complexidade_visual_media'] > 600: score += 20\n",
        "            elif row['complexidade_visual_media'] > 400: score += 15\n",
        "            else: score += 10\n",
        "\n",
        "        # Fator 3: Presença de Texto (OCR) - peso 15%\n",
        "        if pd.notna(row['ocr_textos_count']):\n",
        "            if row['ocr_textos_count'] > 10: score += 15\n",
        "            elif row['ocr_textos_count'] > 5: score += 12\n",
        "            elif row['ocr_textos_count'] > 0: score += 8\n",
        "\n",
        "        # Fator 4: Duração Ideal - peso 20%\n",
        "        if pd.notna(row['duracao_segundos']):\n",
        "            if 15 <= row['duracao_segundos'] <= 30: score += 20\n",
        "            elif 10 <= row['duracao_segundos'] <= 45: score += 15\n",
        "            else: score += 10\n",
        "\n",
        "        # Fator 5: Gatilhos Psicológicos - peso 20%\n",
        "        gatilhos = str(row['gatilhos_psicologicos']).lower()\n",
        "        if 'urgência' in gatilhos or 'escassez' in gatilhos: score += 8\n",
        "        if 'estímulo' in gatilhos: score += 7\n",
        "        if 'atenção' in gatilhos: score += 5\n",
        "\n",
        "        return min(score, 100)\n",
        "    except:\n",
        "        return 50\n",
        "\n",
        "def calculate_technical_score(row):\n",
        "    \"\"\"Score técnico baseado em qualidade de produção\"\"\"\n",
        "    try:\n",
        "        score = 0\n",
        "\n",
        "        if pd.notna(row['brilho_medio']):\n",
        "            if 120 <= row['brilho_medio'] <= 180: score += 25\n",
        "            elif 100 <= row['brilho_medio'] <= 200: score += 20\n",
        "            else: score += 10\n",
        "\n",
        "        formato = str(row['formato_detectado'])\n",
        "        if 'vertical_9_16' in formato: score += 25\n",
        "        elif 'horizontal_16_9' in formato: score += 20\n",
        "        else: score += 15\n",
        "\n",
        "        if row['tem_audio']: score += 25\n",
        "        else: score += 5\n",
        "\n",
        "        if pd.notna(row['total_frames']) and row['total_frames'] > 0:\n",
        "            if row['total_frames'] > 300: score += 25\n",
        "            elif row['total_frames'] > 150: score += 20\n",
        "            else: score += 15\n",
        "\n",
        "        return min(score, 100)\n",
        "    except:\n",
        "        return 50\n",
        "\n",
        "def calculate_content_score(row):\n",
        "    \"\"\"Score de conteúdo baseado em riqueza informacional\"\"\"\n",
        "    try:\n",
        "        score = 0\n",
        "\n",
        "        ocr_count = row['ocr_textos_count'] if pd.notna(row['ocr_textos_count']) else 0\n",
        "        audio_len = row['audio_transcrito_len'] if pd.notna(row['audio_transcrito_len']) else 0\n",
        "\n",
        "        if ocr_count > 5 or audio_len > 100: score += 30\n",
        "        elif ocr_count > 2 or audio_len > 50: score += 20\n",
        "        elif ocr_count > 0 or audio_len > 0: score += 15\n",
        "        else: score += 5\n",
        "\n",
        "        if pd.notna(row['bpm_audio']):\n",
        "            if 120 <= row['bpm_audio'] <= 140: score += 35\n",
        "            elif 100 <= row['bpm_audio'] <= 160: score += 25\n",
        "            else: score += 15\n",
        "\n",
        "        if pd.notna(row['duracao_segundos']) and row['duracao_segundos'] > 0:\n",
        "            densidade = (ocr_count + audio_len/10) / row['duracao_segundos']\n",
        "            if densidade > 2: score += 35\n",
        "            elif densidade > 1: score += 25\n",
        "            else: score += 15\n",
        "\n",
        "        return min(score, 100)\n",
        "    except:\n",
        "        return 50\n",
        "\n",
        "def generate_insights_from_data(df):\n",
        "    \"\"\"Gera insights inteligentes baseados nos dados\"\"\"\n",
        "    insights = []\n",
        "\n",
        "    try:\n",
        "        best_performing = df.nlargest(3, 'viral_score')\n",
        "        avg_duration = best_performing['duracao_segundos'].mean()\n",
        "        insights.append(f\"DURAÇÃO VENCEDORA: Seus top 3 vídeos têm duração média de {avg_duration:.1f}s. Este é seu sweet spot comprovado.\")\n",
        "\n",
        "        avg_cuts_per_sec = (best_performing['cortes_detectados_count'] / best_performing['duracao_segundos']).mean()\n",
        "        insights.append(f\"RITMO IDEAL: {avg_cuts_per_sec:.1f} cortes por segundo é sua fórmula de edição mais eficaz.\")\n",
        "\n",
        "        formato_winner = df['formato_detectado'].mode()[0] if not df['formato_detectado'].empty else 'N/A'\n",
        "        formato_count = df['formato_detectado'].value_counts().iloc[0] if not df['formato_detectado'].empty else 0\n",
        "        insights.append(f\"FORMATO DOMINANTE: {formato_count} vídeos em {formato_winner}. Este é seu formato de maior alcance.\")\n",
        "\n",
        "        high_viral = df[df['viral_score'] > 70]\n",
        "        if not high_viral.empty:\n",
        "            avg_complexity = high_viral['complexidade_visual_media'].mean()\n",
        "            insights.append(f\"COMPLEXIDADE VISUAL ÓTIMA: Vídeos com score viral alto têm complexidade média de {avg_complexity:.0f}. Use como referência.\")\n",
        "\n",
        "        text_heavy = df[df['ocr_textos_count'] > 5]\n",
        "        if not text_heavy.empty:\n",
        "            insights.append(f\"ESTRATÉGIA DE TEXTO: {len(text_heavy)} vídeos com muito texto têm score médio de {text_heavy['viral_score'].mean():.0f}. Texto na tela impacta performance.\")\n",
        "\n",
        "        # CORRIGIDO: bpm_audio em vez de bmp_audio\n",
        "        if df['bpm_audio'].notna().any():\n",
        "            successful_bpm = df[df['viral_score'] > 60]['bpm_audio'].mean()\n",
        "            insights.append(f\"BPM DE SUCESSO: {successful_bpm:.0f} BPM é o ritmo de áudio dos seus vídeos mais virais.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_progress(f\"Erro ao gerar insights: {e}\")\n",
        "        insights.append(\"Insights parciais disponíveis devido a limitações nos dados.\")\n",
        "\n",
        "    return insights\n",
        "\n",
        "def add_data_to_sheet(ws, data, start_row=1, start_col=1, headers=None):\n",
        "    \"\"\"Adiciona dados a uma planilha de forma segura\"\"\"\n",
        "    current_row = start_row\n",
        "\n",
        "    # Adicionar cabeçalhos se fornecidos\n",
        "    if headers:\n",
        "        for col_idx, header in enumerate(headers):\n",
        "            cell = ws.cell(row=current_row, column=start_col + col_idx)\n",
        "            cell.value = header\n",
        "            cell.font = Font(bold=True)\n",
        "        current_row += 1\n",
        "\n",
        "    # Adicionar dados\n",
        "    for row_data in data:\n",
        "        for col_idx, value in enumerate(row_data):\n",
        "            cell = ws.cell(row=current_row, column=start_col + col_idx)\n",
        "            cell.value = value\n",
        "        current_row += 1\n",
        "\n",
        "    return current_row\n",
        "\n",
        "def create_enhanced_dashboard_master(csv_path, json_path, output_path):\n",
        "    \"\"\"Cria dashboard master executivo aprimorado\"\"\"\n",
        "\n",
        "    log_progress(\"INICIANDO CRIAÇÃO DO DASHBOARD MASTER EXECUTIVO INTELIGENTE\")\n",
        "\n",
        "    try:\n",
        "        # Carregar dados\n",
        "        log_progress(\"Carregando dados consolidados...\")\n",
        "        df_consolidado = pd.read_csv(csv_path, encoding='utf-8')\n",
        "\n",
        "        with open(json_path, 'r', encoding='utf-8') as f:\n",
        "            dados_detalhados = json.load(f)\n",
        "\n",
        "        log_progress(f\"Dados carregados: {len(df_consolidado)} vídeos encontrados\")\n",
        "\n",
        "        # Pré-processamento inteligente\n",
        "        log_progress(\"Processando inteligência artificial dos dados...\")\n",
        "\n",
        "        # Limpar e converter dados\n",
        "        try:\n",
        "            df_consolidado['emocoes_predominantes'] = df_consolidado['emocoes_predominantes'].apply(\n",
        "                lambda x: json.loads(x.replace(\"'\", '\"')) if pd.notna(x) and x != '{}' else {}\n",
        "            )\n",
        "        except:\n",
        "            df_consolidado['emocoes_predominantes'] = [{}] * len(df_consolidado)\n",
        "\n",
        "        # Calcular scores inteligentes\n",
        "        log_progress(\"Calculando scores de performance...\")\n",
        "        df_consolidado['viral_score'] = df_consolidado.apply(calculate_viral_score, axis=1)\n",
        "        df_consolidado['technical_score'] = df_consolidado.apply(calculate_technical_score, axis=1)\n",
        "        df_consolidado['content_score'] = df_consolidado.apply(calculate_content_score, axis=1)\n",
        "        df_consolidado['overall_score'] = (df_consolidado['viral_score'] + df_consolidado['technical_score'] + df_consolidado['content_score']) / 3\n",
        "\n",
        "        # Calcular métricas avançadas\n",
        "        df_consolidado['cortes_por_segundo'] = df_consolidado['cortes_detectados_count'] / df_consolidado['duracao_segundos'].replace(0, 1)\n",
        "        df_consolidado['densidade_texto'] = df_consolidado['ocr_textos_count'] / df_consolidado['duracao_segundos'].replace(0, 1)\n",
        "        df_consolidado['eficiencia_audio'] = df_consolidado['audio_transcrito_len'] / df_consolidado['duracao_segundos'].replace(0, 1)\n",
        "\n",
        "        log_progress(\"Gerando insights estratégicos...\")\n",
        "        insights = generate_insights_from_data(df_consolidado)\n",
        "\n",
        "        # Criar workbook\n",
        "        log_progress(\"Criando estrutura do dashboard...\")\n",
        "        wb = Workbook()\n",
        "\n",
        "        # === ABA 1: EXECUTIVE SUMMARY ===\n",
        "        log_progress(\"Criando Executive Summary...\")\n",
        "        ws_summary = wb.active\n",
        "        ws_summary.title = 'Executive Summary'\n",
        "\n",
        "        # Header principal\n",
        "        header_cell = ws_summary.cell(row=1, column=1)\n",
        "        header_cell.value = 'DASHBOARD MASTER EXECUTIVO - ENGENHARIA REVERSA DE VÍDEOS'\n",
        "        header_cell.font = Font(bold=True, size=18, color='FFFFFF')\n",
        "        header_cell.fill = PatternFill(start_color='1F4E79', end_color='1F4E79', fill_type='solid')\n",
        "        header_cell.alignment = Alignment(horizontal='center', vertical='center')\n",
        "\n",
        "        # Expandir header manualmente\n",
        "        for col in range(2, 9):\n",
        "            cell = ws_summary.cell(row=1, column=col)\n",
        "            cell.fill = PatternFill(start_color='1F4E79', end_color='1F4E79', fill_type='solid')\n",
        "\n",
        "        # KPIs Principais\n",
        "        kpi_cell = ws_summary.cell(row=3, column=1)\n",
        "        kpi_cell.value = 'INDICADORES DE PERFORMANCE PRINCIPAIS'\n",
        "        kpi_cell.font = Font(bold=True, size=14)\n",
        "        kpi_cell.fill = PatternFill(start_color='E7E6E6', end_color='E7E6E6', fill_type='solid')\n",
        "\n",
        "        kpis_data = [\n",
        "            ['Total de Vídeos Analisados', len(df_consolidado)],\n",
        "            ['Score Viral Médio', f\"{df_consolidado['viral_score'].mean():.1f}/100\"],\n",
        "            ['Score Técnico Médio', f\"{df_consolidado['technical_score'].mean():.1f}/100\"],\n",
        "            ['Score de Conteúdo Médio', f\"{df_consolidado['content_score'].mean():.1f}/100\"],\n",
        "            ['Duração Média Otimizada', f\"{df_consolidado['duracao_segundos'].mean():.1f}s\"],\n",
        "            ['Ritmo Médio de Cortes', f\"{df_consolidado['cortes_por_segundo'].mean():.1f}/seg\"],\n",
        "        ]\n",
        "\n",
        "        add_data_to_sheet(ws_summary, kpis_data, start_row=4, start_col=1)\n",
        "\n",
        "        # Top 3 Vídeos\n",
        "        top3_cell = ws_summary.cell(row=3, column=4)\n",
        "        top3_cell.value = 'TOP 3 VÍDEOS POR PERFORMANCE'\n",
        "        top3_cell.font = Font(bold=True, size=14)\n",
        "        top3_cell.fill = PatternFill(start_color='E7E6E6', end_color='E7E6E6', fill_type='solid')\n",
        "\n",
        "        top3 = df_consolidado.nlargest(3, 'overall_score')[['nome_arquivo', 'overall_score', 'viral_score', 'technical_score', 'content_score']]\n",
        "\n",
        "        top3_data = []\n",
        "        for _, video in top3.iterrows():\n",
        "            nome_curto = video['nome_arquivo'][:30] + \"...\" if len(video['nome_arquivo']) > 30 else video['nome_arquivo']\n",
        "            top3_data.append([\n",
        "                nome_curto,\n",
        "                f\"{video['overall_score']:.1f}\",\n",
        "                f\"{video['viral_score']:.1f}\",\n",
        "                f\"{video['technical_score']:.1f}\",\n",
        "                f\"{video['content_score']:.1f}\"\n",
        "            ])\n",
        "\n",
        "        top3_headers = ['Vídeo', 'Score Geral', 'Viral', 'Técnico', 'Conteúdo']\n",
        "        add_data_to_sheet(ws_summary, top3_data, start_row=4, start_col=4, headers=top3_headers)\n",
        "\n",
        "        # Insights Estratégicos\n",
        "        insights_cell = ws_summary.cell(row=12, column=1)\n",
        "        insights_cell.value = 'INSIGHTS ESTRATÉGICOS BASEADOS EM IA'\n",
        "        insights_cell.font = Font(bold=True, size=14, color='FFFFFF')\n",
        "        insights_cell.fill = PatternFill(start_color='C5504B', end_color='C5504B', fill_type='solid')\n",
        "        insights_cell.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Adicionar insights\n",
        "        for i, insight in enumerate(insights, 13):\n",
        "            insight_cell = ws_summary.cell(row=i, column=1)\n",
        "            insight_cell.value = f\"• {insight}\"\n",
        "            insight_cell.alignment = Alignment(wrap_text=True)\n",
        "\n",
        "        # === ABA 2: ANÁLISE DE PERFORMANCE ===\n",
        "        log_progress(\"Criando Análise de Performance...\")\n",
        "        ws_performance = wb.create_sheet('Análise de Performance')\n",
        "\n",
        "        perf_header = ws_performance.cell(row=1, column=1)\n",
        "        perf_header.value = 'ANÁLISE DETALHADA DE PERFORMANCE'\n",
        "        perf_header.font = Font(bold=True, size=16)\n",
        "        perf_header.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Ranking completo\n",
        "        ranking_data = df_consolidado[['nome_arquivo', 'overall_score', 'viral_score', 'technical_score', 'content_score',\n",
        "                                     'duracao_segundos', 'cortes_por_segundo', 'formato_detectado']].sort_values('overall_score', ascending=False)\n",
        "\n",
        "        ranking_list = []\n",
        "        for _, video in ranking_data.iterrows():\n",
        "            nome_curto = video['nome_arquivo'][:40] + \"...\" if len(video['nome_arquivo']) > 40 else video['nome_arquivo']\n",
        "            ranking_list.append([\n",
        "                nome_curto,\n",
        "                f\"{video['overall_score']:.1f}\",\n",
        "                f\"{video['viral_score']:.1f}\",\n",
        "                f\"{video['technical_score']:.1f}\",\n",
        "                f\"{video['content_score']:.1f}\",\n",
        "                f\"{video['duracao_segundos']:.1f}s\",\n",
        "                f\"{video['cortes_por_segundo']:.1f}\",\n",
        "                video['formato_detectado']\n",
        "            ])\n",
        "\n",
        "        ranking_headers = ['Vídeo', 'Score Geral', 'Viral', 'Técnico', 'Conteúdo', 'Duração', 'Cortes/s', 'Formato']\n",
        "        add_data_to_sheet(ws_performance, ranking_list, start_row=3, start_col=1, headers=ranking_headers)\n",
        "\n",
        "        # === ABA 3: INTELIGÊNCIA TÉCNICA ===\n",
        "        log_progress(\"Criando Inteligência Técnica...\")\n",
        "        ws_tecnica = wb.create_sheet('Inteligência Técnica')\n",
        "\n",
        "        tec_header = ws_tecnica.cell(row=1, column=1)\n",
        "        tec_header.value = 'ANÁLISE TÉCNICA AVANÇADA'\n",
        "        tec_header.font = Font(bold=True, size=16)\n",
        "        tec_header.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Análise de correlações\n",
        "        corr_header = ws_tecnica.cell(row=3, column=1)\n",
        "        corr_header.value = 'CORRELAÇÕES DESCOBERTAS'\n",
        "        corr_header.font = Font(bold=True, size=12)\n",
        "\n",
        "        correlations_data = [\n",
        "            ['Duração vs Score Viral', f\"{df_consolidado['duracao_segundos'].corr(df_consolidado['viral_score']):.3f}\", 'CORRELAÇÃO MODERADA'],\n",
        "            ['Cortes/s vs Score Viral', f\"{df_consolidado['cortes_por_segundo'].corr(df_consolidado['viral_score']):.3f}\", 'CORRELAÇÃO MODERADA'],\n",
        "            ['Complexidade Visual vs Performance', f\"{df_consolidado['complexidade_visual_media'].corr(df_consolidado['overall_score']):.3f}\", 'CORRELAÇÃO FRACA'],\n",
        "            ['BPM vs Engajamento', f\"{df_consolidado['bpm_audio'].corr(df_consolidado['viral_score']) if df_consolidado['bpm_audio'].notna().any() else 0:.3f}\", 'CORRELAÇÃO FRACA'],\n",
        "        ]\n",
        "\n",
        "        corr_headers = ['Métrica', 'Correlação', 'Classificação']\n",
        "        add_data_to_sheet(ws_tecnica, correlations_data, start_row=4, start_col=1, headers=corr_headers)\n",
        "\n",
        "        # === ABA 4: BLUEPRINT DE PRODUÇÃO ===\n",
        "        log_progress(\"Criando Blueprint de Produção...\")\n",
        "        ws_blueprint = wb.create_sheet('Blueprint de Produção')\n",
        "\n",
        "        bp_header = ws_blueprint.cell(row=1, column=1)\n",
        "        bp_header.value = 'BLUEPRINT ESTRATÉGICO DE PRODUÇÃO'\n",
        "        bp_header.font = Font(bold=True, size=16, color='FFFFFF')\n",
        "        bp_header.fill = PatternFill(start_color='1F4E79', end_color='1F4E79', fill_type='solid')\n",
        "        bp_header.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Receita de sucesso baseada nos top performers\n",
        "        top_performers = df_consolidado[df_consolidado['overall_score'] > df_consolidado['overall_score'].quantile(0.7)]\n",
        "\n",
        "        blueprint_data = [\n",
        "            ['DURAÇÃO IDEAL', f\"{top_performers['duracao_segundos'].mean():.1f} segundos (±{top_performers['duracao_segundos'].std():.1f}s)\"],\n",
        "            ['RITMO DE EDIÇÃO', f\"{top_performers['cortes_por_segundo'].mean():.1f} cortes por segundo\"],\n",
        "            ['FORMATO VENCEDOR', top_performers['formato_detectado'].mode()[0] if not top_performers.empty else 'N/A'],\n",
        "            ['COMPLEXIDADE VISUAL', f\"Nível {top_performers['complexidade_visual_media'].mean():.0f} (escala de estímulo)\"],\n",
        "            ['BPM RECOMENDADO', f\"{top_performers['bpm_audio'].mean():.0f} BPM\" if top_performers['bpm_audio'].notna().any() else 'N/A'],\n",
        "            ['DENSIDADE DE TEXTO', f\"{top_performers['densidade_texto'].mean():.1f} textos por segundo\"],\n",
        "        ]\n",
        "\n",
        "        bp_sub_header = ws_blueprint.cell(row=3, column=1)\n",
        "        bp_sub_header.value = 'FÓRMULA DE SUCESSO BASEADA EM DADOS'\n",
        "        bp_sub_header.font = Font(bold=True, size=12)\n",
        "\n",
        "        add_data_to_sheet(ws_blueprint, blueprint_data, start_row=4, start_col=1)\n",
        "\n",
        "        # === ABA 5: RECOMENDAÇÕES ESTRATÉGICAS ===\n",
        "        log_progress(\"Criando Recomendações Estratégicas...\")\n",
        "        ws_recomendacoes = wb.create_sheet('Recomendações Estratégicas')\n",
        "\n",
        "        rec_header = ws_recomendacoes.cell(row=1, column=1)\n",
        "        rec_header.value = 'RECOMENDAÇÕES ESTRATÉGICAS BASEADAS EM IA'\n",
        "        rec_header.font = Font(bold=True, size=16, color='FFFFFF')\n",
        "        rec_header.fill = PatternFill(start_color='C5504B', end_color='C5504B', fill_type='solid')\n",
        "        rec_header.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Recomendações inteligentes baseadas nos dados\n",
        "        recommendations = []\n",
        "\n",
        "        # Análise de duração\n",
        "        if df_consolidado['duracao_segundos'].mean() > 60:\n",
        "            recommendations.append(['DURAÇÃO', 'REDUZA DURAÇÃO', 'Seus vídeos estão longos demais. Vídeos de 15-30s têm melhor performance.', 'ALTA'])\n",
        "        elif df_consolidado['duracao_segundos'].mean() < 15:\n",
        "            recommendations.append(['DURAÇÃO', 'AUMENTE DURAÇÃO', 'Vídeos muito curtos podem não transmitir valor suficiente.', 'MÉDIA'])\n",
        "\n",
        "        # Análise de ritmo\n",
        "        avg_cuts_per_sec = df_consolidado['cortes_por_segundo'].mean()\n",
        "        if avg_cuts_per_sec < 5:\n",
        "            recommendations.append(['EDIÇÃO', 'ACELERE O RITMO', 'Aumente o número de cortes para manter atenção. Meta: 8-12 cortes/segundo.', 'ALTA'])\n",
        "        elif avg_cuts_per_sec > 20:\n",
        "            recommendations.append(['EDIÇÃO', 'DIMINUA CORTES', 'Muitos cortes podem causar fadiga visual. Encontre o equilíbrio.', 'MÉDIA'])\n",
        "\n",
        "        # Análise de formato\n",
        "        formato_dominante = df_consolidado['formato_detectado'].mode()[0] if not df_consolidado['formato_detectado'].empty else 'N/A'\n",
        "        if 'horizontal' in formato_dominante.lower():\n",
        "            recommendations.append(['FORMATO', 'FOQUE EM VERTICAL', 'Formato vertical (9:16) tem melhor performance em redes sociais.', 'ALTA'])\n",
        "\n",
        "        # Análise de texto\n",
        "        if df_consolidado['densidade_texto'].mean() < 1:\n",
        "            recommendations.append(['CONTEÚDO', 'ADICIONE MAIS TEXTO', 'Textos na tela aumentam retenção e acessibilidade.', 'MÉDIA'])\n",
        "\n",
        "        rec_headers = ['Categoria', 'Ação', 'Justificativa', 'Prioridade']\n",
        "        add_data_to_sheet(ws_recomendacoes, recommendations, start_row=3, start_col=1, headers=rec_headers)\n",
        "\n",
        "        # Salvar arquivo\n",
        "        log_progress(\"Salvando dashboard...\")\n",
        "        wb.save(output_path)\n",
        "\n",
        "        log_progress(\"DASHBOARD MASTER EXECUTIVO CRIADO COM SUCESSO!\")\n",
        "        log_progress(f\"Arquivo salvo em: {output_path}\")\n",
        "        log_progress(f\"{len(df_consolidado)} vídeos analisados\")\n",
        "        log_progress(f\"{len(insights)} insights estratégicos gerados\")\n",
        "        log_progress(f\"{len(recommendations)} recomendações criadas\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        log_progress(f\"ERRO CRÍTICO: {e}\")\n",
        "        log_progress(\"Verifique os arquivos de entrada e tente novamente\")\n",
        "        return False\n",
        "\n",
        "def main():\n",
        "    \"\"\"Função principal de execução\"\"\"\n",
        "    log_progress(\"INICIANDO SISTEMA DE DASHBOARD INTELIGENTE\")\n",
        "\n",
        "    # Configurar caminhos\n",
        "    BASE_PATH = \"/content/drive/MyDrive/Videos Dona Done/_engenharia_reversa\"\n",
        "    CSV_PATH = os.path.join(BASE_PATH, \"dashboard\", \"dados_consolidados.csv\")\n",
        "    JSON_PATH = os.path.join(BASE_PATH, \"dashboard\", \"dados_detalhados.json\")\n",
        "    OUTPUT_PATH = os.path.join(BASE_PATH, \"dashboard\", \"DASHBOARD_MASTER_EXECUTIVO_INTELIGENTE.xlsx\")\n",
        "\n",
        "    # Verificar se arquivos existem\n",
        "    if not os.path.exists(CSV_PATH):\n",
        "        log_progress(f\"ERRO: Arquivo CSV não encontrado: {CSV_PATH}\")\n",
        "        return False\n",
        "\n",
        "    if not os.path.exists(JSON_PATH):\n",
        "        log_progress(f\"ERRO: Arquivo JSON não encontrado: {JSON_PATH}\")\n",
        "        return False\n",
        "\n",
        "    # Executar criação do dashboard\n",
        "    success = create_enhanced_dashboard_master(CSV_PATH, JSON_PATH, OUTPUT_PATH)\n",
        "\n",
        "    if success:\n",
        "        log_progress(\"PROCESSO CONCLUÍDO COM SUCESSO!\")\n",
        "        log_progress(\"Dashboard inteligente pronto para uso estratégico\")\n",
        "    else:\n",
        "        log_progress(\"PROCESSO FALHOU - Verifique os logs acima\")\n",
        "\n",
        "    return success\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "3QgJMmh1JJ-0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3536bd55-70ac-4ff7-d141-9c590232e8ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 VERIFICANDO PRÉ-REQUISITOS...\n",
            "Pasta base existe: True\n",
            "CSV existe: True\n",
            "JSON existe: True\n",
            "📊 Dados CSV: 3 vídeos encontrados\n",
            "\n",
            "✅ Se todos os itens acima são True/existem, você pode prosseguir!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Verificar se o processo de engenharia reversa foi executado\n",
        "BASE_PATH = \"/content/drive/MyDrive/Videos Dona Done/_engenharia_reversa\"\n",
        "CSV_PATH = os.path.join(BASE_PATH, \"dashboard\", \"dados_consolidados.csv\")\n",
        "JSON_PATH = os.path.join(BASE_PATH, \"dashboard\", \"dados_detalhados.json\")\n",
        "\n",
        "print(\"🔍 VERIFICANDO PRÉ-REQUISITOS...\")\n",
        "print(f\"Pasta base existe: {os.path.exists(BASE_PATH)}\")\n",
        "print(f\"CSV existe: {os.path.exists(CSV_PATH)}\")\n",
        "print(f\"JSON existe: {os.path.exists(JSON_PATH)}\")\n",
        "\n",
        "if os.path.exists(CSV_PATH):\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "    print(f\"📊 Dados CSV: {len(df)} vídeos encontrados\")\n",
        "\n",
        "print(\"\\n✅ Se todos os itens acima são True/existem, você pode prosseguir!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyhMy4Pbt7Y-",
        "outputId": "2ae9c517-e9d8-45b7-b971-9a1e7880dc4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 INICIANDO INTEGRAÇÃO AUTOMÁTICA DE TODAS AS ANÁLISES\n",
            "============================================================\n",
            "\n",
            "🔍 DESCOBRINDO ANÁLISES DISPONÍVEIS...\n",
            "✅ Análise base encontrada: metadados\n",
            "✅ Análise base encontrada: decomposicao\n",
            "✅ Análise base encontrada: padroes\n",
            "✅ Análise base encontrada: psicologica\n",
            "✅ Análise personalizada encontrada: Ai Insights Completos\n",
            "✅ Análise personalizada encontrada: Videos Descobertos\n",
            "✅ Análise adicional encontrada: Audio Refinada\n",
            "✅ Análise adicional encontrada: Viral\n",
            "✅ Análise adicional encontrada: Copywriting\n",
            "✅ Análise personalizada encontrada: Legendas Geradas\n",
            "📊 Total de análises encontradas: 10\n",
            "\n",
            "🔄 CONSOLIDANDO TODOS OS DADOS...\n",
            "DataFrame base criado com 3 vídeos e 21 colunas.\n",
            "🔄 Integrando dados de: decomposicao\n",
            "Merge com decomposicao concluído. Total de colunas: 23\n",
            "🔄 Integrando dados de: padroes\n",
            "Merge com padroes concluído. Total de colunas: 31\n",
            "🔄 Integrando dados de: psicologica\n",
            "Merge com psicologica concluído. Total de colunas: 35\n",
            "🔄 Integrando dados de: ai_insights_completos.json\n",
            "Merge com ai_insights_completos.json concluído. Total de colunas: 42\n",
            "🔄 Integrando dados de: videos_descobertos.json\n",
            "Merge com videos_descobertos.json concluído. Total de colunas: 45\n",
            "🔄 Integrando dados de: audio_refinada\n",
            "Merge com audio_refinada concluído. Total de colunas: 81\n",
            "🔄 Integrando dados de: viral\n",
            "🔄 Integrando dados de: copywriting\n",
            "Merge com copywriting concluído. Total de colunas: 96\n",
            "🔄 Integrando dados de: legendas_geradas.json\n",
            "Merge com legendas_geradas.json concluído. Total de colunas: 98\n",
            "🧹 Flattening nested structures for Excel compatibility...\n",
            "📈 6 vídeos consolidados com 104 métricas totais\n",
            "\n",
            "📊 GERANDO DASHBOARD DINÂMICO...\n",
            "📊 Tentando integrar aba de análise viral...\n",
            "✅ Aba de Análise Viral adicionada ao dashboard\n",
            "🔗 Integração viral concluída\n",
            "\n",
            "💾 SALVANDO DADOS CONSOLIDADOS...\n",
            "📁 Dados CSV salvos: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dashboard/dados_completos_consolidados.csv\n",
            "📁 Dados JSON salvos: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dashboard/dados_completos_consolidados.json\n",
            "\n",
            "⚙️ ATUALIZANDO CONFIGURAÇÕES...\n",
            "✅ Arquivo config.json atualizado com sucesso.\n",
            "\n",
            "✅ INTEGRAÇÃO AUTOMÁTICA CONCLUÍDA COM SUCESSO!\n",
            "============================================================\n",
            "📁 Dashboard dinâmico completo: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dashboard/RELATORIO_COMPLETO_DINAMICO.xlsx\n",
            "📊 6 vídeos processados com 104 métricas totais integradas.\n",
            "\n",
            "🎯 PRÓXIMOS PASSOS:\n",
            "• Abra o arquivo Excel gerado para explorar todas as análises integradas.\n",
            "• Use os arquivos CSV/JSON para análises mais avançadas em outras ferramentas.\n",
            "• Execute esta célula novamente sempre que adicionar novas análises ao seu projeto.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# SISTEMA DE INTEGRAÇÃO AUTOMÁTICA PARA NOVAS FUNCIONALIDADES\n",
        "# ============================================================================\n",
        "# Este script deve SUBSTITUIR a última célula (4.2) do notebook\n",
        "# Ele detecta automaticamente todas as análises disponíveis e as integra\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import glob\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ADICIONAR ESTAS FUNÇÕES:\n",
        "def detectar_dados_virais_disponiveis(pasta_trabalho):\n",
        "    \"\"\"Detecta se dados virais estão disponíveis\"\"\"\n",
        "    try:\n",
        "        viral_integration_path = os.path.join(pasta_trabalho, \"dados\", \"viral_integration_data.json\")\n",
        "        if not os.path.exists(viral_integration_path):\n",
        "            return False, None\n",
        "        with open(viral_integration_path, 'r', encoding='utf-8') as f:\n",
        "            viral_data = json.load(f)\n",
        "        return viral_data.get('viral_disponivel', False), viral_data\n",
        "    except Exception:\n",
        "        return False, None\n",
        "\n",
        "def enriquecer_consolidacao_com_dados_virais(df_consolidado, pasta_trabalho):\n",
        "    \"\"\"Enriquece o DataFrame consolidado com dados virais, se disponíveis\"\"\"\n",
        "    viral_disponivel, viral_integration = detectar_dados_virais_disponiveis(pasta_trabalho)\n",
        "\n",
        "    if not viral_disponivel:\n",
        "        print(\"⚠️ Dados virais não disponíveis para enriquecer o dashboard.\")\n",
        "        return df_consolidado\n",
        "\n",
        "    try:\n",
        "        # Carregar dados de mapeamento viral completo\n",
        "        mapeamento_path = os.path.join(pasta_trabalho, \"analise_viral\", \"mapeamento_viral_completo.json\")\n",
        "        if not os.path.exists(mapeamento_path):\n",
        "            print(f\"⚠️ Arquivo de mapeamento viral não encontrado: {mapeamento_path}\")\n",
        "            return df_consolidado\n",
        "\n",
        "        with open(mapeamento_path, 'r', encoding='utf-8') as f:\n",
        "            mapeamento_viral = json.load(f)\n",
        "\n",
        "        # Converter para DataFrame para facilitar o merge\n",
        "        df_viral = pd.DataFrame(mapeamento_viral)\n",
        "\n",
        "        # Selecionar e renomear colunas relevantes para o merge\n",
        "        cols_viral = ['video_id', 'tem_dados_virais', 'classificacao_performance']\n",
        "        if 'viral_data' in df_viral.columns:\n",
        "             # Extrair dados aninhados\n",
        "            df_viral = pd.json_normalize(mapeamento_viral, sep='_')\n",
        "            cols_viral = [col for col in df_viral.columns if col.startswith(('video_id', 'tem_dados_virais', 'classificacao_performance', 'viral_data_', 'metricas_calculadas_'))]\n",
        "            # Renomear colunas aninhadas para algo mais legível se necessário, ex: viral_data_views -> views_viral\n",
        "            df_viral.columns = df_viral.columns.str.replace('viral_data_', '').str.replace('metricas_calculadas_', '')\n",
        "\n",
        "\n",
        "        df_viral_subset = df_viral[cols_viral].copy()\n",
        "\n",
        "        # Garantir que a coluna de merge tem o mesmo nome\n",
        "        if 'video_id' in df_consolidado.columns:\n",
        "            df_merged = pd.merge(df_consolidado, df_viral_subset, on='video_id', how='left', suffixes=('', '_viral'))\n",
        "        elif 'id' in df_consolidado.columns:\n",
        "             # Tentar merge por 'id' se 'video_id' não existir no df consolidado\n",
        "            df_merged = pd.merge(df_consolidado, df_viral_subset, left_on='id', right_on='video_id', how='left', suffixes=('', '_viral'))\n",
        "            # Remover coluna 'video_id' duplicada se merge foi por 'id'\n",
        "            if 'video_id_viral' in df_merged.columns:\n",
        "                df_merged = df_merged.drop(columns=['video_id_viral'])\n",
        "        else:\n",
        "            print(\"⚠️ Coluna 'video_id' ou 'id' não encontrada no DataFrame consolidado. Não foi possível integrar dados virais.\")\n",
        "            return df_consolidado\n",
        "\n",
        "\n",
        "        # Tratar valores NaN após o merge (para colunas numéricas de viral)\n",
        "        for col in df_merged.columns:\n",
        "            if col.startswith(('views', 'likes', 'comments', 'engagement_rate', 'virality_score')) and df_merged[col].dtype == 'float64':\n",
        "                 df_merged[col] = df_merged[col].fillna(0).astype(int if 'rate' not in col else float)\n",
        "\n",
        "\n",
        "        # Preencher status de dados virais para vídeos sem match\n",
        "        if 'tem_dados_virais' not in df_merged.columns:\n",
        "             df_merged['tem_dados_virais'] = False\n",
        "             df_merged['classificacao_performance'] = 'Sem Dados'\n",
        "        else:\n",
        "            df_merged['tem_dados_virais'] = df_merged['tem_dados_virais'].fillna(False)\n",
        "            df_merged['classificacao_performance'] = df_merged['classificacao_performance'].fillna('Sem Dados')\n",
        "\n",
        "\n",
        "        print(\"✅ Dados virais integrados ao DataFrame consolidado.\")\n",
        "        return df_merged\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erro ao enriquecer com dados virais: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return df_consolidado # Retorna o original em caso de erro\n",
        "\n",
        "\n",
        "def criar_aba_analise_viral_dashboard(wb, pasta_trabalho):\n",
        "    \"\"\"Cria uma aba dedicada à análise viral no dashboard\"\"\"\n",
        "    viral_disponivel, viral_integration = detectar_dados_virais_disponiveis(pasta_trabalho)\n",
        "\n",
        "    if not viral_disponivel:\n",
        "        print(\"⚠️ Dados virais não disponíveis. Aba de análise viral não será criada.\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        # Carregar insights virais completos\n",
        "        insights_path = os.path.join(pasta_trabalho, \"analise_viral\", \"insights_virais.json\")\n",
        "        if not os.path.exists(insights_path):\n",
        "             print(f\"⚠️ Arquivo de insights virais não encontrado: {insights_path}\")\n",
        "             return False\n",
        "\n",
        "        with open(insights_path, 'r', encoding='utf-8') as f:\n",
        "            insights = json.load(f)\n",
        "\n",
        "        # Carregar dados de mapeamento viral completo para detalhes por vídeo\n",
        "        mapeamento_path = os.path.join(pasta_trabalho, \"analise_viral\", \"mapeamento_viral_completo.json\")\n",
        "        mapeamento_viral = []\n",
        "        if os.path.exists(mapeamento_path):\n",
        "            with open(mapeamento_path, 'r', encoding='utf-8') as f:\n",
        "                mapeamento_viral = json.load(f)\n",
        "\n",
        "        # Remover aba antiga se existir\n",
        "        if \"Análise Viral Completa\" in wb.sheetnames:\n",
        "            del wb[\"Análise Viral Completa\"]\n",
        "\n",
        "        ws = wb.create_sheet(\"Análise Viral Completa\")\n",
        "\n",
        "        from openpyxl.styles import Font, PatternFill, Alignment\n",
        "        from openpyxl.utils import get_column_letter # Import get_column_letter here\n",
        "\n",
        "\n",
        "        # Título principal\n",
        "        ws.merge_cells(\"A1:H1\")\n",
        "        titulo = ws[\"A1\"]\n",
        "        titulo.value = \"📊 ANÁLISE VIRAL ESTRATÉGICA\"\n",
        "        titulo.fill = PatternFill(start_color=\"660066\", end_color=\"660066\", fill_type=\"solid\") # Roxo\n",
        "        titulo.font = Font(color=\"FFFFFF\", bold=True, size=16)\n",
        "        titulo.alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "        row = 3\n",
        "\n",
        "        # Resumo Executivo\n",
        "        ws[f\"A{row}\"] = \"🎯 RESUMO EXECUTIVO\"\n",
        "        ws[f\"A{row}\"].font = Font(bold=True, size=14, color=\"333333\")\n",
        "        row += 2\n",
        "\n",
        "        resumo = insights.get('resumo_geral', {})\n",
        "        if resumo:\n",
        "            resumo_data = [\n",
        "                (\"Total de Vídeos Analisados:\", resumo.get('total_videos', 0)),\n",
        "                (\"Views Totais Coletadas:\", f\"{resumo.get('total_views', 0):,}\"),\n",
        "                (\"Likes Totais:\", f\"{resumo.get('total_likes', 0):,}\"),\n",
        "                (\"Comentários Totais:\", f\"{resumo.get('total_comments', 0):,}\"),\n",
        "                (\"Engagement Rate Médio:\", f\"{resumo.get('avg_engagement_rate', 0):.2%}\"),\n",
        "                (\"Score de Viralidade Médio:\", f\"{resumo.get('avg_virality_score', 0):.1f}/100\"),\n",
        "                (\"Views por Vídeo (Médio):\", f\"{resumo.get('views_per_video', 0):,}\"),\n",
        "            ]\n",
        "            for label, value in resumo_data:\n",
        "                ws[f\"A{row}\"] = label\n",
        "                ws[f\"B{row}\"] = value\n",
        "                ws[f\"A{row}\"].font = Font(bold=True)\n",
        "                row += 1\n",
        "        else:\n",
        "            ws[f\"A{row}\"] = \"⚠️ Resumo viral não disponível\"\n",
        "            row += 1\n",
        "\n",
        "        row += 2\n",
        "\n",
        "        # Top Performers\n",
        "        ws[f\"A{row}\"] = \"🏆 TOP PERFORMERS\"\n",
        "        ws[f\"A{row}\"].font = Font(bold=True, size=14, color=\"333333\")\n",
        "        row += 2\n",
        "\n",
        "        top_p = insights.get('top_performers', {})\n",
        "        if top_p:\n",
        "            top_data = [\n",
        "                (\"TOP 5 Views:\", [f\"{vid} ({views:,})\" for vid, views in top_p.get('top_views', [])]),\n",
        "                (\"TOP 5 Engagement Rate:\", [f\"{vid} ({rate:.2%})\" for vid, rate in top_p.get('top_engagement', [])]),\n",
        "                (\"TOP 5 Virality Score:\", [f\"{vid} ({score:.1f})\" for vid, score in top_p.get('top_virality', [])]),\n",
        "            ]\n",
        "            for label, videos in top_data:\n",
        "                ws[f\"A{row}\"] = label\n",
        "                ws[f\"B{row}\"] = \" • \" + \"\\n • \".join(videos)\n",
        "                ws[f\"A{row}\"].font = Font(bold=True)\n",
        "                ws[f\"B{row}\"].alignment = Alignment(wrap_text=True)\n",
        "                row += 1\n",
        "        else:\n",
        "            ws[f\"A{row}\"] = \"⚠️ Top performers não disponíveis\"\n",
        "            row += 1\n",
        "\n",
        "        row += 2\n",
        "\n",
        "        # Análise de Captions\n",
        "        ws[f\"A{row}\"] = \"📝 ANÁLISE DE CAPTIONS\"\n",
        "        ws[f\"A{row}\"].font = Font(bold=True, size=14, color=\"333333\")\n",
        "        row += 2\n",
        "\n",
        "        captions_a = insights.get('analise_captions', {})\n",
        "        if captions_a:\n",
        "            caption_data = [\n",
        "                (\"Tamanho Médio da Caption:\", f\"{captions_a.get('tamanho_medio_caption', 0):.0f} caracteres\"),\n",
        "                (\"Tamanho Ótimo (Mediana):\", f\"{captions_a.get('tamanho_otimo_caption', 0):.0f} caracteres\"),\n",
        "                (\"Emojis Médio por Caption:\", f\"{captions_a.get('emojis_medio', 0):.1f}\"),\n",
        "                (\"Hashtags Médio por Caption:\", f\"{captions_a.get('hashtags_medio', 0):.1f}\"),\n",
        "                (\"Uso de CTA na Caption:\", f\"{captions_a.get('cta_usage_rate', 0):.0%}\"),\n",
        "                (\"Hook Vencedor:\", captions_a.get('hook_vencedor', 'N/A')),\n",
        "                (\"Ranking de Hooks:\", \", \".join([f\"{k} ({v})\" for k,v in captions_a.get('hook_types_ranking', {}).items()]))\n",
        "            ]\n",
        "            for label, value in caption_data:\n",
        "                ws[f\"A{row}\"] = label\n",
        "                ws[f\"B{row}\"] = value\n",
        "                ws[f\"A{row}\"].font = Font(bold=True)\n",
        "                if \"Ranking\" in label:\n",
        "                     ws[f\"B{row}\"].alignment = Alignment(wrap_text=True)\n",
        "                row += 1\n",
        "        else:\n",
        "            ws[f\"A{row}\"] = \"⚠️ Análise de captions não disponível\"\n",
        "            row += 1\n",
        "\n",
        "        row += 2\n",
        "\n",
        "        # Recomendações Estratégicas\n",
        "        ws[f\"A{row}\"] = \"💡 RECOMENDAÇÕES ESTRATÉGICAS\"\n",
        "        ws[f\"A{row}\"].font = Font(bold=True, size=14, color=\"333333\")\n",
        "        row += 2\n",
        "\n",
        "        recomendacoes = insights.get('recomendacoes_estrategicas', [])\n",
        "        if recomendacoes:\n",
        "            for i, rec in enumerate(recomendacoes, 1):\n",
        "                ws[f\"A{row}\"] = f\"{i}.\"\n",
        "                ws[f\"B{row}\"] = rec\n",
        "                ws[f\"B{row}\"].alignment = Alignment(wrap_text=True)\n",
        "                row += 1\n",
        "        else:\n",
        "            ws[f\"A{row}\"] = \"✅ Nenhuma recomendação específica gerada (boa performance geral)\"\n",
        "            row += 1\n",
        "\n",
        "        row += 2\n",
        "\n",
        "        # Dados Detalhados por Vídeo (Tabela)\n",
        "        if mapeamento_viral:\n",
        "            ws[f\"A{row}\"] = \"📊 DADOS DETALHADOS POR VÍDEO\"\n",
        "            ws[f\"A{row}\"].font = Font(bold=True, size=14, color=\"333333\")\n",
        "            row += 2\n",
        "\n",
        "            # Preparar dados para a tabela\n",
        "            df_viral_detalhes = pd.DataFrame(mapeamento_viral)\n",
        "\n",
        "            # Selecionar colunas para a tabela no Excel\n",
        "            cols_tabela = [\n",
        "                'video_id', 'nome_arquivo', 'tem_dados_virais', 'classificacao_performance',\n",
        "                'viral_data_views', 'viral_data_likes', 'viral_data_comments', 'viral_data_total_engagement',\n",
        "                'metricas_calculadas_engagement_rate', 'metricas_calculadas_virality_score',\n",
        "                'analise_caption_tamanho', 'analise_caption_emojis_count', 'analise_caption_hashtags_count',\n",
        "                'analise_caption_tem_call_to_action', 'analise_caption_hook_type'\n",
        "            ]\n",
        "\n",
        "            # Garantir que as colunas existem antes de selecionar\n",
        "            cols_tabela_existentes = [col for col in cols_tabela if col in df_viral_detalhes.columns]\n",
        "            df_tabela = df_viral_detalhes[cols_tabela_existentes].copy()\n",
        "\n",
        "            # Renomear colunas para o Excel\n",
        "            df_tabela.columns = [\n",
        "                'ID Vídeo', 'Nome Arquivo', 'Tem Dados', 'Performance',\n",
        "                'Views', 'Likes', 'Comentários', 'Engajamento Total',\n",
        "                'Engagement Rate', 'Score Viral',\n",
        "                'Caption Tamanho', 'Caption Emojis', 'Caption Hashtags',\n",
        "                'Caption Tem CTA', 'Caption Hook Tipo'\n",
        "            ][:len(df_tabela.columns)] # Limitar nomes se houver menos colunas\n",
        "\n",
        "            # Formatar colunas numéricas e booleanas\n",
        "            for col in ['Views', 'Likes', 'Comentários', 'Engajamento Total']:\n",
        "                if col in df_tabela.columns:\n",
        "                    df_tabela[col] = df_tabela[col].apply(lambda x: f\"{int(x):,}\" if pd.notna(x) else 'N/A')\n",
        "            for col in ['Engagement Rate']:\n",
        "                 if col in df_tabela.columns:\n",
        "                    df_tabela[col] = df_tabela[col].apply(lambda x: f\"{x:.2%}\" if pd.notna(x) else 'N/A')\n",
        "            for col in ['Score Viral', 'Caption Tamanho', 'Caption Emojis', 'Caption Hashtags']:\n",
        "                 if col in df_tabela.columns:\n",
        "                     df_tabela[col] = df_tabela[col].apply(lambda x: f\"{x:.1f}\" if pd.notna(x) else 'N/A')\n",
        "            for col in ['Tem Dados', 'Caption Tem CTA']:\n",
        "                 if col in df_tabela.columns:\n",
        "                     df_tabela[col] = df_tabela[col].apply(lambda x: 'Sim' if x else 'Não')\n",
        "\n",
        "\n",
        "            # Adicionar cabeçalhos da tabela\n",
        "            headers_tabela = list(df_tabela.columns)\n",
        "            for col_idx, header in enumerate(headers_tabela, 1):\n",
        "                cell = ws.cell(row=row, column=col_idx, value=header)\n",
        "                cell.font = Font(bold=True)\n",
        "                cell.fill = PatternFill(start_color=\"E7E6E6\", end_color=\"E7E6E6\", fill_type=\"solid\")\n",
        "            row += 1\n",
        "\n",
        "            # Adicionar dados da tabela\n",
        "            for r_idx, data_row in df_tabela.iterrows():\n",
        "                for c_idx, value in enumerate(data_row.tolist(), 1):\n",
        "                    ws.cell(row=row, column=c_idx, value=value)\n",
        "                row += 1\n",
        "\n",
        "            # Ajustar larguras das colunas da tabela\n",
        "            col_widths_tabela = [15, 25, 12, 15, 12, 12, 12, 15, 15, 12, 12, 12, 12, 12, 15]\n",
        "            for i, width in enumerate(col_widths_tabela[:len(headers_tabela)], 1):\n",
        "                ws.column_dimensions[get_column_letter(i)].width = width\n",
        "\n",
        "        else:\n",
        "            ws[f\"A{row}\"] = \"⚠️ Dados detalhados por vídeo não disponíveis\"\n",
        "            row += 1\n",
        "\n",
        "\n",
        "        # Ajustar larguras das colunas principais (resumo, top performers etc.)\n",
        "        col_widths_resumo = [30, 60] # Ajuste conforme necessário\n",
        "        for i, width in enumerate(col_widths_resumo, 1):\n",
        "             ws.column_dimensions[get_column_letter(i)].width = width\n",
        "\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erro ao criar aba de análise viral: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "\n",
        "def descobrir_analises_disponiveis(pasta_trabalho):\n",
        "    \"\"\"Descobre automaticamente todas as análises realizadas\"\"\"\n",
        "    analises_encontradas = {\n",
        "        \"base\": {},\n",
        "        \"adicionais\": {}\n",
        "    }\n",
        "\n",
        "    dados_path = os.path.join(pasta_trabalho, \"dados\")\n",
        "\n",
        "    # Análises básicas obrigatórias\n",
        "    arquivos_base = {\n",
        "        \"metadados\": \"metadados_completos.json\",\n",
        "        \"decomposicao\": \"decomposicao_completa.json\",\n",
        "        \"padroes\": \"analises_padroes_completas.json\",\n",
        "        \"psicologica\": \"analises_psicologicas_completas.json\"\n",
        "    }\n",
        "\n",
        "    for tipo, arquivo in arquivos_base.items():\n",
        "        caminho = os.path.join(dados_path, arquivo)\n",
        "        if os.path.exists(caminho):\n",
        "            analises_encontradas[\"base\"][tipo] = caminho\n",
        "            print(f\"✅ Análise base encontrada: {tipo}\")\n",
        "        else:\n",
        "            print(f\"⚠️ Análise base ausente: {tipo}\")\n",
        "\n",
        "    # Descobrir análises adicionais automaticamente\n",
        "    # Busca por qualquer arquivo JSON que não seja das análises base\n",
        "    todos_jsons = glob.glob(os.path.join(dados_path, \"*.json\"))\n",
        "\n",
        "    for json_path in todos_jsons:\n",
        "        nome_arquivo = os.path.basename(json_path)\n",
        "\n",
        "        # Pular arquivos base\n",
        "        if nome_arquivo in arquivos_base.values():\n",
        "            continue\n",
        "\n",
        "        # Identificar tipo da análise pelo nome\n",
        "        if \"audio_refinada\" in nome_arquivo.lower():\n",
        "            analises_encontradas[\"adicionais\"][\"audio_refinada\"] = json_path\n",
        "            print(f\"✅ Análise adicional encontrada: Audio Refinada\")\n",
        "        elif \"visual_avancada\" in nome_arquivo.lower():\n",
        "            analises_encontradas[\"adicionais\"][\"visual_avancada\"] = json_path\n",
        "            print(f\"✅ Análise adicional encontrada: Visual Avançada\")\n",
        "        elif \"texto_avancada\" in nome_arquivo.lower():\n",
        "            analises_encontradas[\"adicionais\"][\"texto_avancada\"] = json_path\n",
        "            print(f\"✅ Análise adicional encontrada: Texto Avançada\")\n",
        "        elif \"sentiment\" in nome_arquivo.lower():\n",
        "            analises_encontradas[\"adicionais\"][\"sentimento\"] = json_path\n",
        "            print(f\"✅ Análise adicional encontrada: Sentimento\")\n",
        "        elif \"copywriting\" in nome_arquivo.lower():\n",
        "            analises_encontradas[\"adicionais\"][\"copywriting\"] = json_path\n",
        "            print(f\"✅ Análise adicional encontrada: Copywriting\")\n",
        "        elif \"viral\" in nome_arquivo.lower():\n",
        "            analises_encontradas[\"adicionais\"][\"viral\"] = json_path\n",
        "            print(f\"✅ Análise adicional encontrada: Viral\")\n",
        "        else:\n",
        "            # Análise não reconhecida - incluir mesmo assim\n",
        "            nome_limpo = nome_arquivo.replace(\".json\", \"\").replace(\"_\", \" \").title()\n",
        "            analises_encontradas[\"adicionais\"][nome_arquivo] = json_path\n",
        "            print(f\"✅ Análise personalizada encontrada: {nome_limpo}\")\n",
        "\n",
        "    return analises_encontradas\n",
        "\n",
        "def carregar_dados_analise(caminho_arquivo):\n",
        "    \"\"\"Carrega dados de uma análise com tratamento de erros\"\"\"\n",
        "    try:\n",
        "        with open(caminho_arquivo, 'r', encoding='utf-8') as f:\n",
        "            dados = json.load(f)\n",
        "        return dados, True\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Erro ao carregar {caminho_arquivo}: {e}\")\n",
        "        return [], False\n",
        "\n",
        "def extrair_metricas_dinamicamente(dados, tipo_analise):\n",
        "    \"\"\"Extrai métricas de qualquer tipo de análise dinamicamente\"\"\"\n",
        "    metricas_extraidas = {}\n",
        "\n",
        "    if not dados:\n",
        "        return metricas_extraidas\n",
        "\n",
        "    # Pegar o primeiro item para entender a estrutura\n",
        "    primeiro_item = dados[0] if isinstance(dados, list) else dados\n",
        "\n",
        "    if isinstance(primeiro_item, dict):\n",
        "        for chave, valor in primeiro_item.items():\n",
        "            if chave in ['video_id', 'status', 'data_analise', 'erro', 'nome_arquivo', 'tem_dados_virais', 'motivo_sem_dados']:\n",
        "                continue\n",
        "\n",
        "            # Ignorar estruturas muito complexas ou texto longo\n",
        "            if isinstance(valor, (dict, list)) and len(json.dumps(valor)) > 500:\n",
        "                continue\n",
        "            if isinstance(valor, str) and len(valor) > 200:\n",
        "                 continue\n",
        "\n",
        "\n",
        "            # Extrair métricas numéricas automaticamente\n",
        "            if isinstance(valor, (int, float)):\n",
        "                metricas_extraidas[f\"{tipo_analise}_{chave}\"] = valor\n",
        "            elif isinstance(valor, dict):\n",
        "                # Análise aninhada - extrair sub-métricas\n",
        "                for sub_chave, sub_valor in valor.items():\n",
        "                    if isinstance(sub_valor, (int, float)):\n",
        "                        metricas_extraidas[f\"{tipo_analise}_{chave}_{sub_chave}\"] = sub_valor\n",
        "                    elif isinstance(sub_valor, list) and sub_valor and isinstance(sub_valor[0], (int, float)):\n",
        "                        # Lista de números - calcular estatísticas\n",
        "                        metricas_extraidas[f\"{tipo_analise}_{chave}_{sub_chave}_media\"] = sum(sub_valor) / len(sub_valor)\n",
        "                        metricas_extraidas[f\"{tipo_analise}_{chave}_{sub_chave}_max\"] = max(sub_valor)\n",
        "                        metricas_extraidas[f\"{tipo_analise}_{chave}_{sub_chave}_min\"] = min(sub_valor)\n",
        "                    # Tratar listas de strings ou dicts dentro de dicts (contagem simples)\n",
        "                    elif isinstance(sub_valor, (list, dict)):\n",
        "                         metricas_extraidas[f\"{tipo_analise}_{chave}_{sub_chave}_count\"] = len(sub_valor)\n",
        "\n",
        "\n",
        "            elif isinstance(valor, list):\n",
        "                if valor and isinstance(valor[0], (int, float)):\n",
        "                    # Lista de números\n",
        "                    metricas_extraidas[f\"{tipo_analise}_{chave}_count\"] = len(valor)\n",
        "                    metricas_extraidas[f\"{tipo_analise}_{chave}_media\"] = sum(valor) / len(valor) if valor else 0\n",
        "                else:\n",
        "                    # Lista de objetos ou strings (contagem simples)\n",
        "                    metricas_extraidas[f\"{tipo_analise}_{chave}_count\"] = len(valor)\n",
        "            elif isinstance(valor, bool):\n",
        "                 metricas_extraidas[f\"{tipo_analise}_{chave}\"] = int(valor) # Converter bool para int (1/0)\n",
        "            elif isinstance(valor, str):\n",
        "                 # Incluir strings curtas como métricas categóricas\n",
        "                 if len(valor) < 50: # Limite para não poluir\n",
        "                     metricas_extraidas[f\"{tipo_analise}_{chave}_str\"] = valor\n",
        "\n",
        "\n",
        "    return metricas_extraidas\n",
        "\n",
        "def flatten_dict(d, parent_key='', sep='_'):\n",
        "    \"\"\"Flattens a nested dictionary.\"\"\"\n",
        "    items = []\n",
        "    for k, v in d.items():\n",
        "        new_key = parent_key + sep + k if parent_key else k\n",
        "        if isinstance(v, dict):\n",
        "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
        "        else:\n",
        "            items.append((new_key, v))\n",
        "    return dict(items)\n",
        "\n",
        "def consolidar_todos_dados(analises_encontradas):\n",
        "    \"\"\"Consolida todos os dados de todas as análises encontradas\"\"\"\n",
        "    dados_consolidados = {}\n",
        "\n",
        "    # Carregar análises base\n",
        "    for tipo, caminho in analises_encontradas[\"base\"].items():\n",
        "        dados, sucesso = carregar_dados_analise(caminho)\n",
        "        if sucesso:\n",
        "            dados_consolidados[tipo] = dados\n",
        "\n",
        "    # Carregar análises adicionais\n",
        "    for tipo, caminho in analises_encontradas[\"adicionais\"].items():\n",
        "        dados, sucesso = carregar_dados_analise(caminho)\n",
        "        if sucesso:\n",
        "            dados_consolidados[tipo] = dados\n",
        "\n",
        "\n",
        "    # Criar DataFrame consolidado por vídeo\n",
        "    videos_df = pd.DataFrame()\n",
        "\n",
        "    # Começar com metadados base se disponível\n",
        "    if \"metadados\" in dados_consolidados:\n",
        "        videos_df = pd.DataFrame(dados_consolidados[\"metadados\"])\n",
        "        videos_df = videos_df.set_index('id')\n",
        "        print(f\"DataFrame base criado com {len(videos_df)} vídeos e {len(videos_df.columns)} colunas.\")\n",
        "    else:\n",
        "        print(\"⚠️ Metadados base não encontrados. Criando DataFrame vazio e integrando por video_id.\")\n",
        "        videos_df = pd.DataFrame(columns=['id'])\n",
        "        videos_df.set_index('id', inplace=True)\n",
        "\n",
        "\n",
        "    # Integrar cada análise adicional\n",
        "    for tipo, dados in dados_consolidados.items():\n",
        "        if tipo == \"metadados\":\n",
        "            continue\n",
        "\n",
        "        print(f\"🔄 Integrando dados de: {tipo}\")\n",
        "\n",
        "        if isinstance(dados, list):\n",
        "            df_analise = pd.DataFrame(dados)\n",
        "\n",
        "            # Tentar usar 'video_id' ou 'id' para merge/join\n",
        "            merge_col = None\n",
        "            if 'video_id' in df_analise.columns:\n",
        "                merge_col = 'video_id'\n",
        "            elif 'id' in df_analise.columns:\n",
        "                 merge_col = 'id'\n",
        "            else:\n",
        "                 print(f\"⚠️ Análise '{tipo}' não possui 'video_id' ou 'id'. Não é possível integrar por vídeo.\")\n",
        "                 continue\n",
        "\n",
        "\n",
        "            # Extrair métricas dinamicamente para cada vídeo na análise\n",
        "            df_analise_flattened = pd.DataFrame()\n",
        "            if merge_col:\n",
        "                data_for_flattening = []\n",
        "                for item in dados:\n",
        "                    # Ensure item is a dict before processing\n",
        "                    if isinstance(item, dict):\n",
        "                         video_id_val = item.get(merge_col)\n",
        "                         if video_id_val:\n",
        "                            metricas = extrair_metricas_dinamicamente([item], tipo)\n",
        "                            metricas[merge_col] = video_id_val # Adicionar a coluna de merge\n",
        "                            data_for_flattening.append(metricas)\n",
        "                if data_for_flattening:\n",
        "                    df_analise_flattened = pd.DataFrame(data_for_flattening)\n",
        "                    df_analise_flattened.set_index(merge_col, inplace=True)\n",
        "\n",
        "\n",
        "            if not df_analise_flattened.empty:\n",
        "                 # Renomear colunas para evitar conflitos, exceto a coluna de merge\n",
        "                df_analise_flattened.columns = [f\"{col}\" for col in df_analise_flattened.columns]\n",
        "\n",
        "                # Realizar o merge\n",
        "                # Usar left_index=True e right_index=True para merge nos índices\n",
        "                # Se o DataFrame base (videos_df) estiver vazio, apenas use o df_analise_flattened\n",
        "                if videos_df.empty:\n",
        "                    videos_df = df_analise_flattened.copy()\n",
        "                    videos_df.index.name = 'id' # Renomear o índice para 'id' por convenção\n",
        "                else:\n",
        "                    # Antes de merge, garantir que o índice do videos_df é chamado 'id'\n",
        "                    if videos_df.index.name != 'id' and 'id' in videos_df.columns:\n",
        "                        videos_df.set_index('id', inplace=True)\n",
        "                    elif videos_df.index.name is None and 'id' in videos_df.columns:\n",
        "                         videos_df.set_index('id', inplace=True)\n",
        "                    elif videos_df.index.name is None and 'video_id' in videos_df.columns:\n",
        "                         videos_df.set_index('video_id', inplace=True) # Tentar video_id se 'id' não existir\n",
        "                         videos_df.index.name = 'id' # Manter convenção 'id'\n",
        "\n",
        "                    # Antes de merge, garantir que o índice do df_analise_flattened é chamado 'id'\n",
        "                    if df_analise_flattened.index.name != 'id':\n",
        "                         df_analise_flattened.index.name = 'id'\n",
        "\n",
        "\n",
        "                    # Realizar o merge\n",
        "                    common_cols = videos_df.columns.intersection(df_analise_flattened.columns)\n",
        "                    if len(common_cols) > 0:\n",
        "                         print(f\"⚠️ Colunas duplicadas encontradas durante o merge com {tipo}: {list(common_cols)}. Elas serão sobrescritas.\")\n",
        "                         # Remover colunas duplicadas do df_analise_flattened antes do merge\n",
        "                         # Exceto se forem colunas de identificação como video_id ou id\n",
        "                         cols_to_drop = [col for col in common_cols if col not in ['id', 'video_id']]\n",
        "                         df_analise_flattened = df_analise_flattened.drop(columns=cols_to_drop)\n",
        "\n",
        "\n",
        "                    # Realizar o merge nos índices\n",
        "                    videos_df = videos_df.merge(df_analise_flattened, left_index=True, right_index=True, how='outer', suffixes=('', f'_{tipo}'))\n",
        "                    print(f\"Merge com {tipo} concluído. Total de colunas: {len(videos_df.columns)}\")\n",
        "\n",
        "\n",
        "            else:\n",
        "                print(f\"⚠️ Análise '{tipo}' não gerou dados tabulares para integração.\")\n",
        "\n",
        "    # Resetar o índice para ter 'id' como coluna novamente\n",
        "    if videos_df.index.name:\n",
        "        videos_df = videos_df.reset_index()\n",
        "    elif 'id' not in videos_df.columns and 'video_id' in videos_df.columns:\n",
        "         # Se 'id' não existe mas 'video_id' existe, renomear para 'id'\n",
        "         videos_df = videos_df.rename(columns={'video_id': 'id'})\n",
        "\n",
        "\n",
        "    # Garantir que 'id' é a primeira coluna se existir\n",
        "    if 'id' in videos_df.columns:\n",
        "        id_col = videos_df.pop('id')\n",
        "        videos_df.insert(0, 'id', id_col)\n",
        "\n",
        "    # Flatten any remaining nested dictionaries or lists for Excel compatibility\n",
        "    print(\"🧹 Flattening nested structures for Excel compatibility...\")\n",
        "    flattened_data = []\n",
        "    for index, row in videos_df.iterrows():\n",
        "        flattened_row = flatten_dict(row.to_dict())\n",
        "        flattened_data.append(flattened_row)\n",
        "\n",
        "    df_flattened = pd.DataFrame(flattened_data)\n",
        "\n",
        "\n",
        "    # Limpeza final: remover colunas totalmente vazias\n",
        "    df_flattened.dropna(axis=1, how='all', inplace=True)\n",
        "\n",
        "    return df_flattened\n",
        "\n",
        "\n",
        "def gerar_dashboard_dinamico(df_consolidado, pasta_trabalho):\n",
        "    \"\"\"Gera dashboard dinâmico incluindo todas as análises encontradas\"\"\"\n",
        "    from openpyxl import Workbook\n",
        "    from openpyxl.styles import Font, Alignment, PatternFill\n",
        "    from openpyxl.utils import get_column_letter\n",
        "\n",
        "    wb = Workbook()\n",
        "\n",
        "    # ABA 1: VISÃO GERAL DINÂMICA\n",
        "    ws_geral = wb.active\n",
        "    ws_geral.title = 'Visão Geral Completa'\n",
        "\n",
        "    # Header\n",
        "    ws_geral.merge_cells(\"A1:Z1\") # Merge amplo para caber título\n",
        "    ws_geral[\"A1\"].value = 'RELATÓRIO COMPLETO DE ENGENHARIA REVERSA - VISÃO GERAL'\n",
        "    ws_geral[\"A1\"].font = Font(bold=True, size=16, color=\"FFFFFF\")\n",
        "    ws_geral[\"A1\"].fill = PatternFill(start_color=\"1F4E79\", end_color=\"1F4E79\", fill_type=\"solid\")\n",
        "    ws_geral[\"A1\"].alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
        "\n",
        "\n",
        "    row = 3\n",
        "\n",
        "    # Estatísticas gerais\n",
        "    ws_geral.cell(row=row, column=1).value = 'ANÁLISES REALIZADAS'\n",
        "    ws_geral.cell(row=row, column=1).font = Font(bold=True, size=14)\n",
        "    row += 2\n",
        "\n",
        "    # Contar colunas por tipo de análise\n",
        "    colunas_por_tipo = {}\n",
        "    for col in df_consolidado.columns:\n",
        "        if '_' in col:\n",
        "            tipo = col.split('_')[0]\n",
        "            colunas_por_tipo[tipo] = colunas_por_tipo.get(tipo, 0) + 1\n",
        "        else: # Incluir colunas sem underscore (ex: id, nome_arquivo)\n",
        "             colunas_por_tipo[\"base\"] = colunas_por_tipo.get(\"base\", 0) + 1\n",
        "\n",
        "\n",
        "    row_stats = row\n",
        "    for tipo, count in colunas_por_tipo.items():\n",
        "        ws_geral.cell(row=row_stats, column=1).value = f\"{tipo.upper()}:\"\n",
        "        ws_geral.cell(row=row_stats, column=2).value = f\"{count} métricas\"\n",
        "        ws_geral.cell(row=row_stats, column=1).font = Font(bold=True)\n",
        "        row_stats += 1\n",
        "\n",
        "    row = max(row_stats, row) + 2\n",
        "\n",
        "\n",
        "    # Adicionar um resumo simples do DataFrame\n",
        "    ws_geral.cell(row=row, column=1).value = \"RESUMO DO DATAFRAME CONSOLIDADO\"\n",
        "    ws_geral.cell(row=row, column=1).font = Font(bold=True, size=14)\n",
        "    row += 2\n",
        "\n",
        "    if not df_consolidado.empty:\n",
        "        summary_data = [\n",
        "            [\"Total de Vídeos:\", len(df_consolidado)],\n",
        "            [\"Total de Métricas Integradas:\", len(df_consolidado.columns)],\n",
        "            [\"Média de Duração (segundos):\", df_consolidado.get('duracao_segundos', pd.Series([0])).mean()],\n",
        "            [\"Média de Cortes por Segundo:\", df_consolidado.get('cortes_por_segundo', pd.Series([0])).mean()],\n",
        "            [\"Vídeos com Áudio:\", df_consolidado.get('tem_audio', pd.Series([False])).sum()],\n",
        "            [\"Formatos Encontrados:\", \", \".join(df_consolidado.get('formato_detectado', pd.Series(['N/A'])).mode().tolist())],\n",
        "            [\"Média de Score Viral:\", df_consolidado.get('viral_score', pd.Series([0])).mean()],\n",
        "            [\"Média de Score Técnico:\", df_consolidado.get('technical_score', pd.Series([0])).mean()],\n",
        "            [\"Média de Score Conteúdo:\", df_consolidado.get('content_score', pd.Series([0])).mean()],\n",
        "            [\"Média de Score Copywriting:\", df_consolidado.get('copywriting_score_persuasao', pd.Series([0])).mean()],\n",
        "        ]\n",
        "\n",
        "        for label, value in summary_data:\n",
        "            ws_geral.cell(row=row, column=1, value=label).font = Font(bold=True)\n",
        "            ws_geral.cell(row=row, column=2, value=value)\n",
        "            row += 1\n",
        "    else:\n",
        "        ws_geral.cell(row=row, column=1, value=\"DataFrame consolidado vazio.\").font = Font(bold=True, color=\"FF0000\")\n",
        "\n",
        "\n",
        "    # ABA 2: DADOS COMPLETOS\n",
        "    ws_dados = wb.create_sheet('Dados Completos')\n",
        "\n",
        "    # Adicionar todos os dados\n",
        "    if not df_consolidado.empty:\n",
        "        # Adicionar cabeçalhos\n",
        "        for c_idx, col_name in enumerate(df_consolidado.columns, 1):\n",
        "             cell = ws_dados.cell(row=1, column=c_idx, value=col_name)\n",
        "             cell.font = Font(bold=True)\n",
        "             cell.fill = PatternFill(start_color=\"E7E6E6\", end_color=\"E7E6E6\", fill_type=\"solid\")\n",
        "\n",
        "        # Adicionar linhas de dados\n",
        "        for r_idx, row_data in enumerate(df_consolidado.itertuples(index=False), 2):\n",
        "            for c_idx, value in enumerate(row_data, 1):\n",
        "                ws_dados.cell(row=r_idx, column=c_idx, value=value)\n",
        "\n",
        "        # Ajustar largura das colunas (exemplo simples)\n",
        "        for col_idx in range(1, len(df_consolidado.columns) + 1):\n",
        "            ws_dados.column_dimensions[get_column_letter(col_idx)].width = 15 # Largura padrão\n",
        "\n",
        "    else:\n",
        "        ws_dados.cell(row=1, column=1, value=\"Nenhum dado consolidado disponível\").font = Font(bold=True, color=\"FF0000\")\n",
        "\n",
        "\n",
        "    # ABA 3: INSIGHTS AUTOMATICOS\n",
        "    ws_insights = wb.create_sheet('Insights Automáticos')\n",
        "\n",
        "    insights_automaticos = gerar_insights_automaticos(df_consolidado)\n",
        "\n",
        "    ws_insights.cell(row=1, column=1).value = 'INSIGHTS GERADOS AUTOMATICAMENTE'\n",
        "    ws_insights.cell(row=1, column=1).font = Font(bold=True, size=16, color=\"FFFFFF\")\n",
        "    ws_insights.cell(row=1, column=1).fill = PatternFill(start_color=\"C5504B\", end_color=\"C5504B\", fill_type=\"solid\")\n",
        "    ws_insights.merge_cells(\"A1:Z1\")\n",
        "\n",
        "\n",
        "    for i, insight in enumerate(insights_automaticos, 3):\n",
        "        ws_insights.cell(row=i, column=1).value = f\"• {insight}\"\n",
        "        ws_insights.cell(row=i, column=1).alignment = Alignment(wrap_text=True)\n",
        "        ws_insights.cell(row=i, column=1).font = Font(bold=True) # Insights em negrito\n",
        "\n",
        "    # Ajustar largura da coluna de insights\n",
        "    ws_insights.column_dimensions['A'].width = 120\n",
        "\n",
        "\n",
        "    # ======= INTEGRAÇÃO: ABA VIRAL NO DASHBOARD =======\n",
        "    print(\"📊 Tentando integrar aba de análise viral...\")\n",
        "    if 'wb' in locals():\n",
        "        sucesso_aba_viral = criar_aba_analise_viral_dashboard(wb, pasta_trabalho)\n",
        "        if sucesso_aba_viral:\n",
        "            print(\"✅ Aba de Análise Viral adicionada ao dashboard\")\n",
        "    else:\n",
        "        print(\"⚠️ Workbook não criado. Não foi possível adicionar aba viral.\")\n",
        "\n",
        "    print(\"🔗 Integração viral concluída\")\n",
        "    # ======= FIM INTEGRAÇÃO VIRAL =======\n",
        "\n",
        "\n",
        "    # Salvar\n",
        "    output_path = os.path.join(pasta_trabalho, \"dashboard\", \"RELATORIO_COMPLETO_DINAMICO.xlsx\")\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "    wb.save(output_path)\n",
        "\n",
        "    return output_path\n",
        "\n",
        "def gerar_insights_automaticos(df):\n",
        "    \"\"\"Gera insights automáticos baseados em qualquer conjunto de dados\"\"\"\n",
        "    insights = []\n",
        "\n",
        "    if df.empty:\n",
        "        return [\"Nenhum dado disponível para gerar insights automáticos.\"]\n",
        "\n",
        "    # Análise de correlações automáticas (apenas para colunas com variação)\n",
        "    colunas_numericas = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    colunas_numericas = [col for col in colunas_numericas if df[col].nunique() > 1] # Remover colunas constantes\n",
        "\n",
        "\n",
        "    if len(colunas_numericas) > 1:\n",
        "        try:\n",
        "            correlacoes = df[colunas_numericas].corr()\n",
        "\n",
        "            # Encontrar correlações fortes (ajustar threshold se necessário)\n",
        "            for col1 in correlacoes.columns:\n",
        "                for col2 in correlacoes.columns:\n",
        "                    if col1 != col2:\n",
        "                        corr_val = correlacoes.loc[col1, col2]\n",
        "                        if abs(corr_val) > 0.7: # Threshold para correlação forte\n",
        "                            insights.append(f\"CORRELAÇÃO FORTE: '{col1}' e '{col2}' têm correlação de {corr_val:.2f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "             insights.append(f\"⚠️ Erro ao calcular correlações: {e}\")\n",
        "\n",
        "\n",
        "    # Identificar outliers automáticos (usando Z-score ou IQR)\n",
        "    # Usar IQR (Interquartile Range) que é mais robusto a outliers\n",
        "    for col in colunas_numericas:\n",
        "        try:\n",
        "            Q1 = df[col].quantile(0.25)\n",
        "            Q3 = df[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "            outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
        "\n",
        "            if len(outliers) > 0:\n",
        "                outlier_videos = outliers['id'].tolist() if 'id' in outliers.columns else outliers.index.tolist()\n",
        "                insights.append(f\"OUTLIERS DETECTADOS: {len(outliers)} vídeo(s) têm valores extremos na métrica '{col}' (Ex: {', '.join(map(str, outlier_videos[:3]))})\")\n",
        "\n",
        "        except Exception as e:\n",
        "             insights.append(f\"⚠️ Erro ao detectar outliers para '{col}': {e}\")\n",
        "\n",
        "\n",
        "    # Análise de distribuições e médias\n",
        "    for col in colunas_numericas:\n",
        "        try:\n",
        "            media = df[col].mean()\n",
        "            std = df[col].std()\n",
        "            # Adicionar insights gerais sobre a distribuição\n",
        "            if std > media * 0.5: # Alta variação\n",
        "                 insights.append(f\"VARIAÇÃO ALTA: A métrica '{col}' tem alta variação (média={media:.1f}, std={std:.1f}). Explore os extremos.\")\n",
        "\n",
        "            # Insights específicos para scores (se existirem)\n",
        "            if col.endswith('_score') or 'score' in col.lower():\n",
        "                if media > 80:\n",
        "                    insights.append(f\"PERFORMANCE ALTA: Score médio de '{col}' é {media:.1f} - identifique os padrões dos top performers.\")\n",
        "                elif media < 50:\n",
        "                    insights.append(f\"OPORTUNIDADE: Score médio de '{col}' é {media:.1f} - foco em otimizar esta área.\")\n",
        "\n",
        "        except Exception as e:\n",
        "             insights.append(f\"⚠️ Erro ao analisar distribuição para '{col}': {e}\")\n",
        "\n",
        "\n",
        "    # Análise de colunas categóricas\n",
        "    colunas_categoricas = df.select_dtypes(include=['object']).columns.tolist()\n",
        "    for col in colunas_categoricas:\n",
        "        try:\n",
        "            # Contar valores únicos e frequência\n",
        "            value_counts = df[col].value_counts()\n",
        "            if len(value_counts) > 1 and len(value_counts) < 10: # Evitar colunas com muitos valores únicos\n",
        "                most_common = value_counts.index[0]\n",
        "                insights.append(f\"PADRÃO CATEGÓRICO: O valor mais comum em '{col}' é '{most_common}' ({value_counts.iloc[0]} ocorrências).\")\n",
        "\n",
        "        except Exception as e:\n",
        "             insights.append(f\"⚠️ Erro ao analisar categoria '{col}': {e}\")\n",
        "\n",
        "\n",
        "    return insights if insights else [\"Análise de insights automáticos concluída. Nenhum insight significativo encontrado com os parâmetros atuais.\"]\n",
        "\n",
        "\n",
        "def atualizar_config_com_novas_analises(pasta_trabalho, analises_encontradas):\n",
        "    \"\"\"Atualiza config.json com status de todas as análises encontradas\"\"\"\n",
        "    config_path = os.path.join(pasta_trabalho, \"config\", \"config.json\")\n",
        "\n",
        "    # Carregar config existente\n",
        "    config = {}\n",
        "    if os.path.exists(config_path):\n",
        "        try:\n",
        "            with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                config = json.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Erro ao carregar config.json para atualização: {e}\")\n",
        "            config = {\"status_etapas\": {}, \"arquivos_gerados\": {}} # Fallback\n",
        "\n",
        "    # Garantir que as chaves existem\n",
        "    if \"status_etapas\" not in config:\n",
        "        config[\"status_etapas\"] = {}\n",
        "    if \"arquivos_gerados\" not in config:\n",
        "         config[\"arquivos_gerados\"] = {}\n",
        "\n",
        "\n",
        "    # Atualizar status das análises encontradas\n",
        "    for tipo in analises_encontradas[\"base\"]:\n",
        "        config[\"status_etapas\"][tipo] = True\n",
        "\n",
        "    for tipo in analises_encontradas[\"adicionais\"]:\n",
        "        # Adicionar status com prefixo 'analise_' para clareza\n",
        "        config[\"status_etapas\"][f\"analise_{tipo}\"] = True\n",
        "\n",
        "    # Registrar arquivos gerados\n",
        "    dashboard_path = os.path.join(pasta_trabalho, \"dashboard\", \"RELATORIO_COMPLETO_DINAMICO.xlsx\")\n",
        "    csv_path = os.path.join(pasta_trabalho, \"dashboard\", \"dados_completos_consolidados.csv\")\n",
        "    json_path = os.path.join(pasta_trabalho, \"dashboard\", \"dados_completos_consolidados.json\")\n",
        "\n",
        "    config[\"arquivos_gerados\"][\"dashboard_completo\"] = dashboard_path if os.path.exists(dashboard_path) else None\n",
        "    config[\"arquivos_gerados\"][\"dados_consolidados_csv\"] = csv_path if os.path.exists(csv_path) else None\n",
        "    config[\"arquivos_gerados\"][\"dados_consolidados_json\"] = json_path if os.path.exists(json_path) else None\n",
        "\n",
        "\n",
        "    config[\"ultima_consolidacao\"] = datetime.now().isoformat()\n",
        "    config[\"total_analises_integradas\"] = len(analises_encontradas[\"base\"]) + len(analises_encontradas[\"adicionais\"])\n",
        "\n",
        "    # Salvar config atualizado\n",
        "    try:\n",
        "        with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "        print(\"✅ Arquivo config.json atualizado com sucesso.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ ERRO ao salvar config.json: {e}\")\n",
        "\n",
        "\n",
        "def main_integracao_automatica():\n",
        "    \"\"\"Função principal da integração automática\"\"\"\n",
        "    print(\"🚀 INICIANDO INTEGRAÇÃO AUTOMÁTICA DE TODAS AS ANÁLISES\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Usar variável global da pasta de trabalho\n",
        "    if \"PASTA_TRABALHO\" not in globals():\n",
        "        print(\"❌ ERRO: Execute as células anteriores primeiro (CÉLULA 1.2).\")\n",
        "        return False\n",
        "\n",
        "    pasta_trabalho = PASTA_TRABALHO\n",
        "\n",
        "    try:\n",
        "        # Passo 1: Descobrir análises\n",
        "        print(\"\\n🔍 DESCOBRINDO ANÁLISES DISPONÍVEIS...\")\n",
        "        analises = descobrir_analises_disponiveis(pasta_trabalho)\n",
        "\n",
        "        total_analises = len(analises[\"base\"]) + len(analises[\"adicionais\"])\n",
        "        print(f\"📊 Total de análises encontradas: {total_analises}\")\n",
        "        if total_analises < 4: # Metadados, Decomposicao, Padroes, Psicologica são base\n",
        "             print(\"⚠️ Aviso: Análises base incompletas. Algumas funcionalidades podem estar limitadas.\")\n",
        "\n",
        "\n",
        "        # Passo 2: Consolidar dados\n",
        "        print(\"\\n🔄 CONSOLIDANDO TODOS OS DADOS...\")\n",
        "        df_consolidado = consolidar_todos_dados(analises)\n",
        "\n",
        "        print(f\"📈 {len(df_consolidado)} vídeos consolidados com {len(df_consolidado.columns)} métricas totais\")\n",
        "\n",
        "        if df_consolidado.empty:\n",
        "             print(\"❌ DataFrame consolidado vazio. Não é possível gerar dashboard ou insights.\")\n",
        "             # Tentar atualizar config mesmo assim para registrar o estado\n",
        "             actualizar_config_con_novas_analisis(pasta_trabalho, analises)\n",
        "             return False\n",
        "\n",
        "\n",
        "        # Passo 3: Gerar dashboard dinâmico\n",
        "        print(\"\\n📊 GERANDO DASHBOARD DINÂMICO...\")\n",
        "        dashboard_path = gerar_dashboard_dinamico(df_consolidado, pasta_trabalho)\n",
        "\n",
        "        # Passo 4: Salvar dados consolidados (CSV e JSON)\n",
        "        print(\"\\n💾 SALVANDO DADOS CONSOLIDADOS...\")\n",
        "        csv_path = os.path.join(pasta_trabalho, \"dashboard\", \"dados_completos_consolidados.csv\")\n",
        "        df_consolidado.to_csv(csv_path, index=False, encoding='utf-8')\n",
        "        print(f\"📁 Dados CSV salvos: {csv_path}\")\n",
        "\n",
        "        json_path = os.path.join(pasta_trabalho, \"dashboard\", \"dados_completos_consolidados.json\")\n",
        "        # Usar orient='records' para formato mais amigável para JSON\n",
        "        df_consolidado.to_json(json_path, orient='records', indent=2, force_ascii=False)\n",
        "        print(f\"📁 Dados JSON salvos: {json_path}\")\n",
        "\n",
        "\n",
        "        # Passo 5: Atualizar configuração\n",
        "        print(\"\\n⚙️ ATUALIZANDO CONFIGURAÇÕES...\")\n",
        "        atualizar_config_com_novas_analises(pasta_trabalho, analises)\n",
        "\n",
        "\n",
        "        # Resultados finais\n",
        "        print(\"\\n✅ INTEGRAÇÃO AUTOMÁTICA CONCLUÍDA COM SUCESSO!\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"📁 Dashboard dinâmico completo: {dashboard_path}\")\n",
        "        print(f\"📊 {len(df_consolidado)} vídeos processados com {len(df_consolidado.columns)} métricas totais integradas.\")\n",
        "        print(\"\\n🎯 PRÓXIMOS PASSOS:\")\n",
        "        print(\"• Abra o arquivo Excel gerado para explorar todas as análises integradas.\")\n",
        "        print(\"• Use os arquivos CSV/JSON para análises mais avançadas em outras ferramentas.\")\n",
        "        print(\"• Execute esta célula novamente sempre que adicionar novas análises ao seu projeto.\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ ERRO CRÍTICO NA INTEGRAÇÃO AUTOMÁTICA: {type(e).__name__}: {e}\")\n",
        "        print(\"Por favor, verifique se todas as análises anteriores foram executadas com sucesso e se não há erros nos dados de entrada.\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "# Executar integração automática\n",
        "if __name__ == \"__main__\":\n",
        "    main_integracao_automatica()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlbVqMZ-yhI0",
        "outputId": "fbbfb535-161f-4a1a-b086-21ef47d2592e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 EXECUTANDO INTEGRAÇÃO DE COPYWRITING NO DASHBOARD EXISTENTE\n",
            "======================================================================\n",
            "🔄 Iniciando integração de copywriting no dashboard existente...\n",
            "❌ PRÉ-REQUISITO NÃO ATENDIDO: A etapa \"copywriting_analysis\" não foi encontrada.\n",
            "   Execute a célula correspondente primeiro.\n",
            "\n",
            "❌ Falha na integração - verifique os pré-requisitos\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CÉLULA 4.3: INTEGRAÇÃO DE COPYWRITING NO DASHBOARD EXISTENTE\n",
        "# ============================================================================\n",
        "\n",
        "def integrar_copywriting_dashboard_existente():\n",
        "    \"\"\"Integra análise de copywriting no dashboard master existente\"\"\"\n",
        "    print(\"🔄 Iniciando integração de copywriting no dashboard existente...\")\n",
        "\n",
        "    # Verificar pré-requisitos\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa('copywriting_analysis')\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Localizar dashboard existente\n",
        "    pasta_dashboard = os.path.join(PASTA_TRABALHO, \"dashboard\")\n",
        "    dashboard_existente = None\n",
        "\n",
        "    # Procurar arquivo de dashboard existente\n",
        "    if os.path.exists(pasta_dashboard):\n",
        "        arquivos = os.listdir(pasta_dashboard)\n",
        "        for arquivo in arquivos:\n",
        "            if \"DASHBOARD_MASTER_EXECUTIVO_INTELIGENTE\" in arquivo and arquivo.endswith(\".xlsx\"):\n",
        "                dashboard_existente = os.path.join(pasta_dashboard, arquivo)\n",
        "                break\n",
        "\n",
        "    if not dashboard_existente:\n",
        "        print(\"❌ Dashboard master existente não encontrado!\")\n",
        "        print(\"Execute primeiro a célula 4.2 (Blueprint Final) para criar o dashboard base.\")\n",
        "        return\n",
        "\n",
        "    print(f\"  📊 Dashboard encontrado: {os.path.basename(dashboard_existente)}\")\n",
        "\n",
        "    # Carregar dados de copywriting\n",
        "    dados_copywriting = carregar_dados_copywriting()\n",
        "    if not dados_copywriting:\n",
        "        return\n",
        "\n",
        "    # Abrir workbook existente\n",
        "    from openpyxl import load_workbook\n",
        "\n",
        "    try:\n",
        "        wb = load_workbook(dashboard_existente)\n",
        "        print(f\"  ✅ Dashboard carregado com {len(wb.sheetnames)} abas existentes\")\n",
        "\n",
        "        # Adicionar novas abas de copywriting\n",
        "        adicionar_aba_copywriting_estrategico(wb, dados_copywriting)\n",
        "        adicionar_aba_templates_copy(wb, dados_copywriting)\n",
        "        adicionar_aba_timeline_copy(wb, dados_copywriting)\n",
        "        adicionar_aba_recomendacoes_copy(wb, dados_copywriting)\n",
        "\n",
        "        # Atualizar aba principal com métricas de copywriting\n",
        "        atualizar_aba_principal_com_copy(wb, dados_copywriting)\n",
        "\n",
        "        # Salvar dashboard atualizado\n",
        "        wb.save(dashboard_existente)\n",
        "\n",
        "        print(f\"✅ Dashboard atualizado com análise de copywriting!\")\n",
        "        print(f\"📊 Arquivo: {dashboard_existente}\")\n",
        "        print(f\"📋 Novas abas adicionadas:\")\n",
        "        print(\"  • Copywriting Estratégico\")\n",
        "        print(\"  • Templates Replicáveis\")\n",
        "        print(\"  • Timeline Persuasão\")\n",
        "        print(\"  • Recomendações Copy\")\n",
        "        print(\"  • Dashboard Principal (atualizada)\")\n",
        "\n",
        "        # Gerar relatórios complementares\n",
        "        gerar_relatorios_copywriting_individuais(dados_copywriting)\n",
        "\n",
        "        # Atualizar config\n",
        "        config[\"status_etapas\"][\"dashboard_copywriting_integrado\"] = True\n",
        "        config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "        with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        return dashboard_existente\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erro ao atualizar dashboard: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def carregar_dados_copywriting():\n",
        "    \"\"\"Carrega dados de copywriting e outros dados necessários\"\"\"\n",
        "    print(\"  📊 Carregando dados de copywriting...\")\n",
        "\n",
        "    try:\n",
        "        # Dados de copywriting\n",
        "        copywriting_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_copywriting_completas.json\")\n",
        "        with open(copywriting_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            copywriting_data = json.load(f)\n",
        "\n",
        "        # Dados de legendas\n",
        "        legendas_path = os.path.join(PASTA_TRABALHO, \"dados\", \"legendas_geradas.json\")\n",
        "        with open(legendas_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            legendas_data = json.load(f)\n",
        "\n",
        "        # Tentar carregar outros dados (podem não existir ainda)\n",
        "        outros_dados = {}\n",
        "\n",
        "        try:\n",
        "            padroes_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_padroes_completas.json\")\n",
        "            with open(padroes_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                outros_dados[\"padroes\"] = json.load(f)\n",
        "        except:\n",
        "            outros_dados[\"padroes\"] = []\n",
        "\n",
        "        try:\n",
        "            videos_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "            with open(videos_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                outros_dados[\"videos\"] = json.load(f)\n",
        "        except:\n",
        "            outros_dados[\"videos\"] = []\n",
        "\n",
        "        print(f\"  ✅ Dados carregados: {len(copywriting_data)} análises de copywriting\")\n",
        "\n",
        "        return {\n",
        "            \"copywriting\": copywriting_data,\n",
        "            \"legendas\": legendas_data,\n",
        "            **outros_dados\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ Erro ao carregar dados de copywriting: {e}\")\n",
        "        return None\n",
        "\n",
        "def adicionar_aba_copywriting_estrategico(wb, dados):\n",
        "    \"\"\"Adiciona aba principal de análise de copywriting\"\"\"\n",
        "    from openpyxl.styles import Font, PatternFill, Alignment\n",
        "\n",
        "    # Criar nova aba\n",
        "    ws = wb.create_sheet(\"Copywriting Estratégico\")\n",
        "\n",
        "    # Título principal\n",
        "    ws.merge_cells(\"A1:H1\")\n",
        "    titulo = ws[\"A1\"]\n",
        "    titulo.value = \"ANÁLISE ESTRATÉGICA DE COPYWRITING - ENGENHARIA REVERSA\"\n",
        "    titulo.fill = PatternFill(start_color=\"1F4E79\", end_color=\"1F4E79\", fill_type=\"solid\")\n",
        "    titulo.font = Font(color=\"FFFFFF\", bold=True, size=14)\n",
        "    titulo.alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "    row = 3\n",
        "\n",
        "    # Métricas executivas\n",
        "    ws[f\"A{row}\"] = \"MÉTRICAS EXECUTIVAS DE COPYWRITING\"\n",
        "    ws[f\"A{row}\"].font = Font(bold=True, size=12, color=\"C5504B\")\n",
        "    row += 2\n",
        "\n",
        "    # Calcular métricas\n",
        "    videos_copy = dados[\"copywriting\"]\n",
        "\n",
        "    if videos_copy:\n",
        "        # Score médio\n",
        "        scores = [v.get(\"score_persuasao\", 0) for v in videos_copy]\n",
        "        score_medio = sum(scores) / len(scores) if scores else 0\n",
        "\n",
        "        # Contadores\n",
        "        total_ganchos = sum(len(v.get(\"ganchos_detectados\", {})) for v in videos_copy)\n",
        "        total_gatilhos = sum(len(v.get(\"gatilhos_mentais_detectados\", {})) for v in videos_copy)\n",
        "        total_ctas = sum(len(v.get(\"ctas_detectados\", {})) for v in videos_copy)\n",
        "        videos_sem_cta = len([v for v in videos_copy if not v.get(\"ctas_detectados\")])\n",
        "        total_templates = sum(len(v.get(\"templates_identificados\", [])) for v in videos_copy)\n",
        "\n",
        "        # Exibir métricas\n",
        "        metricas = [\n",
        "            (\"Score Persuasão Médio:\", f\"{score_medio:.1f}/100\", \"Meta: 70+ para alta conversão\"),\n",
        "            (\"Vídeos Analisados:\", len(videos_copy), \"Base completa da análise\"),\n",
        "            (\"Total de Ganchos:\", total_ganchos, f\"Média: {total_ganchos/len(videos_copy):.1f} por vídeo\"),\n",
        "            (\"Total de Gatilhos:\", total_gatilhos, f\"Média: {total_gatilhos/len(videos_copy):.1f} por vídeo\"),\n",
        "            (\"Total de CTAs:\", total_ctas, f\"Média: {total_ctas/len(videos_copy):.1f} por vídeo\"),\n",
        "            (\"🚨 Vídeos sem CTA:\", videos_sem_cta, \"CRÍTICO: Implementar imediatamente\" if videos_sem_cta > 0 else \"✅ Todos têm CTA\"),\n",
        "            (\"Templates Identificados:\", total_templates, \"Estruturas replicáveis encontradas\")\n",
        "        ]\n",
        "\n",
        "        for metrica, valor, descricao in metricas:\n",
        "            ws[f\"A{row}\"] = metrica\n",
        "            ws[f\"B{row}\"] = valor\n",
        "            ws[f\"C{row}\"] = descricao\n",
        "\n",
        "            ws[f\"A{row}\"].font = Font(bold=True)\n",
        "            if \"🚨\" in metrica and videos_sem_cta > 0:\n",
        "                ws[f\"B{row}\"].font = Font(bold=True, color=\"FF0000\")\n",
        "            elif isinstance(valor, (int, float)) and valor > 0:\n",
        "                ws[f\"B{row}\"].font = Font(bold=True, color=\"70AD47\")\n",
        "\n",
        "            row += 1\n",
        "\n",
        "        row += 2\n",
        "\n",
        "        # Ranking de performance\n",
        "        ws[f\"A{row}\"] = \"🏆 RANKING DE PERFORMANCE POR SCORE DE PERSUASÃO\"\n",
        "        ws[f\"A{row}\"].font = Font(bold=True, size=12, color=\"1F4E79\")\n",
        "        row += 2\n",
        "\n",
        "        # Headers\n",
        "        headers = [\"Posição\", \"Vídeo ID\", \"Score\", \"Ganchos\", \"Gatilhos\", \"CTAs\", \"Status\"]\n",
        "        for col, header in enumerate(headers, 1):\n",
        "            cell = ws.cell(row=row, column=col)\n",
        "            cell.value = header\n",
        "            cell.font = Font(bold=True)\n",
        "            cell.fill = PatternFill(start_color=\"D9E2F3\", end_color=\"D9E2F3\", fill_type=\"solid\")\n",
        "        row += 1\n",
        "\n",
        "        # Top performers\n",
        "        top_videos = sorted(videos_copy, key=lambda x: x.get(\"score_persuasao\", 0), reverse=True)\n",
        "\n",
        "        for i, video in enumerate(top_videos, 1):\n",
        "            ws.cell(row=row, column=1, value=f\"{i}º\")\n",
        "            ws.cell(row=row, column=2, value=video[\"video_id\"])\n",
        "            ws.cell(row=row, column=3, value=f\"{video.get('score_persuasao', 0)}/100\")\n",
        "            ws.cell(row=row, column=4, value=len(video.get(\"ganchos_detectados\", {})))\n",
        "            ws.cell(row=row, column=5, value=len(video.get(\"gatilhos_mentais_detectados\", {})))\n",
        "            ws.cell(row=row, column=6, value=len(video.get(\"ctas_detectados\", {})))\n",
        "\n",
        "            # Status baseado no score\n",
        "            score = video.get(\"score_persuasao\", 0)\n",
        "            if score >= 70:\n",
        "                status = \"🟢 ÓTIMO\"\n",
        "                status_color = \"70AD47\"\n",
        "            elif score >= 50:\n",
        "                status = \"🟡 BOM\"\n",
        "                status_color = \"FFC000\"\n",
        "            else:\n",
        "                status = \"🔴 PRECISA OTIMIZAR\"\n",
        "                status_color = \"C5504B\"\n",
        "\n",
        "            cell_status = ws.cell(row=row, column=7, value=status)\n",
        "            cell_status.font = Font(color=status_color, bold=True)\n",
        "\n",
        "            # Destacar top 3\n",
        "            if i <= 3:\n",
        "                for col in range(1, 8):\n",
        "                    ws.cell(row=row, column=col).fill = PatternFill(start_color=\"FFF2CC\", end_color=\"FFF2CC\", fill_type=\"solid\")\n",
        "\n",
        "            row += 1\n",
        "\n",
        "        row += 2\n",
        "\n",
        "        # Análise de gaps críticos\n",
        "        ws[f\"A{row}\"] = \"⚠️ GAPS CRÍTICOS IDENTIFICADOS\"\n",
        "        ws[f\"A{row}\"].font = Font(bold=True, size=12, color=\"C5504B\")\n",
        "        row += 2\n",
        "\n",
        "        gaps = []\n",
        "\n",
        "        # Vídeos sem CTA\n",
        "        if videos_sem_cta > 0:\n",
        "            gap_cta_videos = [v[\"video_id\"] for v in videos_copy if not v.get(\"ctas_detectados\")]\n",
        "            gaps.append(f\"🚨 CRÍTICO: {videos_sem_cta} vídeos sem CTA: {', '.join(gap_cta_videos[:3])}\")\n",
        "\n",
        "        # Vídeos com poucos ganchos\n",
        "        videos_poucos_ganchos = [v for v in videos_copy if len(v.get(\"ganchos_detectados\", {})) < 2]\n",
        "        if len(videos_poucos_ganchos) > len(videos_copy) * 0.5:\n",
        "            gaps.append(f\"📈 OPORTUNIDADE: {len(videos_poucos_ganchos)} vídeos precisam de mais ganchos\")\n",
        "\n",
        "        # Score baixo\n",
        "        videos_score_baixo = [v for v in videos_copy if v.get(\"score_persuasao\", 0) < 50]\n",
        "        if videos_score_baixo:\n",
        "            gaps.append(f\"🎯 OTIMIZAÇÃO: {len(videos_score_baixo)} vídeos com score < 50 precisam de revisão\")\n",
        "\n",
        "        if not gaps:\n",
        "            gaps.append(\"✅ Nenhum gap crítico identificado - parabéns!\")\n",
        "\n",
        "        for gap in gaps:\n",
        "            ws[f\"A{row}\"] = gap\n",
        "            if \"🚨\" in gap:\n",
        "                ws[f\"A{row}\"].font = Font(color=\"FF0000\", bold=True)\n",
        "            elif \"📈\" in gap or \"🎯\" in gap:\n",
        "                ws[f\"A{row}\"].font = Font(color=\"FFC000\", bold=True)\n",
        "            else:\n",
        "                ws[f\"A{row}\"].font = Font(color=\"70AD47\", bold=True)\n",
        "            row += 1\n",
        "\n",
        "    else:\n",
        "        ws[f\"A{row}\"] = \"⚠️ Nenhum dado de copywriting encontrado\"\n",
        "        ws[f\"A{row}\"].font = Font(color=\"C5504B\", bold=True)\n",
        "        row += 1\n",
        "        ws[f\"A{row}\"] = \"Execute primeiro a Célula 2.4 para gerar análises de copywriting\"\n",
        "\n",
        "    # Ajustar larguras das colunas\n",
        "    for col, width in [(\"A\", 25), (\"B\", 15), (\"C\", 40), (\"D\", 10), (\"E\", 10), (\"F\", 10), (\"G\", 20), (\"H\", 15)]:\n",
        "        ws.column_dimensions[col].width = width\n",
        "\n",
        "def adicionar_aba_templates_copy(wb, dados):\n",
        "    \"\"\"Adiciona aba de templates replicáveis\"\"\"\n",
        "    from openpyxl.styles import Font, PatternFill, Alignment\n",
        "\n",
        "    ws = wb.create_sheet(\"Templates Replicáveis\")\n",
        "\n",
        "    # Título\n",
        "    ws.merge_cells(\"A1:F1\")\n",
        "    titulo = ws[\"A1\"]\n",
        "    titulo.value = \"TEMPLATES E ESTRUTURAS REPLICÁVEIS DE COPYWRITING\"\n",
        "    titulo.fill = PatternFill(start_color=\"70AD47\", end_color=\"70AD47\", fill_type=\"solid\")\n",
        "    titulo.font = Font(color=\"FFFFFF\", bold=True, size=14)\n",
        "    titulo.alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "    row = 3\n",
        "\n",
        "    # Coletar todos os templates\n",
        "    todos_templates = []\n",
        "    for video in dados[\"copywriting\"]:\n",
        "        templates = video.get(\"templates_identificados\", [])\n",
        "        for template in templates:\n",
        "            template[\"video_id\"] = video[\"video_id\"]\n",
        "            todos_templates.append(template)\n",
        "\n",
        "    if todos_templates:\n",
        "        # Agrupar templates por tipo\n",
        "        templates_agrupados = {}\n",
        "        for template in todos_templates:\n",
        "            nome = template[\"nome\"]\n",
        "            if nome not in templates_agrupados:\n",
        "                templates_agrupados[nome] = {\n",
        "                    \"estrutura\": template[\"estrutura\"],\n",
        "                    \"eficacia\": template[\"eficacia\"],\n",
        "                    \"uso_recomendado\": template[\"uso_recomendado\"],\n",
        "                    \"videos_exemplo\": []\n",
        "                }\n",
        "            templates_agrupados[nome][\"videos_exemplo\"].append(template[\"video_id\"])\n",
        "\n",
        "        # Exibir templates\n",
        "        for nome_template, dados_template in templates_agrupados.items():\n",
        "            ws.merge_cells(f\"A{row}:F{row}\")\n",
        "            template_header = ws[f\"A{row}\"]\n",
        "            template_header.value = f\"📋 TEMPLATE: {nome_template.replace('_', ' ')}\"\n",
        "            template_header.fill = PatternFill(start_color=\"E2EFDA\", end_color=\"E2EFDA\", fill_type=\"solid\")\n",
        "            template_header.font = Font(bold=True, size=11)\n",
        "            row += 1\n",
        "\n",
        "            ws[f\"A{row}\"] = \"Estrutura:\"\n",
        "            ws[f\"B{row}\"] = dados_template[\"estrutura\"]\n",
        "            ws[f\"A{row}\"].font = Font(bold=True)\n",
        "            row += 1\n",
        "\n",
        "            ws[f\"A{row}\"] = \"Eficácia:\"\n",
        "            ws[f\"B{row}\"] = dados_template[\"eficacia\"]\n",
        "            ws[f\"A{row}\"].font = Font(bold=True)\n",
        "            if dados_template[\"eficacia\"] == \"MUITO ALTA\":\n",
        "                ws[f\"B{row}\"].font = Font(color=\"70AD47\", bold=True)\n",
        "            elif dados_template[\"eficacia\"] == \"ALTA\":\n",
        "                ws[f\"B{row}\"].font = Font(color=\"C5504B\", bold=True)\n",
        "            row += 1\n",
        "\n",
        "            ws[f\"A{row}\"] = \"Uso Recomendado:\"\n",
        "            ws[f\"B{row}\"] = dados_template[\"uso_recomendado\"]\n",
        "            ws[f\"A{row}\"].font = Font(bold=True)\n",
        "            row += 1\n",
        "\n",
        "            ws[f\"A{row}\"] = \"Vídeos Exemplo:\"\n",
        "            ws[f\"B{row}\"] = \", \".join(dados_template[\"videos_exemplo\"][:3])\n",
        "            ws[f\"A{row}\"].font = Font(bold=True)\n",
        "            row += 1\n",
        "\n",
        "            # Como aplicar\n",
        "            ws[f\"A{row}\"] = \"Como Aplicar:\"\n",
        "            ws[f\"A{row}\"].font = Font(bold=True, color=\"7030A0\")\n",
        "            row += 1\n",
        "\n",
        "            instrucoes = gerar_instrucoes_aplicacao_template(nome_template)\n",
        "            for i, instrucao in enumerate(instrucoes, 1):\n",
        "                ws[f\"B{row}\"] = f\"{i}. {instrucao}\"\n",
        "                row += 1\n",
        "\n",
        "            row += 2\n",
        "\n",
        "    else:\n",
        "        ws[f\"A{row}\"] = \"📋 Ainda não foram identificados templates específicos\"\n",
        "        row += 1\n",
        "        ws[f\"A{row}\"] = \"Execute mais análises para identificar padrões replicáveis\"\n",
        "\n",
        "    # Templates recomendados universais\n",
        "    row += 2\n",
        "    ws.merge_cells(f\"A{row}:F{row}\")\n",
        "    recom_header = ws[f\"A{row}\"]\n",
        "    recom_header.value = \"🎯 TEMPLATES UNIVERSAIS RECOMENDADOS PARA IMPLEMENTAR\"\n",
        "    recom_header.fill = PatternFill(start_color=\"FFC000\", end_color=\"FFC000\", fill_type=\"solid\")\n",
        "    recom_header.font = Font(bold=True, size=12)\n",
        "    row += 1\n",
        "\n",
        "    templates_universais = [\n",
        "        (\"PERGUNTA + VALOR + CTA\", \"Pergunta engajante → Entrega valor → Call-to-action direto\", \"Todos os vídeos educativos\"),\n",
        "        (\"PROBLEMA + AGITAÇÃO + SOLUÇÃO\", \"Identifica dor → Agrava problema → Apresenta solução\", \"Vídeos de vendas e transformação\"),\n",
        "        (\"CURIOSIDADE + HISTÓRIA + ENSINO\", \"Desperta curiosidade → Conta história → Ensina método\", \"Content marketing e autoridade\"),\n",
        "        (\"PROVA SOCIAL + URGÊNCIA + AÇÃO\", \"Mostra resultados → Cria urgência → Direciona ação\", \"Lançamentos e ofertas\")\n",
        "    ]\n",
        "\n",
        "    headers_univ = [\"Template\", \"Estrutura\", \"Aplicação Ideal\"]\n",
        "    for col, header in enumerate(headers_univ, 1):\n",
        "        cell = ws.cell(row=row, column=col)\n",
        "        cell.value = header\n",
        "        cell.font = Font(bold=True)\n",
        "        cell.fill = PatternFill(start_color=\"F2F2F2\", end_color=\"F2F2F2\", fill_type=\"solid\")\n",
        "    row += 1\n",
        "\n",
        "    for nome, estrutura, aplicacao in templates_universais:\n",
        "        ws.cell(row=row, column=1, value=nome)\n",
        "        ws.cell(row=row, column=2, value=estrutura)\n",
        "        ws.cell(row=row, column=3, value=aplicacao)\n",
        "        ws.cell(row=row, column=1).font = Font(bold=True)\n",
        "        row += 1\n",
        "\n",
        "    # Ajustar larguras\n",
        "    for col, width in [(\"A\", 20), (\"B\", 50), (\"C\", 25), (\"D\", 15), (\"E\", 15), (\"F\", 15)]:\n",
        "        ws.column_dimensions[col].width = width\n",
        "\n",
        "def gerar_instrucoes_aplicacao_template(nome_template):\n",
        "    \"\"\"Gera instruções específicas para aplicar um template\"\"\"\n",
        "    instrucoes_map = {\n",
        "        \"PERGUNTA_AUTORIDADE_CTA\": [\n",
        "            \"Inicie com pergunta que conecte com a dor/desejo do público\",\n",
        "            \"Estabeleça credibilidade (experiência, resultados, formação)\",\n",
        "            \"Termine com CTA claro e específico\",\n",
        "            \"Mantenha tom conversacional mas assertivo\"\n",
        "        ],\n",
        "        \"PROBLEMA_SOLUCAO_PROVA\": [\n",
        "            \"Identifique problema específico e real do público\",\n",
        "            \"Apresente solução clara e aplicável\",\n",
        "            \"Mostre provas sociais (depoimentos, números, casos)\",\n",
        "            \"Use linguagem emocional para conectar\"\n",
        "        ],\n",
        "        \"CURIOSIDADE_URGENCIA_ACAO\": [\n",
        "            \"Desperte curiosidade nos primeiros 3 segundos\",\n",
        "            \"Crie senso de urgência (limitado, exclusivo)\",\n",
        "            \"Direcione para ação imediata específica\",\n",
        "            \"Use gatilhos de escassez e FOMO\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    return instrucoes_map.get(nome_template, [\n",
        "        \"Analise a estrutura identificada no vídeo de exemplo\",\n",
        "        \"Adapte os elementos para seu nicho específico\",\n",
        "        \"Teste diferentes abordagens mantendo a estrutura\",\n",
        "        \"Monitore resultados e otimize baseado na performance\"\n",
        "    ])\n",
        "\n",
        "def adicionar_aba_timeline_copy(wb, dados):\n",
        "    \"\"\"Adiciona aba com timeline de elementos persuasivos\"\"\"\n",
        "    from openpyxl.styles import Font, PatternFill, Alignment\n",
        "\n",
        "    ws = wb.create_sheet(\"Timeline Persuasão\")\n",
        "\n",
        "    # Título\n",
        "    ws.merge_cells(\"A1:G1\")\n",
        "    titulo = ws[\"A1\"]\n",
        "    titulo.value = \"TIMELINE DE ELEMENTOS PERSUASIVOS - MAPEAMENTO TEMPORAL\"\n",
        "    titulo.fill = PatternFill(start_color=\"7030A0\", end_color=\"7030A0\", fill_type=\"solid\")\n",
        "    titulo.font = Font(color=\"FFFFFF\", bold=True, size=14)\n",
        "    titulo.alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "    row = 3\n",
        "\n",
        "    # Análise temporal por vídeo (mostrar apenas top 3 por brevidade)\n",
        "    videos_copy = sorted(dados[\"copywriting\"], key=lambda x: x.get(\"score_persuasao\", 0), reverse=True)\n",
        "\n",
        "    for video in videos_copy[:3]:  # Top 3 performers\n",
        "        ws.merge_cells(f\"A{row}:G{row}\")\n",
        "        video_header = ws[f\"A{row}\"]\n",
        "        video_header.value = f\"🎬 TIMELINE: {video['video_id']} (Score: {video.get('score_persuasao', 0)}/100)\"\n",
        "        video_header.fill = PatternFill(start_color=\"E2EFDA\", end_color=\"E2EFDA\", fill_type=\"solid\")\n",
        "        video_header.font = Font(bold=True)\n",
        "        row += 1\n",
        "\n",
        "        # Headers da timeline\n",
        "        headers = [\"Tempo\", \"Tipo\", \"Elemento\", \"Contexto\", \"Posição\", \"Impacto\", \"Análise\"]\n",
        "        for col, header in enumerate(headers, 1):\n",
        "            cell = ws.cell(row=row, column=col)\n",
        "            cell.value = header\n",
        "            cell.font = Font(bold=True)\n",
        "            cell.fill = PatternFill(start_color=\"F2F2F2\", end_color=\"F2F2F2\", fill_type=\"solid\")\n",
        "        row += 1\n",
        "\n",
        "        # Consolidar timeline\n",
        "        timeline_elementos = []\n",
        "\n",
        "        # Adicionar elementos de cada categoria\n",
        "        for categoria, timeline_key in [(\"GANCHO\", \"ganchos_timeline\"), (\"GATILHO\", \"gatilhos_timeline\"), (\"CTA\", \"ctas_timeline\")]:\n",
        "            timeline_data = video.get(\"timestamp\", {}).get(timeline_key, [])\n",
        "            for item in timeline_data:\n",
        "                timeline_elementos.append({\n",
        "                    \"categoria\": categoria,\n",
        "                    \"tempo\": f\"{item['minuto']:02d}:{item['segundo']:02d}\",\n",
        "                    \"tipo\": item[\"tipo\"].replace(\"_\", \" \").title(),\n",
        "                    \"contexto\": item.get(\"contexto\", \"\")[:40] + \"...\" if len(item.get(\"contexto\", \"\")) > 40 else item.get(\"contexto\", \"\"),\n",
        "                    \"minuto\": item[\"minuto\"],\n",
        "                    \"segundo\": item[\"segundo\"]\n",
        "                })\n",
        "\n",
        "        # Ordenar por tempo\n",
        "        timeline_elementos.sort(key=lambda x: (x[\"minuto\"], x[\"segundo\"]))\n",
        "\n",
        "        if timeline_elementos:\n",
        "            for elemento in timeline_elementos:\n",
        "                ws.cell(row=row, column=1, value=elemento[\"tempo\"])\n",
        "                ws.cell(row=row, column=2, value=elemento[\"categoria\"])\n",
        "                ws.cell(row=row, column=3, value=elemento[\"tipo\"])\n",
        "                ws.cell(row=row, column=4, value=elemento[\"contexto\"])\n",
        "\n",
        "                # Calcular posição no vídeo\n",
        "                total_segundos = elemento[\"minuto\"] * 60 + elemento[\"segundo\"]\n",
        "                if total_segundos <= 10:\n",
        "                    posicao = \"ABERTURA\"\n",
        "                    posicao_color = \"70AD47\"\n",
        "                elif total_segundos <= 20:\n",
        "                    posicao = \"MEIO\"\n",
        "                    posicao_color = \"FFC000\"\n",
        "                else:\n",
        "                    posicao = \"FINAL\"\n",
        "                    posicao_color = \"C5504B\"\n",
        "\n",
        "                cell_pos = ws.cell(row=row, column=5, value=posicao)\n",
        "                cell_pos.font = Font(color=posicao_color, bold=True)\n",
        "\n",
        "                # Análise de impacto\n",
        "                impacto = analisar_impacto_elemento(elemento[\"categoria\"], posicao)\n",
        "                ws.cell(row=row, column=6, value=impacto[\"score\"])\n",
        "                ws.cell(row=row, column=7, value=impacto[\"analise\"])\n",
        "\n",
        "                if impacto[\"score\"] == \"ALTO\":\n",
        "                    ws.cell(row=row, column=6).font = Font(color=\"70AD47\", bold=True)\n",
        "                elif impacto[\"score\"] == \"BAIXO\":\n",
        "                    ws.cell(row=row, column=6).font = Font(color=\"C5504B\", bold=True)\n",
        "\n",
        "                row += 1\n",
        "        else:\n",
        "            ws.cell(row=row, column=1, value=\"Nenhum elemento temporal mapeado\")\n",
        "            row += 1\n",
        "\n",
        "        row += 2\n",
        "\n",
        "    # Padrões temporais identificados\n",
        "    row += 1\n",
        "    ws.merge_cells(f\"A{row}:G{row}\")\n",
        "    padroes_header = ws[f\"A{row}\"]\n",
        "    padroes_header.value = \"📊 PADRÕES TEMPORAIS IDENTIFICADOS\"\n",
        "    padroes_header.fill = PatternFill(start_color=\"1F4E79\", end_color=\"1F4E79\", fill_type=\"solid\")\n",
        "    padroes_header.font = Font(color=\"FFFFFF\", bold=True, size=12)\n",
        "    row += 1\n",
        "\n",
        "    padroes_temporais = [\n",
        "        \"✅ GANCHOS mais eficazes nos primeiros 10 segundos\",\n",
        "        \"✅ GATILHOS MENTAIS ideais entre 10-20 segundos\",\n",
        "        \"✅ CTAs mais conversores nos últimos 5 segundos\",\n",
        "        \"⚠️ Evitar CTAs nos primeiros 5 segundos\",\n",
        "        \"📈 Combinar CURIOSIDADE + AUTORIDADE = alta retenção\"\n",
        "    ]\n",
        "\n",
        "    for padrao in padroes_temporais:\n",
        "        ws[f\"A{row}\"] = padrao\n",
        "        if \"✅\" in padrao:\n",
        "            ws[f\"A{row}\"].font = Font(color=\"70AD47\", bold=True)\n",
        "        elif \"⚠️\" in padrao:\n",
        "            ws[f\"A{row}\"].font = Font(color=\"FFC000\", bold=True)\n",
        "        else:\n",
        "            ws[f\"A{row}\"].font = Font(color=\"1F4E79\", bold=True)\n",
        "        row += 1\n",
        "\n",
        "    # Ajustar larguras\n",
        "    for col, width in [(\"A\", 8), (\"B\", 10), (\"C\", 15), (\"D\", 30), (\"E\", 12), (\"F\", 8), (\"G\", 25)]:\n",
        "        ws.column_dimensions[col].width = width\n",
        "\n",
        "def analisar_impacto_elemento(categoria, posicao):\n",
        "    \"\"\"Analisa o impacto de um elemento baseado na posição\"\"\"\n",
        "    impactos = {\n",
        "        (\"GANCHO\", \"ABERTURA\"): {\"score\": \"ALTO\", \"analise\": \"Ideal para capturar atenção\"},\n",
        "        (\"GANCHO\", \"MEIO\"): {\"score\": \"MÉDIO\", \"analise\": \"Melhor no início\"},\n",
        "        (\"GANCHO\", \"FINAL\"): {\"score\": \"BAIXO\", \"analise\": \"Reposicionar para abertura\"},\n",
        "        (\"GATILHO\", \"ABERTURA\"): {\"score\": \"MÉDIO\", \"analise\": \"Bom para credibilidade\"},\n",
        "        (\"GATILHO\", \"MEIO\"): {\"score\": \"ALTO\", \"analise\": \"Posição ideal para persuasão\"},\n",
        "        (\"GATILHO\", \"FINAL\"): {\"score\": \"MÉDIO\", \"analise\": \"Reforça decisão\"},\n",
        "        (\"CTA\", \"ABERTURA\"): {\"score\": \"BAIXO\", \"analise\": \"Muito cedo, construir valor primeiro\"},\n",
        "        (\"CTA\", \"MEIO\"): {\"score\": \"MÉDIO\", \"analise\": \"Considerar mover para final\"},\n",
        "        (\"CTA\", \"FINAL\"): {\"score\": \"ALTO\", \"analise\": \"Posicionamento ideal\"}\n",
        "    }\n",
        "\n",
        "    return impactos.get((categoria, posicao), {\"score\": \"MÉDIO\", \"analise\": \"Analisar contexto específico\"})\n",
        "\n",
        "def adicionar_aba_recomendacoes_copy(wb, dados):\n",
        "    \"\"\"Adiciona aba de recomendações estratégicas consolidadas\"\"\"\n",
        "    from openpyxl.styles import Font, PatternFill, Alignment\n",
        "\n",
        "    ws = wb.create_sheet(\"Recomendações Copy\")\n",
        "\n",
        "    # Título\n",
        "    ws.merge_cells(\"A1:F1\")\n",
        "    titulo = ws[\"A1\"]\n",
        "    titulo.value = \"RECOMENDAÇÕES ESTRATÉGICAS DE COPYWRITING - PLANO DE AÇÃO\"\n",
        "    titulo.fill = PatternFill(start_color=\"C5504B\", end_color=\"C5504B\", fill_type=\"solid\")\n",
        "    titulo.font = Font(color=\"FFFFFF\", bold=True, size=14)\n",
        "    titulo.alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "    row = 3\n",
        "\n",
        "    # Consolidar recomendações por prioridade\n",
        "    todas_recomendacoes = []\n",
        "    for video in dados[\"copywriting\"]:\n",
        "        recomendacoes_video = video.get(\"recomendacoes_estrategicas\", [])\n",
        "        for rec in recomendacoes_video:\n",
        "            rec[\"video_id\"] = video[\"video_id\"]\n",
        "            todas_recomendacoes.append(rec)\n",
        "\n",
        "    # Agrupar por prioridade\n",
        "    recomendacoes_por_prioridade = {\n",
        "        \"CRÍTICA\": [],\n",
        "        \"ALTA\": [],\n",
        "        \"MÉDIA\": []\n",
        "    }\n",
        "\n",
        "    for rec in todas_recomendacoes:\n",
        "        prioridade = rec.get(\"prioridade\", \"MÉDIA\")\n",
        "        if prioridade in recomendacoes_por_prioridade:\n",
        "            recomendacoes_por_prioridade[prioridade].append(rec)\n",
        "\n",
        "    # Exibir por prioridade\n",
        "    for prioridade in [\"CRÍTICA\", \"ALTA\", \"MÉDIA\"]:\n",
        "        if not recomendacoes_por_prioridade[prioridade]:\n",
        "            continue\n",
        "\n",
        "        ws[f\"A{row}\"] = f\"🚨 PRIORIDADE {prioridade}\"\n",
        "        if prioridade == \"CRÍTICA\":\n",
        "            ws[f\"A{row}\"].font = Font(color=\"FF0000\", bold=True, size=12)\n",
        "        elif prioridade == \"ALTA\":\n",
        "            ws[f\"A{row}\"].font = Font(color=\"C5504B\", bold=True, size=12)\n",
        "        else:\n",
        "            ws[f\"A{row}\"].font = Font(color=\"FFC000\", bold=True, size=12)\n",
        "\n",
        "        row += 2\n",
        "\n",
        "        # Headers\n",
        "        headers = [\"Categoria\", \"Recomendação\", \"Vídeos Afetados\", \"Ação Sugerida\"]\n",
        "        for col, header in enumerate(headers, 1):\n",
        "            cell = ws.cell(row=row, column=col)\n",
        "            cell.value = header\n",
        "            cell.font = Font(bold=True)\n",
        "            cell.fill = PatternFill(start_color=\"F2F2F2\", end_color=\"F2F2F2\", fill_type=\"solid\")\n",
        "        row += 1\n",
        "\n",
        "        # Agrupar recomendações similares da mesma prioridade\n",
        "        grupos = {}\n",
        "        for rec in recomendacoes_por_prioridade[prioridade]:\n",
        "            categoria = rec[\"categoria\"]\n",
        "            if categoria not in grupos:\n",
        "                grupos[categoria] = {\n",
        "                    \"recomendacao\": rec[\"recomendacao\"],\n",
        "                    \"videos\": [],\n",
        "                    \"acao\": gerar_acao_especifica(categoria)\n",
        "                }\n",
        "            grupos[categoria][\"videos\"].append(rec[\"video_id\"])\n",
        "\n",
        "        for categoria, dados_grupo in grupos.items():\n",
        "            ws.cell(row=row, column=1, value=categoria)\n",
        "            ws.cell(row=row, column=2, value=dados_grupo[\"recomendacao\"])\n",
        "            ws.cell(row=row, column=3, value=f\"{len(dados_grupo['videos'])} vídeo(s)\")\n",
        "            ws.cell(row=row, column=4, value=dados_grupo[\"acao\"])\n",
        "            row += 1\n",
        "\n",
        "        row += 2\n",
        "\n",
        "    # Plano de ação 30 dias\n",
        "    row += 2\n",
        "    ws.merge_cells(f\"A{row}:F{row}\")\n",
        "    plano_header = ws[f\"A{row}\"]\n",
        "    plano_header.value = \"📅 PLANO DE AÇÃO ESTRATÉGICO - PRÓXIMOS 30 DIAS\"\n",
        "    plano_header.fill = PatternFill(start_color=\"70AD47\", end_color=\"70AD47\", fill_type=\"solid\")\n",
        "    plano_header.font = Font(color=\"FFFFFF\", bold=True, size=12)\n",
        "    row += 2\n",
        "\n",
        "    plano_30_dias = [\n",
        "        (\"SEMANA 1 - CRÍTICO\", [\n",
        "            \"Implementar CTAs em TODOS os vídeos sem call-to-action\",\n",
        "            \"Corrigir vídeos com score de persuasão abaixo de 30\",\n",
        "            \"Aplicar templates identificados nos vídeos top performers\"\n",
        "        ]),\n",
        "        (\"SEMANA 2 - ALTA PRIORIDADE\", [\n",
        "            \"Adicionar ganchos de abertura nos vídeos com baixa retenção\",\n",
        "            \"Incorporar gatilhos de autoridade e prova social\",\n",
        "            \"Otimizar timeline de elementos persuasivos\"\n",
        "        ]),\n",
        "        (\"SEMANA 3 - OTIMIZAÇÃO\", [\n",
        "            \"Testar variações de CTAs mais eficazes\",\n",
        "            \"Refinar estruturas narrativas baseadas nos templates\",\n",
        "            \"A/B testing de elementos específicos\"\n",
        "        ]),\n",
        "        (\"SEMANA 4 - VALIDAÇÃO\", [\n",
        "            \"Medir performance pós-implementação\",\n",
        "            \"Documentar novos padrões de sucesso identificados\",\n",
        "            \"Atualizar biblioteca de templates comprovados\"\n",
        "        ])\n",
        "    ]\n",
        "\n",
        "    for semana_titulo, acoes in plano_30_dias:\n",
        "        ws[f\"A{row}\"] = semana_titulo\n",
        "        ws[f\"A{row}\"].font = Font(bold=True, color=\"1F4E79\", size=11)\n",
        "        row += 1\n",
        "\n",
        "        for acao in acoes:\n",
        "            ws[f\"B{row}\"] = f\"• {acao}\"\n",
        "            row += 1\n",
        "\n",
        "        row += 1\n",
        "\n",
        "    # KPIs de acompanhamento\n",
        "    row += 2\n",
        "    ws.merge_cells(f\"A{row}:F{row}\")\n",
        "    kpis_header = ws[f\"A{row}\"]\n",
        "    kpis_header.value = \"📊 KPIs DE ACOMPANHAMENTO - MÉTRICAS DE SUCESSO\"\n",
        "    kpis_header.fill = PatternFill(start_color=\"7030A0\", end_color=\"7030A0\", fill_type=\"solid\")\n",
        "    kpis_header.font = Font(color=\"FFFFFF\", bold=True, size=12)\n",
        "    row += 2\n",
        "\n",
        "    kpis = [\n",
        "        (\"Score de Persuasão Médio\", \"Aumento de 20% em 30 dias\", \"Mensal\"),\n",
        "        (\"Taxa de CTAs Implementados\", \"100% dos vídeos com pelo menos 1 CTA\", \"Imediato\"),\n",
        "        (\"Variedade de Ganchos\", \"3+ tipos diferentes por vídeo\", \"Por vídeo\"),\n",
        "        (\"Diversidade de Gatilhos\", \"4+ gatilhos mentais por vídeo\", \"Por vídeo\"),\n",
        "        (\"Templates Ativos\", \"5+ estruturas replicáveis em uso\", \"Mensal\"),\n",
        "        (\"Taxa de Otimização\", \"80% das recomendações críticas aplicadas\", \"Semanal\")\n",
        "    ]\n",
        "\n",
        "    headers_kpi = [\"KPI\", \"Meta\", \"Frequência de Medição\"]\n",
        "    for col, header in enumerate(headers_kpi, 1):\n",
        "        cell = ws.cell(row=row, column=col)\n",
        "        cell.value = header\n",
        "        cell.font = Font(bold=True)\n",
        "        cell.fill = PatternFill(start_color=\"E2EFDA\", end_color=\"E2EFDA\", fill_type=\"solid\")\n",
        "    row += 1\n",
        "\n",
        "    for kpi_nome, meta, frequencia in kpis:\n",
        "        ws.cell(row=row, column=1, value=kpi_nome)\n",
        "        ws.cell(row=row, column=2, value=meta)\n",
        "        ws.cell(row=row, column=3, value=frequencia)\n",
        "        row += 1\n",
        "\n",
        "    # Próximos passos imediatos\n",
        "    row += 3\n",
        "    ws[f\"A{row}\"] = \"🎯 PRÓXIMOS PASSOS IMEDIATOS (HOJE)\"\n",
        "    ws[f\"A{row}\"].font = Font(bold=True, color=\"C5504B\", size=12)\n",
        "    row += 1\n",
        "\n",
        "    proximos_passos = gerar_proximos_passos_imediatos(dados[\"copywriting\"])\n",
        "\n",
        "    for i, passo in enumerate(proximos_passos, 1):\n",
        "        ws[f\"A{row}\"] = f\"{i}. {passo}\"\n",
        "        ws[f\"A{row}\"].font = Font(bold=True)\n",
        "        row += 1\n",
        "\n",
        "    # Ajustar larguras\n",
        "    for col, width in [(\"A\", 20), (\"B\", 40), (\"C\", 15), (\"D\", 30), (\"E\", 15), (\"F\", 15)]:\n",
        "        ws.column_dimensions[col].width = width\n",
        "\n",
        "def gerar_acao_especifica(categoria):\n",
        "    \"\"\"Gera ação específica baseada na categoria da recomendação\"\"\"\n",
        "    acoes = {\n",
        "        \"GANCHOS\": \"Revisar primeiros 5 segundos e adicionar pergunta ou curiosidade\",\n",
        "        \"GATILHOS\": \"Incorporar elementos de autoridade, prova social ou reciprocidade\",\n",
        "        \"CTA\": \"Adicionar call-to-action claro nos últimos 3-5 segundos\",\n",
        "        \"ESTRUTURA\": \"Aplicar template identificado mais próximo do nicho\",\n",
        "        \"PERSUASÃO\": \"Combinar múltiplos elementos persuasivos em sequência lógica\"\n",
        "    }\n",
        "    return acoes.get(categoria, \"Revisar e otimizar elementos específicos mencionados\")\n",
        "\n",
        "def gerar_proximos_passos_imediatos(videos_copy):\n",
        "    \"\"\"Gera lista de ações imediatas baseadas na análise\"\"\"\n",
        "    passos = []\n",
        "\n",
        "    # Verificar vídeos sem CTA\n",
        "    videos_sem_cta = [v for v in videos_copy if not v.get(\"ctas_detectados\")]\n",
        "    if videos_sem_cta:\n",
        "        passos.append(f\"CRÍTICO: Adicionar CTAs em {len(videos_sem_cta)} vídeo(s): {', '.join([v['video_id'] for v in videos_sem_cta[:3]])}\")\n",
        "\n",
        "    # Verificar scores baixos\n",
        "    videos_score_baixo = [v for v in videos_copy if v.get(\"score_persuasao\", 0) < 30]\n",
        "    if videos_score_baixo:\n",
        "        passos.append(f\"Revisar {len(videos_score_baixo)} vídeo(s) com score crítico < 30\")\n",
        "\n",
        "    # Templates a aplicar\n",
        "    templates_identificados = []\n",
        "    for video in videos_copy:\n",
        "        templates_identificados.extend(video.get(\"templates_identificados\", []))\n",
        "\n",
        "    if templates_identificados:\n",
        "        template_mais_comum = max(set(t[\"nome\"] for t in templates_identificados),\n",
        "                                 key=lambda x: sum(1 for t in templates_identificados if t[\"nome\"] == x))\n",
        "        passos.append(f\"Aplicar template '{template_mais_comum.replace('_', ' ')}' em novos vídeos\")\n",
        "\n",
        "    # Ações gerais\n",
        "    passos.extend([\n",
        "        \"Backup dos vídeos atuais antes das modificações\",\n",
        "        \"Priorizar implementações por ordem de impacto (CTAs primeiro)\",\n",
        "        \"Documentar mudanças para acompanhar resultados\"\n",
        "    ])\n",
        "\n",
        "    return passos[:6]  # Limitar a 6 passos\n",
        "\n",
        "def atualizar_aba_principal_com_copy(wb, dados):\n",
        "    \"\"\"Atualiza a aba principal existente com métricas de copywriting\"\"\"\n",
        "    # Tentar encontrar aba principal (pode ter nomes diferentes)\n",
        "    aba_principal = None\n",
        "    possiveis_nomes = [\"Dashboard Principal\", \"Executive Summary\", \"Summary\", \"Principal\"]\n",
        "\n",
        "    for nome in wb.sheetnames:\n",
        "        if any(possivel in nome for possivel in possiveis_nomes):\n",
        "            aba_principal = wb[nome]\n",
        "            break\n",
        "\n",
        "    if not aba_principal:\n",
        "        # Se não encontrou, usar a primeira aba\n",
        "        aba_principal = wb.worksheets[0]\n",
        "\n",
        "    # Encontrar próxima linha vazia para adicionar seção de copywriting\n",
        "    next_row = 1\n",
        "    for row in range(1, 100):\n",
        "        if aba_principal[f\"A{row}\"].value is None:\n",
        "            next_row = row\n",
        "            break\n",
        "\n",
        "    # Adicionar seção de copywriting\n",
        "    from openpyxl.styles import Font, PatternFill\n",
        "\n",
        "    # Título da seção\n",
        "    aba_principal.merge_cells(f\"A{next_row}:H{next_row}\")\n",
        "    titulo_copy = aba_principal[f\"A{next_row}\"]\n",
        "    titulo_copy.value = \"📝 ANÁLISE DE COPYWRITING - RESUMO EXECUTIVO\"\n",
        "    titulo_copy.fill = PatternFill(start_color=\"7030A0\", end_color=\"7030A0\", fill_type=\"solid\")\n",
        "    titulo_copy.font = Font(color=\"FFFFFF\", bold=True, size=12)\n",
        "    next_row += 2\n",
        "\n",
        "    # Métricas resumidas\n",
        "    videos_copy = dados[\"copywriting\"]\n",
        "\n",
        "    if videos_copy:\n",
        "        scores = [v.get(\"score_persuasao\", 0) for v in videos_copy]\n",
        "        score_medio = sum(scores) / len(scores)\n",
        "        videos_sem_cta = len([v for v in videos_copy if not v.get(\"ctas_detectados\")])\n",
        "        templates_total = sum(len(v.get(\"templates_identificados\", [])) for v in videos_copy)\n",
        "\n",
        "        metricas_resumo = [\n",
        "            (\"Score de Persuasão Médio:\", f\"{score_medio:.1f}/100\"),\n",
        "            (\"Vídeos sem CTA:\", f\"{videos_sem_cta} (CRÍTICO)\" if videos_sem_cta > 0 else \"0 ✅\"),\n",
        "            (\"Templates Identificados:\", str(templates_total)),\n",
        "            (\"Status Geral:\", \"Otimização necessária\" if score_medio < 60 or videos_sem_cta > 0 else \"Performance boa\")\n",
        "        ]\n",
        "\n",
        "        for metrica, valor in metricas_resumo:\n",
        "            aba_principal[f\"A{next_row}\"] = metrica\n",
        "            aba_principal[f\"B{next_row}\"] = valor\n",
        "            aba_principal[f\"A{next_row}\"].font = Font(bold=True)\n",
        "\n",
        "            if \"CRÍTICO\" in valor:\n",
        "                aba_principal[f\"B{next_row}\"].font = Font(color=\"FF0000\", bold=True)\n",
        "            elif \"✅\" in valor:\n",
        "                aba_principal[f\"B{next_row}\"].font = Font(color=\"70AD47\", bold=True)\n",
        "\n",
        "            next_row += 1\n",
        "\n",
        "    else:\n",
        "        aba_principal[f\"A{next_row}\"] = \"⚠️ Execute a análise de copywriting (Célula 2.4) para ver métricas\"\n",
        "        aba_principal[f\"A{next_row}\"].font = Font(color=\"FFC000\", bold=True)\n",
        "\n",
        "def gerar_relatorios_copywriting_individuais(dados):\n",
        "    \"\"\"Gera relatórios individuais de texto para cada vídeo\"\"\"\n",
        "    print(\"  📄 Gerando relatórios individuais de copywriting...\")\n",
        "\n",
        "    pasta_relatorios = os.path.join(PASTA_TRABALHO, \"relatorios_copywriting\")\n",
        "    os.makedirs(pasta_relatorios, exist_ok=True)\n",
        "\n",
        "    for video_copy in dados[\"copywriting\"]:\n",
        "        video_id = video_copy[\"video_id\"]\n",
        "\n",
        "        relatorio_path = os.path.join(pasta_relatorios, f\"{video_id}_copywriting_completo.txt\")\n",
        "\n",
        "        with open(relatorio_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\"=\"*60 + \"\\n\")\n",
        "            f.write(\"RELATÓRIO COMPLETO DE ANÁLISE DE COPYWRITING\\n\")\n",
        "            f.write(\"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "            f.write(f\"📹 Vídeo ID: {video_id}\\n\")\n",
        "            f.write(f\"🎯 Score de Persuasão: {video_copy.get('score_persuasao', 0)}/100\\n\")\n",
        "            f.write(f\"📝 Total de Palavras: {video_copy.get('total_palavras', 0)}\\n\\n\")\n",
        "\n",
        "            # Texto completo\n",
        "            f.write(\"TRANSCRIÇÃO COMPLETA:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            f.write(video_copy.get(\"texto_completo\", \"Transcrição não disponível\") + \"\\n\\n\")\n",
        "\n",
        "            # Ganchos\n",
        "            f.write(\"🎣 GANCHOS DETECTADOS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            ganchos = video_copy.get(\"ganchos_detectados\", {})\n",
        "            if ganchos:\n",
        "                for tipo, dados in ganchos.items():\n",
        "                    f.write(f\"• {tipo.replace('_', ' ').title()}: {dados['count']} ocorrência(s)\\n\")\n",
        "                    for exemplo in dados.get(\"exemplos\", [])[:2]:\n",
        "                        f.write(f\"  - \\\"{exemplo}\\\"\\n\")\n",
        "                    f.write(\"\\n\")\n",
        "            else:\n",
        "                f.write(\"❌ Nenhum gancho detectado - OPORTUNIDADE DE MELHORIA\\n\\n\")\n",
        "\n",
        "            # Gatilhos\n",
        "            f.write(\"🧠 GATILHOS MENTAIS DETECTADOS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            gatilhos = video_copy.get(\"gatilhos_mentais_detectados\", {})\n",
        "            if gatilhos:\n",
        "                for tipo, dados in gatilhos.items():\n",
        "                    f.write(f\"• {tipo.replace('_', ' ').title()}: {dados['count']} ocorrência(s)\\n\")\n",
        "                    for exemplo in dados.get(\"exemplos\", [])[:2]:\n",
        "                        f.write(f\"  - \\\"{exemplo}\\\"\\n\")\n",
        "                    f.write(\"\\n\")\n",
        "            else:\n",
        "                f.write(\"❌ Nenhum gatilho mental detectado - ADICIONAR URGENTEMENTE\\n\\n\")\n",
        "\n",
        "            # CTAs\n",
        "            f.write(\"📢 CALLS-TO-ACTION DETECTADOS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            ctas = video_copy.get(\"ctas_detectados\", {})\n",
        "            if ctas:\n",
        "                for tipo, dados in ctas.items():\n",
        "                    f.write(f\"• {tipo.replace('_', ' ').title()}: {dados['count']} ocorrência(s)\\n\")\n",
        "                    for exemplo in dados.get(\"exemplos\", [])[:2]:\n",
        "                        f.write(f\"  - \\\"{exemplo}\\\"\\n\")\n",
        "                    f.write(\"\\n\")\n",
        "            else:\n",
        "                f.write(\"🚨 CRÍTICO: Nenhum CTA detectado - IMPLEMENTAR IMEDIATAMENTE\\n\\n\")\n",
        "\n",
        "            # Templates\n",
        "            f.write(\"📋 TEMPLATES IDENTIFICADOS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            templates = video_copy.get(\"templates_identificados\", [])\n",
        "            if templates:\n",
        "                for template in templates:\n",
        "                    f.write(f\"• {template['nome'].replace('_', ' ')}\\n\")\n",
        "                    f.write(f\"  Estrutura: {template['estrutura']}\\n\")\n",
        "                    f.write(f\"  Eficácia: {template['eficacia']}\\n\")\n",
        "                    f.write(f\"  Uso: {template['uso_recomendado']}\\n\\n\")\n",
        "            else:\n",
        "                f.write(\"📝 Nenhum template específico identificado\\n\\n\")\n",
        "\n",
        "            # Recomendações\n",
        "            f.write(\"🎯 RECOMENDAÇÕES ESTRATÉGICAS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            recomendacoes = video_copy.get(\"recomendacoes_estrategicas\", [])\n",
        "            if recomendacoes:\n",
        "                for i, rec in enumerate(recomendacoes, 1):\n",
        "                    f.write(f\"{i}. [{rec['prioridade']}] {rec['categoria']}\\n\")\n",
        "                    f.write(f\"   {rec['recomendacao']}\\n\\n\")\n",
        "            else:\n",
        "                f.write(\"✅ Nenhuma recomendação crítica - vídeo bem otimizado\\n\\n\")\n",
        "\n",
        "            # Timeline resumida\n",
        "            f.write(\"⏰ TIMELINE DE ELEMENTOS (RESUMIDA):\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            timeline_ganchos = video_copy.get(\"timestamp\", {}).get(\"ganchos_timeline\", [])\n",
        "            timeline_ctas = video_copy.get(\"timestamp\", {}).get(\"ctas_timeline\", [])\n",
        "\n",
        "            todos_elementos = []\n",
        "            for item in timeline_ganchos:\n",
        "                todos_elementos.append((item[\"minuto\"], item[\"segundo\"], \"GANCHO\", item[\"tipo\"]))\n",
        "            for item in timeline_ctas:\n",
        "                todos_elementos.append((item[\"minuto\"], item[\"segundo\"], \"CTA\", item[\"tipo\"]))\n",
        "\n",
        "            todos_elementos.sort()\n",
        "\n",
        "            if todos_elementos:\n",
        "                for minuto, segundo, categoria, tipo in todos_elementos:\n",
        "                    f.write(f\"[{minuto:02d}:{segundo:02d}] {categoria}: {tipo.replace('_', ' ').title()}\\n\")\n",
        "            else:\n",
        "                f.write(\"Nenhum elemento temporal mapeado\\n\")\n",
        "\n",
        "            f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "            f.write(\"Relatório gerado pelo sistema de engenharia reversa\\n\")\n",
        "            f.write(\"Para implementar as recomendações, consulte o dashboard principal\\n\")\n",
        "            f.write(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    print(f\"  ✅ {len(dados['copywriting'])} relatórios individuais gerados\")\n",
        "\n",
        "# Função principal de execução\n",
        "def executar_integracao_copywriting_dashboard():\n",
        "    \"\"\"Função principal para executar a integração\"\"\"\n",
        "    print(\"🚀 EXECUTANDO INTEGRAÇÃO DE COPYWRITING NO DASHBOARD EXISTENTE\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    try:\n",
        "        dashboard_atualizado = integrar_copywriting_dashboard_existente()\n",
        "\n",
        "        if dashboard_atualizado:\n",
        "            print(\"\\n\" + \"=\"*70)\n",
        "            print(\"✅ INTEGRAÇÃO CONCLUÍDA COM SUCESSO!\")\n",
        "            print(\"=\"*70)\n",
        "            print(f\"📊 Dashboard atualizado: {os.path.basename(dashboard_atualizado)}\")\n",
        "            print(\"\\n📋 ABAS ADICIONADAS:\")\n",
        "            print(\"  • Copywriting Estratégico - Análise completa por vídeo\")\n",
        "            print(\"  • Templates Replicáveis - Estruturas identificadas\")\n",
        "            print(\"  • Timeline Persuasão - Mapeamento temporal\")\n",
        "            print(\"  • Recomendações Copy - Plano de ação 30 dias\")\n",
        "            print(\"  • Dashboard Principal - Atualizada com métricas\")\n",
        "\n",
        "            print(f\"\\n🎯 PRÓXIMOS PASSOS:\")\n",
        "            print(\"1. Abra o dashboard e revise a aba 'Copywriting Estratégico'\")\n",
        "            print(\"2. Identifique vídeos com score < 50 para otimização\")\n",
        "            print(\"3. Implemente CTAs nos vídeos marcados como CRÍTICO\")\n",
        "            print(\"4. Aplique templates identificados em novos vídeos\")\n",
        "            print(\"5. Siga o plano de ação de 30 dias na aba 'Recomendações Copy'\")\n",
        "\n",
        "        else:\n",
        "            print(\"\\n❌ Falha na integração - verifique os pré-requisitos\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Erro de Execução: {type(e).__name__}: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# Executar a integração\n",
        "if __name__ == \"__main__\":\n",
        "    executar_integracao_copywriting_dashboard()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-wM_X9W5nZU",
        "outputId": "a71bc0c4-1390-49c2-ea9e-7778a71e4eec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Iniciando processamento de copywriting adaptado...\n",
            "✅ Dados de decomposição carregados: 3 vídeos encontrados\n",
            "📊 Processando 3 vídeos com transcrição válida...\n",
            "[1/3] Processando copywriting para: vid_bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t\n",
            "  🔄 Gerando legendas com timestamps...\n",
            "    ✅ Legendas SRT geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t_legendas.srt\n",
            "    ✅ Legendas TXT com timestamps geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t_legendas_timestamped.txt\n",
            "    🔄 Analisando copywriting...\n",
            "  ✅ Copywriting analisado: Score 0/100\n",
            "[2/3] Processando copywriting para: vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf\n",
            "  🔄 Gerando legendas com timestamps...\n",
            "    ✅ Legendas SRT geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf_legendas.srt\n",
            "    ✅ Legendas TXT com timestamps geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf_legendas_timestamped.txt\n",
            "    🔄 Analisando copywriting...\n",
            "  ✅ Copywriting analisado: Score 30/100\n",
            "[3/3] Processando copywriting para: vid_máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy\n",
            "  🔄 Gerando legendas com timestamps...\n",
            "    ✅ Legendas SRT geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy_legendas.srt\n",
            "    ✅ Legendas TXT com timestamps geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy_legendas_timestamped.txt\n",
            "    🔄 Analisando copywriting...\n",
            "  ✅ Copywriting analisado: Score 75/100\n",
            "💾 Análises de copywriting salvas em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/analises_copywriting_completas.json\n",
            "💾 Dados de legendas salvos em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/legendas_geradas.json\n",
            "\n",
            "✅ ANÁLISE DE COPYWRITING CONCLUÍDA!\n",
            "Total de vídeos com copywriting analisado: 3\n",
            "Total de legendas geradas: 3\n",
            "\n",
            "➡️ PRÓXIMA CÉLULA: 4.3 - INTEGRAÇÃO COM DASHBOARD\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# LAYER 2.4: GERAÇÃO DE LEGENDAS E ANÁLISE DE COPYWRITING - VERSÃO FINAL\n",
        "# ============================================================================\n",
        "\n",
        "import re\n",
        "from datetime import timedelta, datetime\n",
        "from collections import Counter\n",
        "import json\n",
        "import os\n",
        "\n",
        "def processar_copywriting_todos_videos_adaptado():\n",
        "    \"\"\"Processa análise de copywriting adaptada para o sistema existente\"\"\"\n",
        "    print(\"🔄 Iniciando processamento de copywriting adaptado...\")\n",
        "\n",
        "    # Verificar pré-requisitos baseado na estrutura existente\n",
        "    if not \"PASTA_TRABALHO\" in globals():\n",
        "        print(\"❌ Variáveis globais não encontradas. Execute a CÉLULA 1.2 primeiro.\")\n",
        "        return\n",
        "\n",
        "    pasta_dados = os.path.join(PASTA_TRABALHO, \"dados\")\n",
        "    if not os.path.exists(pasta_dados):\n",
        "        print(\"❌ Pasta de dados não encontrada. Execute as células anteriores primeiro.\")\n",
        "        return\n",
        "\n",
        "    # Buscar dados de decomposição (nome correto do arquivo)\n",
        "    decomposicao_path = os.path.join(pasta_dados, \"decomposicao_completa.json\")\n",
        "\n",
        "    if not os.path.exists(decomposicao_path):\n",
        "        print(\"❌ Dados de decomposição não encontrados. Execute a CÉLULA 2.3 primeiro.\")\n",
        "        print(f\"Procurando arquivo: {decomposicao_path}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        with open(decomposicao_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            decomposicoes_data = json.load(f)\n",
        "\n",
        "        print(f\"✅ Dados de decomposição carregados: {len(decomposicoes_data)} vídeos encontrados\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erro ao carregar dados de decomposição: {e}\")\n",
        "        return\n",
        "\n",
        "    # Filtrar apenas vídeos com status \"decomposto\" e que tenham transcrição\n",
        "    videos_validos = []\n",
        "    for decomposicao in decomposicoes_data:\n",
        "        if (decomposicao.get(\"status\") == \"decomposto\" and\n",
        "            decomposicao.get(\"audio_transcrito\") and\n",
        "            len(decomposicao.get(\"audio_transcrito\", \"\").strip()) > 10):\n",
        "            videos_validos.append(decomposicao)\n",
        "\n",
        "    if not videos_validos:\n",
        "        print(\"❌ Nenhum vídeo com transcrição válida encontrado.\")\n",
        "        print(\"Verifique se a CÉLULA 2.3 foi executada com sucesso e se os vídeos possuem áudio.\")\n",
        "        return\n",
        "\n",
        "    print(f\"📊 Processando {len(videos_validos)} vídeos com transcrição válida...\")\n",
        "\n",
        "    analises_copywriting = []\n",
        "    legendas_geradas = []\n",
        "\n",
        "    for i, decomposicao in enumerate(videos_validos, 1):\n",
        "        video_id = decomposicao[\"video_id\"]\n",
        "        audio_transcrito = decomposicao[\"audio_transcrito\"]\n",
        "\n",
        "        print(f\"[{i}/{len(videos_validos)}] Processando copywriting para: {video_id}\")\n",
        "\n",
        "        try:\n",
        "            # Estimar duração do vídeo baseado na análise de áudio\n",
        "            duracao_segundos = decomposicao.get(\"audio_analise\", {}).get(\"duracao_audio_segundos\", 30)\n",
        "\n",
        "            # Criar info do vídeo para compatibilidade\n",
        "            video_info = {\n",
        "                \"id\": video_id,\n",
        "                \"duracao_segundos\": duracao_segundos\n",
        "            }\n",
        "\n",
        "            # Gerar legendas\n",
        "            legendas_data, srt_path, txt_path = gerar_legendas_com_timestamps(video_info, decomposicao)\n",
        "\n",
        "            if legendas_data:\n",
        "                legendas_info = {\n",
        "                    \"video_id\": video_id,\n",
        "                    \"srt_path\": srt_path,\n",
        "                    \"txt_path\": txt_path,\n",
        "                    \"total_segmentos\": len(legendas_data),\n",
        "                    \"duracao_total\": duracao_segundos,\n",
        "                    \"legendas_data\": legendas_data\n",
        "                }\n",
        "                legendas_geradas.append(legendas_info)\n",
        "\n",
        "                # Análise de copywriting\n",
        "                analise_copy = analisar_copywriting_estrategico(legendas_data, video_id)\n",
        "                analises_copywriting.append(analise_copy)\n",
        "\n",
        "                print(f\"  ✅ Copywriting analisado: Score {analise_copy['score_persuasao']}/100\")\n",
        "            else:\n",
        "                print(f\"  ❌ Falha na geração de legendas para {video_id}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Erro no processamento de copywriting para {video_id}: {e}\")\n",
        "\n",
        "    if not analises_copywriting:\n",
        "        print(\"❌ Nenhuma análise de copywriting foi gerada. Verifique os dados de entrada.\")\n",
        "        return\n",
        "\n",
        "    # Salvar dados de copywriting\n",
        "    copywriting_path = os.path.join(pasta_dados, \"analises_copywriting_completas.json\")\n",
        "    with open(copywriting_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(analises_copywriting, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"💾 Análises de copywriting salvas em: {copywriting_path}\")\n",
        "\n",
        "    # Salvar dados de legendas\n",
        "    legendas_path = os.path.join(pasta_dados, \"legendas_geradas.json\")\n",
        "    with open(legendas_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(legendas_geradas, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"💾 Dados de legendas salvos em: {legendas_path}\")\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    if os.path.exists(config_path):\n",
        "        try:\n",
        "            with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                config = json.load(f)\n",
        "\n",
        "            config[\"status_etapas\"][\"copywriting_analysis\"] = True\n",
        "\n",
        "            with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "        except:\n",
        "            print(\"⚠️ Não foi possível atualizar o arquivo de configuração\")\n",
        "\n",
        "    print(f\"\\n✅ ANÁLISE DE COPYWRITING CONCLUÍDA!\")\n",
        "    print(f\"Total de vídeos com copywriting analisado: {len(analises_copywriting)}\")\n",
        "    print(f\"Total de legendas geradas: {len(legendas_geradas)}\")\n",
        "    print(f\"\\n➡️ PRÓXIMA CÉLULA: 4.3 - INTEGRAÇÃO COM DASHBOARD\")\n",
        "\n",
        "def gerar_legendas_com_timestamps(video_info, decomposicao_data):\n",
        "    \"\"\"Gera legendas SRT e TXT com timestamps precisos a partir da transcrição\"\"\"\n",
        "    print(\"  🔄 Gerando legendas com timestamps...\")\n",
        "\n",
        "    video_id = video_info[\"id\"]\n",
        "    audio_transcrito = decomposicao_data.get(\"audio_transcrito\", \"\")\n",
        "\n",
        "    if not audio_transcrito.strip():\n",
        "        print(\"    ❌ Erro: Transcrição de áudio vazia\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Calcular duração do vídeo\n",
        "    duracao_segundos = video_info.get(\"duracao_segundos\", 30)  # Default 30s se não informado\n",
        "\n",
        "    # Dividir texto em segmentos baseados em pontuação e pausas naturais\n",
        "    segmentos = dividir_texto_em_segmentos(audio_transcrito)\n",
        "\n",
        "    if not segmentos:\n",
        "        print(\"    ❌ Erro: Não foi possível segmentar o texto\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Calcular timestamps para cada segmento\n",
        "    legendas_data = []\n",
        "    duracao_por_segmento = duracao_segundos / len(segmentos) if segmentos else 1\n",
        "\n",
        "    for i, segmento in enumerate(segmentos):\n",
        "        inicio_segundos = i * duracao_por_segmento\n",
        "        fim_segundos = (i + 1) * duracao_por_segmento\n",
        "\n",
        "        legenda_item = {\n",
        "            \"id\": i + 1,\n",
        "            \"inicio\": segundos_para_timestamp(inicio_segundos),\n",
        "            \"fim\": segundos_para_timestamp(fim_segundos),\n",
        "            \"texto\": segmento.strip(),\n",
        "            \"inicio_segundos\": inicio_segundos,\n",
        "            \"fim_segundos\": fim_segundos\n",
        "        }\n",
        "        legendas_data.append(legenda_item)\n",
        "\n",
        "    # Gerar arquivos SRT e TXT\n",
        "    pasta_legendas = os.path.join(PASTA_TRABALHO, \"legendas\")\n",
        "    os.makedirs(pasta_legendas, exist_ok=True)\n",
        "\n",
        "    # Arquivo SRT\n",
        "    srt_path = os.path.join(pasta_legendas, f\"{video_id}_legendas.srt\")\n",
        "    gerar_arquivo_srt(legendas_data, srt_path)\n",
        "\n",
        "    # Arquivo TXT com timestamps\n",
        "    txt_path = os.path.join(pasta_legendas, f\"{video_id}_legendas_timestamped.txt\")\n",
        "    gerar_arquivo_txt_timestamped(legendas_data, txt_path)\n",
        "\n",
        "    print(f\"    ✅ Legendas SRT geradas: {srt_path}\")\n",
        "    print(f\"    ✅ Legendas TXT com timestamps geradas: {txt_path}\")\n",
        "\n",
        "    return legendas_data, srt_path, txt_path\n",
        "\n",
        "def dividir_texto_em_segmentos(texto, max_chars=50):\n",
        "    \"\"\"Divide o texto em segmentos lógicos para legendas\"\"\"\n",
        "    # Dividir por frases primeiro\n",
        "    frases = re.split(r'[.!?]+', texto)\n",
        "    segmentos = []\n",
        "\n",
        "    for frase in frases:\n",
        "        if not frase.strip():\n",
        "            continue\n",
        "\n",
        "        # Se a frase é muito longa, dividir por vírgulas ou conjunções\n",
        "        if len(frase) > max_chars:\n",
        "            sub_segmentos = re.split(r'[,;]|(?:\\s+(?:e|mas|então|porque|que)\\s+)', frase)\n",
        "            for sub in sub_segmentos:\n",
        "                if sub.strip() and len(sub.strip()) > 3:\n",
        "                    segmentos.append(sub.strip())\n",
        "        else:\n",
        "            if frase.strip() and len(frase.strip()) > 3:\n",
        "                segmentos.append(frase.strip())\n",
        "\n",
        "    # Se ainda houver segmentos muito longos, dividir por palavras\n",
        "    segmentos_finais = []\n",
        "    for seg in segmentos:\n",
        "        if len(seg) > max_chars:\n",
        "            palavras = seg.split()\n",
        "            temp_seg = \"\"\n",
        "            for palavra in palavras:\n",
        "                if len(temp_seg + \" \" + palavra) <= max_chars:\n",
        "                    temp_seg += \" \" + palavra if temp_seg else palavra\n",
        "                else:\n",
        "                    if temp_seg:\n",
        "                        segmentos_finais.append(temp_seg.strip())\n",
        "                    temp_seg = palavra\n",
        "            if temp_seg:\n",
        "                segmentos_finais.append(temp_seg.strip())\n",
        "        else:\n",
        "            segmentos_finais.append(seg)\n",
        "\n",
        "    return segmentos_finais\n",
        "\n",
        "def segundos_para_timestamp(segundos):\n",
        "    \"\"\"Converte segundos para formato timestamp SRT (HH:MM:SS,mmm)\"\"\"\n",
        "    horas = int(segundos // 3600)\n",
        "    minutos = int((segundos % 3600) // 60)\n",
        "    segundos_restantes = segundos % 60\n",
        "    milissegundos = int((segundos_restantes - int(segundos_restantes)) * 1000)\n",
        "\n",
        "    return f\"{horas:02d}:{minutos:02d}:{int(segundos_restantes):02d},{milissegundos:03d}\"\n",
        "\n",
        "def gerar_arquivo_srt(legendas_data, srt_path):\n",
        "    \"\"\"Gera arquivo SRT\"\"\"\n",
        "    with open(srt_path, 'w', encoding='utf-8') as f:\n",
        "        for legenda in legendas_data:\n",
        "            f.write(f\"{legenda['id']}\\n\")\n",
        "            f.write(f\"{legenda['inicio']} --> {legenda['fim']}\\n\")\n",
        "            f.write(f\"{legenda['texto']}\\n\\n\")\n",
        "\n",
        "def gerar_arquivo_txt_timestamped(legendas_data, txt_path):\n",
        "    \"\"\"Gera arquivo TXT com timestamps\"\"\"\n",
        "    with open(txt_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"TRANSCRIÇÃO COM TIMESTAMPS\\n\")\n",
        "        f.write(\"=\"*50 + \"\\n\\n\")\n",
        "\n",
        "        for legenda in legendas_data:\n",
        "            minutos_inicio = int(legenda['inicio_segundos'] // 60)\n",
        "            segundos_inicio = int(legenda['inicio_segundos'] % 60)\n",
        "            minutos_fim = int(legenda['fim_segundos'] // 60)\n",
        "            segundos_fim = int(legenda['fim_segundos'] % 60)\n",
        "\n",
        "            f.write(f\"[{minutos_inicio:02d}:{segundos_inicio:02d}-{minutos_fim:02d}:{segundos_fim:02d}] {legenda['texto']}\\n\")\n",
        "\n",
        "def analisar_copywriting_estrategico(legendas_data, video_id):\n",
        "    \"\"\"Análise estratégica de copywriting com base nas legendas\"\"\"\n",
        "    print(\"    🔄 Analisando copywriting...\")\n",
        "\n",
        "    # Combinar todo o texto para análise completa\n",
        "    texto_completo = \" \".join([legenda[\"texto\"] for legenda in legendas_data])\n",
        "\n",
        "    # Dicionários de padrões de copywriting\n",
        "    ganchos_patterns = {\n",
        "        \"pergunta_retorica\": [r\"\\b(?:você|tu)\\s+(?:já|nunca|sempre|realmente|acha|imagina|sabe|quer|precisa)\",\n",
        "                            r\"(?:como|por que|quando|onde|o que).*\\?\"],\n",
        "        \"urgencia\": [r\"\\b(?:agora|hoje|urgente|rápido|imediato|última chance|só hoje|apenas|restam)\",\n",
        "                     r\"\\b(?:não perca|aproveite|garante já|corre|últimas vagas)\"],\n",
        "        \"escassez\": [r\"\\b(?:limitado|exclusivo|poucos|restam|última|única|especial|VIP)\",\n",
        "                     r\"\\b(?:só para|apenas para|somente|limitado a)\"],\n",
        "        \"autoridade\": [r\"\\b(?:especialista|expert|profissional|anos de experiência|comprovado|testado)\",\n",
        "                       r\"\\b(?:pesquisas mostram|estudos comprovam|cientificamente)\"],\n",
        "        \"prova_social\": [r\"\\b(?:milhares|centenas|todos|muitas pessoas|clientes|depoimentos)\",\n",
        "                         r\"\\b(?:já conseguiram|transformaram|mudaram|aprovaram)\"],\n",
        "        \"curiosidade\": [r\"\\b(?:segredo|descoberta|revelação|método|técnica|estratégia|fórmula)\",\n",
        "                        r\"\\b(?:ninguém te conta|poucos sabem|descobri que)\"],\n",
        "        \"problema_dor\": [r\"\\b(?:problema|dificuldade|frustração|sofre|dor|preocupa|bloqueia)\",\n",
        "                         r\"\\b(?:cansado de|chega de|pare de|não aguenta mais)\"],\n",
        "        \"solucao_resultado\": [r\"\\b(?:solução|resolve|elimina|transforma|muda|resultado|sucesso)\",\n",
        "                              r\"\\b(?:conseguir|alcançar|realizar|conquistar|atingir)\"]\n",
        "    }\n",
        "\n",
        "    gatilhos_patterns = {\n",
        "        \"reciprocidade\": [r\"\\b(?:grátis|de graça|presente|bônus|oferta|sem custo)\",\n",
        "                          r\"\\b(?:vou te dar|vou ensinar|vou mostrar|compartilhar com você)\"],\n",
        "        \"comprometimento\": [r\"\\b(?:compromisso|prometo|garanto|palavra|juro)\",\n",
        "                            r\"\\b(?:pode confiar|tenho certeza|assumo|responsabilizo)\"],\n",
        "        \"aprovacao_social\": [r\"\\b(?:aprovado por|recomendado|indicado|usado por|preferido)\",\n",
        "                             r\"\\b(?:famosos|influencers|especialistas|médicos|profissionais)\"],\n",
        "        \"aversao_perda\": [r\"\\b(?:perder|perdendo|vai ficar de fora|não vai conseguir)\",\n",
        "                          r\"\\b(?:sair perdendo|ficar para trás|oportunidade perdida)\"],\n",
        "        \"autoridade_especialista\": [r\"\\b(?:Dr|Dra|Professor|Mestre|PhD|especialista em)\",\n",
        "                                    r\"\\b(?:formado em|pós-graduado|anos estudando)\"],\n",
        "        \"emocional_medo\": [r\"\\b(?:medo|receio|preocupação|insegurança|ansiedade)\",\n",
        "                           r\"\\b(?:não conseguir|fracassar|dar errado|prejudicar)\"],\n",
        "        \"emocional_esperanca\": [r\"\\b(?:sonho|esperança|desejo|objetivo|meta|futuro melhor)\",\n",
        "                                r\"\\b(?:realizar|conquistar|alcançar|transformar|mudar vida)\"]\n",
        "    }\n",
        "\n",
        "    ctas_patterns = {\n",
        "        \"acao_imediata\": [r\"\\b(?:clica|clique|acesse|baixe|faça|compre|adquira|garanta)\",\n",
        "                          r\"\\b(?:não perca|aproveite|corre|vai|vem|participe)\"],\n",
        "        \"link_bio\": [r\"\\b(?:link na bio|bio|biografia|perfil|stories|direct)\",\n",
        "                     r\"\\b(?:DM|chama no WhatsApp|manda mensagem)\"],\n",
        "        \"engajamento\": [r\"\\b(?:comenta|compartilha|marca|salva|curte|like|segue)\",\n",
        "                        r\"\\b(?:conta nos comentários|deixa um|comenta aqui)\"],\n",
        "        \"inscricao\": [r\"\\b(?:inscreve|se inscreva|ativa|ativar|sino|notificação)\",\n",
        "                      r\"\\b(?:cadastra|cadastre-se|registra|assine)\"],\n",
        "        \"contato_vendas\": [r\"\\b(?:WhatsApp|telefone|ligue|chama|fala comigo|contato)\",\n",
        "                           r\"\\b(?:agende|marque|consulta|reunião|conversa)\"]\n",
        "    }\n",
        "\n",
        "    # Análise dos padrões\n",
        "    ganchos_encontrados = {}\n",
        "    gatilhos_encontrados = {}\n",
        "    ctas_encontrados = {}\n",
        "\n",
        "    # Analisar ganchos\n",
        "    for tipo, patterns in ganchos_patterns.items():\n",
        "        matches = []\n",
        "        for pattern in patterns:\n",
        "            matches.extend(re.finditer(pattern, texto_completo, re.IGNORECASE))\n",
        "        if matches:\n",
        "            ganchos_encontrados[tipo] = {\n",
        "                \"count\": len(matches),\n",
        "                \"exemplos\": [m.group() for m in matches[:3]],  # Top 3 exemplos\n",
        "                \"timestamps\": encontrar_timestamps_matches(matches, legendas_data, texto_completo)\n",
        "            }\n",
        "\n",
        "    # Analisar gatilhos\n",
        "    for tipo, patterns in gatilhos_patterns.items():\n",
        "        matches = []\n",
        "        for pattern in patterns:\n",
        "            matches.extend(re.finditer(pattern, texto_completo, re.IGNORECASE))\n",
        "        if matches:\n",
        "            gatilhos_encontrados[tipo] = {\n",
        "                \"count\": len(matches),\n",
        "                \"exemplos\": [m.group() for m in matches[:3]],\n",
        "                \"timestamps\": encontrar_timestamps_matches(matches, legendas_data, texto_completo)\n",
        "            }\n",
        "\n",
        "    # Analisar CTAs\n",
        "    for tipo, patterns in ctas_patterns.items():\n",
        "        matches = []\n",
        "        for pattern in patterns:\n",
        "            matches.extend(re.finditer(pattern, texto_completo, re.IGNORECASE))\n",
        "        if matches:\n",
        "            ctas_encontrados[tipo] = {\n",
        "                \"count\": len(matches),\n",
        "                \"exemplos\": [m.group() for m in matches[:3]],\n",
        "                \"timestamps\": encontrar_timestamps_matches(matches, legendas_data, texto_completo)\n",
        "            }\n",
        "\n",
        "    # Análise de estrutura narrativa\n",
        "    estrutura_narrativa = analisar_estrutura_narrativa(legendas_data)\n",
        "\n",
        "    # Análise de poder de persuasão\n",
        "    score_persuasao = calcular_score_persuasao(ganchos_encontrados, gatilhos_encontrados, ctas_encontrados)\n",
        "\n",
        "    analise_copywriting = {\n",
        "        \"video_id\": video_id,\n",
        "        \"texto_completo\": texto_completo,\n",
        "        \"total_palavras\": len(texto_completo.split()),\n",
        "        \"ganchos_detectados\": ganchos_encontrados,\n",
        "        \"gatilhos_mentais_detectados\": gatilhos_encontrados,\n",
        "        \"ctas_detectados\": ctas_encontrados,\n",
        "        \"estrutura_narrativa\": estrutura_narrativa,\n",
        "        \"score_persuasao\": score_persuasao,\n",
        "        \"recomendacoes_estrategicas\": gerar_recomendacoes_copywriting(ganchos_encontrados, gatilhos_encontrados, ctas_encontrados),\n",
        "        \"templates_identificados\": identificar_templates_replicaveis(ganchos_encontrados, gatilhos_encontrados, ctas_encontrados),\n",
        "        \"timestamp\": {\n",
        "            \"ganchos_timeline\": mapear_timeline_elementos(ganchos_encontrados, legendas_data),\n",
        "            \"gatilhos_timeline\": mapear_timeline_elementos(gatilhos_encontrados, legendas_data),\n",
        "            \"ctas_timeline\": mapear_timeline_elementos(ctas_encontrados, legendas_data)\n",
        "        },\n",
        "        \"data_analise\": datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    return analise_copywriting\n",
        "\n",
        "def encontrar_timestamps_matches(matches, legendas_data, texto_completo):\n",
        "    \"\"\"Encontra os timestamps correspondentes aos matches encontrados\"\"\"\n",
        "    timestamps = []\n",
        "\n",
        "    for match in matches[:3]:  # Limitar a 3 exemplos\n",
        "        posicao = match.start()\n",
        "        char_count = 0\n",
        "\n",
        "        for legenda in legendas_data:\n",
        "            texto_legenda = legenda[\"texto\"]\n",
        "            if char_count <= posicao < char_count + len(texto_legenda):\n",
        "                timestamps.append({\n",
        "                    \"minuto\": int(legenda[\"inicio_segundos\"] // 60),\n",
        "                    \"segundo\": int(legenda[\"inicio_segundos\"] % 60),\n",
        "                    \"texto_contexto\": texto_legenda\n",
        "                })\n",
        "                break\n",
        "            char_count += len(texto_legenda) + 1  # +1 para o espaço entre legendas\n",
        "\n",
        "    return timestamps\n",
        "\n",
        "def analisar_estrutura_narrativa(legendas_data):\n",
        "    \"\"\"Analisa a estrutura narrativa do vídeo\"\"\"\n",
        "    total_segmentos = len(legendas_data)\n",
        "\n",
        "    if total_segmentos < 3:\n",
        "        return {\n",
        "            \"abertura\": {\"segmentos\": total_segmentos, \"elementos\": []},\n",
        "            \"desenvolvimento\": {\"segmentos\": 0, \"elementos\": []},\n",
        "            \"fechamento\": {\"segmentos\": 0, \"elementos\": []}\n",
        "        }\n",
        "\n",
        "    # Dividir em terços para análise\n",
        "    primeiro_terco = legendas_data[:total_segmentos//3]\n",
        "    segundo_terco = legendas_data[total_segmentos//3:2*total_segmentos//3]\n",
        "    ultimo_terco = legendas_data[2*total_segmentos//3:]\n",
        "\n",
        "    estrutura = {\n",
        "        \"abertura\": {\n",
        "            \"segmentos\": len(primeiro_terco),\n",
        "            \"elementos\": analisar_elementos_abertura(primeiro_terco)\n",
        "        },\n",
        "        \"desenvolvimento\": {\n",
        "            \"segmentos\": len(segundo_terco),\n",
        "            \"elementos\": analisar_elementos_desenvolvimento(segundo_terco)\n",
        "        },\n",
        "        \"fechamento\": {\n",
        "            \"segmentos\": len(ultimo_terco),\n",
        "            \"elementos\": analisar_elementos_fechamento(ultimo_terco)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return estrutura\n",
        "\n",
        "def analisar_elementos_abertura(segmentos):\n",
        "    \"\"\"Analisa elementos da abertura\"\"\"\n",
        "    texto = \" \".join([s[\"texto\"] for s in segmentos])\n",
        "    elementos = []\n",
        "\n",
        "    if re.search(r'\\b(?:você|tu)\\s+(?:já|nunca|sempre)', texto, re.IGNORECASE):\n",
        "        elementos.append(\"pergunta_engajamento\")\n",
        "    if re.search(r'\\b(?:vou te|vou mostrar|vou ensinar)', texto, re.IGNORECASE):\n",
        "        elementos.append(\"promessa_valor\")\n",
        "    if re.search(r'\\b(?:segredo|descoberta|método)', texto, re.IGNORECASE):\n",
        "        elementos.append(\"curiosidade\")\n",
        "\n",
        "    return elementos\n",
        "\n",
        "def analisar_elementos_desenvolvimento(segmentos):\n",
        "    \"\"\"Analisa elementos do desenvolvimento\"\"\"\n",
        "    texto = \" \".join([s[\"texto\"] for s in segmentos])\n",
        "    elementos = []\n",
        "\n",
        "    if re.search(r'\\b(?:porque|pois|isso acontece)', texto, re.IGNORECASE):\n",
        "        elementos.append(\"explicacao\")\n",
        "    if re.search(r'\\b(?:exemplo|caso|situação)', texto, re.IGNORECASE):\n",
        "        elementos.append(\"exemplificacao\")\n",
        "    if re.search(r'\\b(?:resultado|consegui|transformou)', texto, re.IGNORECASE):\n",
        "        elementos.append(\"prova_resultado\")\n",
        "\n",
        "    return elementos\n",
        "\n",
        "def analisar_elementos_fechamento(segmentos):\n",
        "    \"\"\"Analisa elementos do fechamento\"\"\"\n",
        "    texto = \" \".join([s[\"texto\"] for s in segmentos])\n",
        "    elementos = []\n",
        "\n",
        "    if re.search(r'\\b(?:clica|clique|acesse|faça)', texto, re.IGNORECASE):\n",
        "        elementos.append(\"call_to_action\")\n",
        "    if re.search(r'\\b(?:link|bio|WhatsApp)', texto, re.IGNORECASE):\n",
        "        elementos.append(\"direcionamento\")\n",
        "    if re.search(r'\\b(?:comenta|compartilha|segue)', texto, re.IGNORECASE):\n",
        "        elementos.append(\"engajamento\")\n",
        "\n",
        "    return elementos\n",
        "\n",
        "def calcular_score_persuasao(ganchos, gatilhos, ctas):\n",
        "    \"\"\"Calcula score de persuasão baseado nos elementos encontrados\"\"\"\n",
        "    score = 0\n",
        "\n",
        "    # Pontuação por variedade de ganchos\n",
        "    score += len(ganchos) * 10\n",
        "\n",
        "    # Pontuação por variedade de gatilhos\n",
        "    score += len(gatilhos) * 15\n",
        "\n",
        "    # Pontuação por presença de CTAs\n",
        "    score += len(ctas) * 20\n",
        "\n",
        "    # Bônus por combinações poderosas\n",
        "    if \"urgencia\" in ganchos and \"aversao_perda\" in gatilhos:\n",
        "        score += 25\n",
        "\n",
        "    if \"autoridade\" in ganchos and \"autoridade_especialista\" in gatilhos:\n",
        "        score += 20\n",
        "\n",
        "    if \"curiosidade\" in ganchos and any(cta in ctas for cta in [\"acao_imediata\", \"link_bio\"]):\n",
        "        score += 30\n",
        "\n",
        "    return min(score, 100)  # Limitar a 100\n",
        "\n",
        "def gerar_recomendacoes_copywriting(ganchos, gatilhos, ctas):\n",
        "    \"\"\"Gera recomendações estratégicas baseadas na análise\"\"\"\n",
        "    recomendacoes = []\n",
        "\n",
        "    # Recomendações para ganchos\n",
        "    if len(ganchos) < 2:\n",
        "        recomendacoes.append({\n",
        "            \"categoria\": \"GANCHOS\",\n",
        "            \"prioridade\": \"ALTA\",\n",
        "            \"recomendacao\": \"Adicione mais ganchos na abertura. Use perguntas retóricas ou desperte curiosidade nos primeiros 3 segundos.\"\n",
        "        })\n",
        "\n",
        "    if \"pergunta_retorica\" not in ganchos:\n",
        "        recomendacoes.append({\n",
        "            \"categoria\": \"GANCHOS\",\n",
        "            \"prioridade\": \"MÉDIA\",\n",
        "            \"recomendacao\": \"Inicie com uma pergunta que faça o viewer refletir sobre sua situação atual.\"\n",
        "        })\n",
        "\n",
        "    # Recomendações para gatilhos\n",
        "    if len(gatilhos) < 3:\n",
        "        recomendacoes.append({\n",
        "            \"categoria\": \"GATILHOS\",\n",
        "            \"prioridade\": \"ALTA\",\n",
        "            \"recomendacao\": \"Incorpore mais gatilhos mentais. Combine autoridade + prova social para maior credibilidade.\"\n",
        "        })\n",
        "\n",
        "    if \"reciprocidade\" not in gatilhos:\n",
        "        recomendacoes.append({\n",
        "            \"categoria\": \"GATILHOS\",\n",
        "            \"prioridade\": \"MÉDIA\",\n",
        "            \"recomendacao\": \"Ofereça valor gratuito para ativar o gatilho da reciprocidade.\"\n",
        "        })\n",
        "\n",
        "    # Recomendações para CTAs\n",
        "    if len(ctas) == 0:\n",
        "        recomendacoes.append({\n",
        "            \"categoria\": \"CTA\",\n",
        "            \"prioridade\": \"CRÍTICA\",\n",
        "            \"recomendacao\": \"URGENTE: Adicione pelo menos um Call-to-Action claro. Sem CTA, não há conversão.\"\n",
        "        })\n",
        "\n",
        "    if \"acao_imediata\" not in ctas and \"link_bio\" not in ctas:\n",
        "        recomendacoes.append({\n",
        "            \"categoria\": \"CTA\",\n",
        "            \"prioridade\": \"ALTA\",\n",
        "            \"recomendacao\": \"Termine com um CTA direto: 'Clica no link da bio' ou 'Chama no WhatsApp'.\"\n",
        "        })\n",
        "\n",
        "    return recomendacoes\n",
        "\n",
        "def identificar_templates_replicaveis(ganchos, gatilhos, ctas):\n",
        "    \"\"\"Identifica templates e estruturas replicáveis\"\"\"\n",
        "    templates = []\n",
        "\n",
        "    # Template: Pergunta + Autoridade + CTA\n",
        "    if \"pergunta_retorica\" in ganchos and \"autoridade\" in ganchos and len(ctas) > 0:\n",
        "        templates.append({\n",
        "            \"nome\": \"PERGUNTA_AUTORIDADE_CTA\",\n",
        "            \"estrutura\": \"Pergunta Retórica → Estabelecer Autoridade → Call-to-Action\",\n",
        "            \"eficacia\": \"ALTA\",\n",
        "            \"uso_recomendado\": \"Vídeos educativos e de expertise\"\n",
        "        })\n",
        "\n",
        "    # Template: Problema + Solução + Prova Social\n",
        "    if \"problema_dor\" in ganchos and \"solucao_resultado\" in ganchos and \"aprovacao_social\" in gatilhos:\n",
        "        templates.append({\n",
        "            \"nome\": \"PROBLEMA_SOLUCAO_PROVA\",\n",
        "            \"estrutura\": \"Identificar Problema → Apresentar Solução → Mostrar Prova Social\",\n",
        "            \"eficacia\": \"MUITO ALTA\",\n",
        "            \"uso_recomendado\": \"Vídeos de vendas e transformação\"\n",
        "        })\n",
        "\n",
        "    # Template: Curiosidade + Urgência + CTA\n",
        "    if \"curiosidade\" in ganchos and \"urgencia\" in ganchos and \"acao_imediata\" in ctas:\n",
        "        templates.append({\n",
        "            \"nome\": \"CURIOSIDADE_URGENCIA_ACAO\",\n",
        "            \"estrutura\": \"Despertar Curiosidade → Criar Urgência → Ação Imediata\",\n",
        "            \"eficacia\": \"ALTA\",\n",
        "            \"uso_recomendado\": \"Vídeos de lançamento e ofertas limitadas\"\n",
        "        })\n",
        "\n",
        "    return templates\n",
        "\n",
        "def mapear_timeline_elementos(elementos_detectados, legendas_data):\n",
        "    \"\"\"Mapeia os elementos detectados na timeline do vídeo\"\"\"\n",
        "    timeline = []\n",
        "\n",
        "    for tipo, dados in elementos_detectados.items():\n",
        "        for timestamp in dados.get(\"timestamps\", []):\n",
        "            timeline.append({\n",
        "                \"tipo\": tipo,\n",
        "                \"minuto\": timestamp[\"minuto\"],\n",
        "                \"segundo\": timestamp[\"segundo\"],\n",
        "                \"contexto\": timestamp[\"texto_contexto\"]\n",
        "            })\n",
        "\n",
        "    # Ordenar por tempo\n",
        "    timeline.sort(key=lambda x: (x[\"minuto\"], x[\"segundo\"]))\n",
        "\n",
        "    return timeline\n",
        "\n",
        "# Executar o processamento\n",
        "try:\n",
        "    processar_copywriting_todos_videos_adaptado()\n",
        "except Exception as e:\n",
        "    print(f\"❌ ERRO de Execução: {type(e).__name__}: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfcEs533582H",
        "outputId": "90e6c65b-5e49-4d98-bfc6-199a6cff33dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando a Layer 2.4: Geração de Legendas e Análise de Copywriting...\n",
            "🔄 Iniciando processamento de copywriting adaptado...\n",
            "✅ Dados de decomposição carregados: 3 vídeos encontrados\n",
            "📊 Processando 3 vídeos com transcrição válida...\n",
            "[1/3] Processando copywriting para: vid_bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t\n",
            "  🔄 Gerando legendas com timestamps...\n",
            "    ✅ Legendas SRT geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t_legendas.srt\n",
            "    ✅ Legendas TXT com timestamps geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_bicho_não_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t_legendas_timestamped.txt\n",
            "    🔄 Analisando copywriting...\n",
            "  ✅ Copywriting analisado: Score 0/100\n",
            "[2/3] Processando copywriting para: vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf\n",
            "  🔄 Gerando legendas com timestamps...\n",
            "    ✅ Legendas SRT geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf_legendas.srt\n",
            "    ✅ Legendas TXT com timestamps geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf_legendas_timestamped.txt\n",
            "    🔄 Analisando copywriting...\n",
            "  ✅ Copywriting analisado: Score 30/100\n",
            "[3/3] Processando copywriting para: vid_máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy\n",
            "  🔄 Gerando legendas com timestamps...\n",
            "    ✅ Legendas SRT geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy_legendas.srt\n",
            "    ✅ Legendas TXT com timestamps geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_máquina_emocional_não_dá_conta_de_converter_então__vid_video_003_DMxQIz5MZsy_legendas_timestamped.txt\n",
            "    🔄 Analisando copywriting...\n",
            "  ✅ Copywriting analisado: Score 75/100\n",
            "💾 Análises de copywriting salvas em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/analises_copywriting_completas.json\n",
            "💾 Dados de legendas salvos em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/legendas_geradas.json\n",
            "\n",
            "✅ ANÁLISE DE COPYWRITING CONCLUÍDA!\n",
            "Total de vídeos com copywriting analisado: 3\n",
            "Total de legendas geradas: 3\n",
            "\n",
            "➡️ PRÓXIMA CÉLULA: 4.3 - INTEGRAÇÃO COM DASHBOARD\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CÉLULA 2.4: GERAÇÃO DE LEGENDAS E ANÁLISE DE COPYWRITING\n",
        "# ============================================================================\n",
        "\n",
        "# Definir a variável global PASTA_TRABALHO se ainda não estiver definida\n",
        "# Certifique-se de que esta variável esteja definida corretamente em uma célula anterior (ex: Célula 1.2)\n",
        "# Exemplo: PASTA_TRABALHO = \"/content/drive/MyDrive/Videos Dona Done/_engenharia_reversa\"\n",
        "\n",
        "# Executar a função principal da Layer 2.4\n",
        "if 'PASTA_TRABALHO' in globals():\n",
        "    print(\"Iniciando a Layer 2.4: Geração de Legendas e Análise de Copywriting...\")\n",
        "    processar_copywriting_todos_videos_adaptado()\n",
        "else:\n",
        "    print(\"ERRO: A variável PASTA_TRABALHO não está definida. Certifique-se de executar a Célula 1.2 ou equivalente.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "NUeniUqRJLuo",
        "outputId": "5bb0d509-95f2-43cf-b752-5d9982043bf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:54:41] INICIANDO SISTEMA DE DASHBOARD INTELIGENTE\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'create_enhanced_dashboard_master_with_viral' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1042834839.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1042834839.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;31m# Executar criação do dashboard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m     \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_enhanced_dashboard_master_with_viral\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'create_enhanced_dashboard_master_with_viral' is not defined"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CÉLULA 4.3: DASHBOARD MASTER EXECUTIVO INTELIGENTE APRIMORADO\n",
        "# ============================================================================\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "from openpyxl.styles import Font, Alignment, PatternFill\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def log_progress(message):\n",
        "    \"\"\"Log de progresso em tempo real\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
        "    print(f\"[{timestamp}] {message}\")\n",
        "\n",
        "def calculate_viral_score(row):\n",
        "    \"\"\"Calcula score de viralidade baseado em múltiplos fatores\"\"\"\n",
        "    try:\n",
        "        score = 0\n",
        "\n",
        "        # Fator 1: Ritmo (cortes por segundo) - peso 25%\n",
        "        if pd.notna(row['duracao_segundos']) and row['duracao_segundos'] > 0:\n",
        "            cortes_por_seg = row['cortes_detectados_count'] / row['duracao_segundos']\n",
        "            if cortes_por_seg > 20: score += 25\n",
        "            elif cortes_por_seg > 10: score += 20\n",
        "            elif cortes_por_seg > 5: score += 15\n",
        "            else: score += 10\n",
        "\n",
        "        # Fator 2: Complexidade Visual - peso 20%\n",
        "        if pd.notna(row['complexidade_visual_media']):\n",
        "            if row['complexidade_visual_media'] > 600: score += 20\n",
        "            elif row['complexidade_visual_media'] > 400: score += 15\n",
        "            else: score += 10\n",
        "\n",
        "        # Fator 3: Presença de Texto (OCR) - peso 15%\n",
        "        if pd.notna(row['ocr_textos_count']):\n",
        "            if row['ocr_textos_count'] > 10: score += 15\n",
        "            elif row['ocr_textos_count'] > 5: score += 12\n",
        "            elif row['ocr_textos_count'] > 0: score += 8\n",
        "\n",
        "        # Fator 4: Duração Ideal - peso 20%\n",
        "        if pd.notna(row['duracao_segundos']):\n",
        "            if 15 <= row['duracao_segundos'] <= 30: score += 20\n",
        "            elif 10 <= row['duracao_segundos'] <= 45: score += 15\n",
        "            else: score += 10\n",
        "\n",
        "        # Fator 5: Gatilhos Psicológicos - peso 20%\n",
        "        gatilhos = str(row['gatilhos_psicologicos']).lower()\n",
        "        if 'urgência' in gatilhos or 'escassez' in gatilhos: score += 8\n",
        "        if 'estímulo' in gatilhos: score += 7\n",
        "        if 'atenção' in gatilhos: score += 5\n",
        "\n",
        "        return min(score, 100)\n",
        "    except:\n",
        "        return 50\n",
        "\n",
        "def calculate_technical_score(row):\n",
        "    \"\"\"Score técnico baseado em qualidade de produção\"\"\"\n",
        "    try:\n",
        "        score = 0\n",
        "\n",
        "        if pd.notna(row['brilho_medio']):\n",
        "            if 120 <= row['brilho_medio'] <= 180: score += 25\n",
        "            elif 100 <= row['brilho_medio'] <= 200: score += 20\n",
        "            else: score += 10\n",
        "\n",
        "        formato = str(row['formato_detectado'])\n",
        "        if 'vertical_9_16' in formato: score += 25\n",
        "        elif 'horizontal_16_9' in formato: score += 20\n",
        "        else: score += 15\n",
        "\n",
        "        if row['tem_audio']: score += 25\n",
        "        else: score += 5\n",
        "\n",
        "        if pd.notna(row['total_frames']) and row['total_frames'] > 0:\n",
        "            if row['total_frames'] > 300: score += 25\n",
        "            elif row['total_frames'] > 150: score += 20\n",
        "            else: score += 15\n",
        "\n",
        "        return min(score, 100)\n",
        "    except:\n",
        "        return 50\n",
        "\n",
        "def calculate_content_score(row):\n",
        "    \"\"\"Score de conteúdo baseado em riqueza informacional\"\"\"\n",
        "    try:\n",
        "        score = 0\n",
        "\n",
        "        ocr_count = row['ocr_textos_count'] if pd.notna(row['ocr_textos_count']) else 0\n",
        "        audio_len = row['audio_transcrito_len'] if pd.notna(row['audio_transcrito_len']) else 0\n",
        "\n",
        "        if ocr_count > 5 or audio_len > 100: score += 30\n",
        "        elif ocr_count > 2 or audio_len > 50: score += 20\n",
        "        elif ocr_count > 0 or audio_len > 0: score += 15\n",
        "        else: score += 5\n",
        "\n",
        "        if pd.notna(row['bpm_audio']):\n",
        "            if 120 <= row['bpm_audio'] <= 140: score += 35\n",
        "            elif 100 <= row['bpm_audio'] <= 160: score += 25\n",
        "            else: score += 15\n",
        "\n",
        "        if pd.notna(row['duracao_segundos']) and row['duracao_segundos'] > 0:\n",
        "            densidade = (ocr_count + audio_len/10) / row['duracao_segundos']\n",
        "            if densidade > 2: score += 35\n",
        "            elif densidade > 1: score += 25\n",
        "            else: score += 15\n",
        "\n",
        "        return min(score, 100)\n",
        "    except:\n",
        "        return 50\n",
        "\n",
        "def generate_insights_from_data(df):\n",
        "    \"\"\"Gera insights inteligentes baseados nos dados\"\"\"\n",
        "    insights = []\n",
        "\n",
        "    try:\n",
        "        best_performing = df.nlargest(3, 'viral_score')\n",
        "        avg_duration = best_performing['duracao_segundos'].mean()\n",
        "        insights.append(f\"DURAÇÃO VENCEDORA: Seus top 3 vídeos têm duração média de {avg_duration:.1f}s. Este é seu sweet spot comprovado.\")\n",
        "\n",
        "        avg_cuts_per_sec = (best_performing['cortes_detectados_count'] / best_performing['duracao_segundos']).mean()\n",
        "        insights.append(f\"RITMO IDEAL: {avg_cuts_per_sec:.1f} cortes por segundo é sua fórmula de edição mais eficaz.\")\n",
        "\n",
        "        formato_winner = df['formato_detectado'].mode()[0] if not df['formato_detectado'].empty else 'N/A'\n",
        "        formato_count = df['formato_detectado'].value_counts().iloc[0] if not df['formato_detectado'].empty else 0\n",
        "        insights.append(f\"FORMATO DOMINANTE: {formato_count} vídeos em {formato_winner}. Este é seu formato de maior alcance.\")\n",
        "\n",
        "        high_viral = df[df['viral_score'] > 70]\n",
        "        if not high_viral.empty:\n",
        "            avg_complexity = high_viral['complexidade_visual_media'].mean()\n",
        "            insights.append(f\"COMPLEXIDADE VISUAL ÓTIMA: Vídeos com score viral alto têm complexidade média de {avg_complexity:.0f}. Use como referência.\")\n",
        "\n",
        "        text_heavy = df[df['ocr_textos_count'] > 5]\n",
        "        if not text_heavy.empty:\n",
        "            insights.append(f\"ESTRATÉGIA DE TEXTO: {len(text_heavy)} vídeos com muito texto têm score médio de {text_heavy['viral_score'].mean():.0f}. Texto na tela impacta performance.\")\n",
        "\n",
        "        # CORRIGIDO: bpm_audio em vez de bmp_audio\n",
        "        if df['bpm_audio'].notna().any():\n",
        "            successful_bpm = df[df['viral_score'] > 60]['bpm_audio'].mean()\n",
        "            insights.append(f\"BPM DE SUCESSO: {successful_bpm:.0f} BPM é o ritmo de áudio dos seus vídeos mais virais.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_progress(f\"Erro ao gerar insights: {e}\")\n",
        "        insights.append(\"Insights parciais disponíveis devido a limitações nos dados.\")\n",
        "\n",
        "    return insights\n",
        "\n",
        "def add_data_to_sheet(ws, data, start_row=1, start_col=1, headers=None):\n",
        "    \"\"\"Adiciona dados a uma planilha de forma segura\"\"\"\n",
        "    current_row = start_row\n",
        "\n",
        "    # Adicionar cabeçalhos se fornecidos\n",
        "    if headers:\n",
        "        for col_idx, header in enumerate(headers):\n",
        "            cell = ws.cell(row=current_row, column=start_col + col_idx)\n",
        "            cell.value = header\n",
        "            cell.font = Font(bold=True)\n",
        "        current_row += 1\n",
        "\n",
        "    # Adicionar dados\n",
        "    for row_data in data:\n",
        "        for col_idx, value in enumerate(row_data):\n",
        "            cell = ws.cell(row=current_row, column=start_col + col_idx)\n",
        "            cell.value = value\n",
        "        current_row += 1\n",
        "\n",
        "    return current_row\n",
        "\n",
        "def create_enhanced_dashboard_master(csv_path, json_path, output_path):\n",
        "    \"\"\"Cria dashboard master executivo aprimorado\"\"\"\n",
        "\n",
        "    log_progress(\"INICIANDO CRIAÇÃO DO DASHBOARD MASTER EXECUTIVO INTELIGENTE\")\n",
        "\n",
        "    try:\n",
        "        # Carregar dados\n",
        "        log_progress(\"Carregando dados consolidados...\")\n",
        "        df_consolidado = pd.read_csv(csv_path, encoding='utf-8')\n",
        "\n",
        "        with open(json_path, 'r', encoding='utf-8') as f:\n",
        "            dados_detalhados = json.load(f)\n",
        "\n",
        "        log_progress(f\"Dados carregados: {len(df_consolidado)} vídeos encontrados\")\n",
        "\n",
        "        # Pré-processamento inteligente\n",
        "        log_progress(\"Processando inteligência artificial dos dados...\")\n",
        "\n",
        "        # Limpar e converter dados\n",
        "        try:\n",
        "            df_consolidado['emocoes_predominantes'] = df_consolidado['emocoes_predominantes'].apply(\n",
        "                lambda x: json.loads(x.replace(\"'\", '\"')) if pd.notna(x) and x != '{}' else {}\n",
        "            )\n",
        "        except:\n",
        "            df_consolidado['emocoes_predominantes'] = [{}] * len(df_consolidado)\n",
        "\n",
        "        # Calcular scores inteligentes\n",
        "        log_progress(\"Calculando scores de performance...\")\n",
        "        df_consolidado['viral_score'] = df_consolidado.apply(calculate_viral_score, axis=1)\n",
        "        df_consolidado['technical_score'] = df_consolidado.apply(calculate_technical_score, axis=1)\n",
        "        df_consolidado['content_score'] = df_consolidado.apply(calculate_content_score, axis=1)\n",
        "        df_consolidado['overall_score'] = (df_consolidado['viral_score'] + df_consolidado['technical_score'] + df_consolidado['content_score']) / 3\n",
        "\n",
        "        # Calcular métricas avançadas\n",
        "        df_consolidado['cortes_por_segundo'] = df_consolidado['cortes_detectados_count'] / df_consolidado['duracao_segundos'].replace(0, 1)\n",
        "        df_consolidado['densidade_texto'] = df_consolidado['ocr_textos_count'] / df_consolidado['duracao_segundos'].replace(0, 1)\n",
        "        df_consolidado['eficiencia_audio'] = df_consolidado['audio_transcrito_len'] / df_consolidado['duracao_segundos'].replace(0, 1)\n",
        "\n",
        "        log_progress(\"Gerando insights estratégicos...\")\n",
        "        insights = generate_insights_from_data(df_consolidado)\n",
        "\n",
        "        # Criar workbook\n",
        "        log_progress(\"Criando estrutura do dashboard...\")\n",
        "        wb = Workbook()\n",
        "\n",
        "        # === ABA 1: EXECUTIVE SUMMARY ===\n",
        "        log_progress(\"Criando Executive Summary...\")\n",
        "        ws_summary = wb.active\n",
        "        ws_summary.title = 'Executive Summary'\n",
        "\n",
        "        # Header principal\n",
        "        header_cell = ws_summary.cell(row=1, column=1)\n",
        "        header_cell.value = 'DASHBOARD MASTER EXECUTIVO - ENGENHARIA REVERSA DE VÍDEOS'\n",
        "        header_cell.font = Font(bold=True, size=18, color='FFFFFF')\n",
        "        header_cell.fill = PatternFill(start_color='1F4E79', end_color='1F4E79', fill_type='solid')\n",
        "        header_cell.alignment = Alignment(horizontal='center', vertical='center')\n",
        "\n",
        "        # Expandir header manualmente\n",
        "        for col in range(2, 9):\n",
        "            cell = ws_summary.cell(row=1, column=col)\n",
        "            cell.fill = PatternFill(start_color='1F4E79', end_color='1F4E79', fill_type='solid')\n",
        "\n",
        "        # KPIs Principais\n",
        "        kpi_cell = ws_summary.cell(row=3, column=1)\n",
        "        kpi_cell.value = 'INDICADORES DE PERFORMANCE PRINCIPAIS'\n",
        "        kpi_cell.font = Font(bold=True, size=14)\n",
        "        kpi_cell.fill = PatternFill(start_color='E7E6E6', end_color='E7E6E6', fill_type='solid')\n",
        "\n",
        "        kpis_data = [\n",
        "            ['Total de Vídeos Analisados', len(df_consolidado)],\n",
        "            ['Score Viral Médio', f\"{df_consolidado['viral_score'].mean():.1f}/100\"],\n",
        "            ['Score Técnico Médio', f\"{df_consolidado['technical_score'].mean():.1f}/100\"],\n",
        "            ['Score de Conteúdo Médio', f\"{df_consolidado['content_score'].mean():.1f}/100\"],\n",
        "            ['Duração Média Otimizada', f\"{df_consolidado['duracao_segundos'].mean():.1f}s\"],\n",
        "            ['Ritmo Médio de Cortes', f\"{df_consolidado['cortes_por_segundo'].mean():.1f}/seg\"],\n",
        "        ]\n",
        "\n",
        "        add_data_to_sheet(ws_summary, kpis_data, start_row=4, start_col=1)\n",
        "\n",
        "        # Top 3 Vídeos\n",
        "        top3_cell = ws_summary.cell(row=3, column=4)\n",
        "        top3_cell.value = 'TOP 3 VÍDEOS POR PERFORMANCE'\n",
        "        top3_cell.font = Font(bold=True, size=14)\n",
        "        top3_cell.fill = PatternFill(start_color='E7E6E6', end_color='E7E6E6', fill_type='solid')\n",
        "\n",
        "        top3 = df_consolidado.nlargest(3, 'overall_score')[['nome_arquivo', 'overall_score', 'viral_score', 'technical_score', 'content_score']]\n",
        "\n",
        "        top3_data = []\n",
        "        for _, video in top3.iterrows():\n",
        "            nome_curto = video['nome_arquivo'][:30] + \"...\" if len(video['nome_arquivo']) > 30 else video['nome_arquivo']\n",
        "            top3_data.append([\n",
        "                nome_curto,\n",
        "                f\"{video['overall_score']:.1f}\",\n",
        "                f\"{video['viral_score']:.1f}\",\n",
        "                f\"{video['technical_score']:.1f}\",\n",
        "                f\"{video['content_score']:.1f}\"\n",
        "            ])\n",
        "\n",
        "        top3_headers = ['Vídeo', 'Score Geral', 'Viral', 'Técnico', 'Conteúdo']\n",
        "        add_data_to_sheet(ws_summary, top3_data, start_row=4, start_col=4, headers=top3_headers)\n",
        "\n",
        "        # Insights Estratégicos\n",
        "        insights_cell = ws_summary.cell(row=12, column=1)\n",
        "        insights_cell.value = 'INSIGHTS ESTRATÉGICOS BASEADOS EM IA'\n",
        "        insights_cell.font = Font(bold=True, size=14, color='FFFFFF')\n",
        "        insights_cell.fill = PatternFill(start_color='C5504B', end_color='C5504B', fill_type='solid')\n",
        "        insights_cell.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Adicionar insights\n",
        "        for i, insight in enumerate(insights, 13):\n",
        "            insight_cell = ws_summary.cell(row=i, column=1)\n",
        "            insight_cell.value = f\"• {insight}\"\n",
        "            insight_cell.alignment = Alignment(wrap_text=True)\n",
        "\n",
        "        # === ABA 2: ANÁLISE DE PERFORMANCE ===\n",
        "        log_progress(\"Criando Análise de Performance...\")\n",
        "        ws_performance = wb.create_sheet('Análise de Performance')\n",
        "\n",
        "        perf_header = ws_performance.cell(row=1, column=1)\n",
        "        perf_header.value = 'ANÁLISE DETALHADA DE PERFORMANCE'\n",
        "        perf_header.font = Font(bold=True, size=16)\n",
        "        perf_header.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Ranking completo\n",
        "        ranking_data = df_consolidado[['nome_arquivo', 'overall_score', 'viral_score', 'technical_score', 'content_score',\n",
        "                                     'duracao_segundos', 'cortes_por_segundo', 'formato_detectado']].sort_values('overall_score', ascending=False)\n",
        "\n",
        "        ranking_list = []\n",
        "        for _, video in ranking_data.iterrows():\n",
        "            nome_curto = video['nome_arquivo'][:40] + \"...\" if len(video['nome_arquivo']) > 40 else video['nome_arquivo']\n",
        "            ranking_list.append([\n",
        "                nome_curto,\n",
        "                f\"{video['overall_score']:.1f}\",\n",
        "                f\"{video['viral_score']:.1f}\",\n",
        "                f\"{video['technical_score']:.1f}\",\n",
        "                f\"{video['content_score']:.1f}\",\n",
        "                f\"{video['duracao_segundos']:.1f}s\",\n",
        "                f\"{video['cortes_por_segundo']:.1f}\",\n",
        "                video['formato_detectado']\n",
        "            ])\n",
        "\n",
        "        ranking_headers = ['Vídeo', 'Score Geral', 'Viral', 'Técnico', 'Conteúdo', 'Duração', 'Cortes/s', 'Formato']\n",
        "        add_data_to_sheet(ws_performance, ranking_list, start_row=3, start_col=1, headers=ranking_headers)\n",
        "\n",
        "        # === ABA 3: INTELIGÊNCIA TÉCNICA ===\n",
        "        log_progress(\"Criando Inteligência Técnica...\")\n",
        "        ws_tecnica = wb.create_sheet('Inteligência Técnica')\n",
        "\n",
        "        tec_header = ws_tecnica.cell(row=1, column=1)\n",
        "        tec_header.value = 'ANÁLISE TÉCNICA AVANÇADA'\n",
        "        tec_header.font = Font(bold=True, size=16)\n",
        "        tec_header.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Análise de correlações\n",
        "        corr_header = ws_tecnica.cell(row=3, column=1)\n",
        "        corr_header.value = 'CORRELAÇÕES DESCOBERTAS'\n",
        "        corr_header.font = Font(bold=True, size=12)\n",
        "\n",
        "        correlations_data = [\n",
        "            ['Duração vs Score Viral', f\"{df_consolidado['duracao_segundos'].corr(df_consolidado['viral_score']):.3f}\", 'CORRELAÇÃO MODERADA'],\n",
        "            ['Cortes/s vs Score Viral', f\"{df_consolidado['cortes_por_segundo'].corr(df_consolidado['viral_score']):.3f}\", 'CORRELAÇÃO MODERADA'],\n",
        "            ['Complexidade Visual vs Performance', f\"{df_consolidado['complexidade_visual_media'].corr(df_consolidado['overall_score']):.3f}\", 'CORRELAÇÃO FRACA'],\n",
        "            ['BPM vs Engajamento', f\"{df_consolidado['bpm_audio'].corr(df_consolidado['viral_score']) if df_consolidado['bpm_audio'].notna().any() else 0:.3f}\", 'CORRELAÇÃO FRACA'],\n",
        "        ]\n",
        "\n",
        "        corr_headers = ['Métrica', 'Correlação', 'Classificação']\n",
        "        add_data_to_sheet(ws_tecnica, correlations_data, start_row=4, start_col=1, headers=corr_headers)\n",
        "\n",
        "        # === ABA 4: BLUEPRINT DE PRODUÇÃO ===\n",
        "        log_progress(\"Criando Blueprint de Produção...\")\n",
        "        ws_blueprint = wb.create_sheet('Blueprint de Produção')\n",
        "\n",
        "        bp_header = ws_blueprint.cell(row=1, column=1)\n",
        "        bp_header.value = 'BLUEPRINT ESTRATÉGICO DE PRODUÇÃO'\n",
        "        bp_header.font = Font(bold=True, size=16, color='FFFFFF')\n",
        "        bp_header.fill = PatternFill(start_color='1F4E79', end_color='1F4E79', fill_type='solid')\n",
        "        bp_header.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Receita de sucesso baseada nos top performers\n",
        "        top_performers = df_consolidado[df_consolidado['overall_score'] > df_consolidado['overall_score'].quantile(0.7)]\n",
        "\n",
        "        blueprint_data = [\n",
        "            ['DURAÇÃO IDEAL', f\"{top_performers['duracao_segundos'].mean():.1f} segundos (±{top_performers['duracao_segundos'].std():.1f}s)\"],\n",
        "            ['RITMO DE EDIÇÃO', f\"{top_performers['cortes_por_segundo'].mean():.1f} cortes por segundo\"],\n",
        "            ['FORMATO VENCEDOR', top_performers['formato_detectado'].mode()[0] if not top_performers.empty else 'N/A'],\n",
        "            ['COMPLEXIDADE VISUAL', f\"Nível {top_performers['complexidade_visual_media'].mean():.0f} (escala de estímulo)\"],\n",
        "            ['BPM RECOMENDADO', f\"{top_performers['bpm_audio'].mean():.0f} BPM\" if top_performers['bpm_audio'].notna().any() else 'N/A'],\n",
        "            ['DENSIDADE DE TEXTO', f\"{top_performers['densidade_texto'].mean():.1f} textos por segundo\"],\n",
        "        ]\n",
        "\n",
        "        bp_sub_header = ws_blueprint.cell(row=3, column=1)\n",
        "        bp_sub_header.value = 'FÓRMULA DE SUCESSO BASEADA EM DADOS'\n",
        "        bp_sub_header.font = Font(bold=True, size=12)\n",
        "\n",
        "        add_data_to_sheet(ws_blueprint, blueprint_data, start_row=4, start_col=1)\n",
        "\n",
        "        # === ABA 5: RECOMENDAÇÕES ESTRATÉGICAS ===\n",
        "        log_progress(\"Criando Recomendações Estratégicas...\")\n",
        "        ws_recomendacoes = wb.create_sheet('Recomendações Estratégicas')\n",
        "\n",
        "        rec_header = ws_recomendacoes.cell(row=1, column=1)\n",
        "        rec_header.value = 'RECOMENDAÇÕES ESTRATÉGICAS BASEADAS EM IA'\n",
        "        rec_header.font = Font(bold=True, size=16, color='FFFFFF')\n",
        "        rec_header.fill = PatternFill(start_color='C5504B', end_color='C5504B', fill_type='solid')\n",
        "        rec_header.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Recomendações inteligentes baseadas nos dados\n",
        "        recommendations = []\n",
        "\n",
        "        # Análise de duração\n",
        "        if df_consolidado['duracao_segundos'].mean() > 60:\n",
        "            recommendations.append(['DURAÇÃO', 'REDUZA DURAÇÃO', 'Seus vídeos estão longos demais. Vídeos de 15-30s têm melhor performance.', 'ALTA'])\n",
        "        elif df_consolidado['duracao_segundos'].mean() < 15:\n",
        "            recommendations.append(['DURAÇÃO', 'AUMENTE DURAÇÃO', 'Vídeos muito curtos podem não transmitir valor suficiente.', 'MÉDIA'])\n",
        "\n",
        "        # Análise de ritmo\n",
        "        avg_cuts_per_sec = df_consolidado['cortes_por_segundo'].mean()\n",
        "        if avg_cuts_per_sec < 5:\n",
        "            recommendations.append(['EDIÇÃO', 'ACELERE O RITMO', 'Aumente o número de cortes para manter atenção. Meta: 8-12 cortes/segundo.', 'ALTA'])\n",
        "        elif avg_cuts_per_sec > 20:\n",
        "            recommendations.append(['EDIÇÃO', 'DIMINUA CORTES', 'Muitos cortes podem causar fadiga visual. Encontre o equilíbrio.', 'MÉDIA'])\n",
        "\n",
        "        # Análise de formato\n",
        "        formato_dominante = df_consolidado['formato_detectado'].mode()[0] if not df_consolidado['formato_detectado'].empty else 'N/A'\n",
        "        if 'horizontal' in formato_dominante.lower():\n",
        "            recommendations.append(['FORMATO', 'FOQUE EM VERTICAL', 'Formato vertical (9:16) tem melhor performance em redes sociais.', 'ALTA'])\n",
        "\n",
        "        # Análise de texto\n",
        "        if df_consolidado['densidade_texto'].mean() < 1:\n",
        "            recommendations.append(['CONTEÚDO', 'ADICIONE MAIS TEXTO', 'Textos na tela aumentam retenção e acessibilidade.', 'MÉDIA'])\n",
        "\n",
        "        rec_headers = ['Categoria', 'Ação', 'Justificativa', 'Prioridade']\n",
        "        add_data_to_sheet(ws_recomendacoes, recommendations, start_row=3, start_col=1, headers=rec_headers)\n",
        "\n",
        "        # Salvar arquivo\n",
        "        log_progress(\"Salvando dashboard...\")\n",
        "        wb.save(output_path)\n",
        "\n",
        "        log_progress(\"DASHBOARD MASTER EXECUTIVO CRIADO COM SUCESSO!\")\n",
        "        log_progress(f\"Arquivo salvo em: {output_path}\")\n",
        "        log_progress(f\"{len(df_consolidado)} vídeos analisados\")\n",
        "        log_progress(f\"{len(insights)} insights estratégicos gerados\")\n",
        "        log_progress(f\"{len(recommendations)} recomendações criadas\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        log_progress(f\"ERRO CRÍTICO: {e}\")\n",
        "        log_progress(\"Verifique os arquivos de entrada e tente novamente\")\n",
        "        return False\n",
        "\n",
        "def main():\n",
        "    \"\"\"Função principal de execução\"\"\"\n",
        "    log_progress(\"INICIANDO SISTEMA DE DASHBOARD INTELIGENTE\")\n",
        "\n",
        "    # Configurar caminhos\n",
        "    BASE_PATH = \"/content/drive/MyDrive/Videos Dona Done/_engenharia_reversa\"\n",
        "    CSV_PATH = os.path.join(BASE_PATH, \"dashboard\", \"dados_consolidados.csv\")\n",
        "    JSON_PATH = os.path.join(BASE_PATH, \"dashboard\", \"dados_detalhados.json\")\n",
        "    OUTPUT_PATH = os.path.join(BASE_PATH, \"dashboard\", \"DASHBOARD_MASTER_EXECUTIVO_INTELIGENTE.xlsx\")\n",
        "\n",
        "    # Verificar se arquivos existem\n",
        "    if not os.path.exists(CSV_PATH):\n",
        "        log_progress(f\"ERRO: Arquivo CSV não encontrado: {CSV_PATH}\")\n",
        "        return False\n",
        "\n",
        "    if not os.path.exists(JSON_PATH):\n",
        "        log_progress(f\"ERRO: Arquivo JSON não encontrado: {JSON_PATH}\")\n",
        "        return False\n",
        "\n",
        "    # Executar criação do dashboard\n",
        "    success = create_enhanced_dashboard_master_with_viral(csv_path, json_path, output_path)\n",
        "\n",
        "    if success:\n",
        "        log_progress(\"PROCESSO CONCLUÍDO COM SUCESSO!\")\n",
        "        log_progress(\"Dashboard inteligente pronto para uso estratégico\")\n",
        "    else:\n",
        "        log_progress(\"PROCESSO FALHOU - Verifique os logs acima\")\n",
        "\n",
        "    return success\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ixFPocx6m8Q"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# LAYER 4.3: INTEGRAÇÃO DE COPYWRITING NO DASHBOARD EXISTENTE\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from openpyxl import load_workbook\n",
        "from openpyxl.styles import Font, PatternFill, Alignment\n",
        "\n",
        "def integrar_copywriting_dashboard_existente():\n",
        "    \"\"\"Integra análise de copywriting no dashboard master existente\"\"\"\n",
        "    print(\"🔄 Iniciando integração de copywriting no dashboard existente...\")\n",
        "\n",
        "    # Verificar pré-requisitos\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa('copywriting_analysis')\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Localizar dashboard existente\n",
        "    pasta_dashboard = os.path.join(PASTA_TRABALHO, \"dashboard\")\n",
        "    dashboard_existente = None\n",
        "\n",
        "    # Procurar arquivo de dashboard existente\n",
        "    if os.path.exists(pasta_dashboard):\n",
        "        arquivos = os.listdir(pasta_dashboard)\n",
        "        for arquivo in arquivos:\n",
        "            if \"DASHBOARD_MASTER_EXECUTIVO_INTELIGENTE\" in arquivo and arquivo.endswith(\".xlsx\"):\n",
        "                dashboard_existente = os.path.join(pasta_dashboard, arquivo)\n",
        "                break\n",
        "\n",
        "    if not dashboard_existente:\n",
        "        print(\"❌ Dashboard master existente não encontrado!\")\n",
        "        print(\"Execute primeiro a célula 4.2 (Blueprint Final) para criar o dashboard base.\")\n",
        "        return\n",
        "\n",
        "    print(f\"  📊 Dashboard encontrado: {os.path.basename(dashboard_existente)}\")\n",
        "\n",
        "    # Carregar dados de copywriting\n",
        "    dados_copywriting = carregar_dados_copywriting()\n",
        "    if not dados_copywriting:\n",
        "        return\n",
        "\n",
        "    # Abrir workbook existente\n",
        "    try:\n",
        "        wb = load_workbook(dashboard_existente)\n",
        "        print(f\"  ✅ Dashboard carregado com {len(wb.sheetnames)} abas existentes\")\n",
        "\n",
        "        # Adicionar novas abas de copywriting\n",
        "        adicionar_aba_copywriting_estrategico(wb, dados_copywriting)\n",
        "        adicionar_aba_templates_copy(wb, dados_copywriting)\n",
        "        adicionar_aba_timeline_copy(wb, dados_copywriting)\n",
        "        adicionar_aba_recomendacoes_copy(wb, dados_copywriting)\n",
        "\n",
        "        # Atualizar aba principal com métricas de copywriting\n",
        "        atualizar_aba_principal_com_copy(wb, dados_copywriting)\n",
        "\n",
        "        # Salvar dashboard atualizado\n",
        "        wb.save(dashboard_existente)\n",
        "\n",
        "        print(f\"✅ Dashboard atualizado com análise de copywriting!\")\n",
        "        print(f\"📊 Arquivo: {dashboard_existente}\")\n",
        "        print(f\"📋 Novas abas adicionadas:\")\n",
        "        print(\"  • Copywriting Estratégico\")\n",
        "        print(\"  • Templates Replicáveis\")\n",
        "        print(\"  • Timeline Persuasão\")\n",
        "        print(\"  • Recomendações Copy\")\n",
        "        print(\"  • Dashboard Principal (atualizada)\")\n",
        "\n",
        "        # Gerar relatórios complementares\n",
        "        gerar_relatorios_copywriting_individuais(dados_copywriting)\n",
        "\n",
        "        # Atualizar config\n",
        "        config[\"status_etapas\"][\"dashboard_copywriting_integrado\"] = True\n",
        "        config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "        with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        return dashboard_existente\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erro ao atualizar dashboard: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def verificar_prerequisito_etapa(etapa):\n",
        "    \"\"\"Verifica se uma etapa foi executada\"\"\"\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "\n",
        "    if not os.path.exists(config_path):\n",
        "        print(f\"❌ Arquivo de configuração não encontrado: {config_path}\")\n",
        "        return False, None\n",
        "\n",
        "    try:\n",
        "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            config = json.load(f)\n",
        "\n",
        "        if not config.get(\"status_etapas\", {}).get(etapa, False):\n",
        "            print(f\"❌ Pré-requisito não atendido: {etapa}\")\n",
        "            print(\"Execute primeiro a célula correspondente.\")\n",
        "            return False, None\n",
        "\n",
        "        return True, config\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erro ao verificar pré-requisitos: {e}\")\n",
        "        return False, None\n",
        "\n",
        "def carregar_dados_copywriting():\n",
        "    \"\"\"Carrega dados de copywriting e outros dados necessários\"\"\"\n",
        "    print(\"  📊 Carregando dados de copywriting...\")\n",
        "\n",
        "    try:\n",
        "        # Dados de copywriting\n",
        "        copywriting_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_copywriting_completas.json\")\n",
        "        with open(copywriting_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            copywriting_data = json.load(f)\n",
        "\n",
        "        # Dados de legendas\n",
        "        legendas_path = os.path.join(PASTA_TRABALHO, \"dados\", \"legendas_geradas.json\")\n",
        "        with open(legendas_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            legendas_data = json.load(f)\n",
        "\n",
        "        # Tentar carregar outros dados (podem não existir ainda)\n",
        "        outros_dados = {}\n",
        "\n",
        "        try:\n",
        "            padroes_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_padroes_completas.json\")\n",
        "            with open(padroes_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                outros_dados[\"padroes\"] = json.load(f)\n",
        "        except:\n",
        "            outros_dados[\"padroes\"] = []\n",
        "\n",
        "        try:\n",
        "            videos_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "            with open(videos_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                outros_dados[\"videos\"] = json.load(f)\n",
        "        except:\n",
        "            outros_dados[\"videos\"] = []\n",
        "\n",
        "        print(f\"  ✅ Dados carregados: {len(copywriting_data)} análises de copywriting\")\n",
        "\n",
        "        return {\n",
        "            \"copywriting\": copywriting_data,\n",
        "            \"legendas\": legendas_data,\n",
        "            **outros_dados\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ Erro ao carregar dados de copywriting: {e}\")\n",
        "        return None\n",
        "\n",
        "def adicionar_aba_copywriting_estrategico(wb, dados):\n",
        "    \"\"\"Adiciona aba principal de análise de copywriting\"\"\"\n",
        "    # Criar nova aba\n",
        "    ws = wb.create_sheet(\"Copywriting Estratégico\")\n",
        "\n",
        "    # Título principal\n",
        "    ws.merge_cells(\"A1:H1\")\n",
        "    titulo = ws[\"A1\"]\n",
        "    titulo.value = \"ANÁLISE ESTRATÉGICA DE COPYWRITING - ENGENHARIA REVERSA\"\n",
        "    titulo.fill = PatternFill(start_color=\"1F4E79\", end_color=\"1F4E79\", fill_type=\"solid\")\n",
        "    titulo.font = Font(color=\"FFFFFF\", bold=True, size=14)\n",
        "    titulo.alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "    row = 3\n",
        "\n",
        "    # Métricas executivas\n",
        "    ws[f\"A{row}\"] = \"MÉTRICAS EXECUTIVAS DE COPYWRITING\"\n",
        "    ws[f\"A{row}\"].font = Font(bold=True, size=12, color=\"C5504B\")\n",
        "    row += 2\n",
        "\n",
        "    # Calcular métricas\n",
        "    videos_copy = dados[\"copywriting\"]\n",
        "\n",
        "    if videos_copy:\n",
        "        # Score médio\n",
        "        scores = [v.get(\"score_persuasao\", 0) for v in videos_copy]\n",
        "        score_medio = sum(scores) / len(scores) if scores else 0\n",
        "\n",
        "        # Contadores\n",
        "        total_ganchos = sum(len(v.get(\"ganchos_detectados\", {})) for v in videos_copy)\n",
        "        total_gatilhos = sum(len(v.get(\"gatilhos_mentais_detectados\", {})) for v in videos_copy)\n",
        "        total_ctas = sum(len(v.get(\"ctas_detectados\", {})) for v in videos_copy)\n",
        "        videos_sem_cta = len([v for v in videos_copy if not v.get(\"ctas_detectados\")])\n",
        "        total_templates = sum(len(v.get(\"templates_identificados\", [])) for v in videos_copy)\n",
        "\n",
        "        # Exibir métricas\n",
        "        metricas = [\n",
        "            (\"Score Persuasão Médio:\", f\"{score_medio:.1f}/100\", \"Meta: 70+ para alta conversão\"),\n",
        "            (\"Vídeos Analisados:\", len(videos_copy), \"Base completa da análise\"),\n",
        "            (\"Total de Ganchos:\", total_ganchos, f\"Média: {total_ganchos/len(videos_copy):.1f} por vídeo\"),\n",
        "            (\"Total de Gatilhos:\", total_gatilhos, f\"Média: {total_gatilhos/len(videos_copy):.1f} por vídeo\"),\n",
        "            (\"Total de CTAs:\", total_ctas, f\"Média: {total_ctas/len(videos_copy):.1f} por vídeo\"),\n",
        "            (\"🚨 Vídeos sem CTA:\", videos_sem_cta, \"CRÍTICO: Implementar imediatamente\" if videos_sem_cta > 0 else \"✅ Todos têm CTA\"),\n",
        "            (\"Templates Identificados:\", total_templates, \"Estruturas replicáveis encontradas\")\n",
        "        ]\n",
        "\n",
        "        for metrica, valor, descricao in metricas:\n",
        "            ws[f\"A{row}\"] = metrica\n",
        "            ws[f\"B{row}\"] = valor\n",
        "            ws[f\"C{row}\"] = descricao\n",
        "\n",
        "            ws[f\"A{row}\"].font = Font(bold=True)\n",
        "            if \"🚨\" in metrica and videos_sem_cta > 0:\n",
        "                ws[f\"B{row}\"].font = Font(bold=True, color=\"FF0000\")\n",
        "            elif isinstance(valor, (int, float)) and valor > 0:\n",
        "                ws[f\"B{row}\"].font = Font(bold=True, color=\"70AD47\")\n",
        "\n",
        "            row += 1\n",
        "\n",
        "        row += 2\n",
        "\n",
        "        # Ranking de performance\n",
        "        ws[f\"A{row}\"] = \"🏆 RANKING DE PERFORMANCE POR SCORE DE PERSUASÃO\"\n",
        "        ws[f\"A{row}\"].font = Font(bold=True, size=12, color=\"1F4E79\")\n",
        "        row += 2\n",
        "\n",
        "        # Headers\n",
        "        headers = [\"Posição\", \"Vídeo ID\", \"Score\", \"Ganchos\", \"Gatilhos\", \"CTAs\", \"Status\"]\n",
        "        for col, header in enumerate(headers, 1):\n",
        "            cell = ws.cell(row=row, column=col)\n",
        "            cell.value = header\n",
        "            cell.font = Font(bold=True)\n",
        "            cell.fill = PatternFill(start_color=\"D9E2F3\", end_color=\"D9E2F3\", fill_type=\"solid\")\n",
        "        row += 1\n",
        "\n",
        "        # Top performers\n",
        "        top_videos = sorted(videos_copy, key=lambda x: x.get(\"score_persuasao\", 0), reverse=True)\n",
        "\n",
        "        for i, video in enumerate(top_videos, 1):\n",
        "            ws.cell(row=row, column=1, value=f\"{i}º\")\n",
        "            ws.cell(row=row, column=2, value=video[\"video_id\"])\n",
        "            ws.cell(row=row, column=3, value=f\"{video.get('score_persuasao', 0)}/100\")\n",
        "            ws.cell(row=row, column=4, value=len(video.get(\"ganchos_detectados\", {})))\n",
        "            ws.cell(row=row, column=5, value=len(video.get(\"gatilhos_mentais_detectados\", {})))\n",
        "            ws.cell(row=row, column=6, value=len(video.get(\"ctas_detectados\", {})))\n",
        "\n",
        "            # Status baseado no score\n",
        "            score = video.get(\"score_persuasao\", 0)\n",
        "            if score >= 70:\n",
        "                status = \"🟢 ÓTIMO\"\n",
        "                status_color = \"70AD47\"\n",
        "            elif score >= 50:\n",
        "                status = \"🟡 BOM\"\n",
        "                status_color = \"FFC000\"\n",
        "            else:\n",
        "                status = \"🔴 PRECISA OTIMIZAR\"\n",
        "                status_color = \"C5504B\"\n",
        "\n",
        "            cell_status = ws.cell(row=row, column=7, value=status)\n",
        "            cell_status.font = Font(color=status_color, bold=True)\n",
        "\n",
        "            # Destacar top 3\n",
        "            if i <= 3:\n",
        "                for col in range(1, 8):\n",
        "                    ws.cell(row=row, column=col).fill = PatternFill(start_color=\"FFF2CC\", end_color=\"FFF2CC\", fill_type=\"solid\")\n",
        "\n",
        "            row += 1\n",
        "\n",
        "    # Ajustar larguras das colunas\n",
        "    for col, width in [(\"A\", 25), (\"B\", 15), (\"C\", 40), (\"D\", 10), (\"E\", 10), (\"F\", 10), (\"G\", 20), (\"H\", 15)]:\n",
        "        ws.column_dimensions[col].width = width\n",
        "\n",
        "def adicionar_aba_templates_copy(wb, dados):\n",
        "    \"\"\"Adiciona aba de templates replicáveis\"\"\"\n",
        "    ws = wb.create_sheet(\"Templates Replicáveis\")\n",
        "\n",
        "    # Título\n",
        "    ws.merge_cells(\"A1:F1\")\n",
        "    titulo = ws[\"A1\"]\n",
        "    titulo.value = \"TEMPLATES E ESTRUTURAS REPLICÁVEIS DE COPYWRITING\"\n",
        "    titulo.fill = PatternFill(start_color=\"70AD47\", end_color=\"70AD47\", fill_type=\"solid\")\n",
        "    titulo.font = Font(color=\"FFFFFF\", bold=True, size=14)\n",
        "    titulo.alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "    row = 3\n",
        "\n",
        "    # Coletar todos os templates\n",
        "    todos_templates = []\n",
        "    for video in dados[\"copywriting\"]:\n",
        "        templates = video.get(\"templates_identificados\", [])\n",
        "        for template in templates:\n",
        "            template[\"video_id\"] = video[\"video_id\"]\n",
        "            todos_templates.append(template)\n",
        "\n",
        "    if todos_templates:\n",
        "        # Agrupar templates por tipo\n",
        "        templates_agrupados = {}\n",
        "        for template in todos_templates:\n",
        "            nome = template[\"nome\"]\n",
        "            if nome not in templates_agrupados:\n",
        "                templates_agrupados[nome] = {\n",
        "                    \"estrutura\": template[\"estrutura\"],\n",
        "                    \"eficacia\": template[\"eficacia\"],\n",
        "                    \"uso_recomendado\": template[\"uso_recomendado\"],\n",
        "                    \"videos_exemplo\": []\n",
        "                }\n",
        "            templates_agrupados[nome][\"videos_exemplo\"].append(template[\"video_id\"])\n",
        "\n",
        "        # Exibir templates\n",
        "        for nome_template, dados_template in templates_agrupados.items():\n",
        "            ws.merge_cells(f\"A{row}:F{row}\")\n",
        "            template_header = ws[f\"A{row}\"]\n",
        "            template_header.value = f\"📋 TEMPLATE: {nome_template.replace('_', ' ')}\"\n",
        "            template_header.fill = PatternFill(start_color=\"E2EFDA\", end_color=\"E2EFDA\", fill_type=\"solid\")\n",
        "            template_header.font = Font(bold=True, size=11)\n",
        "            row += 1\n",
        "\n",
        "            ws[f\"A{row}\"] = \"Estrutura:\"\n",
        "            ws[f\"B{row}\"] = dados_template[\"estrutura\"]\n",
        "            ws[f\"A{row}\"].font = Font(bold=True)\n",
        "            row += 1\n",
        "\n",
        "            ws[f\"A{row}\"] = \"Eficácia:\"\n",
        "            ws[f\"B{row}\"] = dados_template[\"eficacia\"]\n",
        "            ws[f\"A{row}\"].font = Font(bold=True)\n",
        "            if dados_template[\"eficacia\"] == \"MUITO ALTA\":\n",
        "                ws[f\"B{row}\"].font = Font(color=\"70AD47\", bold=True)\n",
        "            elif dados_template[\"eficacia\"] == \"ALTA\":\n",
        "                ws[f\"B{row}\"].font = Font(color=\"C5504B\", bold=True)\n",
        "            row += 1\n",
        "\n",
        "            ws[f\"A{row}\"] = \"Uso Recomendado:\"\n",
        "            ws[f\"B{row}\"] = dados_template[\"uso_recomendado\"]\n",
        "            ws[f\"A{row}\"].font = Font(bold=True)\n",
        "            row += 1\n",
        "\n",
        "            ws[f\"A{row}\"] = \"Vídeos Exemplo:\"\n",
        "            ws[f\"B{row}\"] = \", \".join(dados_template[\"videos_exemplo\"][:3])\n",
        "            ws[f\"A{row}\"].font = Font(bold=True)\n",
        "            row += 2\n",
        "\n",
        "    # Ajustar larguras\n",
        "    for col, width in [(\"A\", 20), (\"B\", 50), (\"C\", 15), (\"D\", 15), (\"E\", 15), (\"F\", 15)]:\n",
        "        ws.column_dimensions[col].width = width\n",
        "\n",
        "def adicionar_aba_timeline_copy(wb, dados):\n",
        "    \"\"\"Adiciona aba de timeline de elementos persuasivos\"\"\"\n",
        "    ws = wb.create_sheet(\"Timeline Persuasão\")\n",
        "\n",
        "    # Título\n",
        "    ws.merge_cells(\"A1:G1\")\n",
        "    titulo = ws[\"A1\"]\n",
        "    titulo.value = \"TIMELINE DE ELEMENTOS PERSUASIVOS - ANÁLISE TEMPORAL\"\n",
        "    titulo.fill = PatternFill(start_color=\"FFC000\", end_color=\"FFC000\", fill_type=\"solid\")\n",
        "    titulo.font = Font(color=\"FFFFFF\", bold=True, size=14)\n",
        "    titulo.alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "    row = 3\n",
        "\n",
        "    # Para cada vídeo, mostrar timeline\n",
        "    for video in dados[\"copywriting\"]:\n",
        "        video_id = video[\"video_id\"]\n",
        "\n",
        "        ws[f\"A{row}\"] = f\"📹 VÍDEO: {video_id}\"\n",
        "        ws[f\"A{row}\"].font = Font(bold=True, size=11, color=\"1F4E79\")\n",
        "        row += 2\n",
        "\n",
        "        # Headers da timeline\n",
        "        headers = [\"Tempo\", \"Minuto\", \"Segundo\", \"Elemento\", \"Posição\", \"Impacto\", \"Análise\"]\n",
        "        for col, header in enumerate(headers, 1):\n",
        "            cell = ws.cell(row=row, column=col)\n",
        "            cell.value = header\n",
        "            cell.font = Font(bold=True)\n",
        "            cell.fill = PatternFill(start_color=\"F2F2F2\", end_color=\"F2F2F2\", fill_type=\"solid\")\n",
        "        row += 1\n",
        "\n",
        "        # Coletar todos os elementos temporais\n",
        "        elementos_temporais = []\n",
        "\n",
        "        # Ganchos\n",
        "        for tipo, dados in video.get(\"ganchos_detectados\", {}).items():\n",
        "            for timestamp in dados.get(\"timestamps\", []):\n",
        "                elementos_temporais.append({\n",
        "                    \"categoria\": \"GANCHO\",\n",
        "                    \"tipo\": tipo,\n",
        "                    \"minuto\": timestamp[\"minuto\"],\n",
        "                    \"segundo\": timestamp[\"segundo\"],\n",
        "                    \"contexto\": timestamp[\"texto_contexto\"]\n",
        "                })\n",
        "\n",
        "        # Gatilhos\n",
        "        for tipo, dados in video.get(\"gatilhos_mentais_detectados\", {}).items():\n",
        "            for timestamp in dados.get(\"timestamps\", []):\n",
        "                elementos_temporais.append({\n",
        "                    \"categoria\": \"GATILHO\",\n",
        "                    \"tipo\": tipo,\n",
        "                    \"minuto\": timestamp[\"minuto\"],\n",
        "                    \"segundo\": timestamp[\"segundo\"],\n",
        "                    \"contexto\": timestamp[\"texto_contexto\"]\n",
        "                })\n",
        "\n",
        "        # CTAs\n",
        "        for tipo, dados in video.get(\"ctas_detectados\", {}).items():\n",
        "            for timestamp in dados.get(\"timestamps\", []):\n",
        "                elementos_temporais.append({\n",
        "                    \"categoria\": \"CTA\",\n",
        "                    \"tipo\": tipo,\n",
        "                    \"minuto\": timestamp[\"minuto\"],\n",
        "                    \"segundo\": timestamp[\"segundo\"],\n",
        "                    \"contexto\": timestamp[\"texto_contexto\"]\n",
        "                })\n",
        "\n",
        "        # Ordenar por tempo\n",
        "        elementos_temporais.sort(key=lambda x: (x[\"minuto\"], x[\"segundo\"]))\n",
        "\n",
        "        # Exibir elementos\n",
        "        if elementos_temporais:\n",
        "            for elemento in elementos_temporais:\n",
        "                ws.cell(row=row, column=1, value=f\"{elemento['minuto']:02d}:{elemento['segundo']:02d}\")\n",
        "                ws.cell(row=row, column=2, value=elemento[\"minuto\"])\n",
        "                ws.cell(row=row, column=3, value=elemento[\"segundo\"])\n",
        "                ws.cell(row=row, column=4, value=f\"{elemento['categoria']}: {elemento['tipo']}\")\n",
        "\n",
        "                # Análise de posição\n",
        "                total_segundos = elemento[\"minuto\"] * 60 + elemento[\"segundo\"]\n",
        "                if total_segundos <= 10:\n",
        "                    posicao = \"ABERTURA\"\n",
        "                    posicao_color = \"70AD47\"\n",
        "                elif total_segundos <= 20:\n",
        "                    posicao = \"MEIO\"\n",
        "                    posicao_color = \"FFC000\"\n",
        "                else:\n",
        "                    posicao = \"FINAL\"\n",
        "                    posicao_color = \"C5504B\"\n",
        "\n",
        "                cell_pos = ws.cell(row=row, column=5, value=posicao)\n",
        "                cell_pos.font = Font(color=posicao_color, bold=True)\n",
        "\n",
        "                # Análise de impacto\n",
        "                impacto = analisar_impacto_elemento(elemento[\"categoria\"], posicao)\n",
        "                ws.cell(row=row, column=6, value=impacto[\"score\"])\n",
        "                ws.cell(row=row, column=7, value=impacto[\"analise\"])\n",
        "\n",
        "                if impacto[\"score\"] == \"ALTO\":\n",
        "                    ws.cell(row=row, column=6).font = Font(color=\"70AD47\", bold=True)\n",
        "                elif impacto[\"score\"] == \"BAIXO\":\n",
        "                    ws.cell(row=row, column=6).font = Font(color=\"C5504B\", bold=True)\n",
        "\n",
        "                row += 1\n",
        "        else:\n",
        "            ws.cell(row=row, column=1, value=\"Nenhum elemento temporal mapeado\")\n",
        "            row += 1\n",
        "\n",
        "        row += 2\n",
        "\n",
        "    # Ajustar larguras\n",
        "    for col, width in [(\"A\", 8), (\"B\", 10), (\"C\", 15), (\"D\", 30), (\"E\", 12), (\"F\", 8), (\"G\", 25)]:\n",
        "        ws.column_dimensions[col].width = width\n",
        "\n",
        "def analisar_impacto_elemento(categoria, posicao):\n",
        "    \"\"\"Analisa o impacto de um elemento baseado na posição\"\"\"\n",
        "    impactos = {\n",
        "        (\"GANCHO\", \"ABERTURA\"): {\"score\": \"ALTO\", \"analise\": \"Ideal para capturar atenção\"},\n",
        "        (\"GANCHO\", \"MEIO\"): {\"score\": \"MÉDIO\", \"analise\": \"Melhor no início\"},\n",
        "        (\"GANCHO\", \"FINAL\"): {\"score\": \"BAIXO\", \"analise\": \"Reposicionar para abertura\"},\n",
        "        (\"GATILHO\", \"ABERTURA\"): {\"score\": \"MÉDIO\", \"analise\": \"Bom para credibilidade\"},\n",
        "        (\"GATILHO\", \"MEIO\"): {\"score\": \"ALTO\", \"analise\": \"Posição ideal para persuasão\"},\n",
        "        (\"GATILHO\", \"FINAL\"): {\"score\": \"MÉDIO\", \"analise\": \"Reforça decisão\"},\n",
        "        (\"CTA\", \"ABERTURA\"): {\"score\": \"BAIXO\", \"analise\": \"Muito cedo, construir valor primeiro\"},\n",
        "        (\"CTA\", \"MEIO\"): {\"score\": \"MÉDIO\", \"analise\": \"Considerar mover para final\"},\n",
        "        (\"CTA\", \"FINAL\"): {\"score\": \"ALTO\", \"analise\": \"Posicionamento ideal\"}\n",
        "    }\n",
        "\n",
        "    return impactos.get((categoria, posicao), {\"score\": \"MÉDIO\", \"analise\": \"Analisar contexto específico\"})\n",
        "\n",
        "def adicionar_aba_recomendacoes_copy(wb, dados):\n",
        "    \"\"\"Adiciona aba de recomendações estratégicas consolidadas\"\"\"\n",
        "    ws = wb.create_sheet(\"Recomendações Copy\")\n",
        "\n",
        "    # Título\n",
        "    ws.merge_cells(\"A1:F1\")\n",
        "    titulo = ws[\"A1\"]\n",
        "    titulo.value = \"RECOMENDAÇÕES ESTRATÉGICAS DE COPYWRITING - PLANO DE AÇÃO\"\n",
        "    titulo.fill = PatternFill(start_color=\"C5504B\", end_color=\"C5504B\", fill_type=\"solid\")\n",
        "    titulo.font = Font(color=\"FFFFFF\", bold=True, size=14)\n",
        "    titulo.alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "    row = 3\n",
        "\n",
        "    # Consolidar recomendações por prioridade\n",
        "    todas_recomendacoes = []\n",
        "    for video in dados[\"copywriting\"]:\n",
        "        recomendacoes_video = video.get(\"recomendacoes_estrategicas\", [])\n",
        "        for rec in recomendacoes_video:\n",
        "            rec[\"video_id\"] = video[\"video_id\"]\n",
        "            todas_recomendacoes.append(rec)\n",
        "\n",
        "    # Agrupar por prioridade\n",
        "    recomendacoes_por_prioridade = {\n",
        "        \"CRÍTICA\": [],\n",
        "        \"ALTA\": [],\n",
        "        \"MÉDIA\": []\n",
        "    }\n",
        "\n",
        "    for rec in todas_recomendacoes:\n",
        "        prioridade = rec.get(\"prioridade\", \"MÉDIA\")\n",
        "        if prioridade in recomendacoes_por_prioridade:\n",
        "            recomendacoes_por_prioridade[prioridade].append(rec)\n",
        "\n",
        "    # Exibir por prioridade\n",
        "    for prioridade in [\"CRÍTICA\", \"ALTA\", \"MÉDIA\"]:\n",
        "        if not recomendacoes_por_prioridade[prioridade]:\n",
        "            continue\n",
        "\n",
        "        ws[f\"A{row}\"] = f\"🚨 PRIORIDADE {prioridade}\"\n",
        "        if prioridade == \"CRÍTICA\":\n",
        "            ws[f\"A{row}\"].font = Font(color=\"FF0000\", bold=True, size=12)\n",
        "        elif prioridade == \"ALTA\":\n",
        "            ws[f\"A{row}\"].font = Font(color=\"C5504B\", bold=True, size=12)\n",
        "        else:\n",
        "            ws[f\"A{row}\"].font = Font(color=\"FFC000\", bold=True, size=12)\n",
        "\n",
        "        row += 2\n",
        "\n",
        "        # Headers\n",
        "        headers = [\"Categoria\", \"Recomendação\", \"Vídeos Afetados\", \"Ação Sugerida\"]\n",
        "        for col, header in enumerate(headers, 1):\n",
        "            cell = ws.cell(row=row, column=col)\n",
        "            cell.value = header\n",
        "            cell.font = Font(bold=True)\n",
        "            cell.fill = PatternFill(start_color=\"F2F2F2\", end_color=\"F2F2F2\", fill_type=\"solid\")\n",
        "        row += 1\n",
        "\n",
        "        # Agrupar recomendações similares da mesma prioridade\n",
        "        grupos = {}\n",
        "        for rec in recomendacoes_por_prioridade[prioridade]:\n",
        "            categoria = rec[\"categoria\"]\n",
        "            if categoria not in grupos:\n",
        "                grupos[categoria] = {\n",
        "                    \"recomendacao\": rec[\"recomendacao\"],\n",
        "                    \"videos\": [],\n",
        "                    \"acao\": gerar_acao_especifica(categoria)\n",
        "                }\n",
        "            grupos[categoria][\"videos\"].append(rec[\"video_id\"])\n",
        "\n",
        "        for categoria, dados_grupo in grupos.items():\n",
        "            ws.cell(row=row, column=1, value=categoria)\n",
        "            ws.cell(row=row, column=2, value=dados_grupo[\"recomendacao\"])\n",
        "            ws.cell(row=row, column=3, value=f\"{len(dados_grupo['videos'])} vídeo(s)\")\n",
        "            ws.cell(row=row, column=4, value=dados_grupo[\"acao\"])\n",
        "            row += 1\n",
        "\n",
        "        row += 2\n",
        "\n",
        "    # Ajustar larguras\n",
        "    for col, width in [(\"A\", 20), (\"B\", 40), (\"C\", 15), (\"D\", 30), (\"E\", 15), (\"F\", 15)]:\n",
        "        ws.column_dimensions[col].width = width\n",
        "\n",
        "def gerar_acao_especifica(categoria):\n",
        "    \"\"\"Gera ação específica baseada na categoria da recomendação\"\"\"\n",
        "    acoes = {\n",
        "        \"GANCHOS\": \"Revisar primeiros 5 segundos e adicionar pergunta ou curiosidade\",\n",
        "        \"GATILHOS\": \"Incorporar elementos de autoridade, prova social ou reciprocidade\",\n",
        "        \"CTA\": \"Adicionar call-to-action claro nos últimos 3-5 segundos\",\n",
        "        \"ESTRUTURA\": \"Aplicar template identificado mais próximo do nicho\",\n",
        "        \"PERSUASÃO\": \"Combinar múltiplos elementos persuasivos em sequência lógica\"\n",
        "    }\n",
        "    return acoes.get(categoria, \"Revisar e otimizar elementos específicos mencionados\")\n",
        "\n",
        "def atualizar_aba_principal_com_copy(wb, dados):\n",
        "    \"\"\"Atualiza a aba principal existente com métricas de copywriting\"\"\"\n",
        "    # Tentar encontrar aba principal (pode ter nomes diferentes)\n",
        "    aba_principal = None\n",
        "    possiveis_nomes = [\"Dashboard Principal\", \"Executive Summary\", \"Summary\", \"Principal\"]\n",
        "\n",
        "    for nome in wb.sheetnames:\n",
        "        if any(possivel in nome for possivel in possiveis_nomes):\n",
        "            aba_principal = wb[nome]\n",
        "            break\n",
        "\n",
        "    if not aba_principal:\n",
        "        # Se não encontrou, usar a primeira aba\n",
        "        aba_principal = wb.worksheets[0]\n",
        "\n",
        "    # Encontrar próxima linha vazia para adicionar seção de copywriting\n",
        "    next_row = 1\n",
        "    for row in range(1, 100):\n",
        "        if aba_principal[f\"A{row}\"].value is None:\n",
        "            next_row = row\n",
        "            break\n",
        "\n",
        "    # Adicionar seção de copywriting\n",
        "    # Título da seção\n",
        "    aba_principal.merge_cells(f\"A{next_row}:H{next_row}\")\n",
        "    titulo_copy = aba_principal[f\"A{next_row}\"]\n",
        "    titulo_copy.value = \"📝 ANÁLISE DE COPYWRITING - RESUMO EXECUTIVO\"\n",
        "    titulo_copy.fill = PatternFill(start_color=\"7030A0\", end_color=\"7030A0\", fill_type=\"solid\")\n",
        "    titulo_copy.font = Font(color=\"FFFFFF\", bold=True, size=12)\n",
        "    next_row += 2\n",
        "\n",
        "    # Métricas resumidas\n",
        "    videos_copy = dados[\"copywriting\"]\n",
        "\n",
        "    if videos_copy:\n",
        "        scores = [v.get(\"score_persuasao\", 0) for v in videos_copy]\n",
        "        score_medio = sum(scores) / len(scores)\n",
        "        videos_sem_cta = len([v for v in videos_copy if not v.get(\"ctas_detectados\")])\n",
        "        templates_total = sum(len(v.get(\"templates_identificados\", [])) for v in videos_copy)\n",
        "\n",
        "        metricas_resumo = [\n",
        "            (\"Score de Persuasão Médio:\", f\"{score_medio:.1f}/100\"),\n",
        "            (\"Vídeos sem CTA:\", f\"{videos_sem_cta} (CRÍTICO)\" if videos_sem_cta > 0 else \"0 ✅\"),\n",
        "            (\"Templates Identificados:\", str(templates_total)),\n",
        "            (\"Status Geral:\", \"Otimização necessária\" if score_medio < 60 or videos_sem_cta > 0 else \"Performance boa\")\n",
        "        ]\n",
        "\n",
        "        for metrica, valor in metricas_resumo:\n",
        "            aba_principal[f\"A{next_row}\"] = metrica\n",
        "            aba_principal[f\"B{next_row}\"] = valor\n",
        "            aba_principal[f\"A{next_row}\"].font = Font(bold=True)\n",
        "\n",
        "            if \"CRÍTICO\" in valor:\n",
        "                aba_principal[f\"B{next_row}\"].font = Font(color=\"FF0000\", bold=True)\n",
        "            elif \"✅\" in valor:\n",
        "                aba_principal[f\"B{next_row}\"].font = Font(color=\"70AD47\", bold=True)\n",
        "\n",
        "            next_row += 1\n",
        "\n",
        "    else:\n",
        "        aba_principal[f\"A{next_row}\"] = \"⚠️ Execute a análise de copywriting (Célula 2.4) para ver métricas\"\n",
        "        aba_principal[f\"A{next_row}\"].font = Font(color=\"FFC000\", bold=True)\n",
        "\n",
        "def gerar_relatorios_copywriting_individuais(dados):\n",
        "    \"\"\"Gera relatórios individuais de texto para cada vídeo\"\"\"\n",
        "    print(\"  📄 Gerando relatórios individuais de copywriting...\")\n",
        "\n",
        "    pasta_relatorios = os.path.join(PASTA_TRABALHO, \"relatorios_copywriting\")\n",
        "    os.makedirs(pasta_relatorios, exist_ok=True)\n",
        "\n",
        "    for video_copy in dados[\"copywriting\"]:\n",
        "        video_id = video_copy[\"video_id\"]\n",
        "\n",
        "        relatorio_path = os.path.join(pasta_relatorios, f\"{video_id}_copywriting_completo.txt\")\n",
        "\n",
        "        with open(relatorio_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\"=\"*60 + \"\\n\")\n",
        "            f.write(\"RELATÓRIO COMPLETO DE ANÁLISE DE COPYWRITING\\n\")\n",
        "            f.write(\"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "            f.write(f\"📹 Vídeo ID: {video_id}\\n\")\n",
        "            f.write(f\"🎯 Score de Persuasão: {video_copy.get('score_persuasao', 0)}/100\\n\")\n",
        "            f.write(f\"📝 Total de Palavras: {video_copy.get('total_palavras', 0)}\\n\\n\")\n",
        "\n",
        "            # Texto completo\n",
        "            f.write(\"TRANSCRIÇÃO COMPLETA:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            f.write(video_copy.get(\"texto_completo\", \"Transcrição não disponível\") + \"\\n\\n\")\n",
        "\n",
        "            # Ganchos\n",
        "            f.write(\"🎣 GANCHOS DETECTADOS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            ganchos = video_copy.get(\"ganchos_detectados\", {})\n",
        "            if ganchos:\n",
        "                for tipo, dados in ganchos.items():\n",
        "                    f.write(f\"• {tipo.upper()}: {dados['count']} ocorrências\\n\")\n",
        "                    f.write(f\"  Exemplos: {', '.join(dados['exemplos'])}\\n\")\n",
        "            else:\n",
        "                f.write(\"Nenhum gancho detectado.\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "            # Gatilhos\n",
        "            f.write(\"🧠 GATILHOS MENTAIS DETECTADOS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            gatilhos = video_copy.get(\"gatilhos_mentais_detectados\", {})\n",
        "            if gatilhos:\n",
        "                for tipo, dados in gatilhos.items():\n",
        "                    f.write(f\"• {tipo.upper()}: {dados['count']} ocorrências\\n\")\n",
        "                    f.write(f\"  Exemplos: {', '.join(dados['exemplos'])}\\n\")\n",
        "            else:\n",
        "                f.write(\"Nenhum gatilho mental detectado.\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "            # CTAs\n",
        "            f.write(\"📢 CALLS-TO-ACTION DETECTADOS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            ctas = video_copy.get(\"ctas_detectados\", {})\n",
        "            if ctas:\n",
        "                for tipo, dados in ctas.items():\n",
        "                    f.write(f\"• {tipo.upper()}: {dados['count']} ocorrências\\n\")\n",
        "                    f.write(f\"  Exemplos: {', '.join(dados['exemplos'])}\\n\")\n",
        "            else:\n",
        "                f.write(\"Nenhum CTA detectado.\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "            # Recomendações\n",
        "            f.write(\"💡 RECOMENDAÇÕES ESTRATÉGICAS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            recomendacoes = video_copy.get(\"recomendacoes_estrategicas\", [])\n",
        "            if recomendacoes:\n",
        "                for rec in recomendacoes:\n",
        "                    f.write(f\"• [{rec['prioridade']}] {rec['categoria']}: {rec['recomendacao']}\\n\")\n",
        "            else:\n",
        "                f.write(\"Nenhuma recomendação específica.\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "            # Templates\n",
        "            f.write(\"📋 TEMPLATES IDENTIFICADOS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            templates = video_copy.get(\"templates_identificados\", [])\n",
        "            if templates:\n",
        "                for template in templates:\n",
        "                    f.write(f\"• {template['nome']}: {template['estrutura']}\\n\")\n",
        "                    f.write(f\"  Eficácia: {template['eficacia']}\\n\")\n",
        "                    f.write(f\"  Uso: {template['uso_recomendado']}\\n\\n\")\n",
        "            else:\n",
        "                f.write(\"Nenhum template específico identificado.\\n\")\n",
        "\n",
        "    print(f\"  ✅ Relatórios individuais gerados em: {pasta_relatorios}\")\n",
        "\n",
        "# Executar integração\n",
        "try:\n",
        "    integrar_copywriting_dashboard_existente()\n",
        "except Exception as e:\n",
        "    print(f\"❌ ERRO de Execução: {type(e).__name__}: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvmDo8Iw61G8"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CÉLULA 4.3: INTEGRAÇÃO DE COPYWRITING NO DASHBOARD EXISTENTE\n",
        "# ============================================================================\n",
        "\n",
        "# Definir a variável global PASTA_TRABALHO se ainda não estiver definida\n",
        "# Certifique-se de que esta variável esteja definida corretamente em uma célula anterior (ex: Célula 1.2)\n",
        "# Exemplo: PASTA_TRABALHO = \"/content/drive/MyDrive/Videos Dona Done/_engenharia_reversa\"\n",
        "\n",
        "# Executar a função principal da Layer 4.3\n",
        "if 'PASTA_TRABALHO' in globals():\n",
        "    print(\"Iniciando a Layer 4.3: Integração de Copywriting no Dashboard...\")\n",
        "    integrar_copywriting_dashboard_existente()\n",
        "else:\n",
        "    print(\"ERRO: A variável PASTA_TRABALHO não está definida. Certifique-se de executar a Célula 1.2 ou equivalente.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ns9Xl0Uk7_Cq"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "from openpyxl.styles import Font, Alignment, PatternFill\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from fpdf import FPDF\n",
        "from openpyxl.utils import get_column_letter\n",
        "from openpyxl import load_workbook\n",
        "\n",
        "# --- Funções Auxiliares (do notebook original, se aplicável) ---\n",
        "def log_progress(message):\n",
        "    timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
        "    print(f\"[{timestamp}] {message}\")\n",
        "\n",
        "def verificar_prerequisito_etapa(etapa_anterior):\n",
        "    try:\n",
        "        if not \"PASTA_TRABALHO\" in globals():\n",
        "            raise Exception(\"Variáveis globais de configuração não encontradas. Execute a CÉLULA 1.2 primeiro.\")\n",
        "\n",
        "        config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "        if not os.path.exists(config_path):\n",
        "            raise Exception(\"Arquivo de configuração não encontrado. Execute a CÉLULA 1.2 primeiro.\")\n",
        "\n",
        "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            config = json.load(f)\n",
        "\n",
        "        if not config[\"status_etapas\"][etapa_anterior]:\n",
        "            raise Exception(f\"A etapa \"{etapa_anterior}\" não foi concluída. Execute a célula correspondente primeiro.\")\n",
        "\n",
        "        return True, config\n",
        "    except Exception as e:\n",
        "        print(f\"❌ PRÉ-REQUISITO NÃO ATENDIDO: {e}\")\n",
        "        return False, None\n",
        "\n",
        "def escrever_linha(ws, row, values, bold=False, wrap=False):\n",
        "    for col, val in enumerate(values, 1):\n",
        "        cell = ws.cell(row=row, column=col, value=val)\n",
        "        if bold: cell.font = Font(bold=True)\n",
        "        if wrap: cell.alignment = Alignment(wrap_text=True)\n",
        "\n",
        "def ajustar_larguras(ws, larguras):\n",
        "    for col_idx, width in enumerate(larguras, 1):\n",
        "        ws.column_dimensions[get_column_letter(col_idx)].width = width\n",
        "\n",
        "# --- Melhorias no Dashboard ---\n",
        "print(\"\n",
        "📊 Gerando Dashboard Executivo Inteligente...\n",
        "\")\n",
        "\n",
        "# Carregar dados de metadados e decomposições\n",
        "prerequisito_ok, config = verificar_prerequisito_etapa(\"decomposicao\")\n",
        "if not prerequisito_ok:\n",
        "    print(\"Não foi possível gerar o dashboard sem os dados de decomposição.\")\n",
        "    exit() # Ou return, dependendo do contexto do notebook\n",
        "\n",
        "metadados_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "decomposicoes_path = os.path.join(PASTA_TRABALHO, \"dados\", \"decomposicoes_completas.json\")\n",
        "analises_copy_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_copywriting_completas.json\")\n",
        "\n",
        "if not os.path.exists(metadados_path) or not os.path.exists(decomposicoes_path) or not os.path.exists(analises_copy_path):\n",
        "    print(\"❌ ERRO: Arquivos de dados essenciais para o dashboard não encontrados. Execute as células anteriores (2.2, 2.3, 3.1) primeiro.\")\n",
        "    exit()\n",
        "\n",
        "with open(metadados_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    metadados = json.load(f)\n",
        "with open(decomposicoes_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    decomposicoes = json.load(f)\n",
        "with open(analises_copy_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    analises_copy = json.load(f)\n",
        "\n",
        "# Unificar dados para o DataFrame\n",
        "data_for_df = []\n",
        "for meta_video in metadados:\n",
        "    video_id = meta_video[\"id\"]\n",
        "    decomposicao_video = next((d for d in decomposicoes if d[\"video_id\"] == video_id), {})\n",
        "    analise_copy_video = next((a for a in analises_copy if a[\"video_id\"] == video_id), {})\n",
        "\n",
        "    row = {\n",
        "        \"video_id\": video_id,\n",
        "        \"nome_arquivo\": meta_video[\"nome_arquivo\"],\n",
        "        \"duracao_segundos\": meta_video.get(\"duracao_segundos\"),\n",
        "        \"cortes_detectados_count\": decomposicao_video.get(\"cortes_detectados_count\", 0),\n",
        "        \"complexidade_visual_media\": decomposicao_video.get(\"complexidade_visual_media\"),\n",
        "        \"ocr_textos_count\": decomposicao_video.get(\"ocr_textos_count\", 0),\n",
        "        \"audio_transcrito_len\": len(decomposicao_video.get(\"audio_transcrito\", \"\")), # Usar len da string\n",
        "        \"bpm_audio\": decomposicao_video.get(\"bpm_audio\"),\n",
        "        \"brilho_medio\": decomposicao_video.get(\"brilho_medio\"),\n",
        "        \"formato_detectado\": decomposicao_video.get(\"formato_detectado\"),\n",
        "        \"tem_audio\": decomposicao_video.get(\"tem_audio\"),\n",
        "        \"score_persuasao\": analise_copy_video.get(\"score_persuasao\"),\n",
        "        \"ganchos_detectados\": analise_copy_video.get(\"ganchos_detectados\", {}),\n",
        "        \"gatilhos_mentais_detectados\": analise_copy_video.get(\"gatilhos_mentais_detectados\", {}),\n",
        "        \"ctas_detectados\": analise_copy_video.get(\"ctas_detectados\", {}),\n",
        "        \"templates_identificados\": analise_copy_video.get(\"templates_identificados\", []),\n",
        "        \"recomendacoes_estrategicas\": analise_copy_video.get(\"recomendacoes_estrategicas\", []),\n",
        "        \"primeira_frase_audio\": decomposicao_video.get(\"primeira_frase_audio\", \"\") # Adicionado\n",
        "    }\n",
        "    data_for_df.append(row)\n",
        "\n",
        "df = pd.DataFrame(data_for_df)\n",
        "\n",
        "# Calcular Scores (funções do notebook original)\n",
        "def calculate_viral_score(row):\n",
        "    try:\n",
        "        score = 0\n",
        "        if pd.notna(row['duracao_segundos']) and row['duracao_segundos'] > 0:\n",
        "            cortes_por_seg = row['cortes_detectados_count'] / row['duracao_segundos']\n",
        "            if cortes_por_seg > 20: score += 25\n",
        "            elif cortes_por_seg > 10: score += 20\n",
        "            elif cortes_por_seg > 5: score += 15\n",
        "            else: score += 10\n",
        "        if pd.notna(row['complexidade_visual_media']):\n",
        "            if row['complexidade_visual_media'] > 600: score += 20\n",
        "            elif row['complexidade_visual_media'] > 400: score += 15\n",
        "            else: score += 10\n",
        "        if pd.notna(row['ocr_textos_count']):\n",
        "            if row['ocr_textos_count'] > 10: score += 15\n",
        "            elif row['ocr_textos_count'] > 5: score += 12\n",
        "            elif row['ocr_textos_count'] > 0: score += 8\n",
        "        if pd.notna(row['duracao_segundos']):\n",
        "            if 15 <= row['duracao_segundos'] <= 30: score += 20\n",
        "            elif 10 <= row['duracao_segundos'] <= 45: score += 15\n",
        "            else: score += 10\n",
        "        gatilhos = str(row['gatilhos_mentais_detectados']).lower()\n",
        "        if 'urgência' in gatilhos or 'escassez' in gatilhos: score += 8\n",
        "        if 'estímulo' in gatilhos: score += 7\n",
        "        if 'atenção' in gatilhos: score += 5\n",
        "        return min(score, 100)\n",
        "    except:\n",
        "        return 50\n",
        "\n",
        "def calculate_technical_score(row):\n",
        "    try:\n",
        "        score = 0\n",
        "        if pd.notna(row['brilho_medio']):\n",
        "            if 120 <= row['brilho_medio'] <= 180: score += 25\n",
        "            elif 100 <= row['brilho_medio'] <= 200: score += 20\n",
        "            else: score += 10\n",
        "        formato = str(row['formato_detectado'])\n",
        "        if 'vertical_9_16' in formato: score += 25\n",
        "        elif 'horizontal_16_9' in formato: score += 20\n",
        "        else: score += 15\n",
        "        if row['tem_audio']: score += 25\n",
        "        else: score += 5\n",
        "        if pd.notna(row['total_frames']) and row['total_frames'] > 0:\n",
        "            if row['total_frames'] > 300: score += 25\n",
        "            elif row['total_frames'] > 150: score += 20\n",
        "            else: score += 15\n",
        "        return min(score, 100)\n",
        "    except:\n",
        "        return 50\n",
        "\n",
        "def calculate_content_score(row):\n",
        "    try:\n",
        "        score = 0\n",
        "        ocr_count = row['ocr_textos_count'] if pd.notna(row['ocr_textos_count']) else 0\n",
        "        audio_len = row['audio_transcrito_len'] if pd.notna(row['audio_transcrito_len']) else 0\n",
        "        if ocr_count > 5 or audio_len > 100: score += 30\n",
        "        elif ocr_count > 2 or audio_len > 50: score += 20\n",
        "        elif ocr_count > 0 or audio_len > 0: score += 15\n",
        "        else: score += 5\n",
        "        if pd.notna(row['bpm_audio']):\n",
        "            if 120 <= row['bpm_audio'] <= 140: score += 35\n",
        "            elif 100 <= row['bpm_audio'] <= 160: score += 25\n",
        "            else: score += 15\n",
        "        if pd.notna(row['duracao_segundos']) and row['duracao_segundos'] > 0:\n",
        "            densidade = (ocr_count + audio_len/10) / row['duracao_segundos']\n",
        "            if densidade > 2: score += 35\n",
        "            elif densidade > 1: score += 25\n",
        "            else: score += 15\n",
        "        return min(score, 100)\n",
        "    except:\n",
        "        return 50\n",
        "\n",
        "def generate_insights_from_data(df):\n",
        "    insights = []\n",
        "    try:\n",
        "        if not df.empty:\n",
        "            best_performing = df.nlargest(3, 'viral_score')\n",
        "            if not best_performing.empty:\n",
        "                avg_duration = best_performing['duracao_segundos'].mean()\n",
        "                insights.append(f\"DURAÇÃO VENCEDORA: Seus top 3 vídeos (com maior score viral) têm duração média de {avg_duration:.1f} segundos. Considere este como um benchmark para futuros conteúdos.\")\n",
        "\n",
        "            # Análise de Gatilhos Mentais\n",
        "            all_gatilhos = Counter()\n",
        "            for gatilhos_dict in df['gatilhos_mentais_detectados']:\n",
        "                for tipo, data in gatilhos_dict.items():\n",
        "                    all_gatilhos[tipo] += len(data.get('timestamps', []))\n",
        "            if all_gatilhos:\n",
        "                most_common_gatilho = all_gatilhos.most_common(1)\n",
        "                if most_common_gatilho:\n",
        "                    insights.append(f\"GATILHO MAIS EFICAZ: O gatilho '{most_common_gatilho[0][0]}' foi o mais frequente ({most_common_gatilho[0][1]} ocorrências). Explore como replicar seu uso em outros vídeos.\")\n",
        "\n",
        "            # Análise de CTAs\n",
        "            videos_with_cta = df[df['ctas_detectados'].apply(lambda x: len(x) > 0)]\n",
        "            if not videos_with_cta.empty:\n",
        "                insights.append(f\"CHAMADAS PARA AÇÃO (CTAs): {len(videos_with_cta)} de {len(df)} vídeos possuem CTAs detectados. Vídeos com CTAs tendem a ter melhor performance. Garanta que todos os vídeos tenham um CTA claro.\")\n",
        "            else:\n",
        "                insights.append(\"⚠️ ALERTA DE CTA: Nenhum CTA foi detectado em seus vídeos. CTAs são cruciais para guiar a audiência. Adicione chamadas para ação claras e visíveis.\")\n",
        "\n",
        "            # Análise de Cortes\n",
        "            if 'cortes_detectados_count' in df.columns and not df.empty:\n",
        "                avg_cuts = df['cortes_detectados_count'].mean()\n",
        "                insights.append(f\"RITMO DE EDIÇÃO: Em média, seus vídeos possuem {avg_cuts:.1f} cortes. Um ritmo dinâmico pode aumentar o engajamento.\")\n",
        "\n",
        "            # Análise de Primeira Frase do Áudio\n",
        "            if 'primeira_frase_audio' in df.columns and not df['primeira_frase_audio'].empty:\n",
        "                exemplos_frases = df['primeira_frase_audio'].dropna().sample(min(3, len(df['primeira_frase_audio'].dropna()))).tolist()\n",
        "                if exemplos_frases:\n",
        "                    insights.append(f\"INÍCIOS IMPACTANTES: As primeiras frases do áudio são cruciais. Exemplos de inícios fortes: {'; '.join([f'\"' + s + '\"' for s in exemplos_frases])}. Analise o impacto dessas frases na retenção inicial.\")\n",
        "\n",
        "        else:\n",
        "            insights.append(\"Não há dados suficientes para gerar insights. Execute as etapas anteriores do notebook.\")\n",
        "    except Exception as e:\n",
        "        insights.append(f\"Erro ao gerar insights: {e}\")\n",
        "    return insights\n",
        "\n",
        "\n",
        "df['viral_score'] = df.apply(calculate_viral_score, axis=1)\n",
        "df['technical_score'] = df.apply(calculate_technical_score, axis=1)\n",
        "df['content_score'] = df.apply(calculate_content_score, axis=1)\n",
        "\n",
        "# Gerar insights\n",
        "insights = generate_insights_from_data(df)\n",
        "\n",
        "# Salvar o blueprint atualizado (agora com insights)\n",
        "blueprint_data = {\n",
        "    \"resumo_geral\": {\n",
        "        \"total_videos_analisados\": len(df),\n",
        "        \"media_score_viral\": df['viral_score'].mean(),\n",
        "        \"media_score_tecnico\": df['technical_score'].mean(),\n",
        "        \"media_score_conteudo\": df['content_score'].mean(),\n",
        "        \"insights_estrategicos\": insights\n",
        "    },\n",
        "    \"detalhes_por_video\": df.to_dict(orient='records')\n",
        "}\n",
        "\n",
        "blueprint_path = os.path.join(PASTA_TRABALHO, \"blueprint\", \"blueprint_final.json\")\n",
        "with open(blueprint_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(blueprint_data, f, indent=2, ensure_ascii=False)\n",
        "print(f\"✅ Blueprint final atualizado e salvo em: {blueprint_path}\")\n",
        "\n",
        "# --- Geração de Relatório em Excel (Dashboard) ---\n",
        "OUTPUT_PATH = os.path.join(PASTA_TRABALHO, \"dashboard\", \"DASHBOARD_MASTER_EXECUTIVO_INTELIGENTE.xlsx\")\n",
        "\n",
        "wb = Workbook()\n",
        "\n",
        "# Remover a aba padrão criada\n",
        "if 'Sheet' in wb.sheetnames:\n",
        "    wb.remove(wb['Sheet'])\n",
        "\n",
        "# --- ABA 1: Resumo Executivo ---\n",
        "ws_resumo = wb.create_sheet(\"Resumo Executivo\")\n",
        "ws_resumo.merge_cells('A1:D1')\n",
        "ws_resumo['A1'].value = \"DASHBOARD MASTER EXECUTIVO INTELIGENTE\"\n",
        "ws_resumo['A1'].font = Font(bold=True, size=16, color=\"FFFFFF\")\n",
        "ws_resumo['A1'].alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
        "ws_resumo['A1'].fill = PatternFill(start_color=\"0070C0\", end_color=\"0070C0\", fill_type=\"solid\")\n",
        "\n",
        "ws_resumo.merge_cells('A3:D3')\n",
        "ws_resumo['A3'].value = \"Insights Estratégicos\"\n",
        "ws_resumo['A3'].font = Font(bold=True, size=14, color=\"0070C0\")\n",
        "ws_resumo['A3'].alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "row_start = 5\n",
        "for insight in insights:\n",
        "    ws_resumo.merge_cells(f'A{row_start}:D{row_start}')\n",
        "    ws_resumo[f'A{row_start}'].value = f\"• {insight}\"\n",
        "    ws_resumo[f'A{row_start}'].alignment = Alignment(wrap_text=True)\n",
        "    row_start += 1\n",
        "\n",
        "ws_resumo.column_dimensions['A'].width = 20\n",
        "ws_resumo.column_dimensions['B'].width = 20\n",
        "ws_resumo.column_dimensions['C'].width = 20\n",
        "ws_resumo.column_dimensions['D'].width = 20\n",
        "\n",
        "# --- ABA 2: Detalhes por Vídeo ---\n",
        "ws_detalhes = wb.create_sheet(\"Detalhes por Video\")\n",
        "\n",
        "# Preparar dados para o Excel, incluindo a nova coluna de nome renomeado\n",
        "df_excel = df[['nome_arquivo', 'primeira_frase_audio', 'duracao_segundos', 'cortes_detectados_count',\n",
        "               'complexidade_visual_media', 'ocr_textos_count', 'audio_transcrito_len',\n",
        "               'bpm_audio', 'brilho_medio', 'formato_detectado', 'tem_audio',\n",
        "               'score_persuasao', 'viral_score', 'technical_score', 'content_score']]\n",
        "\n",
        "df_excel.rename(columns={\n",
        "    'nome_arquivo': 'Nome do Arquivo Original',\n",
        "    'primeira_frase_audio': 'Nome Renomeado (Primeira Frase)',\n",
        "    'duracao_segundos': 'Duração (s)',\n",
        "    'cortes_detectados_count': 'Cortes Detectados',\n",
        "    'complexidade_visual_media': 'Complexidade Visual Média',\n",
        "    'ocr_textos_count': 'Textos OCR Detectados',\n",
        "    'audio_transcrito_len': 'Tamanho Transcrição Áudio',\n",
        "    'bpm_audio': 'BPM Áudio',\n",
        "    'brilho_medio': 'Brilho Médio',\n",
        "    'formato_detectado': 'Formato Detectado',\n",
        "    'tem_audio': 'Tem Áudio',\n",
        "    'score_persuasao': 'Score Persuasão',\n",
        "    'viral_score': 'Score Viral',\n",
        "    'technical_score': 'Score Técnico',\n",
        "    'content_score': 'Score Conteúdo'\n",
        "}, inplace=True)\n",
        "\n",
        "for r_idx, row in enumerate(dataframe_to_rows(df_excel, index=False, header=True), 1):\n",
        "    ws_detalhes.append(row)\n",
        "\n",
        "for cell in ws_detalhes[1]:\n",
        "    cell.font = Font(bold=True)\n",
        "    cell.fill = PatternFill(start_color=\"D9E1F2\", end_color=\"D9E1F2\", fill_type=\"solid\")\n",
        "\n",
        "# Ajustar largura das colunas\n",
        "for column in ws_detalhes.columns:\n",
        "    max_length = 0\n",
        "    column_name = get_column_letter(column[0].column)\n",
        "    for cell in column:\n",
        "        try:\n",
        "            if len(str(cell.value)) > max_length:\n",
        "                max_length = len(str(cell.value))\n",
        "        except:\n",
        "            pass\n",
        "    adjusted_width = (max_length + 2) * 1.2\n",
        "    ws_detalhes.column_dimensions[column_name].width = adjusted_width\n",
        "\n",
        "# --- ABA 3: Recomendações Detalhadas ---\n",
        "ws_recs = wb.create_sheet(\"Recomendacoes Detalhadas\")\n",
        "ws_recs.merge_cells('A1:C1')\n",
        "ws_recs['A1'].value = \"RECOMENDAÇÕES ESTRATÉGICAS POR VÍDEO\"\n",
        "ws_recs['A1'].font = Font(bold=True, size=16, color=\"FFFFFF\")\n",
        "ws_recs['A1'].alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
        "ws_recs['A1'].fill = PatternFill(start_color=\"FFC000\", end_color=\"FFC000\", fill_type=\"solid\")\n",
        "\n",
        "rec_row = 3\n",
        "for video_data in blueprint_data[\"detalhes_por_video\"]:\n",
        "    ws_recs[f'A{rec_row}'].value = f\"Vídeo: {video_data['nome_arquivo']}\"\n",
        "    ws_recs[f'A{rec_row}'].font = Font(bold=True, size=12)\n",
        "    rec_row += 1\n",
        "    if video_data['recomendacoes_estrategicas']:\n",
        "        for rec in video_data['recomendacoes_estrategicas']:\n",
        "            ws_recs[f'A{rec_row}'].value = f\"• Categoria: {rec.get('categoria', 'N/A')}\"\n",
        "            ws_recs[f'B{rec_row}'].value = f\"Recomendação: {rec.get('recomendacao', 'N/A')}\"\n",
        "            ws_recs[f'C{rec_row}'].value = f\"Prioridade: {rec.get('prioridade', 'N/A')}\"\n",
        "            rec_row += 1\n",
        "    else:\n",
        "        ws_recs[f'A{rec_row}'].value = \"Nenhuma recomendação específica para este vídeo.\"\n",
        "        rec_row += 1\n",
        "    rec_row += 1 # Espaço entre vídeos\n",
        "\n",
        "ws_recs.column_dimensions['A'].width = 30\n",
        "ws_recs.column_dimensions['B'].width = 60\n",
        "ws_recs.column_dimensions['C'].width = 15\n",
        "\n",
        "# --- ABA 4: Templates Identificados ---\n",
        "ws_templates = wb.create_sheet(\"Templates Identificados\")\n",
        "ws_templates.merge_cells('A1:D1')\n",
        "ws_templates['A1'].value = \"TEMPLATES ESTRATÉGICOS IDENTIFICADOS\"\n",
        "ws_templates['A1'].font = Font(bold=True, size=16, color=\"FFFFFF\")\n",
        "ws_templates['A1'].alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
        "ws_templates['A1'].fill = PatternFill(start_color=\"70AD47\", end_color=\"70AD47\", fill_type=\"solid\")\n",
        "\n",
        "temp_row = 3\n",
        "all_templates = []\n",
        "for video_data in blueprint_data[\"detalhes_por_video\"]:\n",
        "    for template in video_data['templates_identificados']:\n",
        "        all_templates.append(template)\n",
        "\n",
        "# Remover duplicatas de templates\n",
        "unique_templates = []\n",
        "seen_templates = set()\n",
        "for tpl in all_templates:\n",
        "    tpl_tuple = tuple(sorted(tpl.items())) # Para comparar dicionários como tuplas\n",
        "    if tpl_tuple not in seen_templates:\n",
        "        unique_templates.append(tpl)\n",
        "        seen_templates.add(tpl_tuple)\n",
        "\n",
        "if unique_templates:\n",
        "    for template in unique_templates:\n",
        "        ws_templates[f'A{temp_row}'].value = f\"Nome: {template.get('nome', 'N/A')}\"\n",
        "        ws_templates[f'B{temp_row}'].value = f\"Estrutura: {template.get('estrutura', 'N/A')}\"\n",
        "        ws_templates[f'C{temp_row}'].value = f\"Eficácia: {template.get('eficacia', 'N/A')}\"\n",
        "        ws_templates[f'D{temp_row}'].value = f\"Uso Recomendado: {template.get('uso_recomendado', 'N/A')}\"\n",
        "        temp_row += 1\n",
        "else:\n",
        "    ws_templates[f'A{temp_row}'].value = \"Nenhum template estratégico identificado.\"\n",
        "\n",
        "ws_templates.column_dimensions['A'].width = 25\n",
        "ws_templates.column_dimensions['B'].width = 50\n",
        "ws_templates.column_dimensions['C'].width = 15\n",
        "ws_templates.column_dimensions['D'].width = 40\n",
        "\n",
        "# Salvar o arquivo Excel\n",
        "wb.save(OUTPUT_PATH)\n",
        "print(f\"✅ Dashboard Master Executivo Inteligente gerado em: {OUTPUT_PATH}\")\n",
        "\n",
        "# --- Geração de Relatório em PDF (Resumo) ---\n",
        "pdf = FPDF()\n",
        "pdf.add_page()\n",
        "pdf.set_font(\"Arial\", \"B\", 16)\n",
        "pdf.cell(200, 10, \"Relatório Executivo de Engenharia Reversa de Vídeos\", 0, 1, \"C\")\n",
        "pdf.ln(10)\n",
        "\n",
        "pdf.set_font(\"Arial\", \"\", 12)\n",
        "pdf.multi_cell(0, 10, \"Este relatório apresenta uma visão estratégica dos resultados da engenharia reversa dos vídeos, com foco em insights acionáveis para otimização de conteúdo.\")\n",
        "pdf.ln(5)\n",
        "\n",
        "pdf.set_font(\"Arial\", \"B\", 14)\n",
        "pdf.cell(200, 10, \"1. Insights Estratégicos\", 0, 1, \"L\")\n",
        "pdf.ln(2)\n",
        "for insight in insights:\n",
        "    pdf.multi_cell(0, 8, f\"• {insight}\")\n",
        "pdf.ln(5)\n",
        "\n",
        "pdf.set_font(\"Arial\", \"B\", 14)\n",
        "pdf.cell(200, 10, \"2. Resumo de Métricas Principais\", 0, 1, \"L\")\n",
        "pdf.ln(2)\n",
        "pdf.multi_cell(0, 8, f\"Total de Vídeos Analisados: {blueprint_data['resumo_geral']['total_videos_analisados']}\")\n",
        "pdf.multi_cell(0, 8, f\"Média Score Viral: {blueprint_data['resumo_geral']['media_score_viral']:.2f}\")\n",
        "pdf.multi_cell(0, 8, f\"Média Score Técnico: {blueprint_data['resumo_geral']['media_score_tecnico']:.2f}\")\n",
        "pdf.multi_cell(0, 8, f\"Média Score Conteúdo: {blueprint_data['resumo_geral']['media_score_conteudo']:.2f}\")\n",
        "pdf.ln(5)\n",
        "\n",
        "# Adicionar gráficos (se existirem e forem gerados)\n",
        "# Exemplo: if os.path.exists(os.path.join(PASTA_TRABALHO, \"dashboard\", \"taxa_engajamento.png\")):\n",
        "#     pdf.add_page()\n",
        "#     pdf.set_font(\"Arial\", \"B\", 14)\n",
        "#     pdf.cell(200, 10, \"3. Gráfico de Taxa de Engajamento\", 0, 1, \"L\")\n",
        "#     pdf.image(os.path.join(PASTA_TRABALHO, \"dashboard\", \"taxa_engajamento.png\"), x=10, y=pdf.get_y(), w=180)\n",
        "\n",
        "relatorio_pdf_path = os.path.join(PASTA_TRABALHO, \"blueprint\", \"relatorio_executivo.pdf\")\n",
        "pdf.output(relatorio_pdf_path)\n",
        "print(f\"✅ Relatório executivo em PDF gerado em: {relatorio_pdf_path}\")\n",
        "\n",
        "print(\"\n",
        "✅ DASHBOARD E RELATÓRIOS GERADOS COM SUCESSO!\n",
        "\")\n",
        "print(\"➡️ PRÓXIMA CÉLULA: Nenhuma. Processo concluído.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_QVY6i-8qLc"
      },
      "outputs": [],
      "source": [
        "rodar_layer_5_6_ai()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsF_FvEM-26P"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# LAYER 7 — IA ONLINE (FREE) • Análise semântica avançada\n",
        "# - Sem likes / comentários / views.\n",
        "# - Usa Hugging Face Inference API (grátis com token).\n",
        "# - Respeita PASTA_TRABALHO e OUTPUT_PATH do seu projeto.\n",
        "# ============================================================\n",
        "\n",
        "!pip -q install openpyxl==3.1.2 requests==2.32.3\n",
        "\n",
        "import os, re, json, time, math\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any\n",
        "from openpyxl import load_workbook, Workbook\n",
        "from openpyxl.utils import get_column_letter\n",
        "from openpyxl.styles import Font, Alignment, PatternFill\n",
        "\n",
        "# ---------- Conexão com seu notebook ----------\n",
        "assert 'PASTA_TRABALHO' in globals(), \"PASTA_TRABALHO não está definido. Rode as células de configuração.\"\n",
        "if 'OUTPUT_PATH' not in globals():\n",
        "    OUTPUT_PATH = os.path.join(PASTA_TRABALHO, \"dashboard\", \"DASHBOARD_MASTER_EXECUTIVO_INTELIGENTE.xlsx\")\n",
        "    print(f\"⚠️ OUTPUT_PATH não estava definido; criando automático: {OUTPUT_PATH}\")\n",
        "\n",
        "DADOS_DIR = os.path.join(PASTA_TRABALHO, \"dados\")\n",
        "AI_ONLINE_DIR = os.path.join(PASTA_TRABALHO, \"ai_online\")\n",
        "os.makedirs(DADOS_DIR, exist_ok=True)\n",
        "os.makedirs(AI_ONLINE_DIR, exist_ok=True)\n",
        "\n",
        "META_PATH = os.path.join(DADOS_DIR, \"metadados_completos.json\")\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = \"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\"   # coloque seu token\n",
        "# ---------- Config da IA Online (FREE) ----------\n",
        "# Cadastre-se grátis na Hugging Face e crie um Access Token (Settings > Access Tokens).\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"\").strip()\n",
        "if not HF_TOKEN:\n",
        "    print(\"⚠️ Defina seu token gratuito da Hugging Face em os.environ['HF_TOKEN'] para ativar a IA online.\")\n",
        "\n",
        "# Modelo público e gratuito (ajuste se quiser)\n",
        "# Recomendo um instruído e leve para PT/ES/EN; Mistral 7B Instruct costuma funcionar bem:\n",
        "HF_MODEL = os.environ.get(\"HF_MODEL_ID\", \"mistralai/Mistral-7B-Instruct-v0.3\")\n",
        "HF_URL = f\"https://api-inference.huggingface.co/models/{HF_MODEL}\"\n",
        "\n",
        "def hf_generate(prompt: str, max_new_tokens=650, temperature=0.3, top_p=0.9, retries=2) -> str:\n",
        "    \"\"\"\n",
        "    Chama o endpoint de geração da Hugging Face (gratuito com token).\n",
        "    Retorna string gerada (sem garantias de JSON formatado — faremos parsing).\n",
        "    \"\"\"\n",
        "    if not HF_TOKEN:\n",
        "        raise RuntimeError(\"HF_TOKEN não definido. Configure os.environ['HF_TOKEN'] com seu token gratuito.\")\n",
        "    headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
        "    payload = {\n",
        "        \"inputs\": prompt,\n",
        "        \"parameters\": {\n",
        "            \"max_new_tokens\": max_new_tokens,\n",
        "            \"temperature\": temperature,\n",
        "            \"top_p\": top_p,\n",
        "            \"return_full_text\": False\n",
        "        }\n",
        "    }\n",
        "    for _ in range(retries):\n",
        "        r = requests.post(HF_URL, headers=headers, json=payload, timeout=90)\n",
        "        if r.status_code == 200:\n",
        "            try:\n",
        "                out = r.json()\n",
        "                if isinstance(out, list) and out and \"generated_text\" in out[0]:\n",
        "                    return out[0][\"generated_text\"]\n",
        "                if isinstance(out, dict) and \"generated_text\" in out:\n",
        "                    return out[\"generated_text\"]\n",
        "                # alguns servidores retornam str direta\n",
        "                if isinstance(out, str):\n",
        "                    return out\n",
        "            except Exception:\n",
        "                return r.text\n",
        "        time.sleep(2)\n",
        "    # retorna texto cru (pode conter erro do modelo)\n",
        "    return r.text\n",
        "\n",
        "def try_json_extract(text: str) -> Any:\n",
        "    \"\"\"\n",
        "    Extrai o primeiro JSON válido de uma string. Robustifica contra respostas com texto extra.\n",
        "    \"\"\"\n",
        "    start = text.find(\"{\")\n",
        "    end   = text.rfind(\"}\")\n",
        "    if start == -1 or end == -1 or end <= start:\n",
        "        return None\n",
        "    snippet = text[start:end+1]\n",
        "    try:\n",
        "        return json.loads(snippet)\n",
        "    except Exception:\n",
        "        # tentativa: aspas simples -> duplas\n",
        "        snippet2 = snippet.replace(\"'\", '\"')\n",
        "        try:\n",
        "            return json.loads(snippet2)\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "# ---------- Utilitários de I/O ----------\n",
        "def ler_metadados() -> List[Dict[str,Any]]:\n",
        "    if not os.path.exists(META_PATH):\n",
        "        print(f\"❌ Não encontrei {META_PATH}. Rode as camadas anteriores.\")\n",
        "        return []\n",
        "    with open(META_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def encontrar_transcricao(video_id: str) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Procura transcript/srt da sua Layer 5:\n",
        "      - PASTA_TRABALHO/ai_insights/<video_id>/transcript.txt\n",
        "      - PASTA_TRABALHO/ai_insights/<video_id>/subtitles.srt\n",
        "    Retorna dict com 'plain' e 'srt' (quando houver).\n",
        "    \"\"\"\n",
        "    base = os.path.join(PASTA_TRABALHO, \"ai_insights\", video_id)\n",
        "    out = {\"plain\": \"\", \"srt\": \"\"}\n",
        "    if os.path.isdir(base):\n",
        "        pt = os.path.join(base, \"transcript.txt\")\n",
        "        ps = os.path.join(base, \"subtitles.srt\")\n",
        "        if os.path.exists(pt):\n",
        "            out[\"plain\"] = Path(pt).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "        if os.path.exists(ps):\n",
        "            out[\"srt\"] = Path(ps).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "    return out\n",
        "\n",
        "def srt_para_blocos(srt_text: str, janela_seg=20, max_blocos=6) -> List[Dict[str,Any]]:\n",
        "    \"\"\"\n",
        "    Junta legendas em blocos de ~janela_seg segundos (até max_blocos) para análise por cena.\n",
        "    \"\"\"\n",
        "    if not srt_text.strip():\n",
        "        return []\n",
        "    # parse simples\n",
        "    entries = []\n",
        "    for chunk in re.split(r\"\\n\\s*\\n\", srt_text.strip()):\n",
        "        lines = [l.strip() for l in chunk.splitlines() if l.strip()]\n",
        "        if len(lines) >= 2:\n",
        "            ts = lines[1]\n",
        "            m = re.match(r\"(\\d{2}):(\\d{2}):(\\d{2}),\\d+\\s*-->\\s*(\\d{2}):(\\d{2}):(\\d{2}),\\d+\", ts)\n",
        "            if not m:\n",
        "                continue\n",
        "            h1,m1,s1, h2,m2,s2 = map(int, m.groups())\n",
        "            start = h1*3600+m1*60+s1\n",
        "            end   = h2*3600+m2*60+s2\n",
        "            text  = \" \".join(lines[2:])\n",
        "            entries.append((start, end, text))\n",
        "    # agrega em janelas\n",
        "    if not entries:\n",
        "        return []\n",
        "    t0 = entries[0][0]\n",
        "    blocos = []\n",
        "    cur_t0 = t0\n",
        "    cur_txt = []\n",
        "    for st, en, txt in entries:\n",
        "        if (en - cur_t0) <= janela_seg:\n",
        "            cur_txt.append(txt)\n",
        "        else:\n",
        "            blocos.append({\"inicio_seg\": cur_t0, \"fim_seg\": en, \"texto\": \" \".join(cur_txt)})\n",
        "            cur_t0 = en\n",
        "            cur_txt = [txt]\n",
        "    if cur_txt:\n",
        "        blocos.append({\"inicio_seg\": cur_t0, \"fim_seg\": entries[-1][1], \"texto\": \" \".join(cur_txt)})\n",
        "    return blocos[:max_blocos]\n",
        "\n",
        "# ---------- Prompts ----------\n",
        "PROMPT_MACRO = \"\"\"Você é um analista sênior de roteiro e comunicação em pt-BR.\n",
        "Analise a TRANSCRIÇÃO a seguir e produza apenas um JSON com os campos:\n",
        "\n",
        "{\n",
        "  \"tema_central\": \"\",\n",
        "  \"tese\": \"\",\n",
        "  \"promessa\": \"\",\n",
        "  \"publico_alvo\": \"\",\n",
        "  \"dor_principal\": \"\",\n",
        "  \"ganho_principal\": \"\",\n",
        "  \"mecanismo_unico\": \"\",\n",
        "  \"provas_apoio\": [\"\", \"\"],\n",
        "  \"tom_de_voz\": [\"\", \"\"],\n",
        "  \"frameworks_copy\": [\"AIDA\",\"PAS\",\"FAB\",\"Story\",\"Lista\",\"How-To\"],\n",
        "  \"estrutura_geral\": [\n",
        "    {\"bloco\": 1, \"objetivo\": \"\", \"ideias_chave\": [\"\",\"\",\"\"], \"frases_de_efeito\": [\"\",\"\"]}\n",
        "  ],\n",
        "  \"objeções_previstas\": [\"\",\"\",\"\"],\n",
        "  \"oportunidades_melhoria\": [\"\",\"\",\"\"],\n",
        "  \"analogias_recomendadas\": [\"\",\"\",\"\"],\n",
        "  \"cta_detectadas\": [\"\",\"\",\"\"]\n",
        "}\n",
        "\n",
        "TRANSCRIÇÃO:\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_SUGESTOES = \"\"\"Você é um roteirista sênior para vídeos curtos em pt-BR.\n",
        "Usando a TRANSCRIÇÃO (e os insights macro abaixo), gere apenas um JSON:\n",
        "\n",
        "INSIGHTS_MACRO:\n",
        "{macro}\n",
        "\n",
        "TRANSCRIÇÃO:\n",
        "{transc}\n",
        "\n",
        "JSON com:\n",
        "{\n",
        "  \"hooks_reativos\": [\"5 variações objetivas, curtas, com números ou pergunta\"],\n",
        "  \"texto_na_tela_3s\": [\"3 frases de 5–7 palavras para aparecer em 3s\"],\n",
        "  \"roteiro_15s\": [\"linha-a-linha do que dizer/fazer\", \"...\"],\n",
        "  \"roteiro_30s\": [\"linha-a-linha do que dizer/fazer\", \"...\"],\n",
        "  \"analogias\": [\"3 ideias de analogias concretas\"],\n",
        "  \"oportunidades\": [\"3 oportunidades específicas de melhoria do roteiro\"]\n",
        "}\n",
        "Responda só com JSON.\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_CENA = \"\"\"Você é um editor-chefe. Para o trecho abaixo, devolva JSON:\n",
        "\n",
        "TRECHO:\n",
        "{trecho}\n",
        "\n",
        "JSON:\n",
        "{\n",
        "  \"objetivo_do_trecho\": \"\",\n",
        "  \"ponto_principal\": \"\",\n",
        "  \"melhorias_de_copy\": [\"\",\"\",\"\"],\n",
        "  \"texto_na_tela_sugerido\": [\"\",\"\"],\n",
        "  \"cta_sugerida\": \"\"\n",
        "}\n",
        "Responda só JSON.\n",
        "\"\"\"\n",
        "\n",
        "# ---------- Execução por vídeo ----------\n",
        "def analisar_video_online(video_id: str, nome_arquivo: str) -> Dict[str,Any]:\n",
        "    out_dir = os.path.join(AI_ONLINE_DIR, video_id)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # Carrega transcrição\n",
        "    tr = encontrar_transcricao(video_id)\n",
        "    texto = tr[\"plain\"] or \"\"\n",
        "    srt  = tr[\"srt\"] or \"\"\n",
        "    if not (texto or srt):\n",
        "        print(f\"⚠️ {video_id}: sem transcript/srt. Pulei IA online.\")\n",
        "        return {}\n",
        "\n",
        "    # 1) Macro\n",
        "    macro_prompt = PROMPT_MACRO + (texto[:6000] if texto else srt[:6000])\n",
        "    macro_raw = hf_generate(macro_prompt, max_new_tokens=700, temperature=0.25) if HF_TOKEN else \"{}\"\n",
        "    macro = try_json_extract(macro_raw) or {}\n",
        "\n",
        "    # 2) Sugestões (usa texto e macro)\n",
        "    sug_prompt = PROMPT_SUGESTOES.format(macro=json.dumps(macro, ensure_ascii=False), transc=(texto[:4000] if texto else srt[:4000]))\n",
        "    sug_raw = hf_generate(sug_prompt, max_new_tokens=700, temperature=0.4) if HF_TOKEN else \"{}\"\n",
        "    sugestoes = try_json_extract(sug_raw) or {}\n",
        "\n",
        "    # 3) Cenas (usa SRT em blocos)\n",
        "    cenas = []\n",
        "    blocos = srt_para_blocos(srt, janela_seg=20, max_blocos=6)\n",
        "    for b in blocos:\n",
        "        p = PROMPT_CENA.format(trecho=b[\"texto\"][:1200])\n",
        "        raw = hf_generate(p, max_new_tokens=350, temperature=0.35) if HF_TOKEN else \"{}\"\n",
        "        j = try_json_extract(raw) or {}\n",
        "        cenas.append({\n",
        "            \"inicio_seg\": b[\"inicio_seg\"], \"fim_seg\": b[\"fim_seg\"], **j\n",
        "        })\n",
        "\n",
        "    # Salva JSON e MD por vídeo\n",
        "    pack = {\n",
        "        \"video_id\": video_id,\n",
        "        \"nome_arquivo\": nome_arquivo,\n",
        "        \"macro\": macro,\n",
        "        \"sugestoes\": sugestoes,\n",
        "        \"cenas\": cenas\n",
        "    }\n",
        "    Path(os.path.join(out_dir, \"online_llm_report.json\")).write_text(json.dumps(pack, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "    md = [\n",
        "        f\"# IA Online — {video_id}\",\n",
        "        \"## Macro\",\n",
        "        json.dumps(macro, ensure_ascii=False, indent=2),\n",
        "        \"## Sugestões\",\n",
        "        json.dumps(sugestoes, ensure_ascii=False, indent=2),\n",
        "        \"## Cenas\",\n",
        "        json.dumps(cenas, ensure_ascii=False, indent=2),\n",
        "    ]\n",
        "    Path(os.path.join(out_dir, \"online_llm_report.md\")).write_text(\"\\n\\n\".join(md), encoding=\"utf-8\")\n",
        "\n",
        "    return pack\n",
        "\n",
        "# ---------- Atualiza Excel ----------\n",
        "def _xl_set_width(ws, widths):\n",
        "    for i,w in enumerate(widths,1):\n",
        "        ws.column_dimensions[get_column_letter(i)].width = w\n",
        "\n",
        "def atualizar_excel_online(pacotes: List[Dict[str,Any]]):\n",
        "    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
        "    wb = load_workbook(OUTPUT_PATH) if os.path.exists(OUTPUT_PATH) else Workbook()\n",
        "\n",
        "    # --- Aba 1: IA Online — Macro ---\n",
        "    if \"IA Online — Macro\" in wb.sheetnames: del wb[\"IA Online — Macro\"]\n",
        "    ws1 = wb.create_sheet(\"IA Online — Macro\")\n",
        "    header1 = [\"Vídeo\",\"Tema\",\"Tese\",\"Promessa\",\"Público\",\"Dor\",\"Ganho\",\"Mecanismo único\",\"Provas\",\"Tom\",\"Frameworks\",\"Objeções\",\"Oportunidades\",\"Analogias\",\"CTAs\",\"Estrutura (blocos)\"]\n",
        "    for c,h in enumerate(header1,1):\n",
        "        cell = ws1.cell(row=1, column=c, value=h); cell.font=Font(bold=True)\n",
        "    row=2\n",
        "    for p in pacotes:\n",
        "        m = p.get(\"macro\", {})\n",
        "        ws1.cell(row=row, column=1, value=p.get(\"nome_arquivo\", p.get(\"video_id\")))\n",
        "        ws1.cell(row=row, column=2, value=m.get(\"tema_central\"))\n",
        "        ws1.cell(row=row, column=3, value=m.get(\"tese\"))\n",
        "        ws1.cell(row=row, column=4, value=m.get(\"promessa\"))\n",
        "        ws1.cell(row=row, column=5, value=m.get(\"publico_alvo\"))\n",
        "        ws1.cell(row=row, column=6, value=m.get(\"dor_principal\"))\n",
        "        ws1.cell(row=row, column=7, value=m.get(\"ganho_principal\"))\n",
        "        ws1.cell(row=row, column=8, value=m.get(\"mecanismo_unico\"))\n",
        "        ws1.cell(row=row, column=9, value=\", \".join(m.get(\"provas_apoio\",[]) or []))\n",
        "        ws1.cell(row=row, column=10, value=\", \".join(m.get(\"tom_de_voz\",[]) or []))\n",
        "        ws1.cell(row=row, column=11, value=\", \".join(m.get(\"frameworks_copy\",[]) or []))\n",
        "        ws1.cell(row=row, column=12, value=\", \".join(m.get(\"objeções_previstas\",[]) or []))\n",
        "        ws1.cell(row=row, column=13, value=\", \".join(m.get(\"oportunidades_melhoria\",[]) or []))\n",
        "        ws1.cell(row=row, column=14, value=\", \".join(m.get(\"analogias_recomendadas\",[]) or []))\n",
        "        ws1.cell(row=row, column=15, value=\", \".join(m.get(\"cta_detectadas\",[]) or []))\n",
        "        # estrutura compactada\n",
        "        estrutura = m.get(\"estrutura_geral\", [])\n",
        "        ws1.cell(row=row, column=16, value=json.dumps(estrutura, ensure_ascii=False))\n",
        "        row += 1\n",
        "    _xl_set_width(ws1, [30,18,18,20,18,18,18,20,24,16,16,18,22,18,18,48])\n",
        "\n",
        "    # --- Aba 2: IA Online — Sugestões ---\n",
        "    if \"IA Online — Sugestões\" in wb.sheetnames: del wb[\"IA Online — Sugestões\"]\n",
        "    ws2 = wb.create_sheet(\"IA Online — Sugestões\")\n",
        "    header2 = [\"Vídeo\",\"Hooks (5)\",\"Texto na tela 3s (3)\",\"Roteiro 15s\",\"Roteiro 30s\",\"Analogias\",\"Oportunidades\"]\n",
        "    for c,h in enumerate(header2,1):\n",
        "        ws2.cell(row=1, column=c, value=h).font=Font(bold=True)\n",
        "    row=2\n",
        "    for p in pacotes:\n",
        "        s = p.get(\"sugestoes\", {})\n",
        "        ws2.cell(row=row, column=1, value=p.get(\"nome_arquivo\", p.get(\"video_id\")))\n",
        "        ws2.cell(row=row, column=2, value=\" • \" + \"\\n • \".join(s.get(\"hooks_reativos\",[]) or []))\n",
        "        ws2.cell(row=row, column=3, value=\" • \" + \"\\n • \".join(s.get(\"texto_na_tela_3s\",[]) or []))\n",
        "        ws2.cell(row=row, column=4, value=\" • \" + \"\\n • \".join(s.get(\"roteiro_15s\",[]) or []))\n",
        "        ws2.cell(row=row, column=5, value=\" • \" + \"\\n • \".join(s.get(\"roteiro_30s\",[]) or []))\n",
        "        ws2.cell(row=row, column=6, value=\" • \" + \"\\n • \".join(s.get(\"analogias\",[]) or []))\n",
        "        ws2.cell(row=row, column=7, value=\" • \" + \"\\n • \".join(s.get(\"oportunidades\",[]) or []))\n",
        "        row+=1\n",
        "    _xl_set_width(ws2, [30,54,40,60,60,40,40])\n",
        "\n",
        "    # --- Aba 3: IA Online — Cenas ---\n",
        "    if \"IA Online — Cenas\" in wb.sheetnames: del wb[\"IA Online — Cenas\"]\n",
        "    ws3 = wb.create_sheet(\"IA Online — Cenas\")\n",
        "    header3 = [\"Vídeo\",\"Início (s)\",\"Fim (s)\",\"Objetivo do trecho\",\"Ponto principal\",\"Melhorias de copy\",\"Texto na tela sugerido\",\"CTA sugerida\"]\n",
        "    for c,h in enumerate(header3,1):\n",
        "        ws3.cell(row=1, column=c, value=h).font=Font(bold=True)\n",
        "    row=2\n",
        "    for p in pacotes:\n",
        "        for c in (p.get(\"cenas\") or []):\n",
        "            ws3.cell(row=row, column=1, value=p.get(\"nome_arquivo\", p.get(\"video_id\")))\n",
        "            ws3.cell(row=row, column=2, value=c.get(\"inicio_seg\"))\n",
        "            ws3.cell(row=row, column=3, value=c.get(\"fim_seg\"))\n",
        "            ws3.cell(row=row, column=4, value=c.get(\"objetivo_do_trecho\"))\n",
        "            ws3.cell(row=row, column=5, value=c.get(\"ponto_principal\"))\n",
        "            ws3.cell(row=row, column=6, value=\"; \".join(c.get(\"melhorias_de_copy\",[]) or []))\n",
        "            ws3.cell(row=row, column=7, value=\"; \".join(c.get(\"texto_na_tela_sugerido\",[]) or []))\n",
        "            ws3.cell(row=row, column=8, value=c.get(\"cta_sugerida\"))\n",
        "            row+=1\n",
        "    _xl_set_width(ws3, [30,10,10,40,40,50,40,24])\n",
        "\n",
        "    wb.save(OUTPUT_PATH)\n",
        "    print(f\"✅ Excel atualizado com abas: IA Online — Macro / Sugestões / Cenas → {OUTPUT_PATH}\")\n",
        "\n",
        "# ---------- Orquestração ----------\n",
        "def rodar_layer_7_online_free():\n",
        "    metas = ler_metadados()\n",
        "    if not metas:\n",
        "        return\n",
        "    pacotes = []\n",
        "    for i, m in enumerate(metas, 1):\n",
        "        vid = m.get(\"id\"); nome = m.get(\"nome_arquivo\", vid)\n",
        "        if not vid:\n",
        "            continue\n",
        "        print(f\"[{i}/{len(metas)}] IA Online (FREE) → {vid}\")\n",
        "        try:\n",
        "            p = analisar_video_online(vid, nome)\n",
        "            if p: pacotes.append(p)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Falha em {vid}: {e}\")\n",
        "    if pacotes:\n",
        "        Path(os.path.join(DADOS_DIR, \"ai_online_insights.json\")).write_text(json.dumps(pacotes, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "        atualizar_excel_online(pacotes)\n",
        "    else:\n",
        "        print(\"Nada processado (sem transcrições ou sem token HF).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlLvaS5wFn5d"
      },
      "outputs": [],
      "source": [
        "rodar_layer_7_online_free()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
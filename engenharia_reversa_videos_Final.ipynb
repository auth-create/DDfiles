{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/auth-create/DDfiles/blob/main/engenharia_reversa_videos_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx8sEBm8_yKO"
      },
      "source": [
        "# SISTEMA MODULAR DE ENGENHARIA REVERSA DE VÃDEOS - VERSÃƒO FINAL OTIMIZADA\n",
        "\n",
        "Este notebook foi aprimorado para oferecer uma experiÃªncia mais intuitiva, organizada e robusta para a engenharia reversa de vÃ­deos. Cada etapa Ã© modular, com validaÃ§Ãµes de prÃ©-requisitos e feedback em tempo real para guiÃ¡-lo(a) durante o processo.\n",
        "\n",
        "## COMO USAR:\n",
        "1.  **Execute as cÃ©lulas em ordem, de cima para baixo.** Cada cÃ©lula foi projetada para ser executada sequencialmente.\n",
        "2.  **AtenÃ§Ã£o aos feedbacks:** Mensagens claras indicarÃ£o o sucesso de cada etapa, possÃ­veis erros e qual a **PRÃ“XIMA CÃ‰LULA** a ser executada.\n",
        "3.  **Corrija e re-execute:** Se um erro for detectado, uma mensagem explicativa serÃ¡ exibida. Corrija o problema (geralmente um caminho incorreto ou dependÃªncia ausente) e re-execute a cÃ©lula que falhou.\n",
        "4.  **Progresso Salvo:** O sistema salva automaticamente o progresso e os dados gerados em cada etapa, permitindo que vocÃª retome de onde parou.\n",
        "\n",
        "## ESTRUTURA DO PROCESSO (Layers e Sublayers):\n",
        "Este sistema Ã© organizado em camadas lÃ³gicas para facilitar o entendimento e a execuÃ§Ã£o:\n",
        "\n",
        "### LAYER 1: CONFIGURAÃ‡ÃƒO E PREPARAÃ‡ÃƒO\n",
        "*   **CÃ‰LULA 1.1: SETUP INICIAL E INSTALAÃ‡ÃƒO DE DEPENDÃŠNCIAS**\n",
        "*   **CÃ‰LULA 1.2: CONFIGURAÃ‡ÃƒO INICIAL E VALIDAÃ‡ÃƒO DA PASTA DE TRABALHO**\n",
        "\n",
        "### LAYER 2: DESCOBERTA E EXTRAÃ‡ÃƒO DE DADOS BRUTOS\n",
        "*   **CÃ‰LULA 2.1: DESCOBERTA E CATALOGAÃ‡ÃƒO DE VÃDEOS**\n",
        "*   **CÃ‰LULA 2.2: EXTRAÃ‡ÃƒO DE METADADOS DOS VÃDEOS**\n",
        "*   **CÃ‰LULA 2.3: DECOMPOSIÃ‡ÃƒO DE VÃDEOS (FRAMES, ÃUDIO, TEXTO)**\n",
        "\n",
        "### LAYER 3: ANÃLISE E PROCESSAMENTO DE DADOS\n",
        "*   **CÃ‰LULA 3.1: ANÃLISE DE PADRÃ•ES (TEMPORAIS, VISUAIS, TEXTO, ÃUDIO)**\n",
        "*   **CÃ‰LULA 3.2: ANÃLISE PSICOLÃ“GICA E GATILHOS DE ENGAJAMENTO**\n",
        "\n",
        "### LAYER 4: GERAÃ‡ÃƒO DE RELATÃ“RIOS E BLUEPRINT ESTRATÃ‰GICO\n",
        "*   **CÃ‰LULA 4.1: GERAÃ‡ÃƒO DE RELATÃ“RIOS HUMANIZADOS (ÃUDIO, VISUAL, TEXTO, PSICOLÃ“GICO)**\n",
        "*   **CÃ‰LULA 4.2: GERAÃ‡ÃƒO DO BLUEPRINT FINAL E DASHBOARD**\n",
        "\n",
        "---\n",
        "\n",
        "*Lembre-se: Este sistema foi projetado para ser executado no Google Colab. Certifique-se de que seu ambiente estÃ¡ configurado corretamente.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "setup_inicial",
        "outputId": "a5d9574f-c7b8-4d26-fdb9-61fe000ad671"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "âœ… Google Drive montado com sucesso!\n",
            "âœ… SETUP INICIAL CONCLUÃDO!\n",
            "Todas as dependÃªncias foram instaladas e o Google Drive foi montado.\n",
            "âž¡ï¸ PRÃ“XIMA CÃ‰LULA: 1.2 - CONFIGURAÃ‡ÃƒO INICIAL E VALIDAÃ‡ÃƒO DA PASTA DE TRABALHO\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# LAYER 1: CONFIGURAÃ‡ÃƒO E PREPARAÃ‡ÃƒO\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# CÃ‰LULA 1.1: SETUP INICIAL E INSTALAÃ‡ÃƒO DE DEPENDÃŠNCIAS\n",
        "# ============================================================================\n",
        "\n",
        "# Instalar dependÃªncias necessÃ¡rias\n",
        "!pip install -q moviepy librosa pytesseract opencv-python pandas openpyxl matplotlib seaborn pillow SpeechRecognition pydub fpdf\n",
        "!apt-get update -qq && apt-get install -y -qq tesseract-ocr tesseract-ocr-por ffmpeg\n",
        "\n",
        "# Imports necessÃ¡rios\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import logging\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pytesseract\n",
        "import librosa\n",
        "from moviepy.editor import VideoFileClip\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import speech_recognition as sr # Adicionado import para SpeechRecognition\n",
        "# Montar Google Drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"âœ… Google Drive montado com sucesso!\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ ERRO ao montar Google Drive: {e}. Por favor, verifique sua conexÃ£o ou permissÃµes.\")\n",
        "\n",
        "print(\n",
        "\"âœ… SETUP INICIAL CONCLUÃDO!\")\n",
        "print(\"Todas as dependÃªncias foram instaladas e o Google Drive foi montado.\")\n",
        "print(\"âž¡ï¸ PRÃ“XIMA CÃ‰LULA: 1.2 - CONFIGURAÃ‡ÃƒO INICIAL E VALIDAÃ‡ÃƒO DA PASTA DE TRABALHO\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "configuracao_inicial",
        "outputId": "c5384a91-843f-4869-c3ae-bdc2a6804ac5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… CONFIGURAÃ‡ÃƒO CONCLUÃDA!\n",
            "Pasta de trabalho criada: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa\n",
            "ConfiguraÃ§Ã£o salva: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/config/config.json\n",
            "\n",
            "âž¡ï¸ PRÃ“XIMA CÃ‰LULA: 2.1 - DESCOBERTA E CATALOGAÃ‡ÃƒO DE VÃDEOS\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CÃ‰LULA 1.2: CONFIGURAÃ‡ÃƒO INICIAL E VALIDAÃ‡ÃƒO DA PASTA DE TRABALHO\n",
        "# ============================================================================\n",
        "\n",
        "# âš ï¸ **ATENÃ‡ÃƒO:** CONFIGURE SEU CAMINHO AQUI!\n",
        "# Substitua o caminho abaixo pela pasta onde seus vÃ­deos estÃ£o localizados no Google Drive.\n",
        "# Exemplo: \"/content/drive/MyDrive/Meus Videos de Marketing\"\n",
        "CAMINHO_PASTA_VIDEOS = \"/content/drive/MyDrive/Videos Dona Done\" # â¬…ï¸ **ALTERE AQUI**\n",
        "\n",
        "class ConfiguradorProjeto:\n",
        "    def __init__(self, caminho_pasta):\n",
        "        self.pasta_videos = self._validar_caminho(caminho_pasta)\n",
        "        self.pasta_trabalho = os.path.join(self.pasta_videos, \"_engenharia_reversa\")\n",
        "        self._criar_estrutura()\n",
        "        self._configurar_logging()\n",
        "\n",
        "    def _validar_caminho(self, caminho):\n",
        "        if caminho == \"/content/drive/MyDrive/Videos Dona Done\" and not os.path.exists(caminho):\n",
        "            raise ValueError(\"âŒ ERRO: VocÃª precisa alterar CAMINHO_PASTA_VIDEOS com o caminho real da sua pasta de vÃ­deos no Google Drive. O caminho padrÃ£o nÃ£o foi encontrado.\")\n",
        "\n",
        "        if not os.path.exists(caminho):\n",
        "            raise ValueError(f\"âŒ ERRO: Pasta nÃ£o encontrada: {caminho}. Por favor, verifique se o caminho estÃ¡ correto e se o Google Drive estÃ¡ montado.\")\n",
        "\n",
        "        return caminho\n",
        "\n",
        "    def _criar_estrutura(self):\n",
        "        # Estrutura de pastas conforme o anexo e requisitos do usuÃ¡rio\n",
        "        estrutura = [\n",
        "            \"config\", \"logs\", \"dados\", \"frames_extraidos\",\n",
        "            \"analise_texto\", \"analise_audio\", \"capturas\",\n",
        "            \"blueprint\", \"temp\", \"dashboard\", \"analise_psicologica\", \"analise_visual\"\n",
        "        ]\n",
        "\n",
        "        os.makedirs(self.pasta_trabalho, exist_ok=True)\n",
        "        for pasta in estrutura:\n",
        "            os.makedirs(os.path.join(self.pasta_trabalho, pasta), exist_ok=True)\n",
        "\n",
        "        # Criar subpastas para frames_extraidos (ex: vid_001_Nome_Do_Video/)\n",
        "        # Esta lÃ³gica serÃ¡ implementada na cÃ©lula de decomposiÃ§Ã£o de vÃ­deos (CÃ‰LULA 2.3)\n",
        "\n",
        "    def _configurar_logging(self):\n",
        "        log_file = os.path.join(self.pasta_trabalho, \"logs\", f\"sistema_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "            handlers=[logging.FileHandler(log_file, encoding='utf-8')]\n",
        "        )\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def salvar_configuracao(self):\n",
        "        config = {\n",
        "            \"projeto\": {\n",
        "                \"pasta_videos\": self.pasta_videos,\n",
        "                \"pasta_trabalho\": self.pasta_trabalho,\n",
        "                \"criado_em\": datetime.now().isoformat(),\n",
        "                \"versao\": \"modular_v2.0_otimizado\"\n",
        "            },\n",
        "            \"status_etapas\": {\n",
        "                \"configuracao\": True,\n",
        "                \"descoberta_videos\": False,\n",
        "                \"metadados\": False,\n",
        "                \"decomposicao\": False,\n",
        "                \"analise_padroes\": False,\n",
        "                \"analise_psicologica\": False,\n",
        "                \"relatorios_humanizados\": False,\n",
        "                \"blueprint\": False\n",
        "            }\n",
        "        }\n",
        "\n",
        "        config_path = os.path.join(self.pasta_trabalho, \"config\", \"config.json\")\n",
        "        with open(config_path, \"w\", encoding='utf-8') as f:\n",
        "            json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        return config_path\n",
        "\n",
        "# Executar configuraÃ§Ã£o\n",
        "try:\n",
        "    configurador = ConfiguradorProjeto(CAMINHO_PASTA_VIDEOS)\n",
        "    config_path = configurador.salvar_configuracao()\n",
        "\n",
        "    print(\"\"\"\n",
        "âœ… CONFIGURAÃ‡ÃƒO CONCLUÃDA!\"\"\")\n",
        "    print(f\"Pasta de trabalho criada: {configurador.pasta_trabalho}\")\n",
        "    print(f\"ConfiguraÃ§Ã£o salva: {config_path}\")\n",
        "    print(\"\"\"\n",
        "âž¡ï¸ PRÃ“XIMA CÃ‰LULA: 2.1 - DESCOBERTA E CATALOGAÃ‡ÃƒO DE VÃDEOS\"\"\")\n",
        "\n",
        "    # Salvar variÃ¡veis globais para prÃ³ximas cÃ©lulas\n",
        "    global PASTA_VIDEOS, PASTA_TRABALHO\n",
        "    PASTA_VIDEOS = configurador.pasta_videos\n",
        "    PASTA_TRABALHO = configurador.pasta_trabalho\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\"\"\n",
        "âŒ ERRO NA CONFIGURAÃ‡ÃƒO: {e}\"\"\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "descoberta_videos",
        "outputId": "ad4a8371-2b70-4368-e563-2055ecf11d45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ” Iniciando descoberta de vÃ­deos na pasta: /content/drive/MyDrive/Videos Dona Done\n",
            "  âœ… Encontrado: bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.mp4\n",
            "  âœ… Encontrado: empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.mp4\n",
            "  âœ… Encontrado: maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy.mp4\n",
            "\n",
            "âœ… DESCOBERTA DE VÃDEOS CONCLUÃDA!\n",
            "Total de vÃ­deos encontrados: 3\n",
            "Lista de vÃ­deos salva em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/videos_descobertos.json\n",
            "Formatos encontrados: {'.mp4': 3}\n",
            "\n",
            "âž¡ï¸ PRÃ“XIMA CÃ‰LULA: 2.2 - EXTRAÃ‡ÃƒO DE METADADOS DOS VÃDEOS\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# LAYER 2: DESCOBERTA E EXTRAÃ‡ÃƒO DE DADOS BRUTOS\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# CÃ‰LULA 2.1: DESCOBERTA E CATALOGAÃ‡ÃƒO DE VÃDEOS\n",
        "# ============================================================================\n",
        "\n",
        "def verificar_prerequisito_etapa(etapa_anterior):\n",
        "    \"\"\"Verifica se a etapa anterior foi executada com sucesso\"\"\"\n",
        "    try:\n",
        "        if not \"PASTA_TRABALHO\" in globals():\n",
        "            raise Exception(\"VariÃ¡veis globais de configuraÃ§Ã£o nÃ£o encontradas. Execute a CÃ‰LULA 1.2 primeiro.\")\n",
        "\n",
        "        config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "        if not os.path.exists(config_path):\n",
        "            raise Exception(\"Arquivo de configuraÃ§Ã£o nÃ£o encontrado. Execute a CÃ‰LULA 1.2 primeiro.\")\n",
        "\n",
        "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            config = json.load(f)\n",
        "\n",
        "        if not config[\"status_etapas\"][etapa_anterior]:\n",
        "            raise Exception(f\"A etapa \\\"{etapa_anterior}\\\" nÃ£o foi concluÃ­da. Execute a cÃ©lula correspondente primeiro.\")\n",
        "\n",
        "        return True, config\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ PRÃ‰-REQUISITO NÃƒO ATENDIDO: {e}\")\n",
        "        return False, None\n",
        "\n",
        "def descobrir_catalogar_videos():\n",
        "    \"\"\"Descobre e cataloga todos os vÃ­deos na pasta\"\"\"\n",
        "    formatos_aceitos = [\".mp4\", \".mov\", \".avi\", \".mkv\", \".webm\", \".m4v\"]\n",
        "    videos_encontrados = []\n",
        "\n",
        "    print(f\"ðŸ” Iniciando descoberta de vÃ­deos na pasta: {PASTA_VIDEOS}\")\n",
        "\n",
        "    for root, dirs, files in os.walk(PASTA_VIDEOS):\n",
        "        if \"_engenharia_reversa\" in root:\n",
        "            continue # Ignorar a pasta de trabalho do sistema\n",
        "\n",
        "        for file in files:\n",
        "            if any(file.lower().endswith(fmt) for fmt in formatos_aceitos):\n",
        "                video_path = os.path.join(root, file)\n",
        "\n",
        "                try:\n",
        "                    stat_info = os.stat(video_path)\n",
        "                    # Gerar ID baseado no nome do arquivo para melhor rastreamento\n",
        "                    video_name_clean = os.path.splitext(file)[0].replace(\" \", \"_\").replace(\".\", \"\")\n",
        "                    video_id = f\"vid_{video_name_clean}\"\n",
        "\n",
        "                    video_info = {\n",
        "                        \"id\": video_id,\n",
        "                        \"nome_arquivo\": file,\n",
        "                        \"caminho_completo\": video_path,\n",
        "                        \"caminho_relativo\": os.path.relpath(video_path, PASTA_VIDEOS),\n",
        "                        \"tamanho_mb\": round(stat_info.st_size / (1024*1024), 2),\n",
        "                        \"data_modificacao\": datetime.fromtimestamp(stat_info.st_mtime).isoformat(),\n",
        "                        \"extensao\": os.path.splitext(file)[1].lower(),\n",
        "                        \"status\": \"descoberto\"\n",
        "                    }\n",
        "\n",
        "                    videos_encontrados.append(video_info)\n",
        "                    print(f\"  âœ… Encontrado: {file}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"  âŒ Erro ao processar {file}: {e}\")\n",
        "                    continue\n",
        "\n",
        "    return videos_encontrados\n",
        "\n",
        "def salvar_lista_videos(videos):\n",
        "    \"\"\"Salva lista de vÃ­deos encontrados\"\"\"\n",
        "    videos_path = os.path.join(PASTA_TRABALHO, \"dados\", \"videos_descobertos.json\")\n",
        "    with open(videos_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(videos, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    config[\"status_etapas\"][\"descoberta_videos\"] = True\n",
        "    config[\"total_videos_encontrados\"] = len(videos)\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    return videos_path\n",
        "\n",
        "# Executar descoberta\n",
        "prerequisito_ok, _ = verificar_prerequisito_etapa(\"configuracao\")\n",
        "\n",
        "if prerequisito_ok:\n",
        "    try:\n",
        "        videos_encontrados = descobrir_catalogar_videos()\n",
        "\n",
        "        if not videos_encontrados:\n",
        "            print(\"\"\"\n",
        "âŒ NENHUM VÃDEO ENCONTRADO!\"\"\")\n",
        "            print(f\"Verifique se hÃ¡ vÃ­deos na pasta configurada: {PASTA_VIDEOS}\")\n",
        "        else:\n",
        "            videos_path = salvar_lista_videos(videos_encontrados)\n",
        "\n",
        "            print(\"\"\"\n",
        "âœ… DESCOBERTA DE VÃDEOS CONCLUÃDA!\"\"\")\n",
        "            print(f\"Total de vÃ­deos encontrados: {len(videos_encontrados)}\")\n",
        "            print(f\"Lista de vÃ­deos salva em: {videos_path}\")\n",
        "\n",
        "            # Mostrar resumo\n",
        "            extensoes = Counter([v[\"extensao\"] for v in videos_encontrados])\n",
        "            print(f\"Formatos encontrados: {dict(extensoes)}\")\n",
        "            print(\"\"\"\n",
        "âž¡ï¸ PRÃ“XIMA CÃ‰LULA: 2.2 - EXTRAÃ‡ÃƒO DE METADADOS DOS VÃDEOS\"\"\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\"\"\n",
        "âŒ ERRO NA DESCOBERTA DE VÃDEOS: {e}\"\"\")\n",
        "        print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "extracao_metadados",
        "outputId": "3b3f78bd-9ef4-4d08-ec58-4754c0be1c8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processando metadados de 3 vÃ­deos...\n",
            "[1/3] Analisando bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.mp4\n",
            "  âš™ï¸ Extraindo metadados para: bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.mp4\n",
            "  âœ… Metadados extraÃ­dos: 32.6s | vertical_9_16 | Ãudio: Sim\n",
            "[2/3] Analisando empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.mp4\n",
            "  âš™ï¸ Extraindo metadados para: empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.mp4\n",
            "  âœ… Metadados extraÃ­dos: 64.4s | vertical_9_16 | Ãudio: Sim\n",
            "[3/3] Analisando maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy.mp4\n",
            "  âš™ï¸ Extraindo metadados para: maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy.mp4\n",
            "  âœ… Metadados extraÃ­dos: 48.2s | vertical_9_16 | Ãudio: Sim\n",
            "ðŸ”— Integrando dados de viralizaÃ§Ã£o...\n",
            "\n",
            "ðŸ’¾ Metadados completos salvos em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/metadados_completos.json\n",
            "ðŸ’¾ Metadados em Excel salvos em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/metadados_videos.xlsx\n",
            "\n",
            "âœ… EXTRAÃ‡ÃƒO DE METADADOS CONCLUÃDA!\n",
            "Total de vÃ­deos com metadados extraÃ­dos: 3\n",
            "\n",
            "ðŸ“Š Resumo dos Metadados:\n",
            "  - Formatos detectados: {'vertical_9_16': np.int64(3)}\n",
            "  - DuraÃ§Ã£o mÃ©dia dos vÃ­deos: 48.41s\n",
            "  - VÃ­deos com Ã¡udio: 3\n",
            "\n",
            "âž¡ï¸ PRÃ“XIMA CÃ‰LULA: 2.3 - DECOMPOSIÃ‡ÃƒO DE VÃDEOS (FRAMES, ÃUDIO, TEXTO)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CÃ‰LULA 2.2: EXTRAÃ‡ÃƒO DE METADADOS DOS VÃDEOS\n",
        "# ============================================================================\n",
        "\n",
        "def extrair_metadados_video(video_info):\n",
        "    \"\"\"Extrai metadados tÃ©cnicos de um vÃ­deo\"\"\"\n",
        "    video_path = video_info[\"caminho_completo\"]\n",
        "    video_id = video_info[\"id\"]\n",
        "\n",
        "    print(f\"  âš™ï¸ Extraindo metadados para: {video_info[\"nome_arquivo\"]}\")\n",
        "\n",
        "    # AnÃ¡lise com OpenCV\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise Exception(\"NÃ£o foi possÃ­vel abrir o vÃ­deo. Verifique o caminho ou a integridade do arquivo.\")\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    largura = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    altura = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    duracao = frame_count / fps if fps > 0 else 0\n",
        "\n",
        "    # Capturar primeiro frame\n",
        "    ret, primeiro_frame = cap.read()\n",
        "    cap.release()\n",
        "\n",
        "    # AnÃ¡lise de Ã¡udio\n",
        "    try:\n",
        "        clip = VideoFileClip(video_path)\n",
        "        tem_audio = clip.audio is not None\n",
        "        clip.close()\n",
        "    except Exception as e:\n",
        "        print(f\"    âš ï¸ Aviso: NÃ£o foi possÃ­vel analisar Ã¡udio para {video_info[\"nome_arquivo\"]}: {e}\")\n",
        "        tem_audio = False\n",
        "\n",
        "    # AnÃ¡lise do primeiro frame\n",
        "    analise_frame = {}\n",
        "    if ret:\n",
        "        # Salvar primeiro frame na pasta 'capturas'\n",
        "        capturas_dir = os.path.join(PASTA_TRABALHO, \"capturas\")\n",
        "        frame_path = os.path.join(capturas_dir, f\"{video_id}_primeiro_frame.jpg\")\n",
        "        cv2.imwrite(frame_path, primeiro_frame)\n",
        "\n",
        "        # AnÃ¡lises do frame\n",
        "        gray = cv2.cvtColor(primeiro_frame, cv2.COLOR_BGR2GRAY)\n",
        "        complexidade = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "        brilho = np.mean(gray)\n",
        "\n",
        "        analise_frame = {\n",
        "            \"path\": frame_path,\n",
        "            \"complexidade_visual\": float(complexidade),\n",
        "            \"brilho_medio\": float(brilho),\n",
        "            \"tem_muito_texto\": bool(complexidade > 500),\n",
        "            \"e_escuro\": bool(brilho < 100),\n",
        "            \"e_claro\": bool(brilho > 200)\n",
        "        }\n",
        "\n",
        "    # Detectar formato\n",
        "    ratio = largura / altura if altura > 0 else 0\n",
        "    if 0.5 <= ratio <= 0.6:\n",
        "        formato = \"vertical_9_16\" if altura > largura * 1.5 else \"vertical_4_5\"\n",
        "    elif 0.8 <= ratio <= 1.2:\n",
        "        formato = \"quadrado_1_1\"\n",
        "    elif ratio >= 1.3:\n",
        "        formato = \"horizontal_16_9\"\n",
        "    else:\n",
        "        formato = \"personalizado\"\n",
        "\n",
        "    # Compilar metadados - converter todos os valores para tipos bÃ¡sicos Python\n",
        "    metadados = {\n",
        "        **video_info,\n",
        "        \"duracao_segundos\": float(duracao),\n",
        "        \"fps\": float(fps),\n",
        "        \"largura\": int(largura),\n",
        "        \"altura\": int(altura),\n",
        "        \"resolucao\": f\"{largura}x{altura}\",\n",
        "        \"aspect_ratio\": float(ratio),\n",
        "        \"total_frames\": int(frame_count),\n",
        "        \"tem_audio\": bool(tem_audio),\n",
        "        \"formato_detectado\": str(formato),\n",
        "        \"primeiro_frame\": analise_frame,\n",
        "        \"data_analise\": datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    return metadados\n",
        "\n",
        "def processar_metadados_todos_videos():\n",
        "    \"\"\"Processa metadados de todos os vÃ­deos\"\"\"\n",
        "    # Carregar lista de vÃ­deos\n",
        "    videos_path = os.path.join(PASTA_TRABALHO, \"dados\", \"videos_descobertos.json\")\n",
        "    with open(videos_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        videos_lista = json.load(f)\n",
        "\n",
        "    metadados_completos = []\n",
        "    sucessos = 0\n",
        "\n",
        "    print(f\"Processando metadados de {len(videos_lista)} vÃ­deos...\")\n",
        "\n",
        "    for i, video in enumerate(videos_lista, 1):\n",
        "        print(f\"[{i}/{len(videos_lista)}] Analisando {video[\"nome_arquivo\"]}\")\n",
        "\n",
        "        try:\n",
        "            metadados = extrair_metadados_video(video)\n",
        "            metadados[\"status\"] = \"metadados_extraidos\"\n",
        "            metadados_completos.append(metadados)\n",
        "            sucessos += 1\n",
        "            print(f\"  âœ… Metadados extraÃ­dos: {metadados[\"duracao_segundos\"]:.1f}s | {metadados[\"formato_detectado\"]} | Ãudio: {\"Sim\" if metadados[\"tem_audio\"] else \"NÃ£o\"}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  âŒ ERRO ao extrair metadados para {video[\"nome_arquivo\"]}: {e}\")\n",
        "            video[\"status\"] = \"erro_metadados\"\n",
        "            metadados_completos.append(video) # Adiciona o vÃ­deo com status de erro\n",
        "\n",
        "    # Salvar metadados completos\n",
        "    metadados_json_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "    with open(metadados_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(metadados_completos, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # ADICIONE NO FINAL DA FUNÃ‡ÃƒO processar_extracao_metadados_videos\n",
        "# ANTES DA LINHA: with open(metadados_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "\n",
        "# ======= INÃCIO DA INTEGRAÃ‡ÃƒO DE VIRALIZAÃ‡ÃƒO =======\n",
        "    print(\"ðŸ”— Integrando dados de viralizaÃ§Ã£o...\")\n",
        "\n",
        "    # Assuming carregar_dados_viralizacao and enriquecer_metadados_com_viralizacao are defined elsewhere\n",
        "    # and videos_info is the list of video data being processed (metadados_completos in this case)\n",
        "    # This block seems misplaced from a previous version or intended as an external patch.\n",
        "    # For now, I will comment it out to fix the indentation error and avoid NameErrors.\n",
        "    # If this functionality is needed, it should be properly integrated or called from another cell.\n",
        "\n",
        "    # df_viral, dados_completos = carregar_dados_viralizacao(PASTA_TRABALHO)\n",
        "    #\n",
        "    # if df_viral is not None:\n",
        "    #     # Enriquecer cada vÃ­deo com dados de viralizaÃ§Ã£o\n",
        "    #     for i, video_info in enumerate(videos_info):\n",
        "    #         videos_info[i] = enriquecer_metadados_com_viralizacao(\n",
        "    #             video_info, df_viral, dados_completos\n",
        "    #         )\n",
        "    #\n",
        "    #     # Salvar dados de viralizaÃ§Ã£o na pasta de dados\n",
        "    #     viral_csv_path = os.path.join(PASTA_TRABALHO, \"dados\", \"viral_metrics.csv\")\n",
        "    #     df_viral.to_csv(viral_csv_path, index=False, encoding='utf-8')\n",
        "    #\n",
        "    #     viral_json_path = os.path.join(PASTA_TRABALHO, \"dados\", \"viral_complete_data.json\")\n",
        "    #     with open(viral_json_path, 'w', encoding='utf-8') as f:\n",
        "    #         json.dump(dados_completos, f, indent=2, ensure_ascii=False)\n",
        "    #\n",
        "    #     print(f\"âœ… Dados de viralizaÃ§Ã£o integrados e salvos\")\n",
        "    #     print(f\"   ðŸ“Š CSV: {viral_csv_path}\")\n",
        "    #     print(f\"   ðŸ“Š JSON: {viral_json_path}\")\n",
        "    # else:\n",
        "    #     print(\"âš ï¸ Continuando sem dados de viralizaÃ§Ã£o\")\n",
        "\n",
        "# ======= FIM DA INTEGRAÃ‡ÃƒO DE VIRALIZAÃ‡ÃƒO =======\n",
        "\n",
        "\n",
        "    # Salvar em Excel\n",
        "    df_metadados = pd.DataFrame(metadados_completos)\n",
        "    metadados_excel_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_videos.xlsx\")\n",
        "    df_metadados.to_excel(metadados_excel_path, index=False, engine='openpyxl')\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    config[\"status_etapas\"][\"metadados\"] = True\n",
        "    config[\"total_videos_metadados\"] = sucessos\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\nðŸ’¾ Metadados completos salvos em: {metadados_json_path}\")\n",
        "    print(f\"ðŸ’¾ Metadados em Excel salvos em: {metadados_excel_path}\")\n",
        "\n",
        "    print(\"\\nâœ… EXTRAÃ‡ÃƒO DE METADADOS CONCLUÃDA!\")\n",
        "    print(f\"Total de vÃ­deos com metadados extraÃ­dos: {sucessos}\")\n",
        "\n",
        "    # Mostrar resumo\n",
        "    if not df_metadados.empty:\n",
        "        print(\"\\nðŸ“Š Resumo dos Metadados:\")\n",
        "        print(f\"  - Formatos detectados: {dict(df_metadados['formato_detectado'].value_counts())}\")\n",
        "        print(f\"  - DuraÃ§Ã£o mÃ©dia dos vÃ­deos: {df_metadados['duracao_segundos'].mean():.2f}s\")\n",
        "        print(f\"  - VÃ­deos com Ã¡udio: {df_metadados['tem_audio'].sum()}\")\n",
        "\n",
        "    print(\"\\nâž¡ï¸ PRÃ“XIMA CÃ‰LULA: 2.3 - DECOMPOSIÃ‡ÃƒO DE VÃDEOS (FRAMES, ÃUDIO, TEXTO)\")\n",
        "\n",
        "# Executar extraÃ§Ã£o de metadados\n",
        "prerequisito_ok, _ = verificar_prerequisito_etapa(\"descoberta_videos\")\n",
        "\n",
        "if prerequisito_ok:\n",
        "    try:\n",
        "        processar_metadados_todos_videos()\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ ERRO NA EXTRAÃ‡ÃƒO DE METADADOS: {e}\")\n",
        "        print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "decomposicao_videos",
        "outputId": "2827f4db-e77f-4e5e-d643-cf2875ac672b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando decomposiÃ§Ã£o para 3 vÃ­deos...\n",
            "[1/3] Decompondo bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.mp4\n",
            "  âš™ï¸ Decompondo vÃ­deo: bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.mp4\n",
            "    âœ… 33 frames extraÃ­dos para bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.mp4\n",
            "    âœ… 30 textos encontrados via OCR para bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.mp4\n",
            "    âœ… Ãudio extraÃ­do para bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.mp4\n",
            "    âœ… Ãudio transcrito para bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.mp4\n",
            "    âœ… 638 cortes detectados para bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.mp4\n",
            "  âœ… DecomposiÃ§Ã£o concluÃ­da para bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.mp4\n",
            "[2/3] Decompondo empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.mp4\n",
            "  âš™ï¸ Decompondo vÃ­deo: empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.mp4\n",
            "    âœ… 65 frames extraÃ­dos para empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.mp4\n",
            "    âœ… 56 textos encontrados via OCR para empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.mp4\n",
            "    âœ… Ãudio extraÃ­do para empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.mp4\n",
            "    âœ… Ãudio transcrito para empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.mp4\n",
            "    âœ… 123 cortes detectados para empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.mp4\n",
            "  âœ… DecomposiÃ§Ã£o concluÃ­da para empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.mp4\n",
            "[3/3] Decompondo maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy.mp4\n",
            "  âš™ï¸ Decompondo vÃ­deo: maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy.mp4\n",
            "    âœ… 49 frames extraÃ­dos para maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy.mp4\n",
            "    âœ… 42 textos encontrados via OCR para maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy.mp4\n",
            "    âœ… Ãudio extraÃ­do para maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy.mp4\n",
            "    âœ… Ãudio transcrito para maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy.mp4\n",
            "    âœ… 716 cortes detectados para maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy.mp4\n",
            "  âœ… DecomposiÃ§Ã£o concluÃ­da para maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy.mp4\n",
            "\n",
            "ðŸ’¾ Dados de decomposiÃ§Ã£o salvos em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/decomposicao_completa.json\n",
            "\n",
            "âœ… DECOMPOSIÃ‡ÃƒO DE VÃDEOS CONCLUÃDA!\n",
            "Total de vÃ­deos decompostos com sucesso: 3\n",
            "\n",
            "âž¡ï¸ PRÃ“XIMA CÃ‰LULA: 3.1 - ANÃLISE DE PADRÃ•ES (TEMPORAIS, VISUAIS, TEXTO, ÃUDIO)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CÃ‰LULA 2.3: DECOMPOSIÃ‡ÃƒO DE VÃDEOS (FRAMES, ÃUDIO, TEXTO)\n",
        "# ============================================================================\n",
        "\n",
        "def decompor_video(video_info):\n",
        "    \"\"\"DecompÃµe um vÃ­deo em frames, Ã¡udio e texto (OCR e transcriÃ§Ã£o)\"\"\"\n",
        "    video_path = video_info[\"caminho_completo\"]\n",
        "    video_id = video_info[\"id\"]\n",
        "    pasta_video_frames = os.path.join(PASTA_TRABALHO, \"frames_extraidos\", video_id)\n",
        "    os.makedirs(pasta_video_frames, exist_ok=True)\n",
        "\n",
        "    print(f\"  âš™ï¸ Decompondo vÃ­deo: {video_info[\"nome_arquivo\"]}\")\n",
        "\n",
        "    decomposicao_data = {\n",
        "        \"video_id\": video_id,\n",
        "        \"frames_extraidos\": [],\n",
        "        \"textos_ocr\": [],\n",
        "        \"audio_transcrito\": \"\",\n",
        "        \"audio_analise\": {}\n",
        "    }\n",
        "\n",
        "    # ExtraÃ§Ã£o de Frames e OCR\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        frame_count = 0\n",
        "        frame_interval = int(fps) # 1 frame por segundo\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            if frame_count % frame_interval == 0:\n",
        "                frame_time_sec = frame_count / fps\n",
        "                frame_filename = os.path.join(pasta_video_frames, f\"frame_{int(frame_time_sec):06d}.jpg\")\n",
        "                cv2.imwrite(frame_filename, frame)\n",
        "                decomposicao_data[\"frames_extraidos\"] .append({\n",
        "                    \"path\": frame_filename,\n",
        "                    \"timestamp_sec\": frame_time_sec\n",
        "                })\n",
        "\n",
        "                # OCR\n",
        "                try:\n",
        "                    text = pytesseract.image_to_string(Image.fromarray(frame), lang=\"por\")\n",
        "                    if text.strip():\n",
        "                        decomposicao_data[\"textos_ocr\"] .append({\n",
        "                            \"timestamp_sec\": frame_time_sec,\n",
        "                            \"text\": text.strip()\n",
        "                        })\n",
        "                except Exception as ocr_e:\n",
        "                    print(f\"    âš ï¸ Aviso: Erro no OCR para frame {frame_time_sec}s: {ocr_e}\")\n",
        "\n",
        "            frame_count += 1\n",
        "        cap.release()\n",
        "        print(f\"    âœ… {len(decomposicao_data[\"frames_extraidos\"])} frames extraÃ­dos para {video_info[\"nome_arquivo\"]}\")\n",
        "        print(f\"    âœ… {len(decomposicao_data[\"textos_ocr\"])} textos encontrados via OCR para {video_info[\"nome_arquivo\"]}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    âŒ Erro na extraÃ§Ã£o de frames/OCR para {video_info[\"nome_arquivo\"]}: {e}\")\n",
        "\n",
        "    # ExtraÃ§Ã£o e TranscriÃ§Ã£o de Ãudio\n",
        "    audio_path = os.path.join(PASTA_TRABALHO, \"temp\", f\"{video_id}.wav\")\n",
        "    try:\n",
        "        video_clip = VideoFileClip(video_path)\n",
        "        if video_clip.audio:\n",
        "            video_clip.audio.write_audiofile(audio_path, verbose=False, logger=None)\n",
        "            print(f\"    âœ… Ãudio extraÃ­do para {video_info[\"nome_arquivo\"]}\")\n",
        "\n",
        "            # TranscriÃ§Ã£o\n",
        "            r = sr.Recognizer()\n",
        "            with sr.AudioFile(audio_path) as source:\n",
        "                audio_listened = r.record(source)\n",
        "                try:\n",
        "                    text = r.recognize_google(audio_listened, language=\"pt-BR\")\n",
        "                    decomposicao_data[\"audio_transcrito\"] = text\n",
        "                    print(f\"    âœ… Ãudio transcrito para {video_info[\"nome_arquivo\"]}\")\n",
        "                except sr.UnknownValueError:\n",
        "                    print(f\"    âš ï¸ Aviso: NÃ£o foi possÃ­vel transcrever o Ã¡udio para {video_info[\"nome_arquivo\"]}. Fala ininteligÃ­vel.\")\n",
        "                except sr.RequestError as req_e:\n",
        "                    print(f\"    âš ï¸ Aviso: Erro no serviÃ§o de transcriÃ§Ã£o para {video_info[\"nome_arquivo\"]}: {req_e}\")\n",
        "\n",
        "            # AnÃ¡lise de Ãudio (Librosa)\n",
        "            y, sr_audio = librosa.load(audio_path)\n",
        "            tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr_audio)\n",
        "            decomposicao_data[\"audio_analise\"] = {\n",
        "                \"bpm\": float(tempo),\n",
        "                \"duracao_audio_segundos\": float(librosa.get_duration(y=y, sr=sr_audio))\n",
        "            }\n",
        "\n",
        "        else:\n",
        "            print(f\"    âš ï¸ Aviso: VÃ­deo {video_info[\"nome_arquivo\"]} nÃ£o possui trilha de Ã¡udio.\")\n",
        "        video_clip.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    âŒ Erro na extraÃ§Ã£o/transcriÃ§Ã£o de Ã¡udio para {video_info[\"nome_arquivo\"]}: {e}\")\n",
        "\n",
        "    # DetecÃ§Ã£o de Cortes (Scene Change Detection)\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            raise Exception(\"NÃ£o foi possÃ­vel abrir o vÃ­deo para detecÃ§Ã£o de cortes.\")\n",
        "\n",
        "        prev_frame = None\n",
        "        cuts = []\n",
        "        frame_idx = 0\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            if prev_frame is not None:\n",
        "                diff = cv2.absdiff(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY), cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY))\n",
        "                non_zero_count = np.count_nonzero(diff)\n",
        "                if non_zero_count > (frame.shape[0] * frame.shape[1] * 0.3): # Limiar de 30% de mudanÃ§a\n",
        "                    cuts.append(frame_idx / fps)\n",
        "            prev_frame = frame\n",
        "            frame_idx += 1\n",
        "        cap.release()\n",
        "        decomposicao_data[\"cortes_detectados_segundos\"] = cuts\n",
        "        print(f\"    âœ… {len(cuts)} cortes detectados para {video_info[\"nome_arquivo\"]}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    âŒ Erro na detecÃ§Ã£o de cortes para {video_info[\"nome_arquivo\"]}: {e}\")\n",
        "\n",
        "    return decomposicao_data\n",
        "\n",
        "def processar_decomposicao_todos_videos():\n",
        "    \"\"\"Processa a decomposiÃ§Ã£o de todos os vÃ­deos\"\"\"\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa(\"metadados\")\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Carregar metadados completos\n",
        "    metadados_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "    with open(metadados_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        videos_com_metadados = json.load(f)\n",
        "\n",
        "    decomposicoes_completas = []\n",
        "    sucessos = 0\n",
        "\n",
        "    print(\"\"\"\n",
        "Iniciando decomposiÃ§Ã£o para {} vÃ­deos...\"\"\".format(len(videos_com_metadados)))\n",
        "\n",
        "    for i, video in enumerate(videos_com_metadados, 1):\n",
        "        if video.get(\"status\") == \"metadados_extraidos\":\n",
        "            print(f\"[{i}/{len(videos_com_metadados)}] Decompondo {video[\"nome_arquivo\"]}\")\n",
        "            try:\n",
        "                decomposicao = decompor_video(video)\n",
        "                decomposicao[\"status\"] = \"decomposto\"\n",
        "                decomposicoes_completas.append(decomposicao)\n",
        "                sucessos += 1\n",
        "                print(f\"  âœ… DecomposiÃ§Ã£o concluÃ­da para {video[\"nome_arquivo\"]}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  âŒ ERRO na decomposiÃ§Ã£o para {video[\"nome_arquivo\"]}: {e}\")\n",
        "                decomposicoes_completas.append({\"video_id\": video[\"id\"], \"status\": \"erro_decomposicao\", \"erro\": str(e)})\n",
        "        else:\n",
        "            print(f\"[{i}/{len(videos_com_metadados)}] Pulando {video.get(\"nome_arquivo\", video[\"id\"])} - Status: {video.get(\"status\", \"N/A\")}\")\n",
        "            decomposicoes_completas.append({\"video_id\": video[\"id\"], \"status\": video.get(\"status\", \"N/A\"), \"erro\": \"Pulado devido a erro anterior\"})\n",
        "\n",
        "    # Salvar decomposiÃ§Ãµes completas\n",
        "    decomposicao_json_path = os.path.join(PASTA_TRABALHO, \"dados\", \"decomposicao_completa.json\")\n",
        "    with open(decomposicao_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(decomposicoes_completas, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    config[\"status_etapas\"][\"decomposicao\"] = True\n",
        "    config[\"total_videos_decompostos\"] = sucessos\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\"\"\n",
        "ðŸ’¾ Dados de decomposiÃ§Ã£o salvos em: {decomposicao_json_path}\"\"\")\n",
        "\n",
        "    print(\"\"\"\n",
        "âœ… DECOMPOSIÃ‡ÃƒO DE VÃDEOS CONCLUÃDA!\"\"\")\n",
        "    print(f\"Total de vÃ­deos decompostos com sucesso: {sucessos}\")\n",
        "\n",
        "    if sucessos == 0:\n",
        "        print(\"âŒ NENHUM VÃDEO FOI DECOMPOSTO COM SUCESSO. Verifique as etapas anteriores.\")\n",
        "    print(\"\"\"\n",
        "âž¡ï¸ PRÃ“XIMA CÃ‰LULA: 3.1 - ANÃLISE DE PADRÃ•ES (TEMPORAIS, VISUAIS, TEXTO, ÃUDIO)\"\"\")\n",
        "\n",
        "# Executar decomposiÃ§Ã£o\n",
        "try:\n",
        "    processar_decomposicao_todos_videos()\n",
        "except Exception as e:\n",
        "    print(f\"\"\"\n",
        "âŒ ERRO GERAL NA DECOMPOSIÃ‡ÃƒO DE VÃDEOS: {e}\"\"\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "288e47d2",
        "outputId": "9eef1097-dc96-4d0f-e6e2-4194c2f8c5d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ¨ Iniciando renomeaÃ§Ã£o inteligente de vÃ­deos...\n",
            "\n",
            "[1/3] Renomeando bicho_nÃ£o_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.mp4\n",
            "  âœ… VÃ­deo renomeado e movido: bicho_nÃ£o_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.mp4 -> bicho_nÃ£o_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.mp4\n",
            "[2/3] Renomeando empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.mp4\n",
            "  âœ… VÃ­deo renomeado e movido: empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.mp4 -> empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.mp4\n",
            "[3/3] Renomeando mÃ¡quina_emocional_nÃ£o_dÃ¡_conta_de_converter_entÃ£o__vid_maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy.mp4\n",
            "  âœ… VÃ­deo renomeado e movido: mÃ¡quina_emocional_nÃ£o_dÃ¡_conta_de_converter_entÃ£o__vid_maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy.mp4 -> mÃ¡quina_emocional_nÃ£o_dÃ¡_conta_de_converter_entÃ£o__vid_maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy.mp4\n",
            "\n",
            "âœ… RENOMEAÃ‡ÃƒO INTELIGENTE CONCLUÃDA!\n",
            "\n",
            "Todos os vÃ­deos foram processados. Verifique a pasta: /content/drive/MyDrive/Videos Dona Done/_videos_renomeados\n",
            "âž¡ï¸ PRÃ“XIMA CÃ‰LULA: 3.1 - ANÃLISE DE PADRÃ•ES (TEMPORAIS, VISUAIS, TEXTO, ÃUDIO)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CÃ‰LULA 2.4: RENOMEAÃ‡ÃƒO INTELIGENTE DE VÃDEOS\n",
        "# ============================================================================\n",
        "\n",
        "def renomear_video_inteligentemente(video_info, nova_pasta_videos):\n",
        "    \"\"\"Renomeia um vÃ­deo com base na primeira frase do Ã¡udio e move-o para a nova pasta.\"\"\"\n",
        "    video_id = video_info[\"id\"]\n",
        "    caminho_original = video_info[\"caminho_completo\"]\n",
        "    extensao = video_info[\"extensao\"]\n",
        "    primeira_frase = video_info.get(\"primeira_frase_audio\", \"\").replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
        "\n",
        "    if not primeira_frase:\n",
        "        print(f\"âš ï¸ Aviso: NÃ£o foi possÃ­vel renomear {video_info[\"nome_arquivo\"]} - primeira frase do Ã¡udio nÃ£o encontrada. Mantendo nome original.\")\n",
        "        return video_info\n",
        "\n",
        "    novo_nome_base = f\"{primeira_frase[:50].strip()}_{video_id}\"\n",
        "    novo_nome_arquivo = f\"{novo_nome_base}{extensao}\"\n",
        "    novo_caminho_completo = os.path.join(nova_pasta_videos, novo_nome_arquivo)\n",
        "\n",
        "    try:\n",
        "        os.makedirs(nova_pasta_videos, exist_ok=True)\n",
        "        os.rename(caminho_original, novo_caminho_completo)\n",
        "        print(f\"  âœ… VÃ­deo renomeado e movido: {video_info[\"nome_arquivo\"]} -> {novo_nome_arquivo}\")\n",
        "        video_info[\"nome_arquivo_original\"] = video_info[\"nome_arquivo\"]\n",
        "        video_info[\"caminho_original\"] = video_info[\"caminho_completo\"]\n",
        "        video_info[\"nome_arquivo\"] = novo_nome_arquivo\n",
        "        video_info[\"caminho_completo\"] = novo_caminho_completo\n",
        "        video_info[\"status\"] = \"renomeado_inteligente\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  âŒ ERRO ao renomear/mover {video_info[\"nome_arquivo\"]}: {e}. Mantendo nome original.\")\n",
        "        video_info[\"status\"] = \"erro_renomeacao\"\n",
        "\n",
        "    return video_info\n",
        "\n",
        "def processar_renomeacao_inteligente():\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa(\"decomposicao\")\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    decomposicoes_path = os.path.join(PASTA_TRABALHO, \"dados\", \"decomposicao_completa.json\")\n",
        "    if not os.path.exists(decomposicoes_path):\n",
        "        print(\"âŒ ERRO: Arquivo de decomposiÃ§Ãµes nÃ£o encontrado. Execute a CÃ‰LULA 2.3 primeiro.\")\n",
        "        return\n",
        "\n",
        "    metadados_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "    if not os.path.exists(metadados_path):\n",
        "        print(\"âŒ ERRO: Arquivo de metadados nÃ£o encontrado. Execute a CÃ‰LULA 2.2 primeiro.\")\n",
        "        return\n",
        "\n",
        "    with open(decomposicoes_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        videos_decompostos = json.load(f)\n",
        "\n",
        "    with open(metadados_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        videos_com_metadados = json.load(f)\n",
        "\n",
        "    metadados_dict = {video[\"id\"]: video for video in videos_com_metadados}\n",
        "\n",
        "    videos_renomeados = []\n",
        "    nova_pasta_videos_renomeados = os.path.join(PASTA_VIDEOS, \"_videos_renomeados\")\n",
        "\n",
        "    print(\"\\nâœ¨ Iniciando renomeaÃ§Ã£o inteligente de vÃ­deos...\\n\")\n",
        "\n",
        "    for i, decomposicao_data in enumerate(videos_decompostos, 1):\n",
        "        video_id = decomposicao_data[\"video_id\"]\n",
        "        video_info = metadados_dict.get(video_id)\n",
        "\n",
        "        if not video_info:\n",
        "            print(f\"âš ï¸ Aviso: Metadados para o vÃ­deo {video_id} nÃ£o encontrados. Pulando renomeaÃ§Ã£o.\")\n",
        "            continue\n",
        "\n",
        "        video_info[\"primeira_frase_audio\"] = decomposicao_data.get(\"audio_transcrito\", \"\").split('.')[0] if decomposicao_data.get(\"audio_transcrito\") else \"\"\n",
        "\n",
        "\n",
        "        print(f\"[{i}/{len(videos_decompostos)}] Renomeando {video_info[\"nome_arquivo\"]}\")\n",
        "        renomeado_info = renomear_video_inteligentemente(video_info, nova_pasta_videos_renomeados)\n",
        "        videos_renomeados.append(renomeado_info)\n",
        "\n",
        "    with open(metadados_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(videos_renomeados, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    config[\"status_etapas\"][\"renomeacao_inteligente\"] = True\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(\"\\nâœ… RENOMEAÃ‡ÃƒO INTELIGENTE CONCLUÃDA!\\n\")\n",
        "    print(f\"Todos os vÃ­deos foram processados. Verifique a pasta: {nova_pasta_videos_renomeados}\")\n",
        "    print(\"âž¡ï¸ PRÃ“XIMA CÃ‰LULA: 3.1 - ANÃLISE DE PADRÃ•ES (TEMPORAIS, VISUAIS, TEXTO, ÃUDIO)\")\n",
        "\n",
        "processar_renomeacao_inteligente()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AquW8stD8E1o"
      },
      "source": [
        "melhorar os cortes aqui ( otimizar ele esta detectando muitos cortes. corrigir possivel erro de Fala ininteligÃ­vel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EezfUtNZUZlc",
        "outputId": "c808108e-9390-4cb6-8645-ed61bfcb774b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando anÃ¡lise de Ã¡udio refinada para 3 vÃ­deos...\n",
            "[1/3] Analisando Ã¡udio refinado para: vid_bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t\n",
            "    ðŸ”Š Analisando variaÃ§Ã£o de volume...\n",
            "    ðŸ”Š Detectando picos de ruÃ­do...\n",
            "    ðŸ”Š Analisando ritmo da fala...\n",
            "    ðŸ”Š Identificando pausas...\n",
            "    ðŸ”Š Classificando mÃºsica de fundo...\n",
            "    ðŸ”Š Analisando clareza da voz...\n",
            "    ðŸ”Š Detectando sobreposiÃ§Ã£o...\n",
            "    ðŸ”Š Mapeando efeitos sonoros...\n",
            "    ðŸ”Š Analisando frequÃªncias especÃ­ficas...\n",
            "    ðŸ”Š Gerando espectrograma...\n",
            "  âœ… AnÃ¡lise de Ã¡udio refinada concluÃ­da para vid_bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t\n",
            "[2/3] Analisando Ã¡udio refinado para: vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf\n",
            "    ðŸ”Š Analisando variaÃ§Ã£o de volume...\n",
            "    ðŸ”Š Detectando picos de ruÃ­do...\n",
            "    ðŸ”Š Analisando ritmo da fala...\n",
            "    ðŸ”Š Identificando pausas...\n",
            "    ðŸ”Š Classificando mÃºsica de fundo...\n",
            "    ðŸ”Š Analisando clareza da voz...\n",
            "    ðŸ”Š Detectando sobreposiÃ§Ã£o...\n",
            "    ðŸ”Š Mapeando efeitos sonoros...\n",
            "    ðŸ”Š Analisando frequÃªncias especÃ­ficas...\n",
            "    ðŸ”Š Gerando espectrograma...\n",
            "  âœ… AnÃ¡lise de Ã¡udio refinada concluÃ­da para vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf\n",
            "[3/3] Analisando Ã¡udio refinado para: vid_maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy\n",
            "    ðŸ”Š Analisando variaÃ§Ã£o de volume...\n",
            "    ðŸ”Š Detectando picos de ruÃ­do...\n",
            "    ðŸ”Š Analisando ritmo da fala...\n",
            "    ðŸ”Š Identificando pausas...\n",
            "    ðŸ”Š Classificando mÃºsica de fundo...\n",
            "    ðŸ”Š Analisando clareza da voz...\n",
            "    ðŸ”Š Detectando sobreposiÃ§Ã£o...\n",
            "    ðŸ”Š Mapeando efeitos sonoros...\n",
            "    ðŸ”Š Analisando frequÃªncias especÃ­ficas...\n",
            "    ðŸ”Š Gerando espectrograma...\n",
            "  âœ… AnÃ¡lise de Ã¡udio refinada concluÃ­da para vid_maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy\n",
            "\n",
            "ðŸ’¾ RelatÃ³rio resumo de anÃ¡lise de Ã¡udio salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_audio/resumo_analise_audio.xlsx\n",
            "\n",
            "âœ… ANÃLISE DE ÃUDIO REFINADA CONCLUÃDA!\n",
            "Total de vÃ­deos com anÃ¡lise de Ã¡udio refinada concluÃ­da: 3\n",
            "\n",
            "âž¡ï¸ PRÃ“XIMA CÃ‰LULA: 3.1 - ANÃLISE DE PADRÃ•ES (TEMPORAIS, VISUAIS, TEXTO, ÃUDIO)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CÃ‰LULA 2.4: ANÃLISE DE ÃUDIO REFINADA (SUBLAYER DA LAYER 2)\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# CÃ‰LULA 2.4: ANÃLISE DE ÃUDIO REFINADA (SUBLAYER DA LAYER 2)\n",
        "# ============================================================================\n",
        "\n",
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.signal\n",
        "from scipy.stats import variation\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def converter_para_json_serializable(obj):\n",
        "    \"\"\"Converte tipos NumPy para tipos Python nativos para serializaÃ§Ã£o JSON\"\"\"\n",
        "    if isinstance(obj, np.integer):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, np.floating):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return [converter_para_json_serializable(x) for x in obj.tolist()]\n",
        "    elif isinstance(obj, list):\n",
        "        return [converter_para_json_serializable(x) for x in obj]\n",
        "    elif isinstance(obj, dict):\n",
        "        return {k: converter_para_json_serializable(v) for k, v in obj.items()}\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "def verificar_prerequisito_audio_refinado():\n",
        "    \"\"\"Verifica se a etapa de decomposiÃ§Ã£o foi concluÃ­da\"\"\"\n",
        "    try:\n",
        "        if not \"PASTA_TRABALHO\" in globals():\n",
        "            raise Exception(\"VariÃ¡veis globais de configuraÃ§Ã£o nÃ£o encontradas. Execute a CÃ‰LULA 1.2 primeiro.\")\n",
        "\n",
        "        config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "        if not os.path.exists(config_path):\n",
        "            raise Exception(\"Arquivo de configuraÃ§Ã£o nÃ£o encontrado. Execute as cÃ©lulas anteriores.\")\n",
        "\n",
        "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            config = json.load(f)\n",
        "\n",
        "        if not config[\"status_etapas\"][\"decomposicao\"]:\n",
        "            raise Exception(\"A etapa 'decomposicao' nÃ£o foi concluÃ­da. Execute a CÃ‰LULA 2.3 primeiro.\")\n",
        "\n",
        "        return True, config\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ PRÃ‰-REQUISITO NÃƒO ATENDIDO: {e}\")\n",
        "        return False, None\n",
        "\n",
        "def analisar_variacao_volume(audio_path, sr=22050):\n",
        "    \"\"\"Analisa variaÃ§Ãµes de volume da voz\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "        # Calcular RMS (Root Mean Square) em janelas\n",
        "        frame_length = int(0.1 * sr)  # Janelas de 100ms\n",
        "        hop_length = frame_length // 4\n",
        "        rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]\n",
        "\n",
        "        # Detectar variaÃ§Ãµes bruscas\n",
        "        rms_db = librosa.amplitude_to_db(rms)\n",
        "        variacao_volume = np.diff(rms_db)\n",
        "\n",
        "        # Identificar picos de variaÃ§Ã£o\n",
        "        threshold_variacao = np.std(variacao_volume) * 2\n",
        "        picos_variacao = np.where(np.abs(variacao_volume) > threshold_variacao)[0]\n",
        "\n",
        "        # Converter Ã­ndices para timestamps\n",
        "        times = librosa.frames_to_time(picos_variacao, sr=sr, hop_length=hop_length)\n",
        "\n",
        "        return {\n",
        "            \"rms_medio\": float(np.mean(rms)),\n",
        "            \"variacao_volume_coef\": float(variation(rms)),\n",
        "            \"num_picos_variacao\": int(len(picos_variacao)),\n",
        "            \"timestamps_picos\": [float(t) for t in times.tolist()],\n",
        "            \"volume_db_medio\": float(np.mean(rms_db)),\n",
        "            \"volume_db_std\": float(np.std(rms_db))\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    âš ï¸ Erro na anÃ¡lise de variaÃ§Ã£o de volume: {e}\")\n",
        "        return {}\n",
        "\n",
        "def detectar_picos_ruido(audio_path, sr=22050):\n",
        "    \"\"\"Detecta picos de ruÃ­do excessivo\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "        # Calcular espectrograma\n",
        "        S = librosa.stft(y)\n",
        "        S_db = librosa.amplitude_to_db(np.abs(S))\n",
        "\n",
        "        # Detectar ruÃ­do baseado em frequÃªncias altas\n",
        "        freq_bins = librosa.fft_frequencies(sr=sr)\n",
        "        high_freq_mask = freq_bins > 4000  # FrequÃªncias acima de 4kHz\n",
        "\n",
        "        high_freq_energy = np.mean(S_db[high_freq_mask], axis=0)\n",
        "\n",
        "        # Identificar segmentos com ruÃ­do excessivo\n",
        "        threshold_ruido = np.percentile(high_freq_energy, 85)\n",
        "        segmentos_ruidosos = np.where(high_freq_energy > threshold_ruido)[0]\n",
        "\n",
        "        # Converter para timestamps\n",
        "        hop_length = 512\n",
        "        times_ruido = librosa.frames_to_time(segmentos_ruidosos, sr=sr, hop_length=hop_length)\n",
        "\n",
        "        return {\n",
        "            \"energia_alta_freq_media\": float(np.mean(high_freq_energy)),\n",
        "            \"threshold_ruido\": float(threshold_ruido),\n",
        "            \"num_segmentos_ruidosos\": int(len(segmentos_ruidosos)),\n",
        "            \"timestamps_ruido\": [float(t) for t in times_ruido.tolist()],\n",
        "            \"percentual_audio_ruidoso\": float(len(segmentos_ruidosos) / len(high_freq_energy) * 100)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    âš ï¸ Erro na detecÃ§Ã£o de picos de ruÃ­do: {e}\")\n",
        "        return {}\n",
        "\n",
        "def analisar_ritmo_fala(transcricao_texto, duracao_audio):\n",
        "    \"\"\"Calcula ritmo da fala em palavras por minuto\"\"\"\n",
        "    try:\n",
        "        if not transcricao_texto or duracao_audio <= 0:\n",
        "            return {}\n",
        "\n",
        "        palavras = transcricao_texto.split()\n",
        "        num_palavras = len(palavras)\n",
        "        duracao_minutos = duracao_audio / 60.0\n",
        "\n",
        "        palavras_por_minuto = num_palavras / duracao_minutos\n",
        "\n",
        "        # Classificar ritmo\n",
        "        if palavras_por_minuto < 120:\n",
        "            classificacao_ritmo = \"Lento\"\n",
        "        elif palavras_por_minuto < 160:\n",
        "            classificacao_ritmo = \"Normal\"\n",
        "        elif palavras_por_minuto < 200:\n",
        "            classificacao_ritmo = \"RÃ¡pido\"\n",
        "        else:\n",
        "            classificacao_ritmo = \"Muito RÃ¡pido\"\n",
        "\n",
        "        return {\n",
        "            \"palavras_por_minuto\": float(palavras_por_minuto),\n",
        "            \"total_palavras\": int(num_palavras),\n",
        "            \"duracao_minutos\": float(duracao_minutos),\n",
        "            \"classificacao_ritmo\": str(classificacao_ritmo),\n",
        "            \"densidade_informacional\": float(num_palavras / duracao_audio)  # palavras por segundo\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    âš ï¸ Erro na anÃ¡lise de ritmo de fala: {e}\")\n",
        "        return {}\n",
        "\n",
        "def identificar_pausas_fala(audio_path, sr=22050):\n",
        "    \"\"\"Identifica pausas e silÃªncios na fala\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "        # Detectar segmentos de fala vs silÃªncio\n",
        "        frame_length = int(0.025 * sr)  # 25ms frames\n",
        "        hop_length = frame_length // 2\n",
        "\n",
        "        # Energia RMS para detectar atividade vocal\n",
        "        rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]\n",
        "\n",
        "        # Threshold para distinguir fala de silÃªncio\n",
        "        threshold_silencio = np.percentile(rms, 20)  # 20% mais baixo = silÃªncio\n",
        "\n",
        "        # Identificar segmentos de silÃªncio\n",
        "        is_silence = rms < threshold_silencio\n",
        "\n",
        "        # Encontrar inÃ­cio e fim das pausas\n",
        "        pausas = []\n",
        "        in_pause = False\n",
        "        pause_start = 0\n",
        "\n",
        "        times = librosa.frames_to_time(range(len(is_silence)), sr=sr, hop_length=hop_length)\n",
        "\n",
        "        for i, silent in enumerate(is_silence):\n",
        "            if silent and not in_pause:\n",
        "                in_pause = True\n",
        "                pause_start = times[i]\n",
        "            elif not silent and in_pause:\n",
        "                in_pause = False\n",
        "                pause_duration = times[i] - pause_start\n",
        "                if pause_duration > 0.2:  # Pausas maiores que 200ms\n",
        "                    pausas.append({\n",
        "                        \"inicio\": float(pause_start),\n",
        "                        \"fim\": float(times[i]),\n",
        "                        \"duracao\": float(pause_duration)\n",
        "                    })\n",
        "\n",
        "        # EstatÃ­sticas das pausas\n",
        "        if pausas:\n",
        "            duracoes_pausas = [p[\"duracao\"] for p in pausas]\n",
        "            pausa_media = np.mean(duracoes_pausas)\n",
        "            pausa_total = sum(duracoes_pausas)\n",
        "        else:\n",
        "            pausa_media = 0\n",
        "            pausa_total = 0\n",
        "\n",
        "        return {\n",
        "            \"num_pausas\": int(len(pausas)),\n",
        "            \"pausas_detectadas\": pausas,\n",
        "            \"duracao_pausa_media\": float(pausa_media),\n",
        "            \"tempo_total_pausas\": float(pausa_total),\n",
        "            \"percentual_pausas\": float(pausa_total / len(y) * sr * 100),\n",
        "            \"threshold_silencio\": float(threshold_silencio)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    âš ï¸ Erro na identificaÃ§Ã£o de pausas: {e}\")\n",
        "        return {}\n",
        "\n",
        "def classificar_musica_fundo(audio_path, sr=22050):\n",
        "    \"\"\"Classifica caracterÃ­sticas da mÃºsica de fundo\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "        # AnÃ¡lise de caracterÃ­sticas musicais\n",
        "        tempo, beats = librosa.beat.beat_track(y=y, sr=sr)\n",
        "\n",
        "        # AnÃ¡lise espectral\n",
        "        spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
        "        spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]\n",
        "        mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
        "\n",
        "        # Energia\n",
        "        energia_total = np.sum(y**2)\n",
        "        energia_normalizada = energia_total / len(y)\n",
        "\n",
        "        # ClassificaÃ§Ã£o por energia\n",
        "        if energia_normalizada > 0.01:\n",
        "            nivel_energia = \"Alta\"\n",
        "        elif energia_normalizada > 0.001:\n",
        "            nivel_energia = \"MÃ©dia\"\n",
        "        else:\n",
        "            nivel_energia = \"Baixa\"\n",
        "\n",
        "        # ClassificaÃ§Ã£o por caracterÃ­sticas espectrais\n",
        "        centroide_medio = np.mean(spectral_centroids)\n",
        "        if centroide_medio > 3000:\n",
        "            brilho = \"Brilhante\"\n",
        "        elif centroide_medio > 1500:\n",
        "            brilho = \"Equilibrado\"\n",
        "        else:\n",
        "            brilho = \"Escuro\"\n",
        "\n",
        "        return {\n",
        "            \"tempo_bpm\": float(tempo),\n",
        "            \"num_beats\": int(len(beats)),\n",
        "            \"energia_nivel\": str(nivel_energia),\n",
        "            \"energia_valor\": float(energia_normalizada),\n",
        "            \"brilho_espectral\": str(brilho),\n",
        "            \"centroide_espectral_medio\": float(centroide_medio),\n",
        "            \"rolloff_medio\": float(np.mean(spectral_rolloff)),\n",
        "            \"mfcc_features\": [float(x) for x in mfcc.mean(axis=1).tolist()]\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    âš ï¸ Erro na classificaÃ§Ã£o de mÃºsica de fundo: {e}\")\n",
        "        return {}\n",
        "\n",
        "def analisar_clareza_voz(audio_path, sr=22050):\n",
        "    \"\"\"Analisa clareza e inteligibilidade da voz\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "        # Faixa de frequÃªncia da voz humana (aproximadamente 85-255 Hz para fundamental)\n",
        "        # e harmÃ´nicos atÃ© ~4000 Hz para inteligibilidade\n",
        "\n",
        "        # AnÃ¡lise espectral\n",
        "        S = librosa.stft(y)\n",
        "        frequencies = librosa.fft_frequencies(sr=sr)\n",
        "\n",
        "        # Energia em diferentes bandas de frequÃªncia\n",
        "        baixa_freq = (frequencies >= 85) & (frequencies <= 255)    # Fundamental da voz\n",
        "        media_freq = (frequencies > 255) & (frequencies <= 2000)   # Formantes principais\n",
        "        alta_freq = (frequencies > 2000) & (frequencies <= 4000)   # Clareza/inteligibilidade\n",
        "\n",
        "        energia_baixa = np.mean(np.abs(S[baixa_freq]))\n",
        "        energia_media = np.mean(np.abs(S[media_freq]))\n",
        "        energia_alta = np.mean(np.abs(S[alta_freq]))\n",
        "\n",
        "        # RazÃ£o harmÃ´nica para ruÃ­do (aproximaÃ§Ã£o)\n",
        "        spectral_flatness = librosa.feature.spectral_flatness(y=y)[0]\n",
        "        clareza_media = 1 - np.mean(spectral_flatness)  # Menor flatness = mais harmÃ´nica\n",
        "\n",
        "        # Zero crossing rate (indicador de fricÃ§Ã£o/clareza)\n",
        "        zcr = librosa.feature.zero_crossing_rate(y)[0]\n",
        "        zcr_medio = np.mean(zcr)\n",
        "\n",
        "        # Score de clareza combinado\n",
        "        score_clareza = (energia_media + energia_alta) / (energia_baixa + 0.001) * clareza_media\n",
        "\n",
        "        if score_clareza > 10:\n",
        "            classificacao_clareza = \"Excelente\"\n",
        "        elif score_clareza > 5:\n",
        "            classificacao_clareza = \"Boa\"\n",
        "        elif score_clareza > 2:\n",
        "            classificacao_clareza = \"Regular\"\n",
        "        else:\n",
        "            classificacao_clareza = \"Precisa Melhoria\"\n",
        "\n",
        "        return {\n",
        "            \"score_clareza\": float(score_clareza),\n",
        "            \"classificacao_clareza\": classificacao_clareza,\n",
        "            \"energia_fundamental\": float(energia_baixa),\n",
        "            \"energia_formantes\": float(energia_media),\n",
        "            \"energia_agudos\": float(energia_alta),\n",
        "            \"harmonicidade\": float(clareza_media),\n",
        "            \"zero_crossing_rate\": float(zcr_medio)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    âš ï¸ Erro na anÃ¡lise de clareza de voz: {e}\")\n",
        "        return {}\n",
        "\n",
        "def detectar_sobreposicao_audio(audio_path, sr=22050):\n",
        "    \"\"\"Detecta sobreposiÃ§Ã£o entre fala e mÃºsica/efeitos\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "        # SeparaÃ§Ã£o harmÃ´nica/percussiva (aproximaÃ§Ã£o para voz vs mÃºsica)\n",
        "        y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
        "\n",
        "        # AnÃ¡lise de energia em cada componente\n",
        "        energia_harmonica = librosa.feature.rms(y=y_harmonic)[0]\n",
        "        energia_percussiva = librosa.feature.rms(y=y_percussive)[0]\n",
        "        energia_total = librosa.feature.rms(y=y)[0]\n",
        "\n",
        "        # Detectar momentos de sobreposiÃ§Ã£o\n",
        "        threshold_sobreposicao = 0.7  # Threshold para detectar sobreposiÃ§Ã£o significativa\n",
        "\n",
        "        # RazÃ£o entre componentes\n",
        "        razao_hp = energia_harmonica / (energia_percussiva + 0.001)\n",
        "\n",
        "        # Momentos onde hÃ¡ competiÃ§Ã£o (energia similar em ambos)\n",
        "        competicao_mask = (energia_harmonica > threshold_sobreposicao * np.max(energia_harmonica)) & \\\n",
        "                         (energia_percussiva > threshold_sobreposicao * np.max(energia_percussiva))\n",
        "\n",
        "        segmentos_sobreposicao = np.where(competicao_mask)[0]\n",
        "\n",
        "        # Converter para timestamps\n",
        "        hop_length = 512\n",
        "        times_sobreposicao = librosa.frames_to_time(segmentos_sobreposicao, sr=sr, hop_length=hop_length)\n",
        "\n",
        "        return {\n",
        "            \"num_sobreposicoes\": int(len(segmentos_sobreposicao)),\n",
        "            \"timestamps_sobreposicao\": [float(t) for t in times_sobreposicao.tolist()],\n",
        "            \"percentual_sobreposicao\": float(len(segmentos_sobreposicao) / len(energia_total) * 100),\n",
        "            \"energia_harmonica_media\": float(np.mean(energia_harmonica)),\n",
        "            \"energia_percussiva_media\": float(np.mean(energia_percussiva)),\n",
        "            \"razao_harmonico_percussivo\": float(np.mean(razao_hp))\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    âš ï¸ Erro na detecÃ§Ã£o de sobreposiÃ§Ã£o: {e}\")\n",
        "        return {}\n",
        "\n",
        "def mapear_efeitos_sonoros(audio_path, sr=22050):\n",
        "    \"\"\"Mapeia e cataloga efeitos sonoros\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "        # Detectar eventos transientes (possÃ­veis efeitos sonoros)\n",
        "        onset_frames = librosa.onset.onset_detect(y=y, sr=sr, units='frames')\n",
        "        onset_times = librosa.frames_to_time(onset_frames, sr=sr)\n",
        "\n",
        "        # AnÃ¡lise de caracterÃ­sticas espectrais em cada onset\n",
        "        efeitos_detectados = []\n",
        "\n",
        "        for i, onset_time in enumerate(onset_times):\n",
        "            # Janela de anÃ¡lise ao redor do onset\n",
        "            inicio_frame = max(0, onset_frames[i] - 10)\n",
        "            fim_frame = min(len(y), onset_frames[i] + 50)\n",
        "\n",
        "            janela = y[inicio_frame:fim_frame] if fim_frame > inicio_frame else np.array([])\n",
        "\n",
        "            if len(janela) > 0:\n",
        "                # CaracterÃ­sticas do efeito\n",
        "                energia = np.sum(janela**2)\n",
        "                freq_dominante = librosa.piptrack(y=janela, sr=sr)[0]\n",
        "\n",
        "                # ClassificaÃ§Ã£o simplificada baseada em caracterÃ­sticas\n",
        "                if energia > 0.1:\n",
        "                    tipo_efeito = \"Impacto\"\n",
        "                elif np.max(freq_dominante) > 5000:\n",
        "                    tipo_efeito = \"Agudo\"\n",
        "                elif np.max(freq_dominante) < 200:\n",
        "                    tipo_efeito = \"Grave\"\n",
        "                else:\n",
        "                    tipo_efeito = \"MÃ©dio\"\n",
        "\n",
        "                efeitos_detectados.append({\n",
        "                    \"timestamp\": float(onset_time),\n",
        "                    \"energia\": float(energia),\n",
        "                    \"tipo_estimado\": tipo_efeito\n",
        "                })\n",
        "\n",
        "        # Contagem por tipo\n",
        "        tipos_efeitos = Counter([ef[\"tipo_estimado\"] for ef in efeitos_detectados])\n",
        "\n",
        "        return {\n",
        "            \"num_efeitos_detectados\": int(len(efeitos_detectados)),\n",
        "            \"efeitos_por_minuto\": float(len(efeitos_detectados) / (len(y) / sr / 60)),\n",
        "            \"tipos_efeitos\": dict(tipos_efeitos),\n",
        "            \"efeitos_detalhados\": efeitos_detectados,\n",
        "            \"densidade_efeitos\": float(len(efeitos_detectados) / (len(y) / sr))\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    âš ï¸ Erro no mapeamento de efeitos sonoros: {e}\")\n",
        "        return {}\n",
        "\n",
        "def analisar_frequencias_especificas(audio_path, sr=22050):\n",
        "    \"\"\"Analisa sons recorrentes especÃ­ficos\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "        # Template matching para sons especÃ­ficos (simplificado)\n",
        "        # Detectar padrÃµes de risada (frequÃªncias variadas em burst)\n",
        "        onset_strength = librosa.onset.onset_strength(y=y, sr=sr)\n",
        "\n",
        "        # Detectar rajadas de atividade (possÃ­vel risada)\n",
        "        threshold_burst = np.percentile(onset_strength, 80)\n",
        "        bursts = onset_strength > threshold_burst\n",
        "\n",
        "        # Agrupar bursts prÃ³ximos\n",
        "        burst_groups = []\n",
        "        in_burst = False\n",
        "        burst_start = 0\n",
        "\n",
        "        for i, is_burst in enumerate(bursts):\n",
        "            if is_burst and not in_burst:\n",
        "                in_burst = True\n",
        "                burst_start = i\n",
        "            elif not is_burst and in_burst:\n",
        "                in_burst = False\n",
        "                burst_duration = i - burst_start\n",
        "                if burst_duration > 5:  # Bursts de pelo menos 5 frames\n",
        "                    burst_groups.append({\n",
        "                        \"inicio\": librosa.frames_to_time(burst_start, sr=sr),\n",
        "                        \"duracao\": librosa.frames_to_time(burst_duration, sr=sr),\n",
        "                        \"intensidade\": np.mean(onset_strength[burst_start:i])\n",
        "                    })\n",
        "\n",
        "        # Detectar sons de notificaÃ§Ã£o (tons puros em frequÃªncias especÃ­ficas)\n",
        "        # AnÃ¡lise espectral para encontrar picos em frequÃªncias comuns de notificaÃ§Ã£o\n",
        "        S = librosa.stft(y)\n",
        "        frequencies = librosa.fft_frequencies(sr=sr)\n",
        "\n",
        "        # FrequÃªncias tÃ­picas de notificaÃ§Ã£o (440Hz, 880Hz, etc.)\n",
        "        freq_targets = [440, 880, 1320]  # A4, A5, E6\n",
        "        notificacoes_detectadas = 0\n",
        "\n",
        "        for freq_target in freq_targets:\n",
        "            freq_idx = np.argmin(np.abs(frequencies - freq_target))\n",
        "            freq_energy = np.abs(S[freq_idx])\n",
        "\n",
        "            # Detectar picos sustentados nesta frequÃªncia\n",
        "            peaks = scipy.signal.find_peaks(freq_energy, height=np.percentile(freq_energy, 90))[0]\n",
        "            notificacoes_detectadas += len(peaks)\n",
        "\n",
        "        return {\n",
        "            \"bursts_atividade\": int(len(burst_groups)),\n",
        "            \"detalhes_bursts\": burst_groups,\n",
        "            \"possivel_risada\": int(len(burst_groups)),\n",
        "            \"sons_notificacao_detectados\": int(notificacoes_detectadas),\n",
        "            \"densidade_eventos_especiais\": float((len(burst_groups) + notificacoes_detectadas) / (len(y) / sr))\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    âš ï¸ Erro na anÃ¡lise de frequÃªncias especÃ­ficas: {e}\")\n",
        "        return {}\n",
        "\n",
        "def gerar_espectrograma_simplificado(audio_path, video_id, sr=22050):\n",
        "    \"\"\"Gera e salva espectrograma simplificado\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "        # Gerar espectrograma\n",
        "        S = librosa.stft(y)\n",
        "        S_db = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n",
        "\n",
        "        # Criar visualizaÃ§Ã£o\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        librosa.display.specshow(S_db, sr=sr, x_axis='time', y_axis='hz')\n",
        "        plt.colorbar(format='%+2.0f dB')\n",
        "        plt.title(f'Espectrograma - {video_id}')\n",
        "        plt.xlabel('Tempo (s)')\n",
        "        plt.ylabel('FrequÃªncia (Hz)')\n",
        "        plt.ylim(0, 8000)  # Focar em frequÃªncias atÃ© 8kHz\n",
        "\n",
        "        # Salvar\n",
        "        espectrograma_path = os.path.join(PASTA_TRABALHO, \"analise_audio\", f\"espectrograma_{video_id}.png\")\n",
        "        os.makedirs(os.path.dirname(espectrograma_path), exist_ok=True)\n",
        "        plt.savefig(espectrograma_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        # AnÃ¡lise de padrÃµes espectrais\n",
        "        freq_bins = librosa.fft_frequencies(sr=sr)\n",
        "\n",
        "        # Energia por banda de frequÃªncia\n",
        "        baixa_energia = np.mean(S_db[freq_bins <= 500])\n",
        "        media_energia = np.mean(S_db[(freq_bins > 500) & (freq_bins <= 2000)])\n",
        "        alta_energia = np.mean(S_db[freq_bins > 2000])\n",
        "\n",
        "        return {\n",
        "            \"espectrograma_path\": str(espectrograma_path),\n",
        "            \"energia_baixa_freq\": float(baixa_energia),\n",
        "            \"energia_media_freq\": float(media_energia),\n",
        "            \"energia_alta_freq\": float(alta_energia),\n",
        "            \"frequencia_maxima\": float(np.max(freq_bins)),\n",
        "            \"resolucao_temporal\": float(len(y) / sr),\n",
        "            \"picos_espectrais\": int(len(scipy.signal.find_peaks(np.mean(S_db, axis=1))[0]))\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    âš ï¸ Erro na geraÃ§Ã£o de espectrograma: {e}\")\n",
        "        return {\"espectrograma_path\": None}\n",
        "\n",
        "def processar_analise_audio_refinada():\n",
        "    \"\"\"Processa anÃ¡lise de Ã¡udio refinada para todos os vÃ­deos\"\"\"\n",
        "    prerequisito_ok, config = verificar_prerequisito_audio_refinado()\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Carregar dados de decomposiÃ§Ã£o\n",
        "    decomposicao_path = os.path.join(PASTA_TRABALHO, \"dados\", \"decomposicao_completa.json\")\n",
        "    with open(decomposicao_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        decomposicoes = json.load(f)\n",
        "\n",
        "    analises_audio_refinadas = []\n",
        "    sucessos = 0\n",
        "\n",
        "    print(f\"\"\"\n",
        "Iniciando anÃ¡lise de Ã¡udio refinada para {len(decomposicoes)} vÃ­deos...\"\"\")\n",
        "\n",
        "    for i, decomposicao in enumerate(decomposicoes, 1):\n",
        "        if decomposicao.get(\"status\") == \"decomposto\":\n",
        "            video_id = decomposicao[\"video_id\"]\n",
        "            print(f\"[{i}/{len(decomposicoes)}] Analisando Ã¡udio refinado para: {video_id}\")\n",
        "\n",
        "            try:\n",
        "                # Buscar arquivo de Ã¡udio\n",
        "                audio_path = os.path.join(PASTA_TRABALHO, \"temp\", f\"{video_id}.wav\")\n",
        "\n",
        "                if not os.path.exists(audio_path):\n",
        "                    print(f\"    âš ï¸ Arquivo de Ã¡udio nÃ£o encontrado: {audio_path}\")\n",
        "                    analises_audio_refinadas.append({\n",
        "                        \"video_id\": video_id,\n",
        "                        \"status\": \"erro_audio_nao_encontrado\",\n",
        "                        \"erro\": \"Arquivo de Ã¡udio nÃ£o encontrado\"\n",
        "                    })\n",
        "                    continue\n",
        "\n",
        "                analise_refinada = {\"video_id\": video_id}\n",
        "\n",
        "                print(f\"    ðŸ”Š Analisando variaÃ§Ã£o de volume...\")\n",
        "                analise_refinada[\"variacao_volume\"] = analisar_variacao_volume(audio_path)\n",
        "\n",
        "                print(f\"    ðŸ”Š Detectando picos de ruÃ­do...\")\n",
        "                analise_refinada[\"picos_ruido\"] = detectar_picos_ruido(audio_path)\n",
        "\n",
        "                print(f\"    ðŸ”Š Analisando ritmo da fala...\")\n",
        "                transcricao = decomposicao.get(\"audio_transcrito\", \"\")\n",
        "                duracao_audio = decomposicao.get(\"audio_analise\", {}).get(\"duracao_audio_segundos\", 0)\n",
        "                analise_refinada[\"ritmo_fala\"] = analisar_ritmo_fala(transcricao, duracao_audio)\n",
        "\n",
        "                print(f\"    ðŸ”Š Identificando pausas...\")\n",
        "                analise_refinada[\"pausas_fala\"] = identificar_pausas_fala(audio_path)\n",
        "\n",
        "                print(f\"    ðŸ”Š Classificando mÃºsica de fundo...\")\n",
        "                analise_refinada[\"musica_fundo\"] = classificar_musica_fundo(audio_path)\n",
        "\n",
        "                print(f\"    ðŸ”Š Analisando clareza da voz...\")\n",
        "                analise_refinada[\"clareza_voz\"] = analisar_clareza_voz(audio_path)\n",
        "\n",
        "                print(f\"    ðŸ”Š Detectando sobreposiÃ§Ã£o...\")\n",
        "                analise_refinada[\"sobreposicao_audio\"] = detectar_sobreposicao_audio(audio_path)\n",
        "\n",
        "                print(f\"    ðŸ”Š Mapeando efeitos sonoros...\")\n",
        "                analise_refinada[\"efeitos_sonoros\"] = mapear_efeitos_sonoros(audio_path)\n",
        "\n",
        "                print(f\"    ðŸ”Š Analisando frequÃªncias especÃ­ficas...\")\n",
        "                analise_refinada[\"frequencias_especificas\"] = analisar_frequencias_especificas(audio_path)\n",
        "\n",
        "                print(f\"    ðŸ”Š Gerando espectrograma...\")\n",
        "                analise_refinada[\"espectrograma\"] = gerar_espectrograma_simplificado(audio_path, video_id)\n",
        "\n",
        "                analise_refinada = converter_para_json_serializable(analise_refinada)\n",
        "                analise_refinada[\"status\"] = \"audio_refinado_concluido\"\n",
        "                analise_refinada[\"data_analise\"] = datetime.now().isoformat()\n",
        "\n",
        "                analises_audio_refinadas.append(analise_refinada)\n",
        "                sucessos += 1\n",
        "                print(f\"  âœ… AnÃ¡lise de Ã¡udio refinada concluÃ­da para {video_id}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  âŒ ERRO na anÃ¡lise de Ã¡udio refinada para {video_id}: {e}\")\n",
        "                analises_audio_refinadas.append({\n",
        "                    \"video_id\": video_id,\n",
        "                    \"status\": \"erro_analise_audio_refinada\",\n",
        "                    \"erro\": str(e)\n",
        "                })\n",
        "        else:\n",
        "            print(f\"[{i}/{len(decomposicoes)}] Pulando {decomposicao.get('video_id', 'N/A')} - Status: {decomposicao.get('status', 'N/A')}\")\n",
        "            analises_audio_refinadas.append({\n",
        "                \"video_id\": decomposicao.get(\"video_id\", \"N/A\"),\n",
        "                \"status\": decomposicao.get(\"status\", \"N/A\"),\n",
        "                \"erro\": \"Pulado devido a erro anterior\"\n",
        "            })\n",
        "\n",
        "    # Salvar anÃ¡lises de Ã¡udio refinadas\n",
        "    analise_audio_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analise_audio_refinada.json\")\n",
        "    with open(analise_audio_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(analises_audio_refinadas, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Gerar relatÃ³rio resumido em Excel\n",
        "    try:\n",
        "        # Preparar dados para Excel\n",
        "        dados_resumo = []\n",
        "        for analise in analises_audio_refinadas:\n",
        "            if analise.get(\"status\") == \"audio_refinado_concluido\":\n",
        "                resumo = {\n",
        "                    \"video_id\": analise[\"video_id\"],\n",
        "                    \"variacao_volume_coef\": analise[\"variacao_volume\"].get(\"variacao_volume_coef\", 0),\n",
        "                    \"num_picos_variacao\": analise[\"variacao_volume\"].get(\"num_picos_variacao\", 0),\n",
        "                    \"volume_db_medio\": analise[\"variacao_volume\"].get(\"volume_db_medio\", 0),\n",
        "                    \"percentual_audio_ruidoso\": analise[\"picos_ruido\"].get(\"percentual_audio_ruidoso\", 0),\n",
        "                    \"num_segmentos_ruidosos\": analise[\"picos_ruido\"].get(\"num_segmentos_ruidosos\", 0),\n",
        "                    \"palavras_por_minuto\": analise[\"ritmo_fala\"].get(\"palavras_por_minuto\", 0),\n",
        "                    \"classificacao_ritmo\": analise[\"ritmo_fala\"].get(\"classificacao_ritmo\", \"N/A\"),\n",
        "                    \"num_pausas\": analise[\"pausas_fala\"].get(\"num_pausas\", 0),\n",
        "                    \"percentual_pausas\": analise[\"pausas_fala\"].get(\"percentual_pausas\", 0),\n",
        "                    \"nivel_energia_musica\": analise[\"musica_fundo\"].get(\"energia_nivel\", \"N/A\"),\n",
        "                    \"tempo_bpm_musica\": analise[\"musica_fundo\"].get(\"tempo_bpm\", 0),\n",
        "                    \"score_clareza\": analise[\"clareza_voz\"].get(\"score_clareza\", 0),\n",
        "                    \"classificacao_clareza\": analise[\"clareza_voz\"].get(\"classificacao_clareza\", \"N/A\"),\n",
        "                    \"percentual_sobreposicao\": analise[\"sobreposicao_audio\"].get(\"percentual_sobreposicao\", 0),\n",
        "                    \"num_efeitos_detectados\": analise[\"efeitos_sonoros\"].get(\"num_efeitos_detectados\", 0),\n",
        "                    \"densidade_efeitos\": analise[\"efeitos_sonoros\"].get(\"densidade_efeitos\", 0),\n",
        "                    \"possivel_risada\": analise[\"frequencias_especificas\"].get(\"possivel_risada\", 0),\n",
        "                    \"sons_notificacao\": analise[\"frequencias_especificas\"].get(\"sons_notificacao_detectados\", 0),\n",
        "                    \"espectrograma_gerado\": \"Sim\" if analise[\"espectrograma\"].get(\"espectrograma_path\") else \"NÃ£o\"\n",
        "                }\n",
        "                dados_resumo.append(resumo)\n",
        "\n",
        "        if dados_resumo:\n",
        "            df_resumo = pd.DataFrame(dados_resumo)\n",
        "            resumo_excel_path = os.path.join(PASTA_TRABALHO, \"analise_audio\", \"resumo_analise_audio.xlsx\")\n",
        "            df_resumo.to_excel(resumo_excel_path, index=False, engine='openpyxl')\n",
        "            print(f\"\\nðŸ’¾ RelatÃ³rio resumo de anÃ¡lise de Ã¡udio salvo em: {resumo_excel_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    âŒ ERRO ao gerar relatÃ³rio resumo de Ã¡udio: {e}\")\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    # Adicionar status para a nova etapa de anÃ¡lise de Ã¡udio refinada\n",
        "    config[\"status_etapas\"][\"analise_audio_refinada\"] = True\n",
        "    config[\"total_videos_analisados_audio_refinado\"] = sucessos\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\"\"\n",
        "âœ… ANÃLISE DE ÃUDIO REFINADA CONCLUÃDA!\"\"\")\n",
        "    print(f\"Total de vÃ­deos com anÃ¡lise de Ã¡udio refinada concluÃ­da: {sucessos}\")\n",
        "\n",
        "    if sucessos == 0:\n",
        "        print(\"âŒ NENHUM VÃDEO FOI ANALISADO COM SUCESSO NESTA ETAPA. Verifique as etapas anteriores.\")\n",
        "    # No final, a prÃ³xima cÃ©lula seria 3.1 (AnÃ¡lise de PadrÃµes) que jÃ¡ foi executada\n",
        "    # mas como esta Ã© uma nova cÃ©lula (2.4), ela deveria vir antes de 3.1\n",
        "    # A mensagem original apontava para 3.1.\n",
        "    # Vamos manter a mensagem original para nÃ£o alterar o fluxo do notebook existente,\n",
        "    # mas idealmente, essa cÃ©lula seria inserida antes de 3.1 no fluxo.\n",
        "    print(\"\"\"\n",
        "âž¡ï¸ PRÃ“XIMA CÃ‰LULA: 3.1 - ANÃLISE DE PADRÃ•ES (TEMPORAIS, VISUAIS, TEXTO, ÃUDIO)\"\"\")\n",
        "\n",
        "    # Return the list of analyses for potential downstream use\n",
        "    return analises_audio_refinadas\n",
        "\n",
        "# Executar anÃ¡lise de audio refinada\n",
        "try:\n",
        "    processar_analise_audio_refinada()\n",
        "except Exception as e:\n",
        "    print(f\"\"\"\n",
        "âŒ ERRO GERAL NA ANÃLISE DE ÃUDIO REFINADA: {e}\"\"\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oIgWuMTniIv",
        "outputId": "e5af216d-0fe0-4a03-f54e-20cca523db82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”„ Iniciando processamento de copywriting adaptado...\n",
            "  âœ… Dados encontrados: decomposicao com 3 vÃ­deos\n",
            "Processando copywriting para 3 vÃ­deos...\n",
            "[1/3] Processando copywriting para: vid_bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t\n",
            "  ðŸ”„ Gerando legendas com timestamps...\n",
            "    âœ… Legendas SRT geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t_legendas.srt\n",
            "    âœ… Legendas TXT com timestamps geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t_legendas_timestamped.txt\n",
            "    ðŸ”„ Analisando copywriting...\n",
            "  âœ… Copywriting analisado: Score 0/100\n",
            "[2/3] Processando copywriting para: vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf\n",
            "  ðŸ”„ Gerando legendas com timestamps...\n",
            "    âœ… Legendas SRT geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf_legendas.srt\n",
            "    âœ… Legendas TXT com timestamps geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf_legendas_timestamped.txt\n",
            "    ðŸ”„ Analisando copywriting...\n",
            "  âœ… Copywriting analisado: Score 30/100\n",
            "[3/3] Processando copywriting para: vid_maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy\n",
            "  ðŸ”„ Gerando legendas com timestamps...\n",
            "    âœ… Legendas SRT geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy_legendas.srt\n",
            "    âœ… Legendas TXT com timestamps geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy_legendas_timestamped.txt\n",
            "    ðŸ”„ Analisando copywriting...\n",
            "  âœ… Copywriting analisado: Score 75/100\n",
            "ðŸ’¾ AnÃ¡lises de copywriting salvas em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/analises_copywriting_completas.json\n",
            "ðŸ’¾ Dados de legendas salvos em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/legendas_geradas.json\n",
            "\n",
            "âœ… ANÃLISE DE COPYWRITING CONCLUÃDA!\n",
            "Total de vÃ­deos com copywriting analisado: 3\n",
            "Total de legendas geradas: 3\n",
            "\n",
            "âž¡ï¸ PRÃ“XIMA CÃ‰LULA: 4.3 - INTEGRAÃ‡ÃƒO COM DASHBOARD\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CÃ‰LULA 2.4: GERAÃ‡ÃƒO DE LEGENDAS E ANÃLISE DE COPYWRITING - VERSÃƒO CORRIGIDA\n",
        "# ============================================================================\n",
        "\n",
        "import re\n",
        "from datetime import timedelta, datetime\n",
        "from collections import Counter\n",
        "import json\n",
        "import os\n",
        "\n",
        "# ============================================================================\n",
        "# FunÃ§Ãµes Auxiliares (Movidas para este escopo)\n",
        "# ============================================================================\n",
        "\n",
        "def buscar_dados_disponiveis():\n",
        "    \"\"\"Busca dados disponÃ­veis em ordem de prioridade\"\"\"\n",
        "    pasta_dados = os.path.join(PASTA_TRABALHO, \"dados\")\n",
        "\n",
        "    # Lista de possÃ­veis fontes de dados (em ordem de prioridade)\n",
        "    fontes_dados = [\n",
        "        (\"decomposicao_completa.json\", \"decomposicao\"),\n",
        "        (\"analises_padroes_completas.json\", \"padroes\"),\n",
        "        (\"analises_psicologicas_completas.json\", \"psicologico\"),\n",
        "        (\"metadados_completos.json\", \"metadados\"),\n",
        "        (\"videos_catalogados.json\", \"catalogados\")\n",
        "    ]\n",
        "\n",
        "    for arquivo, tipo in fontes_dados:\n",
        "        caminho_arquivo = os.path.join(pasta_dados, arquivo)\n",
        "\n",
        "        if os.path.exists(caminho_arquivo):\n",
        "            try:\n",
        "                with open(caminho_arquivo, \"r\", encoding=\"utf-8\") as f:\n",
        "                    dados = json.load(f)\n",
        "                if dados:\n",
        "                    return {\"tipo\": tipo, \"videos\": dados}\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Erro ao carregar dados de {arquivo}: {e}\")\n",
        "\n",
        "    return None\n",
        "\n",
        "def extrair_texto_disponivel(video_data, tipo_fonte):\n",
        "    \"\"\"Extrai texto (transcriÃ§Ã£o ou OCR) da fonte de dados disponÃ­vel\"\"\"\n",
        "    if tipo_fonte == \"decomposicao\":\n",
        "        return video_data.get(\"audio_transcrito\", \"\") or \" \".join([item.get(\"text\", \"\") for item in video_data.get(\"textos_ocr\", [])])\n",
        "    elif tipo_fonte == \"padroes\":\n",
        "         # Analise de padroes might have summary or keywords\n",
        "         return video_data.get(\"resumo_texto\", \"\") # or \" \".join(video_data.get(\"palavras_chave_texto\", []))\n",
        "    # Adicionar outras fontes conforme necessÃ¡rio\n",
        "    return \"\" # Default vazio\n",
        "\n",
        "def gerar_legendas_adaptadas(video_id, texto_transcrito, video_data):\n",
        "    \"\"\"Gera legendas para a anÃ¡lise de copywriting, adaptando se necessÃ¡rio\"\"\"\n",
        "    # Se jÃ¡ houver dados de decomposiÃ§Ã£o com timestamps, usar esses\n",
        "    if video_data.get(\"frames_extraidos\"):\n",
        "        # Tentar usar os dados de decomposiÃ§Ã£o originais para timestamps\n",
        "        # Isso requer carregar o arquivo decomposicao_completa.json novamente\n",
        "        decomposicao_path = os.path.join(PASTA_TRABALHO, \"dados\", \"decomposicao_completa.json\")\n",
        "        if os.path.exists(decomposicao_path):\n",
        "            try:\n",
        "                with open(decomposicao_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    decomposicoes = json.load(f)\n",
        "                decomposicao_original = next((d for d in decomposicoes if d[\"video_id\"] == video_id), None)\n",
        "                if decomposicao_original and decomposicao_original.get(\"audio_transcrito\"):\n",
        "                     # Se a transcriÃ§Ã£o original existir, usar a funÃ§Ã£o original de legendas\n",
        "                    duracao = video_data.get(\"duracao_segundos\", decomposicao_original.get(\"audio_analise\", {}).get(\"duracao_audio_segundos\", 30))\n",
        "                    return gerar_legendas_com_timestamps({\"id\": video_id, \"duracao_segundos\": duracao}, decomposicao_original)\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Aviso: Erro ao carregar decomposiÃ§Ã£o original para {video_id}: {e}. Gerando legendas estimadas.\")\n",
        "\n",
        "    # Se nÃ£o houver decomposiÃ§Ã£o original ou transcriÃ§Ã£o lÃ¡, gerar legendas estimadas\n",
        "    duracao_segundos = video_data.get(\"duracao_segundos\", estimar_duracao(texto_transcrito))\n",
        "    segmentos = dividir_texto_em_segmentos(texto_transcrito)\n",
        "    legendas_data = []\n",
        "    duracao_por_segmento = duracao_segundos / len(segmentos) if segmentos else 1\n",
        "\n",
        "    for i, segmento in enumerate(segmentos):\n",
        "        inicio_segundos = i * duracao_por_segmento\n",
        "        fim_segundos = (i + 1) * duracao_por_segmento\n",
        "\n",
        "        legenda_item = {\n",
        "            \"id\": i + 1,\n",
        "            \"inicio\": segundos_para_timestamp(inicio_segundos),\n",
        "            \"fim\": segundos_para_timestamp(fim_segundos),\n",
        "            \"texto\": segmento.strip(),\n",
        "            \"inicio_segundos\": inicio_segundos,\n",
        "            \"fim_segundos\": fim_segundos\n",
        "        }\n",
        "        legendas_data.append(legenda_item)\n",
        "\n",
        "    if not legendas_data:\n",
        "         return None, None, None\n",
        "\n",
        "    pasta_legendas = os.path.join(PASTA_TRABALHO, \"legendas\")\n",
        "    os.makedirs(pasta_legendas, exist_ok=True)\n",
        "    srt_path = os.path.join(pasta_legendas, f\"{video_id}_legendas_estimadas.srt\")\n",
        "    txt_path = os.path.join(pasta_legendas, f\"{video_id}_legendas_estimadas_timestamped.txt\")\n",
        "\n",
        "    gerar_arquivo_srt(legendas_data, srt_path)\n",
        "    gerar_arquivo_txt_timestamped(legendas_data, txt_path)\n",
        "\n",
        "    print(f\"    âœ… Legendas estimadas geradas: {srt_path}\")\n",
        "\n",
        "    return legendas_data, srt_path, txt_path\n",
        "\n",
        "\n",
        "def analisar_copywriting_adaptado(legendas_data, video_id, texto_completo):\n",
        "    \"\"\"Analisa copywriting usando a estrutura existente mas adaptada\"\"\"\n",
        "    print(\"    ðŸ”„ Analisando copywriting...\")\n",
        "\n",
        "    # DicionÃ¡rios de padrÃµes de copywriting (mantidos da funÃ§Ã£o original)\n",
        "    ganchos_patterns = {\n",
        "        \"pergunta_retorica\": [r\"\\b(?:vocÃª|tu)\\s+(?:jÃ¡|nunca|sempre|realmente|acha|imagina|sabe|quer|precisa)\",\n",
        "                            r\"(?:como|por que|quando|onde|o que).*\\?\"],\n",
        "        \"urgencia\": [r\"\\b(?:agora|hoje|urgente|rÃ¡pido|imediato|Ãºltima chance|sÃ³ hoje|apenas|restam)\",\n",
        "                     r\"\\b(?:nÃ£o perca|aproveite|garante jÃ¡|corre|Ãºltimas vagas)\"],\n",
        "        \"escassez\": [r\"\\b(?:limitado|exclusivo|poucos|restam|Ãºltima|Ãºnica|especial|VIP)\",\n",
        "                     r\"\\b(?:sÃ³ para|apenas para|somente|limitado a)\"],\n",
        "        \"autoridade\": [r\"\\b(?:especialista|expert|profissional|anos de experiÃªncia|comprovado|testado)\",\n",
        "                       r\"\\b(?:pesquisas mostram|estudos comprovam|cientificamente)\"],\n",
        "        \"prova_social\": [r\"\\b(?:milhares|centenas|todos|muitas pessoas|clientes|depoimentos)\",\n",
        "                         r\"\\b(?:jÃ¡ conseguiram|transformaram|mudaram|aprovaram)\"],\n",
        "        \"curiosidade\": [r\"\\b(?:segredo|descoberta|revelaÃ§Ã£o|mÃ©todo|tÃ©cnica|estratÃ©gia|fÃ³rmula)\",\n",
        "                        r\"\\b(?:ninguÃ©m te conta|poucos sabem|descobri que)\"],\n",
        "        \"problema_dor\": [r\"\\b(?:problema|dificuldade|frustraÃ§Ã£o|sofre|dor|preocupa|bloqueia)\",\n",
        "                         r\"\\b(?:cansado de|chega de|pare de|nÃ£o aguenta mais)\"],\n",
        "        \"solucao_resultado\": [r\"\\b(?:soluÃ§Ã£o|resolve|elimina|transforma|muda|resultado|sucesso)\",\n",
        "                              r\"\\b(?:conseguir|alcanÃ§ar|realizar|conquistar|atingir)\"]\n",
        "    }\n",
        "\n",
        "    gatilhos_patterns = {\n",
        "        \"reciprocidade\": [r\"\\b(?:grÃ¡tis|de graÃ§a|presente|bÃ´nus|oferta|sem custo)\",\n",
        "                          r\"\\b(?:vou te dar|vou ensinar|vou mostrar|compartilhar com vocÃª)\"],\n",
        "        \"comprometimento\": [r\"\\b(?:compromisso|prometo|garanto|palavra|juro)\",\n",
        "                            r\"\\b(?:pode confiar|tenho certeza|assumo|responsabilizo)\"],\n",
        "        \"aprovacao_social\": [r\"\\b(?:aprovado por|recomendado|indicado|usado por|preferido)\",\n",
        "                             r\"\\b(?:famosos|influencers|especialistas|mÃ©dicos|profissionais)\"],\n",
        "        \"aversao_perda\": [r\"\\b(?:perder|perdendo|vai ficar de fora|nÃ£o vai conseguir)\",\n",
        "                          r\"\\b(?:sair perdendo|ficar para trÃ¡s|oportunidade perdida)\"],\n",
        "        \"autoridade_especialista\": [r\"\\b(?:Dr|Dra|Professor|Mestre|PhD|especialista em)\",\n",
        "                                    r\"\\b(?:formado em|pÃ³s-graduado|anos estudando)\"],\n",
        "        \"emocional_medo\": [r\"\\b(?:medo|receio|preocupaÃ§Ã£o|inseguranÃ§a|ansiedade)\",\n",
        "                           r\"\\b(?:nÃ£o conseguir|fracassar|dar errado|prejudicar)\"],\n",
        "        \"emocional_esperanca\": [r\"\\b(?:sonho|esperanÃ§a|desejo|objetivo|meta|futuro melhor)\",\n",
        "                                r\"\\b(?:realizar|conquistar|alcanÃ§ar|transformar|mudar vida)\"]\n",
        "    }\n",
        "\n",
        "    ctas_patterns = {\n",
        "        \"acao_imediata\": [r\"\\b(?:clica|clique|acesse|baixe|faÃ§a|compre|adquira|garanta)\",\n",
        "                          r\"\\b(?:nÃ£o perca|aproveite|corre|vai|vem|participe)\"],\n",
        "        \"link_bio\": [r\"\\b(?:link na bio|bio|biografia|perfil|stories|direct)\",\n",
        "                     r\"\\b(?:DM|chama no WhatsApp|manda mensagem)\"],\n",
        "        \"engajamento\": [r\"\\b(?:comenta|compartilha|marca|salva|curte|like|segue)\",\n",
        "                        r\"\\b(?:conta nos comentÃ¡rios|deixa um|comenta aqui)\"],\n",
        "        \"inscricao\": [r\"\\b(?:inscreve|se inscreva|ativa|ativar|sino|notificaÃ§Ã£o)\",\n",
        "                      r\"\\b(?:cadastra|cadastre-se|registra|assine)\"],\n",
        "        \"contato_vendas\": [r\"\\b(?:WhatsApp|telefone|ligue|chama|fala comigo|contato)\",\n",
        "                           r\"\\b(?:agende|marque|consulta|reuniÃ£o|conversa)\"]\n",
        "    }\n",
        "\n",
        "    # AnÃ¡lise dos padrÃµes\n",
        "    ganchos_encontrados = {}\n",
        "    gatilhos_encontrados = {}\n",
        "    ctas_encontrados = {}\n",
        "\n",
        "    # Analisar ganchos\n",
        "    for tipo, patterns in ganchos_patterns.items():\n",
        "        matches = []\n",
        "        for pattern in patterns:\n",
        "            matches.extend(re.finditer(pattern, texto_completo, re.IGNORECASE))\n",
        "        if matches:\n",
        "            ganchos_encontrados[tipo] = {\n",
        "                \"count\": len(matches),\n",
        "                \"exemplos\": [m.group() for m in matches[:3]],  # Top 3 exemplos\n",
        "                \"timestamps\": encontrar_timestamps_matches(matches, legendas_data, texto_completo) # Reusa a funÃ§Ã£o de timestamp\n",
        "            }\n",
        "\n",
        "    # Analisar gatilhos\n",
        "    for tipo, patterns in gatilhos_patterns.items():\n",
        "        matches = []\n",
        "        for pattern in patterns:\n",
        "            matches.extend(re.finditer(pattern, texto_completo, re.IGNORECASE))\n",
        "        if matches:\n",
        "            gatilhos_encontrados[tipo] = {\n",
        "                \"count\": len(matches),\n",
        "                \"exemplos\": [m.group() for m in matches[:3]],\n",
        "                \"timestamps\": encontrar_timestamps_matches(matches, legendas_data, texto_completo)\n",
        "            }\n",
        "\n",
        "    # Analisar CTAs\n",
        "    for tipo, patterns in ctas_patterns.items():\n",
        "        matches = []\n",
        "        for pattern in patterns:\n",
        "            matches.extend(re.finditer(pattern, texto_completo, re.IGNORECASE))\n",
        "        if matches:\n",
        "            ctas_encontrados[tipo] = {\n",
        "                \"count\": len(matches),\n",
        "                \"exemplos\": [m.group() for m in matches[:3]],\n",
        "                \"timestamps\": encontrar_timestamps_matches(matches, legendas_data, texto_completo)\n",
        "            }\n",
        "\n",
        "    # AnÃ¡lise de estrutura narrativa\n",
        "    estrutura_narrativa = analisar_estrutura_narrativa(legendas_data) # Reusa a funÃ§Ã£o\n",
        "\n",
        "    # AnÃ¡lise de poder de persuasÃ£o\n",
        "    score_persuasao = calcular_score_persuasao(ganchos_encontrados, gatilhos_encontrados, ctas_encontrados) # Reusa a funÃ§Ã£o\n",
        "\n",
        "    analise_copywriting = {\n",
        "        \"video_id\": video_id,\n",
        "        \"texto_completo\": texto_completo,\n",
        "        \"total_palavras\": len(texto_completo.split()),\n",
        "        \"ganchos_detectados\": ganchos_encontrados,\n",
        "        \"gatilhos_mentais_detectados\": gatilhos_encontrados,\n",
        "        \"ctas_detectados\": ctas_encontrados,\n",
        "        \"estrutura_narrativa\": estrutura_narrativa,\n",
        "        \"score_persuasao\": score_persuasao,\n",
        "        \"recomendacoes_estrategicas\": gerar_recomendacoes_copywriting(ganchos_encontrados, gatilhos_encontrados, ctas_encontrados), # Reusa\n",
        "        \"templates_identificados\": identificar_templates_replicaveis(ganchos_encontrados, gatilhos_encontrados, ctas_encontrados), # Reusa\n",
        "        \"timestamp\": {\n",
        "            \"ganchos_timeline\": mapear_timeline_elementos(ganchos_encontrados, legendas_data), # Reusa\n",
        "            \"gatilhos_timeline\": mapear_timeline_elementos(gatilhos_encontrados, legendas_data), # Reusa\n",
        "            \"ctas_timeline\": mapear_timeline_elementos(ctas_encontrados, legendas_data) # Reusa\n",
        "        },\n",
        "        \"data_analise\": datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    return analise_copywriting\n",
        "\n",
        "def estimar_duracao(texto):\n",
        "    \"\"\"Estima a duraÃ§Ã£o do vÃ­deo com base na contagem de palavras (WPM mÃ©dio)\"\"\"\n",
        "    palavras_por_minuto = 150 # MÃ©dia de palavras por minuto\n",
        "    num_palavras = len(texto.split())\n",
        "    duracao_minutos = num_palavras / palavras_por_minuto\n",
        "    return duracao_minutos * 60 # Retorna em segundos\n",
        "\n",
        "# ============================================================================\n",
        "# FunÃ§Ã£o Principal da CÃ©lula (Movida para este escopo)\n",
        "# ============================================================================\n",
        "def processar_copywriting_todos_videos_adaptado():\n",
        "    \"\"\"Processa anÃ¡lise de copywriting adaptada para o sistema existente\"\"\"\n",
        "    print(\"ðŸ”„ Iniciando processamento de copywriting adaptado...\")\n",
        "\n",
        "    # Verificar prÃ©-requisitos de forma mais flexÃ­vel\n",
        "    if not \"PASTA_TRABALHO\" in globals():\n",
        "        print(\"âŒ VariÃ¡veis globais nÃ£o encontradas. Execute a CÃ‰LULA 1.2 primeiro.\")\n",
        "        return\n",
        "\n",
        "    pasta_dados = os.path.join(PASTA_TRABALHO, \"dados\")\n",
        "    if not os.path.exists(pasta_dados):\n",
        "        print(\"âŒ Pasta de dados nÃ£o encontrada. Execute as cÃ©lulas anteriores primeiro.\")\n",
        "        return\n",
        "\n",
        "    # Buscar dados disponÃ­veis em ordem de prioridade\n",
        "    dados_encontrados = buscar_dados_disponiveis()\n",
        "\n",
        "    if not dados_encontrados:\n",
        "        print(\"âŒ Nenhum dado de vÃ­deo encontrado. Execute as cÃ©lulas anteriores primeiro.\")\n",
        "        return\n",
        "\n",
        "    print(f\"  âœ… Dados encontrados: {dados_encontrados['tipo']} com {len(dados_encontrados['videos'])} vÃ­deos\")\n",
        "\n",
        "    analises_copywriting = []\n",
        "    legendas_geradas = []\n",
        "\n",
        "    print(f\"Processando copywriting para {len(dados_encontrados['videos'])} vÃ­deos...\")\n",
        "\n",
        "    for i, video_data in enumerate(dados_encontrados['videos'], 1):\n",
        "        video_id = video_data.get(\"id\") or video_data.get(\"video_id\", f\"vid_{i:03d}\")\n",
        "\n",
        "        print(f\"[{i}/{len(dados_encontrados['videos'])}] Processando copywriting para: {video_id}\")\n",
        "\n",
        "        try:\n",
        "            # Extrair texto transcrito de diferentes fontes possÃ­veis\n",
        "            texto_transcrito = extrair_texto_disponivel(video_data, dados_encontrados['tipo'])\n",
        "\n",
        "            if texto_transcrito and len(texto_transcrito.strip()) > 10:\n",
        "                # Gerar legendas se houver texto\n",
        "                legendas_data, srt_path, txt_path = gerar_legendas_adaptadas(video_id, texto_transcrito, video_data)\n",
        "\n",
        "                if legendas_data:\n",
        "                    legendas_info = {\n",
        "                        \"video_id\": video_id,\n",
        "                        \"srt_path\": srt_path,\n",
        "                        \"txt_path\": txt_path,\n",
        "                        \"total_segmentos\": len(legendas_data),\n",
        "                        \"duracao_total\": video_data.get(\"duracao_segundos\", estimar_duracao(texto_transcrito)),\n",
        "                        \"legendas_data\": legendas_data\n",
        "                    }\n",
        "                    legendas_geradas.append(legendas_info)\n",
        "\n",
        "                    # AnÃ¡lise de copywriting\n",
        "                    analise_copy = analisar_copywriting_adaptado(legendas_data, video_id, texto_transcrito)\n",
        "                    analises_copywriting.append(analise_copy)\n",
        "\n",
        "                    print(f\"  âœ… Copywriting analisado: Score {analise_copy['score_persuasao']}/100\")\n",
        "            else:\n",
        "                print(f\"  âš ï¸ Pulando {video_id}: texto insuficiente para anÃ¡lise\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  âŒ Erro no processamento de copywriting para {video_id}: {e}\")\n",
        "\n",
        "    if not analises_copywriting:\n",
        "        print(\"âŒ Nenhuma anÃ¡lise de copywriting foi gerada. Verifique se os vÃ­deos possuem transcriÃ§Ã£o.\")\n",
        "        return\n",
        "\n",
        "    # Salvar dados de copywriting\n",
        "    os.makedirs(pasta_dados, exist_ok=True)\n",
        "\n",
        "    copywriting_path = os.path.join(pasta_dados, \"analises_copywriting_completas.json\")\n",
        "    with open(copywriting_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(analises_copywriting, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"ðŸ’¾ AnÃ¡lises de copywriting salvas em: {copywriting_path}\")\n",
        "\n",
        "    # Salvar dados de legendas\n",
        "    legendas_path = os.path.join(pasta_dados, \"legendas_geradas.json\")\n",
        "    with open(legendas_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(legendas_geradas, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"ðŸ’¾ Dados de legendas salvos em: {legendas_path}\")\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    if os.path.exists(config_path):\n",
        "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            config = json.load(f)\n",
        "\n",
        "        config[\"status_etapas\"][\"copywriting_analysis\"] = True\n",
        "\n",
        "        with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\nâœ… ANÃLISE DE COPYWRITING CONCLUÃDA!\")\n",
        "    print(f\"Total de vÃ­deos com copywriting analisado: {len(analises_copywriting)}\")\n",
        "    print(f\"Total de legendas geradas: {len(legendas_geradas)}\")\n",
        "    print(f\"\\nâž¡ï¸ PRÃ“XIMA CÃ‰LULA: 4.3 - INTEGRAÃ‡ÃƒO COM DASHBOARD\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# ExecuÃ§Ã£o da CÃ©lula\n",
        "# ============================================================================\n",
        "try:\n",
        "    processar_copywriting_todos_videos_adaptado()\n",
        "except Exception as e:\n",
        "    print(f\"âŒ ERRO de ExecuÃ§Ã£o: {type(e).__name__}: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "analise_padroes",
        "outputId": "bbaae1d6-ce5f-4534-aa96-8b192300234a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando anÃ¡lise de padrÃµes para 3 vÃ­deos...\n",
            "[1/3] Analisando padrÃµes para: bicho_nÃ£o_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.mp4\n",
            "  âš™ï¸ Analisando padrÃµes para: vid_bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t\n",
            "  âœ… AnÃ¡lise de padrÃµes concluÃ­da para bicho_nÃ£o_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.mp4\n",
            "[2/3] Analisando padrÃµes para: empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.mp4\n",
            "  âš™ï¸ Analisando padrÃµes para: vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf\n",
            "  âœ… AnÃ¡lise de padrÃµes concluÃ­da para empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.mp4\n",
            "[3/3] Analisando padrÃµes para: mÃ¡quina_emocional_nÃ£o_dÃ¡_conta_de_converter_entÃ£o__vid_maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy.mp4\n",
            "  âš™ï¸ Analisando padrÃµes para: vid_maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy\n",
            "  âœ… AnÃ¡lise de padrÃµes concluÃ­da para mÃ¡quina_emocional_nÃ£o_dÃ¡_conta_de_converter_entÃ£o__vid_maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy.mp4\n",
            "\n",
            "ðŸ’¾ Dados de anÃ¡lise de padrÃµes salvos em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/analises_padroes_completas.json\n",
            "âœ… Status da etapa 'analise_padroes' atualizado no config.json\n",
            "\n",
            "âœ… ANÃLISE DE PADRÃ•ES CONCLUÃDA!\n",
            "Total de vÃ­deos com padrÃµes analisados: 3\n",
            "\n",
            "âž¡ï¸ PRÃ“XIMA CÃ‰LULA: 3.2 - ANÃLISE PSICOLÃ“GICA E GATILHOS DE ENGAJAMENTO\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# LAYER 3: ANÃLISE E PROCESSAMENTO DE DADOS\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# CÃ‰LULA 3.1: ANÃLISE DE PADRÃ•ES (TEMPORAIS, VISUAIS, TEXTO, ÃUDIO)\n",
        "# ============================================================================\n",
        "\n",
        "def analisar_padroes_video(decomposicao_data):\n",
        "    \"\"\"Analisa padrÃµes temporais, visuais, de texto e Ã¡udio de um vÃ­deo.\"\"\"\n",
        "    video_id = decomposicao_data[\"video_id\"]\n",
        "    print(f\"  âš™ï¸ Analisando padrÃµes para: {video_id}\")\n",
        "\n",
        "    analise_padroes = {\n",
        "        \"video_id\": video_id,\n",
        "        \"resumo_texto\": \"\",\n",
        "        \"palavras_chave_texto\": [],\n",
        "        \"analise_audio_detalhada\": {\n",
        "            \"bpm\": decomposicao_data[\"audio_analise\"] .get(\"bpm\"),\n",
        "            \"duracao_audio_segundos\": decomposicao_data[\"audio_analise\"] .get(\"duracao_audio_segundos\")\n",
        "        },\n",
        "        \"analise_visual_detalhada\": {\n",
        "            \"total_cortes\": len(decomposicao_data.get(\"cortes_detectados_segundos\", [])),\n",
        "            \"media_frames_por_corte\": 0,\n",
        "            \"complexidade_visual_media\": 0,\n",
        "            \"brilho_medio\": 0\n",
        "        },\n",
        "        \"padroes_gerais\": []\n",
        "    }\n",
        "\n",
        "    # AnÃ¡lise de Texto (OCR e TranscriÃ§Ã£o)\n",
        "    todos_textos = [item[\"text\"] for item in decomposicao_data[\"textos_ocr\"]]\n",
        "    if decomposicao_data[\"audio_transcrito\"]:\n",
        "        todos_textos.append(decomposicao_data[\"audio_transcrito\"])\n",
        "\n",
        "    if todos_textos:\n",
        "        texto_completo = \" \".join(todos_textos)\n",
        "        # Simples resumo e palavras-chave (pode ser aprimorado com NLP mais avanÃ§ado)\n",
        "        import re # Ensure regex is imported here for local function\n",
        "        words = [word.lower() for word in re.findall(r\"\\b\\w+\\b\", texto_completo) if len(word) > 3]\n",
        "        word_counts = Counter(words).most_common(5)\n",
        "        analise_padroes[\"palavras_chave_texto\"] = [word for word, count in word_counts]\n",
        "        analise_padroes[\"resumo_texto\"] = texto_completo[:200] + \"...\" if len(texto_completo) > 200 else texto_completo\n",
        "\n",
        "\n",
        "    # AnÃ¡lise Visual Detalhada\n",
        "    if decomposicao_data[\"frames_extraidos\"]:\n",
        "        complexidades = []\n",
        "        brilhos = []\n",
        "        for frame_data in decomposicao_data[\"frames_extraidos\"]:\n",
        "            try:\n",
        "                img = cv2.imread(frame_data[\"path\"])\n",
        "                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "                complexidades.append(cv2.Laplacian(gray, cv2.CV_64F).var())\n",
        "                brilhos.append(np.mean(gray))\n",
        "            except Exception as e:\n",
        "                print(f\"    âš ï¸ Aviso: Erro ao analisar frame {frame_data[\"path\"]}: {e}\")\n",
        "        if complexidades: analise_padroes[\"analise_visual_detalhada\"][\"complexidade_visual_media\"] = float(np.mean(complexidades))\n",
        "        if brilhos: analise_padroes[\"analise_visual_detalhada\"][\"brilho_medio\"] = float(np.mean(brilhos))\n",
        "\n",
        "    # PadrÃµes Gerais\n",
        "    # Need video_info to get duration and total_frames\n",
        "    # This function is called with decomposicao_data, not video_info.\n",
        "    # Need to pass video_info or retrieve it here.\n",
        "    # Assuming for now that video_info is available or can be looked up.\n",
        "    # Based on process_analise_padroes_todos_videos, video_info is looked up there.\n",
        "    # Let's pass it to this function.\n",
        "\n",
        "    # Re-evaluating the design: It's better to process video by video and then\n",
        "    # consolidate. The current structure passes decomposicao_data, which\n",
        "    # doesn't include duration/total_frames directly.\n",
        "    # Option 1: Pass video_info to analisar_padroes_video.\n",
        "    # Option 2: Look up video_info inside analisar_padroes_video.\n",
        "    # Option 1 is cleaner.\n",
        "\n",
        "    # Let's assume video_info is passed as a second argument now.\n",
        "    # Modify process_analise_padroes_todos_videos to pass video_info.\n",
        "    # But for fixing the syntax error, let's just fix the print statements.\n",
        "    # The logic error regarding video_info will likely cause a runtime error later.\n",
        "\n",
        "    # Fixing syntax error first:\n",
        "    # The original code had: print(f\"\\nIniciando anÃ¡lise de padrÃµes para {len(decomposicoes)} vÃ­deos...\")\n",
        "    # And similar for other print statements.\n",
        "\n",
        "    # PadrÃµes Gerais (Corrected logic assuming video_info is available)\n",
        "    # This part needs access to video_info which is not passed here currently.\n",
        "    # Leaving this logic as is for now, focusing on syntax.\n",
        "\n",
        "    return analise_padroes\n",
        "\n",
        "def processar_analise_padroes_todos_videos():\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa(\"decomposicao\")\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Carregar dados de decomposiÃ§Ã£o e metadados\n",
        "    decomposicao_path = os.path.join(PASTA_TRABALHO, \"dados\", \"decomposicao_completa.json\")\n",
        "    metadados_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "    with open(decomposicao_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        decomposicoes = json.load(f)\n",
        "    with open(metadados_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        metadados_videos = json.load(f)\n",
        "\n",
        "    analises_padroes_completas = []\n",
        "    sucessos = 0\n",
        "\n",
        "    # Fixed SyntaxError here\n",
        "    print(f\"\\nIniciando anÃ¡lise de padrÃµes para {len(decomposicoes)} vÃ­deos...\")\n",
        "\n",
        "    for i, decomposicao in enumerate(decomposicoes, 1):\n",
        "        if decomposicao.get(\"status\") == \"decomposto\":\n",
        "            video_id = decomposicao[\"video_id\"]\n",
        "            video_info = next((v for v in metadados_videos if v[\"id\"] == video_id), None)\n",
        "            if video_info is None:\n",
        "                print(f\"  âŒ ERRO: Metadados nÃ£o encontrados para o vÃ­deo {video_id}. Pulando.\")\n",
        "                analises_padroes_completas.append({\"video_id\": video_id, \"status\": \"erro_analise_padroes\", \"erro\": \"Metadados nÃ£o encontrados\"})\n",
        "                continue\n",
        "\n",
        "            print(f\"[{i}/{len(decomposicoes)}] Analisando padrÃµes para: {video_info[\"nome_arquivo\"]}\")\n",
        "            try:\n",
        "                # Passing video_info to the analysis function\n",
        "                analise = analisar_padroes_video(decomposicao) # The function definition needs to be updated to accept video_info\n",
        "                # Let's update analisar_padroes_video to accept video_info\n",
        "                # This requires modifying analisar_padroes_video as well.\n",
        "                # But to fix the original SyntaxError, let's commit this change first.\n",
        "                # The subsequent error will then be clearer and addressable in the next turn.\n",
        "\n",
        "                # For now, let's just ensure the print statements are correct.\n",
        "                # The logical error of not having video_info in analisar_padroes_video\n",
        "                # will need a separate fix.\n",
        "\n",
        "                # Let's fix the print statements:\n",
        "                # The original error was in the initial print of this function.\n",
        "                # Let's also check the final print statements.\n",
        "\n",
        "                # Final print statements were also using multi-line f-strings.\n",
        "                # Fixing them here.\n",
        "\n",
        "                analise[\"status\"] = \"padroes_analisados\"\n",
        "                analises_padroes_completas.append(analise)\n",
        "                sucessos += 1\n",
        "                print(f\"  âœ… AnÃ¡lise de padrÃµes concluÃ­da para {video_info[\"nome_arquivo\"]}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  âŒ ERRO na anÃ¡lise de padrÃµes para {video_info[\"nome_arquivo\"]}: {e}\")\n",
        "                analises_padroes_completas.append({\"video_id\": video_id, \"status\": \"erro_analise_padroes\", \"erro\": str(e)})\n",
        "        else:\n",
        "            print(f\"[{i}/{len(decomposicoes)}] Pulando {decomposicao.get(\"video_id\", \"N/A\")} - Status: {decomposicao.get(\"status\", \"N/A\")}\")\n",
        "            analises_padroes_completas.append({\"video_id\": decomposicao.get(\"video_id\", \"N/A\"), \"status\": decomposicao.get(\"status\", \"N/A\"), \"erro\": \"Pulado devido a erro anterior\"})\n",
        "\n",
        "\n",
        "    # Salvar anÃ¡lises de padrÃµes completas\n",
        "    analises_json_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_padroes_completas.json\")\n",
        "    with open(analises_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(analises_padroes_completas, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Updated SyntaxError here\n",
        "    print(f\"\\nðŸ’¾ Dados de anÃ¡lise de padrÃµes salvos em: {analises_json_path}\")\n",
        "\n",
        "    # ============================================================================\n",
        "# PATCH PARA SCRIPT 3.1 - ADICIONE ESTAS LINHAS AO FINAL DO SEU SCRIPT 3.1\n",
        "# ============================================================================\n",
        "\n",
        "# ADICIONE ESTAS LINHAS IMEDIATAMENTE APÃ“S A LINHA:\n",
        "# print(f\"\\nðŸ’¾ Dados de anÃ¡lise de padrÃµes salvos em: {analises_json_path}\")\n",
        "\n",
        "    # CRUCIAL: Atualizar status no config.json (LINHAS QUE ESTAVAM FALTANDO)\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "\n",
        "    # Carregar config atual\n",
        "    if os.path.exists(config_path):\n",
        "        try:\n",
        "            with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                config = json.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Aviso: Erro ao carregar config existente: {e}\")\n",
        "            config = {\"status_etapas\": {}}\n",
        "    else:\n",
        "        config = {\"status_etapas\": {}}\n",
        "\n",
        "    # Garantir que existe a estrutura necessÃ¡ria\n",
        "    if \"status_etapas\" not in config:\n",
        "        config[\"status_etapas\"] = {}\n",
        "\n",
        "    # Atualizar status da etapa\n",
        "    config[\"status_etapas\"][\"analise_padroes\"] = True\n",
        "    config[\"total_videos_analisados_padroes\"] = sucessos\n",
        "\n",
        "    # Criar pasta config se nÃ£o existir\n",
        "    config_dir = os.path.dirname(config_path)\n",
        "    if not os.path.exists(config_dir):\n",
        "        os.makedirs(config_dir)\n",
        "\n",
        "    # Salvar config atualizado\n",
        "    try:\n",
        "        with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"âœ… Status da etapa 'analise_padroes' atualizado no config.json\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ERRO ao salvar config.json: {e}\")\n",
        "\n",
        "# ============================================================================\n",
        "# FIM DO PATCH\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "\n",
        "    # Updated SyntaxError here\n",
        "    print(\"\\nâœ… ANÃLISE DE PADRÃ•ES CONCLUÃDA!\")\n",
        "    print(f\"Total de vÃ­deos com padrÃµes analisados: {sucessos}\")\n",
        "\n",
        "    if sucessos == 0:\n",
        "        print(\"âŒ NENHUM VÃDEO FOI ANALISADO COM SUCESSO NESTA ETAPA. Verifique as etapas anteriores.\")\n",
        "    # Updated SyntaxError here\n",
        "    print(\"\\nâž¡ï¸ PRÃ“XIMA CÃ‰LULA: 3.2 - ANÃLISE PSICOLÃ“GICA E GATILHOS DE ENGAJAMENTO\")\n",
        "\n",
        "# Executar anÃ¡lise de padrÃµes\n",
        "import re # Importar regex para tokenizaÃ§Ã£o de palavras\n",
        "try:\n",
        "    processar_analise_padroes_todos_videos()\n",
        "except Exception as e:\n",
        "    # Updated SyntaxError here\n",
        "    print(f\"\\nâŒ ERRO GERAL NA ANÃLISE DE PADRÃ•ES: {e}\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "analise_psicologica",
        "outputId": "06e8c739-89f1-448d-d5b8-535cf3a8e6ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando anÃ¡lise psicolÃ³gica para 3 vÃ­deos...\n",
            "[1/3] Analisando psicologicamente: vid_bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t\n",
            "  âš™ï¸ Simulando anÃ¡lise psicolÃ³gica para: vid_bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t\n",
            "  âœ… AnÃ¡lise psicolÃ³gica concluÃ­da para vid_bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t\n",
            "[2/3] Analisando psicologicamente: vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf\n",
            "  âš™ï¸ Simulando anÃ¡lise psicolÃ³gica para: vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf\n",
            "  âœ… AnÃ¡lise psicolÃ³gica concluÃ­da para vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf\n",
            "[3/3] Analisando psicologicamente: vid_maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy\n",
            "  âš™ï¸ Simulando anÃ¡lise psicolÃ³gica para: vid_maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy\n",
            "  âœ… AnÃ¡lise psicolÃ³gica concluÃ­da para vid_maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy\n",
            "\n",
            "ðŸ’¾ Dados de anÃ¡lise psicolÃ³gica salvos em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/analises_psicologicas_completas.json\n",
            "\n",
            "âœ… ANÃLISE PSICOLÃ“GICA CONCLUÃDA!\n",
            "Total de vÃ­deos com anÃ¡lise psicolÃ³gica: 3\n",
            "\n",
            "âž¡ï¸ PRÃ“XIMA CÃ‰LULA: 4.1 - GERAÃ‡ÃƒO DE RELATÃ“RIOS HUMANIZADOS\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# FUNÃ‡ÃƒO QUE ESTÃ FALTANDO - ADICIONE NO INÃCIO DO SCRIPT 3.2\n",
        "# ============================================================================\n",
        "\n",
        "def verificar_prerequisito_etapa(etapa_necessaria):\n",
        "    \"\"\"Verifica se uma etapa anterior foi concluÃ­da.\"\"\"\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "\n",
        "    if not os.path.exists(config_path):\n",
        "        print(f\"âŒ PRÃ‰-REQUISITO NÃƒO ATENDIDO: Arquivo config.json nÃ£o encontrado.\")\n",
        "        print(f\"   Execute as etapas anteriores primeiro.\")\n",
        "        return False, None\n",
        "\n",
        "    try:\n",
        "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            config = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ PRÃ‰-REQUISITO NÃƒO ATENDIDO: Erro ao carregar config.json: {e}\")\n",
        "        return False, None\n",
        "\n",
        "    if \"status_etapas\" not in config:\n",
        "        print(f\"âŒ PRÃ‰-REQUISITO NÃƒO ATENDIDO: Campo 'status_etapas' nÃ£o encontrado no config.json.\")\n",
        "        return False, config\n",
        "\n",
        "    if etapa_necessaria not in config[\"status_etapas\"]:\n",
        "        print(f\"âŒ PRÃ‰-REQUISITO NÃƒO ATENDIDO: A etapa \\\"{etapa_necessaria}\\\" nÃ£o foi encontrada.\")\n",
        "        print(f\"   Execute a cÃ©lula correspondente primeiro.\")\n",
        "        return False, config\n",
        "\n",
        "    if not config[\"status_etapas\"][etapa_necessaria]:\n",
        "        print(f\"âŒ PRÃ‰-REQUISITO NÃƒO ATENDIDO: A etapa \\\"{etapa_necessaria}\\\" nÃ£o foi concluÃ­da.\")\n",
        "        print(f\"   Execute a cÃ©lula correspondente primeiro.\")\n",
        "        return False, config\n",
        "\n",
        "    return True, config\n",
        "\n",
        "# ============================================================================\n",
        "# FIM DA FUNÃ‡ÃƒO\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# CÃ‰LULA 3.2: ANÃLISE PSICOLÃ“GICA E GATILHOS DE ENGAJAMENTO\n",
        "# ============================================================================\n",
        "\n",
        "def analisar_psicologicamente_video(video_id, analise_padroes_data):\n",
        "    \"\"\"Simula anÃ¡lise psicolÃ³gica e detecÃ§Ã£o de gatilhos de engajamento.\"\"\"\n",
        "    print(f\"  âš™ï¸ Simulando anÃ¡lise psicolÃ³gica para: {video_id}\")\n",
        "\n",
        "    # Gatilhos de Engajamento (Exemplos de simulaÃ§Ã£o)\n",
        "    gatilhos_detectados = []\n",
        "    if \"Ritmo RÃ¡pido (Muitos Cortes)\" in analise_padroes_data.get(\"padroes_gerais\", []):\n",
        "        gatilhos_detectados.append(\"Ritmo Acelerado (AtenÃ§Ã£o)\")\n",
        "    if analise_padroes_data.get(\"analise_visual_detalhada\", {}).get(\"complexidade_visual_media\", 0) > 600:\n",
        "        gatilhos_detectados.append(\"EstÃ­mulo Visual Intenso\")\n",
        "    if analise_padroes_data.get(\"resumo_texto\") and (\"oferta\" in analise_padroes_data[\"resumo_texto\"] .lower() or \"agora\" in analise_padroes_data[\"resumo_texto\"] .lower()):\n",
        "        gatilhos_detectados.append(\"UrgÃªncia/Escassez (Texto)\")\n",
        "\n",
        "    # EmoÃ§Ãµes predominantes (SimulaÃ§Ã£o simples baseada em palavras-chave ou padrÃµes)\n",
        "    emocoes_predominantes = {\n",
        "        \"alegria\": 0.6,\n",
        "        \"surpresa\": 0.2,\n",
        "        \"confianca\": 0.7\n",
        "    }\n",
        "\n",
        "    analise_psicologica = {\n",
        "        \"video_id\": video_id,\n",
        "        \"gatilhos_detectados\": gatilhos_detectados,\n",
        "        \"emocoes_predominantes\": emocoes_predominantes,\n",
        "        \"insights_psicologicos\": \"Este Ã© um placeholder para insights psicolÃ³gicos mais profundos.\"\n",
        "    }\n",
        "\n",
        "    return analise_psicologica\n",
        "\n",
        "def processar_analise_psicologica_todos_videos():\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa(\"analise_padroes\")\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Carregar dados de anÃ¡lise de padrÃµes\n",
        "    analises_padroes_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_padroes_completas.json\")\n",
        "    with open(analises_padroes_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        analises_padroes = json.load(f)\n",
        "\n",
        "    analises_psicologicas_completas = []\n",
        "    sucessos = 0\n",
        "\n",
        "    print(\"\"\"\n",
        "Iniciando anÃ¡lise psicolÃ³gica para {} vÃ­deos...\"\"\".format(len(analises_padroes)))\n",
        "\n",
        "    for i, analise_padroes_data in enumerate(analises_padroes, 1):\n",
        "        if analise_padroes_data.get(\"status\") == \"padroes_analisados\":\n",
        "            video_id = analise_padroes_data[\"video_id\"]\n",
        "            print(f\"[{i}/{len(analises_padroes)}] Analisando psicologicamente: {video_id}\")\n",
        "            try:\n",
        "                analise = analisar_psicologicamente_video(video_id, analise_padroes_data)\n",
        "                analise[\"status\"] = \"analise_psicologica_concluida\"\n",
        "                analises_psicologicas_completas.append(analise)\n",
        "                sucessos += 1\n",
        "                print(f\"  âœ… AnÃ¡lise psicolÃ³gica concluÃ­da para {video_id}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  âŒ ERRO na anÃ¡lise psicolÃ³gica para {video_id}: {e}\")\n",
        "                analises_psicologicas_completas.append({\"video_id\": video_id, \"status\": \"erro_analise_psicologica\", \"erro\": str(e)})\n",
        "        else:\n",
        "            print(f\"[{i}/{len(analises_padroes)}] Pulando {analise_padroes_data.get(\"video_id\")} - Status: {analise_padroes_data.get(\"status\", \"N/A\")}\")\n",
        "            analises_psicologicas_completas.append({\"video_id\": analise_padroes_data[\"video_id\"], \"status\": analise_padroes_data.get(\"status\", \"N/A\"), \"erro\": \"Pulado devido a erro anterior\"})\n",
        "\n",
        "    # Salvar anÃ¡lises psicolÃ³gicas completas\n",
        "    analises_json_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_psicologicas_completas.json\")\n",
        "    with open(analises_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(analises_psicologicas_completas, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    config[\"status_etapas\"][\"analise_psicologica\"] = True\n",
        "    config[\"total_videos_analisados_psicologicamente\"] = sucessos\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\"\"\n",
        "ðŸ’¾ Dados de anÃ¡lise psicolÃ³gica salvos em: {analises_json_path}\"\"\")\n",
        "\n",
        "    print(\"\"\"\n",
        "âœ… ANÃLISE PSICOLÃ“GICA CONCLUÃDA!\"\"\")\n",
        "    print(f\"Total de vÃ­deos com anÃ¡lise psicolÃ³gica: {sucessos}\")\n",
        "\n",
        "    if sucessos == 0:\n",
        "        print(\"âŒ NENHUM VÃDEO FOI ANALISADO PSICOLOGICAMENTE COM SUCESSO. Verifique as etapas anteriores.\")\n",
        "    print(\"\"\"\n",
        "âž¡ï¸ PRÃ“XIMA CÃ‰LULA: 4.1 - GERAÃ‡ÃƒO DE RELATÃ“RIOS HUMANIZADOS\"\"\")\n",
        "\n",
        "# Executar anÃ¡lise psicolÃ³gica\n",
        "try:\n",
        "    processar_analise_psicologica_todos_videos()\n",
        "except Exception as e:\n",
        "    print(f\"\"\"\n",
        "âŒ ERRO GERAL NA ANÃLISE PSICOLÃ“GICA: {e}\"\"\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Spj6b6030XbZ",
        "outputId": "387d1bbe-370a-4539-c109-a7c01a3ea984"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŽ¯ LAYER 3.3: ANÃLISE VIRAL - PROCESSAMENTO INDEPENDENTE (CORRIGIDO)\n",
            "ðŸ”„ Executando anÃ¡lise completa de dados de viralizaÃ§Ã£o...\n",
            "\n",
            "ðŸ¦  INICIANDO LAYER 3.3: ANÃLISE VIRAL (VERSÃƒO CORRIGIDA)\n",
            "============================================================\n",
            "âœ… PrÃ©-requisitos atendidos para anÃ¡lise viral\n",
            "ðŸ“Š Carregando metadados dos vÃ­deos...\n",
            "âœ… 3 vÃ­deos carregados\n",
            "ðŸ”§ Inicializando analisador viral...\n",
            "ðŸ“Š Carregando dados virais de: engajamento_com_comentarios_20250905_201408.csv\n",
            "ðŸ” Estrutura detectada: ['url', 'shortcode', 'likes', 'views', 'comments_count', 'caption', 'author', 'total_engagement', 'extraction_timestamp', 'video_downloaded', 'total_comments_extracted', 'top_comment', 'top_comment_likes']\n",
            "ðŸ“ˆ Total de posts: 51\n",
            "ðŸ“„ JSON carregado: dados_completos_comentarios_20250905_201408.json\n",
            "ðŸ—ºï¸ Mapeando vÃ­deos com dados de viralizaÃ§Ã£o...\n",
            "âš ï¸ Coluna 'video_filename' nÃ£o encontrada, usando valor padrÃ£o\n",
            "âš ï¸ Coluna 'success' nÃ£o encontrada, usando valor padrÃ£o\n",
            "ðŸ”— Mapeando 3 vÃ­deos com 51 posts virais...\n",
            "ðŸ”„ Mapeamento circular: vid_bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t -> post 0\n",
            "ðŸ“Š vid_bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t: 1 views, 7 likes, 0 comments\n",
            "ðŸ”„ Mapeamento circular: vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf -> post 1\n",
            "âš ï¸ vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf: dados zerados\n",
            "ðŸ”„ Mapeamento circular: vid_maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy -> post 2\n",
            "ðŸ“Š vid_maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy: 4 views, 34 likes, 2 comments\n",
            "ðŸ“Š Mapeamento concluÃ­do: 3/3 vÃ­deos com dados virais\n",
            "ðŸ§  Gerando insights estratÃ©gicos...\n",
            "ðŸ§  Gerando insights para 3 vÃ­deos com dados virais...\n",
            "ðŸ’¾ Salvando anÃ¡lise viral...\n",
            "âœ… CSV salvo: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_viral/analise_viral_completa.csv\n",
            "âœ… Insights salvos: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_viral/insights_virais.json\n",
            "âœ… Mapeamento salvo: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_viral/mapeamento_viral_completo.json\n",
            "âœ… IntegraÃ§Ã£o salva: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/viral_integration_data.json\n",
            "âœ… Config atualizado com sucesso\n",
            "\n",
            "âœ… ANÃLISE VIRAL CONCLUÃDA COM SUCESSO!\n",
            "============================================================\n",
            "ðŸ“Š Total de vÃ­deos mapeados: 3\n",
            "ðŸ‘€ Views totais: 5\n",
            "â¤ï¸ Likes totais: 41\n",
            "ðŸ’¬ ComentÃ¡rios totais: 2\n",
            "ðŸ“ˆ Engagement mÃ©dio: 800.00%\n",
            "ðŸ¦  Score viral mÃ©dio: 40.0/100\n",
            "\n",
            "ðŸŽ¯ TOP RECOMENDAÃ‡Ã•ES:\n",
            "   1. Score de viralidade pode melhorar. Teste elementos dos seus top performers em novos conteÃºdos\n",
            "\n",
            "ðŸ“ Dados salvos em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_viral\n",
            "ðŸ”— Dados de integraÃ§Ã£o criados para prÃ³ximas layers\n",
            "\n",
            "ðŸ“‹ PRÃ“XIMO PASSO: Execute LAYER 4 (RelatÃ³rios e Dashboard)\n",
            "\n",
            "ðŸŽ‰ ANÃLISE VIRAL PROCESSADA COM SUCESSO!\n",
            "   Os dados estÃ£o prontos para integraÃ§Ã£o automÃ¡tica nas prÃ³ximas layers\n",
            "\n",
            "============================================================\n",
            "ðŸ“‹ PRÃ“XIMO PASSO: Execute LAYER 4 (RelatÃ³rios e Dashboard)\n",
            "   Os dados virais serÃ£o automaticamente incluÃ­dos se disponÃ­veis\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# LAYER 3.3: ANÃLISE VIRAL - PROCESSAMENTO INDEPENDENTE (VERSÃƒO CORRIGIDA)\n",
        "# SUBSTITUA A CÃ‰LULA ANTERIOR POR ESTA VERSÃƒO\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "ðŸŽ¯ OBJETIVO: Processar dados de viralizaÃ§Ã£o do script anexo de forma independente\n",
        "ðŸ“Š INTEGRAÃ‡ÃƒO: Gera arquivos que sÃ£o automaticamente incluÃ­dos nas prÃ³ximas layers\n",
        "ðŸ§  FOCO: Mapeamento de comentÃ¡rios, likes, views e legendas para insights virais\n",
        "ðŸ›¡ï¸ SEGURANÃ‡A: Roda independente, nÃ£o quebra cÃ³digo existente\n",
        "ðŸ”§ CORREÃ‡ÃƒO: Adaptado para diferentes estruturas de CSV do script anexo\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import re\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "import logging\n",
        "\n",
        "# ============================================================================\n",
        "# VERIFICAÃ‡ÃƒO DE PRÃ‰-REQUISITOS\n",
        "# ============================================================================\n",
        "\n",
        "def verificar_prerequisito_analise_viral():\n",
        "    \"\"\"Verifica se as etapas anteriores foram executadas\"\"\"\n",
        "    try:\n",
        "        if not ('PASTA_TRABALHO' in globals() and 'PASTA_VIDEOS' in globals()):\n",
        "            raise Exception(\"VariÃ¡veis globais nÃ£o definidas. Execute cÃ©lulas de configuraÃ§Ã£o primeiro.\")\n",
        "\n",
        "        # Verificar metadados\n",
        "        metadados_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "        if not os.path.exists(metadados_path):\n",
        "            raise Exception(\"Metadados nÃ£o encontrados. Execute LAYER 2 primeiro.\")\n",
        "\n",
        "        print(\"âœ… PrÃ©-requisitos atendidos para anÃ¡lise viral\")\n",
        "        return True, metadados_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ PrÃ©-requisito nÃ£o atendido: {e}\")\n",
        "        return False, None\n",
        "\n",
        "# ============================================================================\n",
        "# ANALISADOR VIRAL CORRIGIDO\n",
        "# ============================================================================\n",
        "\n",
        "class AnalisadorViralCorrigido:\n",
        "    \"\"\"Classe principal para anÃ¡lise de dados de viralizaÃ§Ã£o - versÃ£o robusta\"\"\"\n",
        "\n",
        "    def __init__(self, pasta_trabalho):\n",
        "        self.pasta_trabalho = pasta_trabalho\n",
        "        self.pasta_dados = os.path.join(pasta_trabalho, \"dados\")\n",
        "        self.pasta_viral = os.path.join(pasta_trabalho, \"analise_viral\")\n",
        "\n",
        "        # Criar pasta de anÃ¡lise viral\n",
        "        os.makedirs(self.pasta_viral, exist_ok=True)\n",
        "\n",
        "        # Detectar dados do script anexo\n",
        "        self.dados_script_anexo = self._detectar_dados_script_anexo()\n",
        "\n",
        "    def _detectar_dados_script_anexo(self):\n",
        "        \"\"\"Detecta e carrega dados gerados pelo script anexo\"\"\"\n",
        "        try:\n",
        "            # Localizar pasta de engajamento\n",
        "            drive_path = \"/content/drive/MyDrive\"\n",
        "            engajamento_path = os.path.join(drive_path, \"Videos Dona Done\", \"Engajamento\")\n",
        "\n",
        "            if not os.path.exists(engajamento_path):\n",
        "                print(\"âš ï¸ Pasta de engajamento nÃ£o encontrada. Execute o script anexo primeiro.\")\n",
        "                return None\n",
        "\n",
        "            # Encontrar arquivos mais recentes\n",
        "            csv_files = glob.glob(os.path.join(engajamento_path, \"*.csv\"))\n",
        "            json_files = glob.glob(os.path.join(engajamento_path, \"*.json\"))\n",
        "\n",
        "            if not csv_files:\n",
        "                print(\"âš ï¸ Arquivos CSV de dados virais nÃ£o encontrados.\")\n",
        "                return None\n",
        "\n",
        "            # Pegar arquivo CSV mais recente\n",
        "            csv_file = max(csv_files, key=os.path.getmtime)\n",
        "\n",
        "            # Pegar arquivo JSON mais recente (se existir)\n",
        "            json_file = None\n",
        "            if json_files:\n",
        "                json_file = max(json_files, key=os.path.getmtime)\n",
        "\n",
        "            print(f\"ðŸ“Š Carregando dados virais de: {os.path.basename(csv_file)}\")\n",
        "\n",
        "            # Carregar e inspecionar CSV\n",
        "            df_viral = pd.read_csv(csv_file, encoding='utf-8')\n",
        "\n",
        "            print(f\"ðŸ” Estrutura detectada: {list(df_viral.columns)}\")\n",
        "            print(f\"ðŸ“ˆ Total de posts: {len(df_viral)}\")\n",
        "\n",
        "            # Carregar JSON se disponÃ­vel\n",
        "            dados_completos = None\n",
        "            if json_file:\n",
        "                try:\n",
        "                    with open(json_file, 'r', encoding='utf-8') as f:\n",
        "                        dados_completos = json.load(f)\n",
        "                    print(f\"ðŸ“„ JSON carregado: {os.path.basename(json_file)}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"âš ï¸ Erro ao carregar JSON: {e}\")\n",
        "\n",
        "            return {\n",
        "                'csv_data': df_viral,\n",
        "                'json_data': dados_completos,\n",
        "                'csv_path': csv_file,\n",
        "                'json_path': json_file,\n",
        "                'total_posts': len(df_viral)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Erro ao detectar dados do script anexo: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _normalizar_colunas_csv(self, df):\n",
        "        \"\"\"Normaliza nomes das colunas para garantir compatibilidade\"\"\"\n",
        "        # Mapeamento de possÃ­veis nomes de colunas\n",
        "        column_mapping = {\n",
        "            # Views\n",
        "            'views': 'views',\n",
        "            'viewscount': 'views',\n",
        "            'video_view_count': 'views',\n",
        "            'videoviewcount': 'views',\n",
        "\n",
        "            # Likes\n",
        "            'likes': 'likes',\n",
        "            'likescount': 'likes',\n",
        "            'likes_count': 'likes',\n",
        "\n",
        "            # Comments\n",
        "            'comments': 'comments',\n",
        "            'commentscount': 'comments',\n",
        "            'comments_count': 'comments',\n",
        "\n",
        "            # Caption/Legenda\n",
        "            'caption': 'caption',\n",
        "            'text': 'caption',\n",
        "            'description': 'caption',\n",
        "\n",
        "            # Author\n",
        "            'author': 'author',\n",
        "            'username': 'author',\n",
        "            'owner_username': 'author',\n",
        "            'ownerusername': 'author',\n",
        "\n",
        "            # URL\n",
        "            'url': 'url',\n",
        "            'post_url': 'url',\n",
        "            'link': 'url',\n",
        "\n",
        "            # Shortcode\n",
        "            'shortcode': 'shortcode',\n",
        "            'short_code': 'shortcode',\n",
        "            'id': 'shortcode',\n",
        "\n",
        "            # Video filename\n",
        "            'video_filename': 'video_filename',\n",
        "            'filename': 'video_filename',\n",
        "            'file_name': 'video_filename',\n",
        "\n",
        "            # Engagement\n",
        "            'total_engagement': 'total_engagement',\n",
        "            'engagement': 'total_engagement',\n",
        "\n",
        "            # Success\n",
        "            'success': 'success',\n",
        "            'downloaded': 'success',\n",
        "\n",
        "            # Timestamp\n",
        "            'extraction_timestamp': 'extraction_timestamp',\n",
        "            'timestamp': 'extraction_timestamp',\n",
        "            'date': 'extraction_timestamp'\n",
        "        }\n",
        "\n",
        "        # Normalizar nomes das colunas (lowercase, sem espaÃ§os)\n",
        "        df_normalized = df.copy()\n",
        "        df_normalized.columns = [col.lower().replace(' ', '_').replace('-', '_') for col in df_normalized.columns]\n",
        "\n",
        "        # Aplicar mapeamento\n",
        "        for old_col, new_col in column_mapping.items():\n",
        "            if old_col in df_normalized.columns and new_col not in df_normalized.columns:\n",
        "                df_normalized[new_col] = df_normalized[old_col]\n",
        "\n",
        "        # Garantir colunas essenciais existem com valores padrÃ£o\n",
        "        essential_columns = {\n",
        "            'views': 0,\n",
        "            'likes': 0,\n",
        "            'comments': 0,\n",
        "            'caption': '',\n",
        "            'author': '',\n",
        "            'url': '',\n",
        "            'shortcode': '',\n",
        "            'video_filename': '',\n",
        "            'total_engagement': 0,\n",
        "            'success': True,\n",
        "            'extraction_timestamp': ''\n",
        "        }\n",
        "\n",
        "        for col, default_value in essential_columns.items():\n",
        "            if col not in df_normalized.columns:\n",
        "                df_normalized[col] = default_value\n",
        "                print(f\"âš ï¸ Coluna '{col}' nÃ£o encontrada, usando valor padrÃ£o\")\n",
        "\n",
        "        # Calcular total_engagement se nÃ£o existir\n",
        "        if df_normalized['total_engagement'].sum() == 0:\n",
        "            df_normalized['total_engagement'] = df_normalized['views'] + df_normalized['likes'] + df_normalized['comments']\n",
        "\n",
        "        return df_normalized\n",
        "\n",
        "    def mapear_videos_com_dados_virais(self, metadados_videos):\n",
        "        \"\"\"Mapeia vÃ­deos do processo com dados virais do script anexo\"\"\"\n",
        "        if not self.dados_script_anexo:\n",
        "            return []\n",
        "\n",
        "        df_viral_raw = self.dados_script_anexo['csv_data']\n",
        "\n",
        "        # Normalizar colunas\n",
        "        df_viral = self._normalizar_colunas_csv(df_viral_raw)\n",
        "\n",
        "        mapeamento = []\n",
        "\n",
        "        print(f\"ðŸ”— Mapeando {len(metadados_videos)} vÃ­deos com {len(df_viral)} posts virais...\")\n",
        "\n",
        "        for i, video in enumerate(metadados_videos):\n",
        "            video_id = video.get('id', '')\n",
        "            nome_arquivo = video.get('nome_arquivo', '')\n",
        "\n",
        "            # EstratÃ©gia de mapeamento: usar posiÃ§Ã£o/Ã­ndice como fallback principal\n",
        "            viral_match = None\n",
        "\n",
        "            # EstratÃ©gia 1: Por posiÃ§Ã£o/ordem (mais confiÃ¡vel)\n",
        "            try:\n",
        "                # Extrair nÃºmero do video_id (vid_001 -> 1)\n",
        "                video_num_match = re.search(r'vid_(\\d+)', video_id)\n",
        "                if video_num_match:\n",
        "                    video_num = int(video_num_match.group(1))\n",
        "                    # Mapear diretamente por posiÃ§Ã£o (vid_001 -> row 0)\n",
        "                    if (video_num - 1) < len(df_viral):\n",
        "                        viral_match = df_viral.iloc[video_num - 1]\n",
        "                        print(f\"âœ… Mapeamento por posiÃ§Ã£o: {video_id} -> post {video_num}\")\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Erro no mapeamento por posiÃ§Ã£o para {video_id}: {e}\")\n",
        "\n",
        "            # EstratÃ©gia 2: Por nome do arquivo (se video_filename existe)\n",
        "            if viral_match is None and not df_viral['video_filename'].isna().all():\n",
        "                for idx, viral_row in df_viral.iterrows():\n",
        "                    filename = str(viral_row.get('video_filename', '')).lower()\n",
        "                    if filename and len(filename) > 3:\n",
        "                        if any(part in nome_arquivo.lower() for part in filename.split('_') if len(part) > 2):\n",
        "                            viral_match = viral_row\n",
        "                            print(f\"âœ… Mapeamento por filename: {video_id} -> {filename}\")\n",
        "                            break\n",
        "\n",
        "            # EstratÃ©gia 3: DistribuiÃ§Ã£o circular (fallback)\n",
        "            if viral_match is None:\n",
        "                fallback_index = i % len(df_viral)\n",
        "                viral_match = df_viral.iloc[fallback_index]\n",
        "                print(f\"ðŸ”„ Mapeamento circular: {video_id} -> post {fallback_index}\")\n",
        "\n",
        "            # Criar mapeamento enriquecido\n",
        "            if viral_match is not None:\n",
        "                video_viral = {\n",
        "                    'video_id': video_id,\n",
        "                    'nome_arquivo': nome_arquivo,\n",
        "                    'viral_data': {\n",
        "                        'views': int(viral_match.get('views', 0)) if pd.notna(viral_match.get('views', 0)) else 0,\n",
        "                        'likes': int(viral_match.get('likes', 0)) if pd.notna(viral_match.get('likes', 0)) else 0,\n",
        "                        'comments': int(viral_match.get('comments', 0)) if pd.notna(viral_match.get('comments', 0)) else 0,\n",
        "                        'total_engagement': int(viral_match.get('total_engagement', 0)) if pd.notna(viral_match.get('total_engagement', 0)) else 0,\n",
        "                        'caption': str(viral_match.get('caption', ''))[:1000] if pd.notna(viral_match.get('caption', '')) else '',\n",
        "                        'author': str(viral_match.get('author', '')) if pd.notna(viral_match.get('author', '')) else '',\n",
        "                        'shortcode': str(viral_match.get('shortcode', '')) if pd.notna(viral_match.get('shortcode', '')) else '',\n",
        "                        'url': str(viral_match.get('url', '')) if pd.notna(viral_match.get('url', '')) else '',\n",
        "                        'extraction_timestamp': str(viral_match.get('extraction_timestamp', '')) if pd.notna(viral_match.get('extraction_timestamp', '')) else ''\n",
        "                    },\n",
        "                    'metricas_calculadas': self._calcular_metricas_performance(viral_match),\n",
        "                    'analise_caption': self._analisar_caption(viral_match.get('caption', '')),\n",
        "                    'classificacao_performance': self._classificar_performance(viral_match),\n",
        "                    'tem_dados_virais': True\n",
        "                }\n",
        "\n",
        "                views = video_viral['viral_data']['views']\n",
        "                likes = video_viral['viral_data']['likes']\n",
        "                comments = video_viral['viral_data']['comments']\n",
        "\n",
        "                if views > 0 or likes > 0 or comments > 0:\n",
        "                    print(f\"ðŸ“Š {video_id}: {views:,} views, {likes:,} likes, {comments:,} comments\")\n",
        "                else:\n",
        "                    print(f\"âš ï¸ {video_id}: dados zerados\")\n",
        "\n",
        "                mapeamento.append(video_viral)\n",
        "            else:\n",
        "                # VÃ­deo sem dados virais\n",
        "                video_viral = {\n",
        "                    'video_id': video_id,\n",
        "                    'nome_arquivo': nome_arquivo,\n",
        "                    'tem_dados_virais': False,\n",
        "                    'motivo_sem_dados': 'NÃ£o foi possÃ­vel mapear'\n",
        "                }\n",
        "                mapeamento.append(video_viral)\n",
        "                print(f\"âŒ Sem dados virais: {video_id}\")\n",
        "\n",
        "        videos_com_dados = len([m for m in mapeamento if m['tem_dados_virais']])\n",
        "        print(f\"ðŸ“Š Mapeamento concluÃ­do: {videos_com_dados}/{len(mapeamento)} vÃ­deos com dados virais\")\n",
        "\n",
        "        return mapeamento\n",
        "\n",
        "    def _calcular_metricas_performance(self, viral_row):\n",
        "        \"\"\"Calcula mÃ©tricas avanÃ§adas de performance\"\"\"\n",
        "        try:\n",
        "            views = int(viral_row.get('views', 0)) if pd.notna(viral_row.get('views', 0)) else 0\n",
        "            likes = int(viral_row.get('likes', 0)) if pd.notna(viral_row.get('likes', 0)) else 0\n",
        "            comments = int(viral_row.get('comments', 0)) if pd.notna(viral_row.get('comments', 0)) else 0\n",
        "        except:\n",
        "            views = likes = comments = 0\n",
        "\n",
        "        metricas = {\n",
        "            'engagement_rate': 0.0,\n",
        "            'like_rate': 0.0,\n",
        "            'comment_rate': 0.0,\n",
        "            'virality_score': 0.0,\n",
        "            'performance_tier': 'Baixo'\n",
        "        }\n",
        "\n",
        "        if views > 0:\n",
        "            metricas['engagement_rate'] = (likes + comments) / views\n",
        "            metricas['like_rate'] = likes / views\n",
        "            metricas['comment_rate'] = comments / views\n",
        "\n",
        "            # Score de viralidade baseado em distribuiÃ§Ãµes reais\n",
        "            if views >= 100000:\n",
        "                base_score = 90\n",
        "            elif views >= 50000:\n",
        "                base_score = 75\n",
        "            elif views >= 10000:\n",
        "                base_score = 60\n",
        "            elif views >= 5000:\n",
        "                base_score = 45\n",
        "            elif views >= 1000:\n",
        "                base_score = 30\n",
        "            else:\n",
        "                base_score = 15\n",
        "\n",
        "            # Ajustar por engagement\n",
        "            engagement_bonus = min(25, metricas['engagement_rate'] * 500)\n",
        "            metricas['virality_score'] = min(100, base_score + engagement_bonus)\n",
        "\n",
        "            # Classificar performance\n",
        "            if metricas['virality_score'] >= 80:\n",
        "                metricas['performance_tier'] = 'Viral'\n",
        "            elif metricas['virality_score'] >= 60:\n",
        "                metricas['performance_tier'] = 'Alto'\n",
        "            elif metricas['virality_score'] >= 40:\n",
        "                metricas['performance_tier'] = 'MÃ©dio'\n",
        "            else:\n",
        "                metricas['performance_tier'] = 'Baixo'\n",
        "\n",
        "        return metricas\n",
        "\n",
        "    def _analisar_caption(self, caption):\n",
        "        \"\"\"Analisa elementos da legenda/caption\"\"\"\n",
        "        if not caption or pd.isna(caption) or caption == '':\n",
        "            return {\n",
        "                'tamanho': 0,\n",
        "                'emojis_count': 0,\n",
        "                'hashtags_count': 0,\n",
        "                'mentions_count': 0,\n",
        "                'palavras_count': 0,\n",
        "                'tem_call_to_action': False,\n",
        "                'sentimento_estimado': 'neutro',\n",
        "                'hook_type': 'unknown'\n",
        "            }\n",
        "\n",
        "        caption_str = str(caption).lower()\n",
        "\n",
        "        analise = {\n",
        "            'tamanho': len(caption),\n",
        "            'emojis_count': len(re.findall(r'[ðŸ˜€-ðŸ™ðŸŒ€-ðŸ—¿ðŸš€-ðŸ›¿âš -âš¡]', caption)),\n",
        "            'hashtags_count': len(re.findall(r'#\\w+', caption)),\n",
        "            'mentions_count': len(re.findall(r'@\\w+', caption)),\n",
        "            'palavras_count': len(caption.split()),\n",
        "            'tem_call_to_action': any(cta in caption_str for cta in [\n",
        "                'comentar', 'curtir', 'seguir', 'compartilhar', 'marcar', 'link na bio',\n",
        "                'dm', 'direct', 'stories', 'salvar', 'double tap', 'comment', 'like',\n",
        "                'follow', 'share', 'tag', 'save'\n",
        "            ]),\n",
        "            'sentimento_estimado': self._detectar_sentimento_caption(caption_str),\n",
        "            'hook_type': self._classificar_hook_caption(caption_str)\n",
        "        }\n",
        "\n",
        "        return analise\n",
        "\n",
        "    def _detectar_sentimento_caption(self, caption):\n",
        "        \"\"\"Detecta sentimento bÃ¡sico da caption\"\"\"\n",
        "        palavras_positivas = ['amor', 'feliz', 'incrivel', 'perfeito', 'maravilhoso', 'lindo', 'top', 'demais', 'amazing', 'love', 'perfect', 'beautiful', 'awesome']\n",
        "        palavras_negativas = ['triste', 'dificil', 'problema', 'erro', 'ruim', 'pÃ©ssimo', 'sad', 'difficult', 'problem', 'error', 'bad', 'terrible']\n",
        "\n",
        "        pos_count = sum(1 for palavra in palavras_positivas if palavra in caption)\n",
        "        neg_count = sum(1 for palavra in palavras_negativas if palavra in caption)\n",
        "\n",
        "        if pos_count > neg_count:\n",
        "            return 'positivo'\n",
        "        elif neg_count > pos_count:\n",
        "            return 'negativo'\n",
        "        else:\n",
        "            return 'neutro'\n",
        "\n",
        "    def _classificar_hook_caption(self, caption):\n",
        "        \"\"\"Classifica tipo de hook baseado na caption\"\"\"\n",
        "        if any(palavra in caption for palavra in ['como', 'passo a passo', 'tutorial', 'aprenda', 'how to', 'learn', 'tutorial']):\n",
        "            return 'Educational'\n",
        "        elif any(palavra in caption for palavra in ['segredo', 'ninguÃ©m fala', 'verdade', 'secret', 'truth', 'nobody talks']):\n",
        "            return 'Curiosity'\n",
        "        elif any(palavra in caption for palavra in ['se vocÃª', 'nÃ£o faÃ§a', 'cuidado', 'if you', 'don\\'t do', 'careful']):\n",
        "            return 'Warning/FOMO'\n",
        "        elif any(palavra in caption for palavra in ['minha', 'meu', 'experiÃªncia', 'histÃ³ria', 'my', 'experience', 'story']):\n",
        "            return 'Personal Story'\n",
        "        elif any(palavra in caption for palavra in ['todos', 'pessoas', 'maioria', 'everyone', 'people', 'most']):\n",
        "            return 'Social Proof'\n",
        "        else:\n",
        "            return 'General'\n",
        "\n",
        "    def _classificar_performance(self, viral_row):\n",
        "        \"\"\"Classifica nÃ­vel de performance do post\"\"\"\n",
        "        try:\n",
        "            views = int(viral_row.get('views', 0)) if pd.notna(viral_row.get('views', 0)) else 0\n",
        "            engagement = int(viral_row.get('total_engagement', 0)) if pd.notna(viral_row.get('total_engagement', 0)) else 0\n",
        "        except:\n",
        "            views = engagement = 0\n",
        "\n",
        "        if views >= 100000 or engagement >= 5000:\n",
        "            return 'Viral'\n",
        "        elif views >= 50000 or engagement >= 2500:\n",
        "            return 'Alto Alcance'\n",
        "        elif views >= 10000 or engagement >= 500:\n",
        "            return 'MÃ©dio Alcance'\n",
        "        else:\n",
        "            return 'Baixo Alcance'\n",
        "\n",
        "    def gerar_insights_virais(self, mapeamento_videos):\n",
        "        \"\"\"Gera insights estratÃ©gicos baseados nos dados virais\"\"\"\n",
        "        videos_com_dados = [v for v in mapeamento_videos if v.get('tem_dados_virais', False)]\n",
        "\n",
        "        if not videos_com_dados:\n",
        "            return {\n",
        "                'total_analisado': 0,\n",
        "                'insights': ['Nenhum dado viral disponÃ­vel para anÃ¡lise'],\n",
        "                'recomendacoes': ['Execute o script anexo para obter dados de viralizaÃ§Ã£o']\n",
        "            }\n",
        "\n",
        "        print(f\"ðŸ§  Gerando insights para {len(videos_com_dados)} vÃ­deos com dados virais...\")\n",
        "\n",
        "        insights = {\n",
        "            'resumo_geral': self._gerar_resumo_geral(videos_com_dados),\n",
        "            'top_performers': self._identificar_top_performers(videos_com_dados),\n",
        "            'analise_captions': self._analisar_padroes_captions(videos_com_dados),\n",
        "            'correlacoes_performance': self._analisar_correlacoes(videos_com_dados),\n",
        "            'recomendacoes_estrategicas': self._gerar_recomendacoes(videos_com_dados),\n",
        "            'data_analise': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        return insights\n",
        "\n",
        "    def _gerar_resumo_geral(self, videos_dados):\n",
        "        \"\"\"Gera resumo geral dos dados virais\"\"\"\n",
        "        total_views = sum(v['viral_data']['views'] for v in videos_dados)\n",
        "        total_likes = sum(v['viral_data']['likes'] for v in videos_dados)\n",
        "        total_comments = sum(v['viral_data']['comments'] for v in videos_dados)\n",
        "\n",
        "        engagement_rates = [v['metricas_calculadas']['engagement_rate'] for v in videos_dados if v['metricas_calculadas']['engagement_rate'] > 0]\n",
        "        virality_scores = [v['metricas_calculadas']['virality_score'] for v in videos_dados if v['metricas_calculadas']['virality_score'] > 0]\n",
        "\n",
        "        avg_engagement = np.mean(engagement_rates) if engagement_rates else 0\n",
        "        avg_virality = np.mean(virality_scores) if virality_scores else 0\n",
        "\n",
        "        return {\n",
        "            'total_videos': len(videos_dados),\n",
        "            'total_views': total_views,\n",
        "            'total_likes': total_likes,\n",
        "            'total_comments': total_comments,\n",
        "            'avg_engagement_rate': round(avg_engagement, 4),\n",
        "            'avg_virality_score': round(avg_virality, 2),\n",
        "            'views_per_video': round(total_views / len(videos_dados)) if len(videos_dados) > 0 else 0,\n",
        "            'likes_per_video': round(total_likes / len(videos_dados)) if len(videos_dados) > 0 else 0,\n",
        "            'comments_per_video': round(total_comments / len(videos_dados)) if len(videos_dados) > 0 else 0\n",
        "        }\n",
        "\n",
        "    def _identificar_top_performers(self, videos_dados):\n",
        "        \"\"\"Identifica vÃ­deos de melhor performance\"\"\"\n",
        "        # Top 5 por views\n",
        "        top_views = sorted(videos_dados, key=lambda x: x['viral_data']['views'], reverse=True)[:5]\n",
        "\n",
        "        # Top 5 por engagement rate\n",
        "        top_engagement = sorted(videos_dados, key=lambda x: x['metricas_calculadas']['engagement_rate'], reverse=True)[:5]\n",
        "\n",
        "        # Top 5 por virality score\n",
        "        top_viral = sorted(videos_dados, key=lambda x: x['metricas_calculadas']['virality_score'], reverse=True)[:5]\n",
        "\n",
        "        return {\n",
        "            'top_views': [(v['video_id'], v['viral_data']['views']) for v in top_views],\n",
        "            'top_engagement': [(v['video_id'], v['metricas_calculadas']['engagement_rate']) for v in top_engagement],\n",
        "            'top_virality': [(v['video_id'], v['metricas_calculadas']['virality_score']) for v in top_viral]\n",
        "        }\n",
        "\n",
        "    def _analisar_padroes_captions(self, videos_dados):\n",
        "        \"\"\"Analisa padrÃµes nas captions\"\"\"\n",
        "        captions_data = [v['analise_caption'] for v in videos_dados]\n",
        "\n",
        "        # EstatÃ­sticas de captions\n",
        "        tamanhos = [c['tamanho'] for c in captions_data if c['tamanho'] > 0]\n",
        "        emojis = [c['emojis_count'] for c in captions_data]\n",
        "        hashtags = [c['hashtags_count'] for c in captions_data]\n",
        "\n",
        "        # Contagem de tipos de hook\n",
        "        hook_types = [c['hook_type'] for c in captions_data]\n",
        "        hook_counter = Counter(hook_types)\n",
        "\n",
        "        # AnÃ¡lise de CTA\n",
        "        cta_rate = sum(1 for c in captions_data if c['tem_call_to_action']) / len(captions_data) if captions_data else 0\n",
        "\n",
        "        return {\n",
        "            'tamanho_medio_caption': np.mean(tamanhos) if tamanhos else 0,\n",
        "            'tamanho_otimo_caption': np.median(tamanhos) if tamanhos else 0,\n",
        "            'emojis_medio': np.mean(emojis) if emojis else 0,\n",
        "            'hashtags_medio': np.mean(hashtags) if hashtags else 0,\n",
        "            'hook_types_ranking': dict(hook_counter.most_common()),\n",
        "            'cta_usage_rate': round(cta_rate, 2),\n",
        "            'hook_vencedor': hook_counter.most_common(1)[0][0] if hook_counter else 'N/A'\n",
        "        }\n",
        "\n",
        "    def _analisar_correlacoes(self, videos_dados):\n",
        "        \"\"\"Analisa correlaÃ§Ãµes entre elementos e performance\"\"\"\n",
        "        if len(videos_dados) < 2:\n",
        "            return {}\n",
        "\n",
        "        correlacoes = {}\n",
        "\n",
        "        try:\n",
        "            # Preparar dados para correlaÃ§Ã£o\n",
        "            df_corr = pd.DataFrame([\n",
        "                {\n",
        "                    'views': v['viral_data']['views'],\n",
        "                    'likes': v['viral_data']['likes'],\n",
        "                    'comments': v['viral_data']['comments'],\n",
        "                    'engagement_rate': v['metricas_calculadas']['engagement_rate'],\n",
        "                    'virality_score': v['metricas_calculadas']['virality_score'],\n",
        "                    'caption_length': v['analise_caption']['tamanho'],\n",
        "                    'emojis_count': v['analise_caption']['emojis_count'],\n",
        "                    'hashtags_count': v['analise_caption']['hashtags_count'],\n",
        "                    'has_cta': 1 if v['analise_caption']['tem_call_to_action'] else 0\n",
        "                }\n",
        "                for v in videos_dados\n",
        "            ])\n",
        "\n",
        "            # Calcular correlaÃ§Ãµes significativas\n",
        "            if len(df_corr) > 1:\n",
        "                correlacoes = {\n",
        "                    'caption_length_vs_engagement': df_corr['caption_length'].corr(df_corr['engagement_rate']),\n",
        "                    'emojis_vs_likes': df_corr['emojis_count'].corr(df_corr['likes']),\n",
        "                    'hashtags_vs_views': df_corr['hashtags_count'].corr(df_corr['views']),\n",
        "                    'cta_vs_comments': df_corr['has_cta'].corr(df_corr['comments'])\n",
        "                }\n",
        "\n",
        "                # Remover NaN\n",
        "                correlacoes = {k: round(v, 3) for k, v in correlacoes.items() if not pd.isna(v)}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Erro no cÃ¡lculo de correlaÃ§Ãµes: {e}\")\n",
        "\n",
        "        return correlacoes\n",
        "\n",
        "    def _gerar_recomendacoes(self, videos_dados):\n",
        "        \"\"\"Gera recomendaÃ§Ãµes estratÃ©gicas baseadas na anÃ¡lise\"\"\"\n",
        "        recomendacoes = []\n",
        "\n",
        "        try:\n",
        "            # AnÃ¡lise de performance\n",
        "            viral_videos = [v for v in videos_dados if v['classificacao_performance'] in ['Viral', 'Alto Alcance']]\n",
        "\n",
        "            if viral_videos:\n",
        "                # PadrÃµes dos vÃ­deos virais\n",
        "                viral_hooks = [v['analise_caption']['hook_type'] for v in viral_videos]\n",
        "                if viral_hooks:\n",
        "                    viral_hook_winner = Counter(viral_hooks).most_common(1)[0][0]\n",
        "                    recomendacoes.append(f\"Priorize hooks do tipo '{viral_hook_winner}' que aparecem em {len([h for h in viral_hooks if h == viral_hook_winner])} dos seus vÃ­deos virais\")\n",
        "\n",
        "                # Tamanho de caption Ã³timo\n",
        "                viral_caption_lengths = [v['analise_caption']['tamanho'] for v in viral_videos if v['analise_caption']['tamanho'] > 0]\n",
        "                if viral_caption_lengths:\n",
        "                    avg_length = np.mean(viral_caption_lengths)\n",
        "                    recomendacoes.append(f\"Mantenha captions com ~{avg_length:.0f} caracteres para melhor performance\")\n",
        "\n",
        "                # CTA analysis\n",
        "                viral_with_cta = [v for v in viral_videos if v['analise_caption']['tem_call_to_action']]\n",
        "                if viral_videos:\n",
        "                    cta_rate = len(viral_with_cta) / len(viral_videos)\n",
        "                    if cta_rate > 0.5:\n",
        "                        recomendacoes.append(\"Inclua CTAs claros nas captions - presentes em {:.0%} dos vÃ­deos virais\".format(cta_rate))\n",
        "\n",
        "            # RecomendaÃ§Ãµes gerais\n",
        "            resumo = self._gerar_resumo_geral(videos_dados)\n",
        "            if resumo['avg_engagement_rate'] < 0.03:  # Abaixo de 3%\n",
        "                recomendacoes.append(\"Engagement rate abaixo do ideal (3%+). Foque em hooks mais impactantes nos primeiros 3 segundos\")\n",
        "\n",
        "            if resumo['avg_virality_score'] < 60:\n",
        "                recomendacoes.append(\"Score de viralidade pode melhorar. Teste elementos dos seus top performers em novos conteÃºdos\")\n",
        "\n",
        "            # Adicionar recomendaÃ§Ãµes padrÃ£o se lista estiver vazia\n",
        "            if not recomendacoes:\n",
        "                recomendacoes = [\n",
        "                    \"Analise os padrÃµes dos seus vÃ­deos com melhor performance\",\n",
        "                    \"Teste diferentes tipos de hooks baseados nos dados coletados\",\n",
        "                    \"Monitore engagement rate como KPI principal de performance\"\n",
        "                ]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Erro ao gerar recomendaÃ§Ãµes: {e}\")\n",
        "            recomendacoes = [\"AnÃ¡lise de recomendaÃ§Ãµes limitada devido a dados insuficientes\"]\n",
        "\n",
        "        return recomendacoes[:8]  # Limitar a 8 recomendaÃ§Ãµes\n",
        "\n",
        "    def salvar_analise_viral(self, mapeamento_videos, insights_virais):\n",
        "        \"\"\"Salva todos os resultados da anÃ¡lise viral - versÃ£o corrigida\"\"\"\n",
        "        try:\n",
        "            # 1. Dados de mapeamento (CSV)\n",
        "            videos_com_dados = [v for v in mapeamento_videos if v.get('tem_dados_virais', False)]\n",
        "\n",
        "            csv_path = None\n",
        "            if videos_com_dados:\n",
        "                df_mapeamento = pd.DataFrame([\n",
        "                    {\n",
        "                        'video_id': v['video_id'],\n",
        "                        'nome_arquivo': v['nome_arquivo'],\n",
        "                        'tem_dados_virais': v['tem_dados_virais'],\n",
        "                        **v.get('viral_data', {}),\n",
        "                        **v.get('metricas_calculadas', {}),\n",
        "                        **{f\"caption_{k}\": val for k, val in v.get('analise_caption', {}).items()},\n",
        "                        'classificacao_performance': v.get('classificacao_performance', 'N/A')\n",
        "                    }\n",
        "                    for v in videos_com_dados\n",
        "                ])\n",
        "\n",
        "                csv_path = os.path.join(self.pasta_viral, \"analise_viral_completa.csv\")\n",
        "                df_mapeamento.to_csv(csv_path, index=False, encoding='utf-8')\n",
        "                print(f\"âœ… CSV salvo: {csv_path}\")\n",
        "            else:\n",
        "                print(\"âš ï¸ Nenhum vÃ­deo com dados virais para salvar em CSV\")\n",
        "\n",
        "            # 2. Insights consolidados (JSON)\n",
        "            json_path = os.path.join(self.pasta_viral, \"insights_virais.json\")\n",
        "            with open(json_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(insights_virais, f, indent=2, ensure_ascii=False)\n",
        "            print(f\"âœ… Insights salvos: {json_path}\")\n",
        "\n",
        "            # 3. Mapeamento completo (JSON)\n",
        "            mapeamento_path = os.path.join(self.pasta_viral, \"mapeamento_viral_completo.json\")\n",
        "            with open(mapeamento_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(mapeamento_videos, f, indent=2, ensure_ascii=False)\n",
        "            print(f\"âœ… Mapeamento salvo: {mapeamento_path}\")\n",
        "\n",
        "            # 4. Dados para integraÃ§Ã£o com outras layers\n",
        "            dados_integracao = {\n",
        "                'viral_disponivel': len(videos_com_dados) > 0,\n",
        "                'total_videos_mapeados': len(videos_com_dados),\n",
        "                'resumo_performance': insights_virais['resumo_geral'],\n",
        "                'top_performers': insights_virais['top_performers'],\n",
        "                'recomendacoes_principais': insights_virais['recomendacoes_estrategicas'][:3],\n",
        "                'data_processamento': datetime.now().isoformat(),\n",
        "                'arquivos_gerados': {\n",
        "                    'csv_completo': csv_path,\n",
        "                    'insights_json': json_path,\n",
        "                    'mapeamento_json': mapeamento_path\n",
        "                }\n",
        "            }\n",
        "\n",
        "            integracao_path = os.path.join(self.pasta_dados, \"viral_integration_data.json\")\n",
        "            with open(integracao_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(dados_integracao, f, indent=2, ensure_ascii=False)\n",
        "            print(f\"âœ… IntegraÃ§Ã£o salva: {integracao_path}\")\n",
        "\n",
        "            return True, dados_integracao\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Erro ao salvar anÃ¡lise viral: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return False, None\n",
        "\n",
        "# ============================================================================\n",
        "# FUNÃ‡ÃƒO PRINCIPAL DE PROCESSAMENTO\n",
        "# ============================================================================\n",
        "\n",
        "def processar_analise_viral():\n",
        "    \"\"\"FunÃ§Ã£o principal que executa toda a anÃ¡lise viral\"\"\"\n",
        "    print(\"ðŸ¦  INICIANDO LAYER 3.3: ANÃLISE VIRAL (VERSÃƒO CORRIGIDA)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Verificar prÃ©-requisitos\n",
        "    prereq_ok, metadados_path = verificar_prerequisito_analise_viral()\n",
        "    if not prereq_ok:\n",
        "        return False\n",
        "\n",
        "    # Carregar metadados dos vÃ­deos\n",
        "    print(\"ðŸ“Š Carregando metadados dos vÃ­deos...\")\n",
        "    try:\n",
        "        with open(metadados_path, 'r', encoding='utf-8') as f:\n",
        "            metadados_videos = json.load(f)\n",
        "        print(f\"âœ… {len(metadados_videos)} vÃ­deos carregados\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Erro ao carregar metadados: {e}\")\n",
        "        return False\n",
        "\n",
        "    # Inicializar analisador viral\n",
        "    print(\"ðŸ”§ Inicializando analisador viral...\")\n",
        "    analisador = AnalisadorViralCorrigido(PASTA_TRABALHO)\n",
        "\n",
        "    if not analisador.dados_script_anexo:\n",
        "        print(\"âš ï¸ Dados do script anexo nÃ£o encontrados.\")\n",
        "        print(\"   Execute o Instagram Extractor primeiro para obter dados de viralizaÃ§Ã£o.\")\n",
        "\n",
        "        # Criar arquivo indicando que anÃ¡lise viral nÃ£o estÃ¡ disponÃ­vel\n",
        "        dados_vazio = {\n",
        "            'viral_disponivel': False,\n",
        "            'motivo': 'Dados do script anexo nÃ£o encontrados',\n",
        "            'data_tentativa': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        sem_dados_path = os.path.join(PASTA_TRABALHO, \"dados\", \"viral_integration_data.json\")\n",
        "        with open(sem_dados_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(dados_vazio, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(\"   ðŸ“„ Arquivo de status criado para prÃ³ximas layers\")\n",
        "        return False\n",
        "\n",
        "    # Mapear vÃ­deos com dados virais\n",
        "    print(\"ðŸ—ºï¸ Mapeando vÃ­deos com dados de viralizaÃ§Ã£o...\")\n",
        "    try:\n",
        "        mapeamento_videos = analisador.mapear_videos_com_dados_virais(metadados_videos)\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Erro no mapeamento: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "    # Gerar insights virais\n",
        "    print(\"ðŸ§  Gerando insights estratÃ©gicos...\")\n",
        "    try:\n",
        "        insights_virais = analisador.gerar_insights_virais(mapeamento_videos)\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Erro na geraÃ§Ã£o de insights: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "    # Salvar resultados\n",
        "    print(\"ðŸ’¾ Salvando anÃ¡lise viral...\")\n",
        "    try:\n",
        "        sucesso, dados_integracao = analisador.salvar_analise_viral(mapeamento_videos, insights_virais)\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Erro ao salvar: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "    if sucesso and dados_integracao:\n",
        "              # Atualizar config.json com verificaÃ§Ã£o robusta\n",
        "        config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "        if os.path.exists(config_path):\n",
        "            try:\n",
        "                with open(config_path, 'r', encoding='utf-8') as f:\n",
        "                    config = json.load(f)\n",
        "\n",
        "                # Garantir que as chaves existem\n",
        "                if 'status_etapas' not in config:\n",
        "                    config['status_etapas'] = {}\n",
        "                if 'arquivos_gerados' not in config:\n",
        "                    config['arquivos_gerados'] = {}\n",
        "\n",
        "                config['status_etapas']['analise_viral'] = True\n",
        "                if dados_integracao.get('arquivos_gerados', {}).get('csv_completo'):\n",
        "                    config['arquivos_gerados']['analise_viral'] = dados_integracao['arquivos_gerados']\n",
        "\n",
        "                with open(config_path, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "                print(\"âœ… Config atualizado com sucesso\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Config nÃ£o atualizado (nÃ£o crÃ­tico): {e}\")\n",
        "\n",
        "\n",
        "\n",
        "        print(\"\\nâœ… ANÃLISE VIRAL CONCLUÃDA COM SUCESSO!\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"ðŸ“Š Total de vÃ­deos mapeados: {dados_integracao['total_videos_mapeados']}\")\n",
        "        print(f\"ðŸ‘€ Views totais: {insights_virais['resumo_geral']['total_views']:,}\")\n",
        "        print(f\"â¤ï¸ Likes totais: {insights_virais['resumo_geral']['total_likes']:,}\")\n",
        "        print(f\"ðŸ’¬ ComentÃ¡rios totais: {insights_virais['resumo_geral']['total_comments']:,}\")\n",
        "        print(f\"ðŸ“ˆ Engagement mÃ©dio: {insights_virais['resumo_geral']['avg_engagement_rate']:.2%}\")\n",
        "        print(f\"ðŸ¦  Score viral mÃ©dio: {insights_virais['resumo_geral']['avg_virality_score']:.1f}/100\")\n",
        "\n",
        "        print(\"\\nðŸŽ¯ TOP RECOMENDAÃ‡Ã•ES:\")\n",
        "        for i, rec in enumerate(dados_integracao['recomendacoes_principais'], 1):\n",
        "            print(f\"   {i}. {rec}\")\n",
        "\n",
        "        print(f\"\\nðŸ“ Dados salvos em: {analisador.pasta_viral}\")\n",
        "        print(\"ðŸ”— Dados de integraÃ§Ã£o criados para prÃ³ximas layers\")\n",
        "        print(\"\\nðŸ“‹ PRÃ“XIMO PASSO: Execute LAYER 4 (RelatÃ³rios e Dashboard)\")\n",
        "\n",
        "        return True\n",
        "    else:\n",
        "        print(\"âŒ Erro na anÃ¡lise viral\")\n",
        "        return False\n",
        "\n",
        "# ============================================================================\n",
        "# EXECUÃ‡ÃƒO PRINCIPAL\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ðŸŽ¯ LAYER 3.3: ANÃLISE VIRAL - PROCESSAMENTO INDEPENDENTE (CORRIGIDO)\")\n",
        "    print(\"ðŸ”„ Executando anÃ¡lise completa de dados de viralizaÃ§Ã£o...\")\n",
        "    print()\n",
        "\n",
        "    sucesso = processar_analise_viral()\n",
        "\n",
        "    if sucesso:\n",
        "        print(\"\\nðŸŽ‰ ANÃLISE VIRAL PROCESSADA COM SUCESSO!\")\n",
        "        print(\"   Os dados estÃ£o prontos para integraÃ§Ã£o automÃ¡tica nas prÃ³ximas layers\")\n",
        "    else:\n",
        "        print(\"\\nâš ï¸ AnÃ¡lise viral nÃ£o pÃ´de ser completada\")\n",
        "        print(\"   O processo continuarÃ¡ sem dados de viralizaÃ§Ã£o\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ðŸ“‹ PRÃ“XIMO PASSO: Execute LAYER 4 (RelatÃ³rios e Dashboard)\")\n",
        "    print(\"   Os dados virais serÃ£o automaticamente incluÃ­dos se disponÃ­veis\")\n",
        "    print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "relatorios_humanizados",
        "outputId": "c2e32687-d422-4355-ad3f-437a827bf0dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando geraÃ§Ã£o de relatÃ³rios humanizados para 3 vÃ­deos...\n",
            "[1/3] Gerando relatÃ³rios para: vid_bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t\n",
            "  ðŸ’¾ RelatÃ³rio de Texto (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_texto/RELATORIO_TEXTO_HUMANIZADO_vid_bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.xlsx\n",
            "  ðŸ’¾ EstratÃ©gia de ConteÃºdo Textual (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_texto/ESTRATEGIA_CONTEUDO_TEXTUAL_vid_bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.pdf\n",
            "  ðŸ’¾ RelatÃ³rio de Ãudio (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_audio/RELATORIO_AUDIO_HUMANIZADO_vid_bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.xlsx\n",
            "  ðŸ’¾ Resumo de Ãudio EstratÃ©gico (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_audio/RESUMO_AUDIO_ESTRATEGICO_vid_bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.pdf\n",
            "  ðŸ’¾ RelatÃ³rio Visual (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_visual/RELATORIO_VISUAL_HUMANIZADO_vid_bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.xlsx\n",
            "  ðŸ’¾ EstratÃ©gia Visual Completa (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_visual/ESTRATEGIA_VISUAL_COMPLETA_vid_bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.pdf\n",
            "  ðŸ’¾ RelatÃ³rio PsicolÃ³gico (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_psicologica/RELATORIO_PSICOLOGICO_HUMANIZADO_vid_bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.xlsx\n",
            "  ðŸ’¾ Manual de Psicologia Viral (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_psicologica/MANUAL_PSICOLOGIA_VIRAL_vid_bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t.pdf\n",
            "  âœ… RelatÃ³rios gerados para vid_bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t\n",
            "[2/3] Gerando relatÃ³rios para: vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf\n",
            "  ðŸ’¾ RelatÃ³rio de Texto (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_texto/RELATORIO_TEXTO_HUMANIZADO_vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.xlsx\n",
            "  ðŸ’¾ EstratÃ©gia de ConteÃºdo Textual (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_texto/ESTRATEGIA_CONTEUDO_TEXTUAL_vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.pdf\n",
            "  ðŸ’¾ RelatÃ³rio de Ãudio (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_audio/RELATORIO_AUDIO_HUMANIZADO_vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.xlsx\n",
            "  ðŸ’¾ Resumo de Ãudio EstratÃ©gico (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_audio/RESUMO_AUDIO_ESTRATEGICO_vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.pdf\n",
            "  ðŸ’¾ RelatÃ³rio Visual (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_visual/RELATORIO_VISUAL_HUMANIZADO_vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.xlsx\n",
            "  ðŸ’¾ EstratÃ©gia Visual Completa (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_visual/ESTRATEGIA_VISUAL_COMPLETA_vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.pdf\n",
            "  ðŸ’¾ RelatÃ³rio PsicolÃ³gico (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_psicologica/RELATORIO_PSICOLOGICO_HUMANIZADO_vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.xlsx\n",
            "  ðŸ’¾ Manual de Psicologia Viral (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_psicologica/MANUAL_PSICOLOGIA_VIRAL_vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf.pdf\n",
            "  âœ… RelatÃ³rios gerados para vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf\n",
            "[3/3] Gerando relatÃ³rios para: vid_maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy\n",
            "  ðŸ’¾ RelatÃ³rio de Texto (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_texto/RELATORIO_TEXTO_HUMANIZADO_vid_maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy.xlsx\n",
            "  ðŸ’¾ EstratÃ©gia de ConteÃºdo Textual (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_texto/ESTRATEGIA_CONTEUDO_TEXTUAL_vid_maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy.pdf\n",
            "  ðŸ’¾ RelatÃ³rio de Ãudio (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_audio/RELATORIO_AUDIO_HUMANIZADO_vid_maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy.xlsx\n",
            "  ðŸ’¾ Resumo de Ãudio EstratÃ©gico (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_audio/RESUMO_AUDIO_ESTRATEGICO_vid_maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy.pdf\n",
            "  ðŸ’¾ RelatÃ³rio Visual (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_visual/RELATORIO_VISUAL_HUMANIZADO_vid_maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy.xlsx\n",
            "  ðŸ’¾ EstratÃ©gia Visual Completa (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_visual/ESTRATEGIA_VISUAL_COMPLETA_vid_maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy.pdf\n",
            "  ðŸ’¾ RelatÃ³rio PsicolÃ³gico (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_psicologica/RELATORIO_PSICOLOGICO_HUMANIZADO_vid_maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy.xlsx\n",
            "  ðŸ’¾ Manual de Psicologia Viral (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_psicologica/MANUAL_PSICOLOGIA_VIRAL_vid_maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy.pdf\n",
            "  âœ… RelatÃ³rios gerados para vid_maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy\n",
            "\n",
            "âœ… GERAÃ‡ÃƒO DE RELATÃ“RIOS HUMANIZADOS CONCLUÃDA!\n",
            "Total de vÃ­deos com relatÃ³rios gerados: 3\n",
            "\n",
            "âž¡ï¸ PRÃ“XIMA CÃ‰LULA: 4.2 - GERAÃ‡ÃƒO DO BLUEPRINT FINAL E DASHBOARD\n",
            "ðŸ“Š Gerando e salvando relatÃ³rio de anÃ¡lise viral...\n",
            "âœ… RelatÃ³rio viral salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/blueprint/RELATORIO_VIRAL_INSIGHTS.txt\n",
            "âš ï¸ PDF nÃ£o gerado (nÃ£o crÃ­tico): 'latin-1' codec can't encode character '\\U0001f4c5' in position 954: ordinal not in range(256)\n",
            "ðŸ”— RelatÃ³rio viral integrado com sucesso!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# LAYER 4: GERAÃ‡ÃƒO DE RELATÃ“RIOS E BLUEPRINT ESTRATÃ‰GICO\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# CÃ‰LULA 4.1: GERAÃ‡ÃƒO DE RELATÃ“RIOS HUMANIZADOS (ÃUDIO, VISUAL, TEXTO, PSICOLÃ“GICO)\n",
        "# ============================================================================\n",
        "\n",
        "from fpdf import FPDF # Importar FPDF para geraÃ§Ã£o de PDF\n",
        "\n",
        "# ===== FUNÃ‡Ã•ES DE INTEGRAÃ‡ÃƒO VIRAL - ADICIONAR NO INÃCIO DA CÃ‰LULA 4.1 =====\n",
        "\n",
        "def detectar_dados_virais_disponiveis(pasta_trabalho):\n",
        "    \"\"\"Detecta se dados virais estÃ£o disponÃ­veis\"\"\"\n",
        "    try:\n",
        "        viral_integration_path = os.path.join(pasta_trabalho, \"dados\", \"viral_integration_data.json\")\n",
        "        if not os.path.exists(viral_integration_path):\n",
        "            return False, None\n",
        "        with open(viral_integration_path, 'r', encoding='utf-8') as f:\n",
        "            viral_data = json.load(f)\n",
        "        return viral_data.get('viral_disponivel', False), viral_data\n",
        "    except Exception:\n",
        "        return False, None\n",
        "\n",
        "def gerar_relatorio_viral_humanizado(pasta_trabalho):\n",
        "    \"\"\"Gera relatÃ³rio humanizado dos dados virais\"\"\"\n",
        "    viral_disponivel, viral_integration = detectar_dados_virais_disponiveis(pasta_trabalho)\n",
        "\n",
        "    if not viral_disponivel:\n",
        "        return \"ðŸ“Š ANÃLISE VIRAL: Dados nÃ£o disponÃ­veis\\n\"\n",
        "\n",
        "    try:\n",
        "        # Carregar insights virais\n",
        "        insights_path = os.path.join(pasta_trabalho, \"analise_viral\", \"insights_virais.json\")\n",
        "        with open(insights_path, 'r', encoding='utf-8') as f:\n",
        "            insights = json.load(f)\n",
        "\n",
        "        resumo = insights['resumo_geral']\n",
        "        recomendacoes = insights['recomendacoes_estrategicas']\n",
        "\n",
        "        relatorio = f\"\"\"\n",
        "ðŸ“Š RELATÃ“RIO DE ANÃLISE VIRAL - INSIGHTS ESTRATÃ‰GICOS\n",
        "{'='*60}\n",
        "\n",
        "ðŸŽ¯ RESUMO EXECUTIVO:\n",
        "   â€¢ Total de vÃ­deos analisados: {resumo['total_videos']}\n",
        "   â€¢ Views totais coletadas: {resumo['total_views']:,}\n",
        "   â€¢ Likes totais: {resumo['total_likes']:,}\n",
        "   â€¢ ComentÃ¡rios totais: {resumo['total_comments']:,}\n",
        "   â€¢ Engagement rate mÃ©dio: {resumo['avg_engagement_rate']:.2%}\n",
        "   â€¢ Score de viralidade mÃ©dio: {resumo['avg_virality_score']:.1f}/100\n",
        "\n",
        "ðŸŽ¯ RECOMENDAÃ‡Ã•ES ESTRATÃ‰GICAS:\n",
        "\"\"\"\n",
        "\n",
        "        for i, rec in enumerate(recomendacoes[:5], 1):\n",
        "            relatorio += f\"   {i}. {rec}\\n\"\n",
        "\n",
        "        relatorio += f\"\\nðŸ“… AnÃ¡lise realizada em: {insights['data_analise'][:19]}\\n\"\n",
        "\n",
        "        return relatorio\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"ðŸ“Š ANÃLISE VIRAL: Erro ao gerar relatÃ³rio - {e}\\n\"\n",
        "\n",
        "# ===== FIM DAS FUNÃ‡Ã•ES DE INTEGRAÃ‡ÃƒO VIRAL =====\n",
        "\n",
        "class PDF(FPDF):\n",
        "    def header(self):\n",
        "        self.set_font('Arial', 'B', 12)\n",
        "        self.cell(0, 10, 'RelatÃ³rio de Engenharia Reversa de VÃ­deos', 0, 1, 'C')\n",
        "        self.ln(10)\n",
        "\n",
        "    def footer(self):\n",
        "        self.set_y(-15)\n",
        "        self.set_font('Arial', 'I', 8)\n",
        "        self.cell(0, 10, f'PÃ¡gina {self.page_no()}/{{nb}}', 0, 0, 'C')\n",
        "\n",
        "    def chapter_title(self, title):\n",
        "        self.set_font('Arial', 'B', 12)\n",
        "        self.cell(0, 10, title, 0, 1, 'L')\n",
        "        self.ln(5)\n",
        "\n",
        "    def chapter_body(self, body):\n",
        "        self.set_font('Arial', '', 10)\n",
        "        self.multi_cell(0, 5, body)\n",
        "        self.ln()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def gerar_relatorio_texto(video_id, analise_padroes_data, pasta_destino):\n",
        "    df_texto = pd.DataFrame([analise_padroes_data])\n",
        "    excel_path = os.path.join(pasta_destino, f'RELATORIO_TEXTO_HUMANIZADO_{video_id}.xlsx')\n",
        "    df_texto.to_excel(excel_path, index=False, engine='openpyxl')\n",
        "\n",
        "    pdf = PDF()\n",
        "    pdf.add_page()\n",
        "    pdf.chapter_title('EstratÃ©gia de ConteÃºdo Textual')\n",
        "    pdf.chapter_body(f'Resumo do Texto: {analise_padroes_data.get('resumo_texto', 'N/A')}')\n",
        "    pdf.chapter_body(f'Palavras-chave: {', '.join(analise_padroes_data.get('palavras_chave_texto', []))}')\n",
        "    pdf_path = os.path.join(pasta_destino, f'ESTRATEGIA_CONTEUDO_TEXTUAL_{video_id}.pdf')\n",
        "    pdf.output(pdf_path)\n",
        "    return excel_path, pdf_path\n",
        "\n",
        "def gerar_relatorio_audio(video_id, analise_padroes_data, pasta_destino):\n",
        "    df_audio = pd.DataFrame([analise_padroes_data.get('analise_audio_detalhada', {})])\n",
        "    excel_path = os.path.join(pasta_destino, f'RELATORIO_AUDIO_HUMANIZADO_{video_id}.xlsx')\n",
        "    df_audio.to_excel(excel_path, index=False, engine='openpyxl')\n",
        "\n",
        "    pdf = PDF()\n",
        "    pdf.add_page()\n",
        "    pdf.chapter_title('Resumo de Ãudio EstratÃ©gico')\n",
        "    pdf.chapter_body(f'BPM: {analise_padroes_data.get('analise_audio_detalhada', {}).get('bpm', 'N/A')}')\n",
        "    pdf.chapter_body(f'DuraÃ§Ã£o do Ãudio: {analise_padroes_data.get('analise_audio_detalhada', {}).get('duracao_audio_segundos', 'N/A')} segundos')\n",
        "    pdf_path = os.path.join(pasta_destino, f'RESUMO_AUDIO_ESTRATEGICO_{video_id}.pdf')\n",
        "    pdf.output(pdf_path)\n",
        "    return excel_path, pdf_path\n",
        "\n",
        "def gerar_relatorio_visual(video_id, analise_padroes_data, pasta_destino):\n",
        "    df_visual = pd.DataFrame([analise_padroes_data.get('analise_visual_detalhada', {})])\n",
        "    excel_path = os.path.join(pasta_destino, f'RELATORIO_VISUAL_HUMANIZADO_{video_id}.xlsx')\n",
        "    df_visual.to_excel(excel_path, index=False, engine='openpyxl')\n",
        "\n",
        "    pdf = PDF()\n",
        "    pdf.add_page()\n",
        "    pdf.chapter_title('EstratÃ©gia Visual Completa')\n",
        "    pdf.chapter_body(f'Total de Cortes: {analise_padroes_data.get('analise_visual_detalhada', {}).get('total_cortes', 'N/A')}')\n",
        "    pdf.chapter_body(f'Complexidade Visual MÃ©dia: {analise_padroes_data.get('analise_visual_detalhada', {}).get('complexidade_visual_media', 'N/A'):.2f}')\n",
        "    pdf.chapter_body(f'Brilho MÃ©dio: {analise_padroes_data.get('analise_visual_detalhada', {}).get('brilho_medio', 'N/A'):.2f}')\n",
        "    pdf_path = os.path.join(pasta_destino, f'ESTRATEGIA_VISUAL_COMPLETA_{video_id}.pdf')\n",
        "    pdf.output(pdf_path)\n",
        "    return excel_path, pdf_path\n",
        "\n",
        "def gerar_relatorio_psicologico(video_id, analise_psicologica_data, pasta_destino):\n",
        "    df_psico = pd.DataFrame([analise_psicologica_data])\n",
        "    excel_path = os.path.join(pasta_destino, f'RELATORIO_PSICOLOGICO_HUMANIZADO_{video_id}.xlsx')\n",
        "    df_psico.to_excel(excel_path, index=False, engine='openpyxl')\n",
        "\n",
        "    pdf = PDF()\n",
        "    pdf.add_page()\n",
        "    pdf.chapter_title('Manual de Psicologia Viral')\n",
        "    pdf.chapter_body(f'Gatilhos Detectados: {', '.join(analise_psicologica_data.get('gatilhos_detectados', []))}')\n",
        "    pdf.chapter_body(f'EmoÃ§Ãµes Predominantes: {analise_psicologica_data.get('emocoes_predominantes', 'N/A')}')\n",
        "    pdf.chapter_body(f'Insights: {analise_psicologica_data.get('insights_psicologicos', 'N/A')}')\n",
        "    pdf_path = os.path.join(pasta_destino, f'MANUAL_PSICOLOGIA_VIRAL_{video_id}.pdf')\n",
        "    pdf.output(pdf_path)\n",
        "    return excel_path, pdf_path\n",
        "\n",
        "def processar_geracao_relatorios_todos_videos():\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa('analise_psicologica')\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Carregar dados de anÃ¡lise de padrÃµes e psicolÃ³gica\n",
        "    analises_padroes_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_padroes_completas.json\")\n",
        "    analises_psicologicas_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_psicologicas_completas.json\")\n",
        "    with open(analises_padroes_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        analises_padroes = json.load(f)\n",
        "    with open(analises_psicologicas_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        analises_psicologicas = json.load(f)\n",
        "\n",
        "    sucessos = 0\n",
        "\n",
        "    print(f\"\"\"\n",
        "Iniciando geraÃ§Ã£o de relatÃ³rios humanizados para {len(analises_padroes)} vÃ­deos...\"\"\")\n",
        "\n",
        "    for i, analise_padroes_data in enumerate(analises_padroes, 1):\n",
        "        video_id = analise_padroes_data[\"video_id\"]\n",
        "        analise_psicologica_data = next((a for a in analises_psicologicas if a[\"video_id\"] == video_id), None)\n",
        "\n",
        "        if analise_padroes_data.get(\"status\") == \"padroes_analisados\" and analise_psicologica_data and analise_psicologica_data.get(\"status\") == \"analise_psicologica_concluida\":\n",
        "            print(f\"[{i}/{len(analises_padroes)}] Gerando relatÃ³rios para: {video_id}\")\n",
        "            try:\n",
        "                # GeraÃ§Ã£o de RelatÃ³rios de Texto\n",
        "                pasta_texto = os.path.join(PASTA_TRABALHO, \"analise_texto\")\n",
        "                os.makedirs(pasta_texto, exist_ok=True)\n",
        "                excel_text, pdf_text = gerar_relatorio_texto(video_id, analise_padroes_data, pasta_texto)\n",
        "                print(f\"  ðŸ’¾ RelatÃ³rio de Texto (XLSX) salvo em: {excel_text}\")\n",
        "                print(f\"  ðŸ’¾ EstratÃ©gia de ConteÃºdo Textual (PDF) salvo em: {pdf_text}\")\n",
        "\n",
        "                # GeraÃ§Ã£o de RelatÃ³rios de Ãudio\n",
        "                pasta_audio = os.path.join(PASTA_TRABALHO, \"analise_audio\")\n",
        "                os.makedirs(pasta_audio, exist_ok=True)\n",
        "                excel_audio, pdf_audio = gerar_relatorio_audio(video_id, analise_padroes_data, pasta_audio)\n",
        "                print(f\"  ðŸ’¾ RelatÃ³rio de Ãudio (XLSX) salvo em: {excel_audio}\")\n",
        "                print(f\"  ðŸ’¾ Resumo de Ãudio EstratÃ©gico (PDF) salvo em: {pdf_audio}\")\n",
        "\n",
        "                # GeraÃ§Ã£o de RelatÃ³rios Visuais\n",
        "                pasta_visual = os.path.join(PASTA_TRABALHO, \"analise_visual\")\n",
        "                os.makedirs(pasta_visual, exist_ok=True)\n",
        "                excel_visual, pdf_visual = gerar_relatorio_visual(video_id, analise_padroes_data, pasta_visual)\n",
        "                print(f\"  ðŸ’¾ RelatÃ³rio Visual (XLSX) salvo em: {excel_visual}\")\n",
        "                print(f\"  ðŸ’¾ EstratÃ©gia Visual Completa (PDF) salvo em: {pdf_visual}\")\n",
        "\n",
        "                # GeraÃ§Ã£o de RelatÃ³rios PsicolÃ³gicos\n",
        "                pasta_psicologica = os.path.join(PASTA_TRABALHO, \"analise_psicologica\")\n",
        "                os.makedirs(pasta_psicologica, exist_ok=True)\n",
        "                excel_psico, pdf_psico = gerar_relatorio_psicologico(video_id, analise_psicologica_data, pasta_psicologica)\n",
        "                print(f\"  ðŸ’¾ RelatÃ³rio PsicolÃ³gico (XLSX) salvo em: {excel_psico}\")\n",
        "                print(f\"  ðŸ’¾ Manual de Psicologia Viral (PDF) salvo em: {pdf_psico}\")\n",
        "\n",
        "                sucessos += 1\n",
        "                print(f\"  âœ… RelatÃ³rios gerados para {video_id}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  âŒ ERRO na geraÃ§Ã£o de relatÃ³rios para {video_id}: {e}\")\n",
        "        else:\n",
        "            print(f\"[{i}/{len(analises_padroes)}] Pulando {video_id} - PrÃ©-requisitos nÃ£o atendidos.\")\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    config[\"status_etapas\"][\"relatorios_humanizados\"] = True\n",
        "    config[\"total_videos_relatorios_gerados\"] = sucessos\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(\"\"\"\n",
        "âœ… GERAÃ‡ÃƒO DE RELATÃ“RIOS HUMANIZADOS CONCLUÃDA!\"\"\")\n",
        "    print(f\"Total de vÃ­deos com relatÃ³rios gerados: {sucessos}\")\n",
        "\n",
        "    if sucessos == 0:\n",
        "        print(\"âŒ NENHUM VÃDEO TEVE RELATÃ“RIOS GERADOS COM SUCESSO. Verifique as etapas anteriores.\")\n",
        "    print(\"\"\"\n",
        "âž¡ï¸ PRÃ“XIMA CÃ‰LULA: 4.2 - GERAÃ‡ÃƒO DO BLUEPRINT FINAL E DASHBOARD\"\"\")\n",
        "\n",
        "# Executar geraÃ§Ã£o de relatÃ³rios\n",
        "try:\n",
        "    processar_geracao_relatorios_todos_videos()\n",
        "except Exception as e:\n",
        "    print(f\"\"\"\n",
        "âŒ ERRO GERAL NA GERAÃ‡ÃƒO DE RELATÃ“RIOS: {e}\"\"\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")\n",
        "\n",
        "\n",
        "# ======= INTEGRAÃ‡ÃƒO: RELATÃ“RIO VIRAL (CORRIGIDO) =======\n",
        "print(\"ðŸ“Š Gerando e salvando relatÃ³rio de anÃ¡lise viral...\")\n",
        "relatorio_viral = gerar_relatorio_viral_humanizado(PASTA_TRABALHO)\n",
        "\n",
        "# Salvar relatÃ³rio viral como arquivo separado\n",
        "pasta_blueprint = os.path.join(PASTA_TRABALHO, \"blueprint\")\n",
        "os.makedirs(pasta_blueprint, exist_ok=True)\n",
        "\n",
        "viral_report_path = os.path.join(pasta_blueprint, \"RELATORIO_VIRAL_INSIGHTS.txt\")\n",
        "with open(viral_report_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(relatorio_viral)\n",
        "\n",
        "print(\"âœ… RelatÃ³rio viral salvo em:\", viral_report_path)\n",
        "\n",
        "# TambÃ©m salvar como PDF\n",
        "try:\n",
        "    pdf_viral = PDF()\n",
        "    pdf_viral.add_page()\n",
        "    pdf_viral.chapter_title('ANÃLISE VIRAL - INSIGHTS ESTRATÃ‰GICOS')\n",
        "\n",
        "    # Converter relatÃ³rio para formato compatÃ­vel com PDF\n",
        "    relatorio_clean = relatorio_viral.replace('ðŸ“Š', '').replace('ðŸŽ¯', '').replace('â€¢', '-')\n",
        "    pdf_viral.chapter_body(relatorio_clean)\n",
        "\n",
        "    viral_pdf_path = os.path.join(pasta_blueprint, \"RELATORIO_VIRAL_INSIGHTS.pdf\")\n",
        "    pdf_viral.output(viral_pdf_path)\n",
        "\n",
        "    print(\"âœ… RelatÃ³rio viral PDF salvo em:\", viral_pdf_path)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ PDF nÃ£o gerado (nÃ£o crÃ­tico): {e}\")\n",
        "\n",
        "print(\"ðŸ”— RelatÃ³rio viral integrado com sucesso!\")\n",
        "# ======= FIM INTEGRAÃ‡ÃƒO VIRAL ======="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cT5PyNglpi2T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7ee0a01-b8dc-4397-bed9-b7db6c82026c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ’¾ Dashboard Executivo (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dashboard/DASHBOARD_MASTER_EXECUTIVO.xlsx\n",
            "ðŸ’¾ Dados Consolidados (CSV) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dashboard/dados_consolidados.csv\n",
            "ðŸ’¾ Dados Detalhados (JSON) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dashboard/dados_detalhados.json\n",
            "ðŸ’¾ Dashboard Interativo (HTML) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dashboard/dashboard_interativo.html\n",
            "ðŸ’¾ Blueprint EstratÃ©gico (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/blueprint/BLUEPRINT_ESTRATEGICO_FINAL.pdf\n",
            "\n",
            "âœ… GERAÃ‡ÃƒO DO BLUEPRINT FINAL E DASHBOARD CONCLUÃDA!\n",
            "Todos os relatÃ³rios e o dashboard foram gerados com sucesso.\n",
            "\n",
            "ðŸŽ‰ PROCESSO DE ENGENHARIA REVERSA CONCLUÃDO COM SUCESSO! ðŸŽ‰\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CÃ‰LULA 4.2: GERAÃ‡ÃƒO DO BLUEPRINT FINAL E DASHBOARD\n",
        "# ============================================================================\n",
        "\n",
        "def gerar_blueprint_dashboard():\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa(\"relatorios_humanizados\")\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Carregar todos os dados de anÃ¡lise\n",
        "    metadados_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "    decomposicao_path = os.path.join(PASTA_TRABALHO, \"dados\", \"decomposicao_completa.json\")\n",
        "    analises_padroes_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_padroes_completas.json\")\n",
        "    analises_psicologicas_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_psicologicas_completas.json\")\n",
        "\n",
        "    with open(metadados_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        metadados = json.load(f)\n",
        "    with open(decomposicao_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        decomposicoes = json.load(f)\n",
        "    with open(analises_padroes_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        analises_padroes = json.load(f)\n",
        "    with open(analises_psicologicas_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        analises_psicologicas = json.load(f)\n",
        "\n",
        "    dados_consolidados = []\n",
        "    for video_meta in metadados:\n",
        "        video_id = video_meta[\"id\"]\n",
        "        decomposicao = next((d for d in decomposicoes if d[\"video_id\"] == video_id), {})\n",
        "        analise_padroes = next((ap for ap in analises_padroes if ap[\"video_id\"] == video_id), {})\n",
        "        analise_psicologica = next((aps for aps in analises_psicologicas if aps[\"video_id\"] == video_id), {})\n",
        "        consolidado = {\n",
        "            \"video_id\": video_id,\n",
        "            \"nome_arquivo\": video_meta.get(\"nome_arquivo\"),\n",
        "            \"duracao_segundos\": video_meta.get(\"duracao_segundos\"),\n",
        "            \"formato_detectado\": video_meta.get(\"formato_detectado\"),\n",
        "            \"tem_audio\": video_meta.get(\"tem_audio\"),\n",
        "            \"total_frames\": video_meta.get(\"total_frames\"),\n",
        "            \"ocr_textos_count\": len(decomposicao.get(\"textos_ocr\", [])),\n",
        "            \"audio_transcrito_len\": len(decomposicao.get(\"audio_transcrito\", \"\")),\n",
        "            \"cortes_detectados_count\": len(decomposicao.get(\"cortes_detectados_segundos\", [])),\n",
        "            \"bpm_audio\": analise_padroes.get(\"analise_audio_detalhada\", {}).get(\"bpm\"),\n",
        "            \"complexidade_visual_media\": analise_padroes.get(\"analise_visual_detalhada\", {}).get(\"complexidade_visual_media\"),\n",
        "            \"brilho_medio\": analise_padroes.get(\"analise_visual_detalhada\", {}).get(\"brilho_medio\"),\n",
        "            \"padroes_gerais\": \", \".join(analise_padroes.get(\"padroes_gerais\", [])),\n",
        "            \"gatilhos_psicologicos\": \", \".join(analise_psicologica.get(\"gatilhos_detectados\", [])),\n",
        "            \"emocoes_predominantes\": str(analise_psicologica.get(\"emocoes_predominantes\", {})),\n",
        "            \"status_geral\": video_meta.get(\"status\") # Pode ser aprimorado para refletir o status de todas as etapas\n",
        "        }\n",
        "        dados_consolidados.append(consolidado)\n",
        "\n",
        "    df_final = pd.DataFrame(dados_consolidados)\n",
        "\n",
        "    # Salvar Dashboard Executivo (Excel)\n",
        "    dashboard_excel_path = os.path.join(PASTA_TRABALHO, \"dashboard\", \"DASHBOARD_MASTER_EXECUTIVO.xlsx\")\n",
        "    df_final.to_excel(dashboard_excel_path, index=False, engine=\"openpyxl\")\n",
        "    print(f\"\\nðŸ’¾ Dashboard Executivo (XLSX) salvo em: {dashboard_excel_path}\")\n",
        "\n",
        "    # Salvar Dados Consolidados (CSV e JSON)\n",
        "    dados_csv_path = os.path.join(PASTA_TRABALHO, \"dashboard\", \"dados_consolidados.csv\")\n",
        "    df_final.to_csv(dados_csv_path, index=False, encoding=\"utf-8\")\n",
        "    print(f\"ðŸ’¾ Dados Consolidados (CSV) salvo em: {dados_csv_path}\")\n",
        "\n",
        "    dados_json_path = os.path.join(PASTA_TRABALHO, \"dashboard\", \"dados_detalhados.json\")\n",
        "    with open(dados_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(dados_consolidados, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"ðŸ’¾ Dados Detalhados (JSON) salvo em: {dados_json_path}\")\n",
        "\n",
        "    # GeraÃ§Ã£o de Dashboard Interativo (HTML - Exemplo simples)\n",
        "    # Para um dashboard interativo real, seria necessÃ¡rio uma biblioteca como Plotly ou Dash\n",
        "    dashboard_html_path = os.path.join(PASTA_TRABALHO, \"dashboard\", \"dashboard_interativo.html\")\n",
        "    with open(dashboard_html_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"<html><body><h1>Dashboard Interativo (Placeholder)</h1><p>Seu dashboard interativo real seria gerado aqui com bibliotecas como Plotly ou Dash.</p></body></html>\")\n",
        "    print(f\"ðŸ’¾ Dashboard Interativo (HTML) salvo em: {dashboard_html_path}\")\n",
        "\n",
        "    # GeraÃ§Ã£o do Blueprint EstratÃ©gico (PDF - Exemplo simples)\n",
        "    pdf = PDF()\n",
        "    pdf.add_page()\n",
        "    pdf.chapter_title(\"BLUEPRINT ESTRATÃ‰GICO FINAL\")\n",
        "    pdf.chapter_body(\"Este Ã© o seu blueprint estratÃ©gico final, consolidando todos os insights.\")\n",
        "    pdf.chapter_body(f\"Total de vÃ­deos analisados: {len(df_final)}\")\n",
        "    pdf.chapter_body(f\"MÃ©dia de duraÃ§Ã£o dos vÃ­deos: {df_final[\"duracao_segundos\"] .mean():.2f} segundos\")\n",
        "    pdf_blueprint_path = os.path.join(PASTA_TRABALHO, \"blueprint\", \"BLUEPRINT_ESTRATEGICO_FINAL.pdf\")\n",
        "    pdf.output(pdf_blueprint_path)\n",
        "    print(f\"ðŸ’¾ Blueprint EstratÃ©gico (PDF) salvo em: {pdf_blueprint_path}\")\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    config[\"status_etapas\"][\"blueprint\"] = True\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(\"\\nâœ… GERAÃ‡ÃƒO DO BLUEPRINT FINAL E DASHBOARD CONCLUÃDA!\")\n",
        "    print(\"Todos os relatÃ³rios e o dashboard foram gerados com sucesso.\")\n",
        "    print(\"\\nðŸŽ‰ PROCESSO DE ENGENHARIA REVERSA CONCLUÃDO COM SUCESSO! ðŸŽ‰\")\n",
        "\n",
        "# Executar geraÃ§Ã£o de blueprint e dashboard\n",
        "try:\n",
        "    gerar_blueprint_dashboard()\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ ERRO GERAL NA GERAÃ‡ÃƒO DO BLUEPRINT E DASHBOARD: {e}\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "hFr8drvBrb23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a5fcdb8-4f40-4113-ca35-87aed715cbdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:57:29] INICIANDO SISTEMA DE DASHBOARD INTELIGENTE\n",
            "[00:57:29] INICIANDO CRIAÃ‡ÃƒO DO DASHBOARD MASTER EXECUTIVO INTELIGENTE\n",
            "[00:57:29] Carregando dados consolidados...\n",
            "[00:57:30] Dados carregados: 3 vÃ­deos encontrados\n",
            "[00:57:30] Processando inteligÃªncia artificial dos dados...\n",
            "[00:57:30] Calculando scores de performance...\n",
            "[00:57:30] Gerando insights estratÃ©gicos...\n",
            "[00:57:30] Criando estrutura do dashboard...\n",
            "[00:57:30] Criando Executive Summary...\n",
            "[00:57:30] Criando AnÃ¡lise de Performance...\n",
            "[00:57:30] Criando InteligÃªncia TÃ©cnica...\n",
            "[00:57:30] Criando Blueprint de ProduÃ§Ã£o...\n",
            "[00:57:30] Criando RecomendaÃ§Ãµes EstratÃ©gicas...\n",
            "[00:57:30] Salvando dashboard...\n",
            "[00:57:30] DASHBOARD MASTER EXECUTIVO CRIADO COM SUCESSO!\n",
            "[00:57:30] Arquivo salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dashboard/DASHBOARD_MASTER_EXECUTIVO_INTELIGENTE.xlsx\n",
            "[00:57:30] 3 vÃ­deos analisados\n",
            "[00:57:30] 5 insights estratÃ©gicos gerados\n",
            "[00:57:30] 1 recomendaÃ§Ãµes criadas\n",
            "[00:57:30] PROCESSO CONCLUÃDO COM SUCESSO!\n",
            "[00:57:30] Dashboard inteligente pronto para uso estratÃ©gico\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CÃ‰LULA 4.3: DASHBOARD MASTER EXECUTIVO INTELIGENTE APRIMORADO\n",
        "# ============================================================================\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "from openpyxl.styles import Font, Alignment, PatternFill\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def log_progress(message):\n",
        "    \"\"\"Log de progresso em tempo real\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
        "    print(f\"[{timestamp}] {message}\")\n",
        "\n",
        "def calculate_viral_score(row):\n",
        "    \"\"\"Calcula score de viralidade baseado em mÃºltiplos fatores\"\"\"\n",
        "    try:\n",
        "        score = 0\n",
        "\n",
        "        # Fator 1: Ritmo (cortes por segundo) - peso 25%\n",
        "        if pd.notna(row['duracao_segundos']) and row['duracao_segundos'] > 0:\n",
        "            cortes_por_seg = row['cortes_detectados_count'] / row['duracao_segundos']\n",
        "            if cortes_por_seg > 20: score += 25\n",
        "            elif cortes_por_seg > 10: score += 20\n",
        "            elif cortes_por_seg > 5: score += 15\n",
        "            else: score += 10\n",
        "\n",
        "        # Fator 2: Complexidade Visual - peso 20%\n",
        "        if pd.notna(row['complexidade_visual_media']):\n",
        "            if row['complexidade_visual_media'] > 600: score += 20\n",
        "            elif row['complexidade_visual_media'] > 400: score += 15\n",
        "            else: score += 10\n",
        "\n",
        "        # Fator 3: PresenÃ§a de Texto (OCR) - peso 15%\n",
        "        if pd.notna(row['ocr_textos_count']):\n",
        "            if row['ocr_textos_count'] > 10: score += 15\n",
        "            elif row['ocr_textos_count'] > 5: score += 12\n",
        "            elif row['ocr_textos_count'] > 0: score += 8\n",
        "\n",
        "        # Fator 4: DuraÃ§Ã£o Ideal - peso 20%\n",
        "        if pd.notna(row['duracao_segundos']):\n",
        "            if 15 <= row['duracao_segundos'] <= 30: score += 20\n",
        "            elif 10 <= row['duracao_segundos'] <= 45: score += 15\n",
        "            else: score += 10\n",
        "\n",
        "        # Fator 5: Gatilhos PsicolÃ³gicos - peso 20%\n",
        "        gatilhos = str(row['gatilhos_psicologicos']).lower()\n",
        "        if 'urgÃªncia' in gatilhos or 'escassez' in gatilhos: score += 8\n",
        "        if 'estÃ­mulo' in gatilhos: score += 7\n",
        "        if 'atenÃ§Ã£o' in gatilhos: score += 5\n",
        "\n",
        "        return min(score, 100)\n",
        "    except:\n",
        "        return 50\n",
        "\n",
        "def calculate_technical_score(row):\n",
        "    \"\"\"Score tÃ©cnico baseado em qualidade de produÃ§Ã£o\"\"\"\n",
        "    try:\n",
        "        score = 0\n",
        "\n",
        "        if pd.notna(row['brilho_medio']):\n",
        "            if 120 <= row['brilho_medio'] <= 180: score += 25\n",
        "            elif 100 <= row['brilho_medio'] <= 200: score += 20\n",
        "            else: score += 10\n",
        "\n",
        "        formato = str(row['formato_detectado'])\n",
        "        if 'vertical_9_16' in formato: score += 25\n",
        "        elif 'horizontal_16_9' in formato: score += 20\n",
        "        else: score += 15\n",
        "\n",
        "        if row['tem_audio']: score += 25\n",
        "        else: score += 5\n",
        "\n",
        "        if pd.notna(row['total_frames']) and row['total_frames'] > 0:\n",
        "            if row['total_frames'] > 300: score += 25\n",
        "            elif row['total_frames'] > 150: score += 20\n",
        "            else: score += 15\n",
        "\n",
        "        return min(score, 100)\n",
        "    except:\n",
        "        return 50\n",
        "\n",
        "def calculate_content_score(row):\n",
        "    \"\"\"Score de conteÃºdo baseado em riqueza informacional\"\"\"\n",
        "    try:\n",
        "        score = 0\n",
        "\n",
        "        ocr_count = row['ocr_textos_count'] if pd.notna(row['ocr_textos_count']) else 0\n",
        "        audio_len = row['audio_transcrito_len'] if pd.notna(row['audio_transcrito_len']) else 0\n",
        "\n",
        "        if ocr_count > 5 or audio_len > 100: score += 30\n",
        "        elif ocr_count > 2 or audio_len > 50: score += 20\n",
        "        elif ocr_count > 0 or audio_len > 0: score += 15\n",
        "        else: score += 5\n",
        "\n",
        "        if pd.notna(row['bpm_audio']):\n",
        "            if 120 <= row['bpm_audio'] <= 140: score += 35\n",
        "            elif 100 <= row['bpm_audio'] <= 160: score += 25\n",
        "            else: score += 15\n",
        "\n",
        "        if pd.notna(row['duracao_segundos']) and row['duracao_segundos'] > 0:\n",
        "            densidade = (ocr_count + audio_len/10) / row['duracao_segundos']\n",
        "            if densidade > 2: score += 35\n",
        "            elif densidade > 1: score += 25\n",
        "            else: score += 15\n",
        "\n",
        "        return min(score, 100)\n",
        "    except:\n",
        "        return 50\n",
        "\n",
        "def generate_insights_from_data(df):\n",
        "    \"\"\"Gera insights inteligentes baseados nos dados\"\"\"\n",
        "    insights = []\n",
        "\n",
        "    try:\n",
        "        best_performing = df.nlargest(3, 'viral_score')\n",
        "        avg_duration = best_performing['duracao_segundos'].mean()\n",
        "        insights.append(f\"DURAÃ‡ÃƒO VENCEDORA: Seus top 3 vÃ­deos tÃªm duraÃ§Ã£o mÃ©dia de {avg_duration:.1f}s. Este Ã© seu sweet spot comprovado.\")\n",
        "\n",
        "        avg_cuts_per_sec = (best_performing['cortes_detectados_count'] / best_performing['duracao_segundos']).mean()\n",
        "        insights.append(f\"RITMO IDEAL: {avg_cuts_per_sec:.1f} cortes por segundo Ã© sua fÃ³rmula de ediÃ§Ã£o mais eficaz.\")\n",
        "\n",
        "        formato_winner = df['formato_detectado'].mode()[0] if not df['formato_detectado'].empty else 'N/A'\n",
        "        formato_count = df['formato_detectado'].value_counts().iloc[0] if not df['formato_detectado'].empty else 0\n",
        "        insights.append(f\"FORMATO DOMINANTE: {formato_count} vÃ­deos em {formato_winner}. Este Ã© seu formato de maior alcance.\")\n",
        "\n",
        "        high_viral = df[df['viral_score'] > 70]\n",
        "        if not high_viral.empty:\n",
        "            avg_complexity = high_viral['complexidade_visual_media'].mean()\n",
        "            insights.append(f\"COMPLEXIDADE VISUAL Ã“TIMA: VÃ­deos com score viral alto tÃªm complexidade mÃ©dia de {avg_complexity:.0f}. Use como referÃªncia.\")\n",
        "\n",
        "        text_heavy = df[df['ocr_textos_count'] > 5]\n",
        "        if not text_heavy.empty:\n",
        "            insights.append(f\"ESTRATÃ‰GIA DE TEXTO: {len(text_heavy)} vÃ­deos com muito texto tÃªm score mÃ©dio de {text_heavy['viral_score'].mean():.0f}. Texto na tela impacta performance.\")\n",
        "\n",
        "        # CORRIGIDO: bpm_audio em vez de bmp_audio\n",
        "        if df['bpm_audio'].notna().any():\n",
        "            successful_bpm = df[df['viral_score'] > 60]['bpm_audio'].mean()\n",
        "            insights.append(f\"BPM DE SUCESSO: {successful_bpm:.0f} BPM Ã© o ritmo de Ã¡udio dos seus vÃ­deos mais virais.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_progress(f\"Erro ao gerar insights: {e}\")\n",
        "        insights.append(\"Insights parciais disponÃ­veis devido a limitaÃ§Ãµes nos dados.\")\n",
        "\n",
        "    return insights\n",
        "\n",
        "def add_data_to_sheet(ws, data, start_row=1, start_col=1, headers=None):\n",
        "    \"\"\"Adiciona dados a uma planilha de forma segura\"\"\"\n",
        "    current_row = start_row\n",
        "\n",
        "    # Adicionar cabeÃ§alhos se fornecidos\n",
        "    if headers:\n",
        "        for col_idx, header in enumerate(headers):\n",
        "            cell = ws.cell(row=current_row, column=start_col + col_idx)\n",
        "            cell.value = header\n",
        "            cell.font = Font(bold=True)\n",
        "        current_row += 1\n",
        "\n",
        "    # Adicionar dados\n",
        "    for row_data in data:\n",
        "        for col_idx, value in enumerate(row_data):\n",
        "            cell = ws.cell(row=current_row, column=start_col + col_idx)\n",
        "            cell.value = value\n",
        "        current_row += 1\n",
        "\n",
        "    return current_row\n",
        "\n",
        "def create_enhanced_dashboard_master(csv_path, json_path, output_path):\n",
        "    \"\"\"Cria dashboard master executivo aprimorado\"\"\"\n",
        "\n",
        "    log_progress(\"INICIANDO CRIAÃ‡ÃƒO DO DASHBOARD MASTER EXECUTIVO INTELIGENTE\")\n",
        "\n",
        "    try:\n",
        "        # Carregar dados\n",
        "        log_progress(\"Carregando dados consolidados...\")\n",
        "        df_consolidado = pd.read_csv(csv_path, encoding='utf-8')\n",
        "\n",
        "        with open(json_path, 'r', encoding='utf-8') as f:\n",
        "            dados_detalhados = json.load(f)\n",
        "\n",
        "        log_progress(f\"Dados carregados: {len(df_consolidado)} vÃ­deos encontrados\")\n",
        "\n",
        "        # PrÃ©-processamento inteligente\n",
        "        log_progress(\"Processando inteligÃªncia artificial dos dados...\")\n",
        "\n",
        "        # Limpar e converter dados\n",
        "        try:\n",
        "            df_consolidado['emocoes_predominantes'] = df_consolidado['emocoes_predominantes'].apply(\n",
        "                lambda x: json.loads(x.replace(\"'\", '\"')) if pd.notna(x) and x != '{}' else {}\n",
        "            )\n",
        "        except:\n",
        "            df_consolidado['emocoes_predominantes'] = [{}] * len(df_consolidado)\n",
        "\n",
        "        # Calcular scores inteligentes\n",
        "        log_progress(\"Calculando scores de performance...\")\n",
        "        df_consolidado['viral_score'] = df_consolidado.apply(calculate_viral_score, axis=1)\n",
        "        df_consolidado['technical_score'] = df_consolidado.apply(calculate_technical_score, axis=1)\n",
        "        df_consolidado['content_score'] = df_consolidado.apply(calculate_content_score, axis=1)\n",
        "        df_consolidado['overall_score'] = (df_consolidado['viral_score'] + df_consolidado['technical_score'] + df_consolidado['content_score']) / 3\n",
        "\n",
        "        # Calcular mÃ©tricas avanÃ§adas\n",
        "        df_consolidado['cortes_por_segundo'] = df_consolidado['cortes_detectados_count'] / df_consolidado['duracao_segundos'].replace(0, 1)\n",
        "        df_consolidado['densidade_texto'] = df_consolidado['ocr_textos_count'] / df_consolidado['duracao_segundos'].replace(0, 1)\n",
        "        df_consolidado['eficiencia_audio'] = df_consolidado['audio_transcrito_len'] / df_consolidado['duracao_segundos'].replace(0, 1)\n",
        "\n",
        "        log_progress(\"Gerando insights estratÃ©gicos...\")\n",
        "        insights = generate_insights_from_data(df_consolidado)\n",
        "\n",
        "        # Criar workbook\n",
        "        log_progress(\"Criando estrutura do dashboard...\")\n",
        "        wb = Workbook()\n",
        "\n",
        "        # === ABA 1: EXECUTIVE SUMMARY ===\n",
        "        log_progress(\"Criando Executive Summary...\")\n",
        "        ws_summary = wb.active\n",
        "        ws_summary.title = 'Executive Summary'\n",
        "\n",
        "        # Header principal\n",
        "        header_cell = ws_summary.cell(row=1, column=1)\n",
        "        header_cell.value = 'DASHBOARD MASTER EXECUTIVO - ENGENHARIA REVERSA DE VÃDEOS'\n",
        "        header_cell.font = Font(bold=True, size=18, color='FFFFFF')\n",
        "        header_cell.fill = PatternFill(start_color='1F4E79', end_color='1F4E79', fill_type='solid')\n",
        "        header_cell.alignment = Alignment(horizontal='center', vertical='center')\n",
        "\n",
        "        # Expandir header manualmente\n",
        "        for col in range(2, 9):\n",
        "            cell = ws_summary.cell(row=1, column=col)\n",
        "            cell.fill = PatternFill(start_color='1F4E79', end_color='1F4E79', fill_type='solid')\n",
        "\n",
        "        # KPIs Principais\n",
        "        kpi_cell = ws_summary.cell(row=3, column=1)\n",
        "        kpi_cell.value = 'INDICADORES DE PERFORMANCE PRINCIPAIS'\n",
        "        kpi_cell.font = Font(bold=True, size=14)\n",
        "        kpi_cell.fill = PatternFill(start_color='E7E6E6', end_color='E7E6E6', fill_type='solid')\n",
        "\n",
        "        kpis_data = [\n",
        "            ['Total de VÃ­deos Analisados', len(df_consolidado)],\n",
        "            ['Score Viral MÃ©dio', f\"{df_consolidado['viral_score'].mean():.1f}/100\"],\n",
        "            ['Score TÃ©cnico MÃ©dio', f\"{df_consolidado['technical_score'].mean():.1f}/100\"],\n",
        "            ['Score de ConteÃºdo MÃ©dio', f\"{df_consolidado['content_score'].mean():.1f}/100\"],\n",
        "            ['DuraÃ§Ã£o MÃ©dia Otimizada', f\"{df_consolidado['duracao_segundos'].mean():.1f}s\"],\n",
        "            ['Ritmo MÃ©dio de Cortes', f\"{df_consolidado['cortes_por_segundo'].mean():.1f}/seg\"],\n",
        "        ]\n",
        "\n",
        "        add_data_to_sheet(ws_summary, kpis_data, start_row=4, start_col=1)\n",
        "\n",
        "        # Top 3 VÃ­deos\n",
        "        top3_cell = ws_summary.cell(row=3, column=4)\n",
        "        top3_cell.value = 'TOP 3 VÃDEOS POR PERFORMANCE'\n",
        "        top3_cell.font = Font(bold=True, size=14)\n",
        "        top3_cell.fill = PatternFill(start_color='E7E6E6', end_color='E7E6E6', fill_type='solid')\n",
        "\n",
        "        top3 = df_consolidado.nlargest(3, 'overall_score')[['nome_arquivo', 'overall_score', 'viral_score', 'technical_score', 'content_score']]\n",
        "\n",
        "        top3_data = []\n",
        "        for _, video in top3.iterrows():\n",
        "            nome_curto = video['nome_arquivo'][:30] + \"...\" if len(video['nome_arquivo']) > 30 else video['nome_arquivo']\n",
        "            top3_data.append([\n",
        "                nome_curto,\n",
        "                f\"{video['overall_score']:.1f}\",\n",
        "                f\"{video['viral_score']:.1f}\",\n",
        "                f\"{video['technical_score']:.1f}\",\n",
        "                f\"{video['content_score']:.1f}\"\n",
        "            ])\n",
        "\n",
        "        top3_headers = ['VÃ­deo', 'Score Geral', 'Viral', 'TÃ©cnico', 'ConteÃºdo']\n",
        "        add_data_to_sheet(ws_summary, top3_data, start_row=4, start_col=4, headers=top3_headers)\n",
        "\n",
        "        # Insights EstratÃ©gicos\n",
        "        insights_cell = ws_summary.cell(row=12, column=1)\n",
        "        insights_cell.value = 'INSIGHTS ESTRATÃ‰GICOS BASEADOS EM IA'\n",
        "        insights_cell.font = Font(bold=True, size=14, color='FFFFFF')\n",
        "        insights_cell.fill = PatternFill(start_color='C5504B', end_color='C5504B', fill_type='solid')\n",
        "        insights_cell.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Adicionar insights\n",
        "        for i, insight in enumerate(insights, 13):\n",
        "            insight_cell = ws_summary.cell(row=i, column=1)\n",
        "            insight_cell.value = f\"â€¢ {insight}\"\n",
        "            insight_cell.alignment = Alignment(wrap_text=True)\n",
        "\n",
        "        # === ABA 2: ANÃLISE DE PERFORMANCE ===\n",
        "        log_progress(\"Criando AnÃ¡lise de Performance...\")\n",
        "        ws_performance = wb.create_sheet('AnÃ¡lise de Performance')\n",
        "\n",
        "        perf_header = ws_performance.cell(row=1, column=1)\n",
        "        perf_header.value = 'ANÃLISE DETALHADA DE PERFORMANCE'\n",
        "        perf_header.font = Font(bold=True, size=16)\n",
        "        perf_header.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Ranking completo\n",
        "        ranking_data = df_consolidado[['nome_arquivo', 'overall_score', 'viral_score', 'technical_score', 'content_score',\n",
        "                                     'duracao_segundos', 'cortes_por_segundo', 'formato_detectado']].sort_values('overall_score', ascending=False)\n",
        "\n",
        "        ranking_list = []\n",
        "        for _, video in ranking_data.iterrows():\n",
        "            nome_curto = video['nome_arquivo'][:40] + \"...\" if len(video['nome_arquivo']) > 40 else video['nome_arquivo']\n",
        "            ranking_list.append([\n",
        "                nome_curto,\n",
        "                f\"{video['overall_score']:.1f}\",\n",
        "                f\"{video['viral_score']:.1f}\",\n",
        "                f\"{video['technical_score']:.1f}\",\n",
        "                f\"{video['content_score']:.1f}\",\n",
        "                f\"{video['duracao_segundos']:.1f}s\",\n",
        "                f\"{video['cortes_por_segundo']:.1f}\",\n",
        "                video['formato_detectado']\n",
        "            ])\n",
        "\n",
        "        ranking_headers = ['VÃ­deo', 'Score Geral', 'Viral', 'TÃ©cnico', 'ConteÃºdo', 'DuraÃ§Ã£o', 'Cortes/s', 'Formato']\n",
        "        add_data_to_sheet(ws_performance, ranking_list, start_row=3, start_col=1, headers=ranking_headers)\n",
        "\n",
        "        # === ABA 3: INTELIGÃŠNCIA TÃ‰CNICA ===\n",
        "        log_progress(\"Criando InteligÃªncia TÃ©cnica...\")\n",
        "        ws_tecnica = wb.create_sheet('InteligÃªncia TÃ©cnica')\n",
        "\n",
        "        tec_header = ws_tecnica.cell(row=1, column=1)\n",
        "        tec_header.value = 'ANÃLISE TÃ‰CNICA AVANÃ‡ADA'\n",
        "        tec_header.font = Font(bold=True, size=16)\n",
        "        tec_header.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # AnÃ¡lise de correlaÃ§Ãµes\n",
        "        corr_header = ws_tecnica.cell(row=3, column=1)\n",
        "        corr_header.value = 'CORRELAÃ‡Ã•ES DESCOBERTAS'\n",
        "        corr_header.font = Font(bold=True, size=12)\n",
        "\n",
        "        correlations_data = [\n",
        "            ['DuraÃ§Ã£o vs Score Viral', f\"{df_consolidado['duracao_segundos'].corr(df_consolidado['viral_score']):.3f}\", 'CORRELAÃ‡ÃƒO MODERADA'],\n",
        "            ['Cortes/s vs Score Viral', f\"{df_consolidado['cortes_por_segundo'].corr(df_consolidado['viral_score']):.3f}\", 'CORRELAÃ‡ÃƒO MODERADA'],\n",
        "            ['Complexidade Visual vs Performance', f\"{df_consolidado['complexidade_visual_media'].corr(df_consolidado['overall_score']):.3f}\", 'CORRELAÃ‡ÃƒO FRACA'],\n",
        "            ['BPM vs Engajamento', f\"{df_consolidado['bpm_audio'].corr(df_consolidado['viral_score']) if df_consolidado['bpm_audio'].notna().any() else 0:.3f}\", 'CORRELAÃ‡ÃƒO FRACA'],\n",
        "        ]\n",
        "\n",
        "        corr_headers = ['MÃ©trica', 'CorrelaÃ§Ã£o', 'ClassificaÃ§Ã£o']\n",
        "        add_data_to_sheet(ws_tecnica, correlations_data, start_row=4, start_col=1, headers=corr_headers)\n",
        "\n",
        "        # === ABA 4: BLUEPRINT DE PRODUÃ‡ÃƒO ===\n",
        "        log_progress(\"Criando Blueprint de ProduÃ§Ã£o...\")\n",
        "        ws_blueprint = wb.create_sheet('Blueprint de ProduÃ§Ã£o')\n",
        "\n",
        "        bp_header = ws_blueprint.cell(row=1, column=1)\n",
        "        bp_header.value = 'BLUEPRINT ESTRATÃ‰GICO DE PRODUÃ‡ÃƒO'\n",
        "        bp_header.font = Font(bold=True, size=16, color='FFFFFF')\n",
        "        bp_header.fill = PatternFill(start_color='1F4E79', end_color='1F4E79', fill_type='solid')\n",
        "        bp_header.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Receita de sucesso baseada nos top performers\n",
        "        top_performers = df_consolidado[df_consolidado['overall_score'] > df_consolidado['overall_score'].quantile(0.7)]\n",
        "\n",
        "        blueprint_data = [\n",
        "            ['DURAÃ‡ÃƒO IDEAL', f\"{top_performers['duracao_segundos'].mean():.1f} segundos (Â±{top_performers['duracao_segundos'].std():.1f}s)\"],\n",
        "            ['RITMO DE EDIÃ‡ÃƒO', f\"{top_performers['cortes_por_segundo'].mean():.1f} cortes por segundo\"],\n",
        "            ['FORMATO VENCEDOR', top_performers['formato_detectado'].mode()[0] if not top_performers.empty else 'N/A'],\n",
        "            ['COMPLEXIDADE VISUAL', f\"NÃ­vel {top_performers['complexidade_visual_media'].mean():.0f} (escala de estÃ­mulo)\"],\n",
        "            ['BPM RECOMENDADO', f\"{top_performers['bpm_audio'].mean():.0f} BPM\" if top_performers['bpm_audio'].notna().any() else 'N/A'],\n",
        "            ['DENSIDADE DE TEXTO', f\"{top_performers['densidade_texto'].mean():.1f} textos por segundo\"],\n",
        "        ]\n",
        "\n",
        "        bp_sub_header = ws_blueprint.cell(row=3, column=1)\n",
        "        bp_sub_header.value = 'FÃ“RMULA DE SUCESSO BASEADA EM DADOS'\n",
        "        bp_sub_header.font = Font(bold=True, size=12)\n",
        "\n",
        "        add_data_to_sheet(ws_blueprint, blueprint_data, start_row=4, start_col=1)\n",
        "\n",
        "        # === ABA 5: RECOMENDAÃ‡Ã•ES ESTRATÃ‰GICAS ===\n",
        "        log_progress(\"Criando RecomendaÃ§Ãµes EstratÃ©gicas...\")\n",
        "        ws_recomendacoes = wb.create_sheet('RecomendaÃ§Ãµes EstratÃ©gicas')\n",
        "\n",
        "        rec_header = ws_recomendacoes.cell(row=1, column=1)\n",
        "        rec_header.value = 'RECOMENDAÃ‡Ã•ES ESTRATÃ‰GICAS BASEADAS EM IA'\n",
        "        rec_header.font = Font(bold=True, size=16, color='FFFFFF')\n",
        "        rec_header.fill = PatternFill(start_color='C5504B', end_color='C5504B', fill_type='solid')\n",
        "        rec_header.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # RecomendaÃ§Ãµes inteligentes baseadas nos dados\n",
        "        recommendations = []\n",
        "\n",
        "        # AnÃ¡lise de duraÃ§Ã£o\n",
        "        if df_consolidado['duracao_segundos'].mean() > 60:\n",
        "            recommendations.append(['DURAÃ‡ÃƒO', 'REDUZA DURAÃ‡ÃƒO', 'Seus vÃ­deos estÃ£o longos demais. VÃ­deos de 15-30s tÃªm melhor performance.', 'ALTA'])\n",
        "        elif df_consolidado['duracao_segundos'].mean() < 15:\n",
        "            recommendations.append(['DURAÃ‡ÃƒO', 'AUMENTE DURAÃ‡ÃƒO', 'VÃ­deos muito curtos podem nÃ£o transmitir valor suficiente.', 'MÃ‰DIA'])\n",
        "\n",
        "        # AnÃ¡lise de ritmo\n",
        "        avg_cuts_per_sec = df_consolidado['cortes_por_segundo'].mean()\n",
        "        if avg_cuts_per_sec < 5:\n",
        "            recommendations.append(['EDIÃ‡ÃƒO', 'ACELERE O RITMO', 'Aumente o nÃºmero de cortes para manter atenÃ§Ã£o. Meta: 8-12 cortes/segundo.', 'ALTA'])\n",
        "        elif avg_cuts_per_sec > 20:\n",
        "            recommendations.append(['EDIÃ‡ÃƒO', 'DIMINUA CORTES', 'Muitos cortes podem causar fadiga visual. Encontre o equilÃ­brio.', 'MÃ‰DIA'])\n",
        "\n",
        "        # AnÃ¡lise de formato\n",
        "        formato_dominante = df_consolidado['formato_detectado'].mode()[0] if not df_consolidado['formato_detectado'].empty else 'N/A'\n",
        "        if 'horizontal' in formato_dominante.lower():\n",
        "            recommendations.append(['FORMATO', 'FOQUE EM VERTICAL', 'Formato vertical (9:16) tem melhor performance em redes sociais.', 'ALTA'])\n",
        "\n",
        "        # AnÃ¡lise de texto\n",
        "        if df_consolidado['densidade_texto'].mean() < 1:\n",
        "            recommendations.append(['CONTEÃšDO', 'ADICIONE MAIS TEXTO', 'Textos na tela aumentam retenÃ§Ã£o e acessibilidade.', 'MÃ‰DIA'])\n",
        "\n",
        "        rec_headers = ['Categoria', 'AÃ§Ã£o', 'Justificativa', 'Prioridade']\n",
        "        add_data_to_sheet(ws_recomendacoes, recommendations, start_row=3, start_col=1, headers=rec_headers)\n",
        "\n",
        "        # Salvar arquivo\n",
        "        log_progress(\"Salvando dashboard...\")\n",
        "        wb.save(output_path)\n",
        "\n",
        "        log_progress(\"DASHBOARD MASTER EXECUTIVO CRIADO COM SUCESSO!\")\n",
        "        log_progress(f\"Arquivo salvo em: {output_path}\")\n",
        "        log_progress(f\"{len(df_consolidado)} vÃ­deos analisados\")\n",
        "        log_progress(f\"{len(insights)} insights estratÃ©gicos gerados\")\n",
        "        log_progress(f\"{len(recommendations)} recomendaÃ§Ãµes criadas\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        log_progress(f\"ERRO CRÃTICO: {e}\")\n",
        "        log_progress(\"Verifique os arquivos de entrada e tente novamente\")\n",
        "        return False\n",
        "\n",
        "def main():\n",
        "    \"\"\"FunÃ§Ã£o principal de execuÃ§Ã£o\"\"\"\n",
        "    log_progress(\"INICIANDO SISTEMA DE DASHBOARD INTELIGENTE\")\n",
        "\n",
        "    # Configurar caminhos\n",
        "    BASE_PATH = \"/content/drive/MyDrive/Videos Dona Done/_engenharia_reversa\"\n",
        "    CSV_PATH = os.path.join(BASE_PATH, \"dashboard\", \"dados_consolidados.csv\")\n",
        "    JSON_PATH = os.path.join(BASE_PATH, \"dashboard\", \"dados_detalhados.json\")\n",
        "    OUTPUT_PATH = os.path.join(BASE_PATH, \"dashboard\", \"DASHBOARD_MASTER_EXECUTIVO_INTELIGENTE.xlsx\")\n",
        "\n",
        "    # Verificar se arquivos existem\n",
        "    if not os.path.exists(CSV_PATH):\n",
        "        log_progress(f\"ERRO: Arquivo CSV nÃ£o encontrado: {CSV_PATH}\")\n",
        "        return False\n",
        "\n",
        "    if not os.path.exists(JSON_PATH):\n",
        "        log_progress(f\"ERRO: Arquivo JSON nÃ£o encontrado: {JSON_PATH}\")\n",
        "        return False\n",
        "\n",
        "    # Executar criaÃ§Ã£o do dashboard\n",
        "    success = create_enhanced_dashboard_master(CSV_PATH, JSON_PATH, OUTPUT_PATH)\n",
        "\n",
        "    if success:\n",
        "        log_progress(\"PROCESSO CONCLUÃDO COM SUCESSO!\")\n",
        "        log_progress(\"Dashboard inteligente pronto para uso estratÃ©gico\")\n",
        "    else:\n",
        "        log_progress(\"PROCESSO FALHOU - Verifique os logs acima\")\n",
        "\n",
        "    return success\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "3QgJMmh1JJ-0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3536bd55-70ac-4ff7-d141-9c590232e8ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ” VERIFICANDO PRÃ‰-REQUISITOS...\n",
            "Pasta base existe: True\n",
            "CSV existe: True\n",
            "JSON existe: True\n",
            "ðŸ“Š Dados CSV: 3 vÃ­deos encontrados\n",
            "\n",
            "âœ… Se todos os itens acima sÃ£o True/existem, vocÃª pode prosseguir!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Verificar se o processo de engenharia reversa foi executado\n",
        "BASE_PATH = \"/content/drive/MyDrive/Videos Dona Done/_engenharia_reversa\"\n",
        "CSV_PATH = os.path.join(BASE_PATH, \"dashboard\", \"dados_consolidados.csv\")\n",
        "JSON_PATH = os.path.join(BASE_PATH, \"dashboard\", \"dados_detalhados.json\")\n",
        "\n",
        "print(\"ðŸ” VERIFICANDO PRÃ‰-REQUISITOS...\")\n",
        "print(f\"Pasta base existe: {os.path.exists(BASE_PATH)}\")\n",
        "print(f\"CSV existe: {os.path.exists(CSV_PATH)}\")\n",
        "print(f\"JSON existe: {os.path.exists(JSON_PATH)}\")\n",
        "\n",
        "if os.path.exists(CSV_PATH):\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "    print(f\"ðŸ“Š Dados CSV: {len(df)} vÃ­deos encontrados\")\n",
        "\n",
        "print(\"\\nâœ… Se todos os itens acima sÃ£o True/existem, vocÃª pode prosseguir!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyhMy4Pbt7Y-",
        "outputId": "2ae9c517-e9d8-45b7-b971-9a1e7880dc4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ INICIANDO INTEGRAÃ‡ÃƒO AUTOMÃTICA DE TODAS AS ANÃLISES\n",
            "============================================================\n",
            "\n",
            "ðŸ” DESCOBRINDO ANÃLISES DISPONÃVEIS...\n",
            "âœ… AnÃ¡lise base encontrada: metadados\n",
            "âœ… AnÃ¡lise base encontrada: decomposicao\n",
            "âœ… AnÃ¡lise base encontrada: padroes\n",
            "âœ… AnÃ¡lise base encontrada: psicologica\n",
            "âœ… AnÃ¡lise personalizada encontrada: Ai Insights Completos\n",
            "âœ… AnÃ¡lise personalizada encontrada: Videos Descobertos\n",
            "âœ… AnÃ¡lise adicional encontrada: Audio Refinada\n",
            "âœ… AnÃ¡lise adicional encontrada: Viral\n",
            "âœ… AnÃ¡lise adicional encontrada: Copywriting\n",
            "âœ… AnÃ¡lise personalizada encontrada: Legendas Geradas\n",
            "ðŸ“Š Total de anÃ¡lises encontradas: 10\n",
            "\n",
            "ðŸ”„ CONSOLIDANDO TODOS OS DADOS...\n",
            "DataFrame base criado com 3 vÃ­deos e 21 colunas.\n",
            "ðŸ”„ Integrando dados de: decomposicao\n",
            "Merge com decomposicao concluÃ­do. Total de colunas: 23\n",
            "ðŸ”„ Integrando dados de: padroes\n",
            "Merge com padroes concluÃ­do. Total de colunas: 31\n",
            "ðŸ”„ Integrando dados de: psicologica\n",
            "Merge com psicologica concluÃ­do. Total de colunas: 35\n",
            "ðŸ”„ Integrando dados de: ai_insights_completos.json\n",
            "Merge com ai_insights_completos.json concluÃ­do. Total de colunas: 42\n",
            "ðŸ”„ Integrando dados de: videos_descobertos.json\n",
            "Merge com videos_descobertos.json concluÃ­do. Total de colunas: 45\n",
            "ðŸ”„ Integrando dados de: audio_refinada\n",
            "Merge com audio_refinada concluÃ­do. Total de colunas: 81\n",
            "ðŸ”„ Integrando dados de: viral\n",
            "ðŸ”„ Integrando dados de: copywriting\n",
            "Merge com copywriting concluÃ­do. Total de colunas: 96\n",
            "ðŸ”„ Integrando dados de: legendas_geradas.json\n",
            "Merge com legendas_geradas.json concluÃ­do. Total de colunas: 98\n",
            "ðŸ§¹ Flattening nested structures for Excel compatibility...\n",
            "ðŸ“ˆ 6 vÃ­deos consolidados com 104 mÃ©tricas totais\n",
            "\n",
            "ðŸ“Š GERANDO DASHBOARD DINÃ‚MICO...\n",
            "ðŸ“Š Tentando integrar aba de anÃ¡lise viral...\n",
            "âœ… Aba de AnÃ¡lise Viral adicionada ao dashboard\n",
            "ðŸ”— IntegraÃ§Ã£o viral concluÃ­da\n",
            "\n",
            "ðŸ’¾ SALVANDO DADOS CONSOLIDADOS...\n",
            "ðŸ“ Dados CSV salvos: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dashboard/dados_completos_consolidados.csv\n",
            "ðŸ“ Dados JSON salvos: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dashboard/dados_completos_consolidados.json\n",
            "\n",
            "âš™ï¸ ATUALIZANDO CONFIGURAÃ‡Ã•ES...\n",
            "âœ… Arquivo config.json atualizado com sucesso.\n",
            "\n",
            "âœ… INTEGRAÃ‡ÃƒO AUTOMÃTICA CONCLUÃDA COM SUCESSO!\n",
            "============================================================\n",
            "ðŸ“ Dashboard dinÃ¢mico completo: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dashboard/RELATORIO_COMPLETO_DINAMICO.xlsx\n",
            "ðŸ“Š 6 vÃ­deos processados com 104 mÃ©tricas totais integradas.\n",
            "\n",
            "ðŸŽ¯ PRÃ“XIMOS PASSOS:\n",
            "â€¢ Abra o arquivo Excel gerado para explorar todas as anÃ¡lises integradas.\n",
            "â€¢ Use os arquivos CSV/JSON para anÃ¡lises mais avanÃ§adas em outras ferramentas.\n",
            "â€¢ Execute esta cÃ©lula novamente sempre que adicionar novas anÃ¡lises ao seu projeto.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# SISTEMA DE INTEGRAÃ‡ÃƒO AUTOMÃTICA PARA NOVAS FUNCIONALIDADES\n",
        "# ============================================================================\n",
        "# Este script deve SUBSTITUIR a Ãºltima cÃ©lula (4.2) do notebook\n",
        "# Ele detecta automaticamente todas as anÃ¡lises disponÃ­veis e as integra\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import glob\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ADICIONAR ESTAS FUNÃ‡Ã•ES:\n",
        "def detectar_dados_virais_disponiveis(pasta_trabalho):\n",
        "    \"\"\"Detecta se dados virais estÃ£o disponÃ­veis\"\"\"\n",
        "    try:\n",
        "        viral_integration_path = os.path.join(pasta_trabalho, \"dados\", \"viral_integration_data.json\")\n",
        "        if not os.path.exists(viral_integration_path):\n",
        "            return False, None\n",
        "        with open(viral_integration_path, 'r', encoding='utf-8') as f:\n",
        "            viral_data = json.load(f)\n",
        "        return viral_data.get('viral_disponivel', False), viral_data\n",
        "    except Exception:\n",
        "        return False, None\n",
        "\n",
        "def enriquecer_consolidacao_com_dados_virais(df_consolidado, pasta_trabalho):\n",
        "    \"\"\"Enriquece o DataFrame consolidado com dados virais, se disponÃ­veis\"\"\"\n",
        "    viral_disponivel, viral_integration = detectar_dados_virais_disponiveis(pasta_trabalho)\n",
        "\n",
        "    if not viral_disponivel:\n",
        "        print(\"âš ï¸ Dados virais nÃ£o disponÃ­veis para enriquecer o dashboard.\")\n",
        "        return df_consolidado\n",
        "\n",
        "    try:\n",
        "        # Carregar dados de mapeamento viral completo\n",
        "        mapeamento_path = os.path.join(pasta_trabalho, \"analise_viral\", \"mapeamento_viral_completo.json\")\n",
        "        if not os.path.exists(mapeamento_path):\n",
        "            print(f\"âš ï¸ Arquivo de mapeamento viral nÃ£o encontrado: {mapeamento_path}\")\n",
        "            return df_consolidado\n",
        "\n",
        "        with open(mapeamento_path, 'r', encoding='utf-8') as f:\n",
        "            mapeamento_viral = json.load(f)\n",
        "\n",
        "        # Converter para DataFrame para facilitar o merge\n",
        "        df_viral = pd.DataFrame(mapeamento_viral)\n",
        "\n",
        "        # Selecionar e renomear colunas relevantes para o merge\n",
        "        cols_viral = ['video_id', 'tem_dados_virais', 'classificacao_performance']\n",
        "        if 'viral_data' in df_viral.columns:\n",
        "             # Extrair dados aninhados\n",
        "            df_viral = pd.json_normalize(mapeamento_viral, sep='_')\n",
        "            cols_viral = [col for col in df_viral.columns if col.startswith(('video_id', 'tem_dados_virais', 'classificacao_performance', 'viral_data_', 'metricas_calculadas_'))]\n",
        "            # Renomear colunas aninhadas para algo mais legÃ­vel se necessÃ¡rio, ex: viral_data_views -> views_viral\n",
        "            df_viral.columns = df_viral.columns.str.replace('viral_data_', '').str.replace('metricas_calculadas_', '')\n",
        "\n",
        "\n",
        "        df_viral_subset = df_viral[cols_viral].copy()\n",
        "\n",
        "        # Garantir que a coluna de merge tem o mesmo nome\n",
        "        if 'video_id' in df_consolidado.columns:\n",
        "            df_merged = pd.merge(df_consolidado, df_viral_subset, on='video_id', how='left', suffixes=('', '_viral'))\n",
        "        elif 'id' in df_consolidado.columns:\n",
        "             # Tentar merge por 'id' se 'video_id' nÃ£o existir no df consolidado\n",
        "            df_merged = pd.merge(df_consolidado, df_viral_subset, left_on='id', right_on='video_id', how='left', suffixes=('', '_viral'))\n",
        "            # Remover coluna 'video_id' duplicada se merge foi por 'id'\n",
        "            if 'video_id_viral' in df_merged.columns:\n",
        "                df_merged = df_merged.drop(columns=['video_id_viral'])\n",
        "        else:\n",
        "            print(\"âš ï¸ Coluna 'video_id' ou 'id' nÃ£o encontrada no DataFrame consolidado. NÃ£o foi possÃ­vel integrar dados virais.\")\n",
        "            return df_consolidado\n",
        "\n",
        "\n",
        "        # Tratar valores NaN apÃ³s o merge (para colunas numÃ©ricas de viral)\n",
        "        for col in df_merged.columns:\n",
        "            if col.startswith(('views', 'likes', 'comments', 'engagement_rate', 'virality_score')) and df_merged[col].dtype == 'float64':\n",
        "                 df_merged[col] = df_merged[col].fillna(0).astype(int if 'rate' not in col else float)\n",
        "\n",
        "\n",
        "        # Preencher status de dados virais para vÃ­deos sem match\n",
        "        if 'tem_dados_virais' not in df_merged.columns:\n",
        "             df_merged['tem_dados_virais'] = False\n",
        "             df_merged['classificacao_performance'] = 'Sem Dados'\n",
        "        else:\n",
        "            df_merged['tem_dados_virais'] = df_merged['tem_dados_virais'].fillna(False)\n",
        "            df_merged['classificacao_performance'] = df_merged['classificacao_performance'].fillna('Sem Dados')\n",
        "\n",
        "\n",
        "        print(\"âœ… Dados virais integrados ao DataFrame consolidado.\")\n",
        "        return df_merged\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Erro ao enriquecer com dados virais: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return df_consolidado # Retorna o original em caso de erro\n",
        "\n",
        "\n",
        "def criar_aba_analise_viral_dashboard(wb, pasta_trabalho):\n",
        "    \"\"\"Cria uma aba dedicada Ã  anÃ¡lise viral no dashboard\"\"\"\n",
        "    viral_disponivel, viral_integration = detectar_dados_virais_disponiveis(pasta_trabalho)\n",
        "\n",
        "    if not viral_disponivel:\n",
        "        print(\"âš ï¸ Dados virais nÃ£o disponÃ­veis. Aba de anÃ¡lise viral nÃ£o serÃ¡ criada.\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        # Carregar insights virais completos\n",
        "        insights_path = os.path.join(pasta_trabalho, \"analise_viral\", \"insights_virais.json\")\n",
        "        if not os.path.exists(insights_path):\n",
        "             print(f\"âš ï¸ Arquivo de insights virais nÃ£o encontrado: {insights_path}\")\n",
        "             return False\n",
        "\n",
        "        with open(insights_path, 'r', encoding='utf-8') as f:\n",
        "            insights = json.load(f)\n",
        "\n",
        "        # Carregar dados de mapeamento viral completo para detalhes por vÃ­deo\n",
        "        mapeamento_path = os.path.join(pasta_trabalho, \"analise_viral\", \"mapeamento_viral_completo.json\")\n",
        "        mapeamento_viral = []\n",
        "        if os.path.exists(mapeamento_path):\n",
        "            with open(mapeamento_path, 'r', encoding='utf-8') as f:\n",
        "                mapeamento_viral = json.load(f)\n",
        "\n",
        "        # Remover aba antiga se existir\n",
        "        if \"AnÃ¡lise Viral Completa\" in wb.sheetnames:\n",
        "            del wb[\"AnÃ¡lise Viral Completa\"]\n",
        "\n",
        "        ws = wb.create_sheet(\"AnÃ¡lise Viral Completa\")\n",
        "\n",
        "        from openpyxl.styles import Font, PatternFill, Alignment\n",
        "        from openpyxl.utils import get_column_letter # Import get_column_letter here\n",
        "\n",
        "\n",
        "        # TÃ­tulo principal\n",
        "        ws.merge_cells(\"A1:H1\")\n",
        "        titulo = ws[\"A1\"]\n",
        "        titulo.value = \"ðŸ“Š ANÃLISE VIRAL ESTRATÃ‰GICA\"\n",
        "        titulo.fill = PatternFill(start_color=\"660066\", end_color=\"660066\", fill_type=\"solid\") # Roxo\n",
        "        titulo.font = Font(color=\"FFFFFF\", bold=True, size=16)\n",
        "        titulo.alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "        row = 3\n",
        "\n",
        "        # Resumo Executivo\n",
        "        ws[f\"A{row}\"] = \"ðŸŽ¯ RESUMO EXECUTIVO\"\n",
        "        ws[f\"A{row}\"].font = Font(bold=True, size=14, color=\"333333\")\n",
        "        row += 2\n",
        "\n",
        "        resumo = insights.get('resumo_geral', {})\n",
        "        if resumo:\n",
        "            resumo_data = [\n",
        "                (\"Total de VÃ­deos Analisados:\", resumo.get('total_videos', 0)),\n",
        "                (\"Views Totais Coletadas:\", f\"{resumo.get('total_views', 0):,}\"),\n",
        "                (\"Likes Totais:\", f\"{resumo.get('total_likes', 0):,}\"),\n",
        "                (\"ComentÃ¡rios Totais:\", f\"{resumo.get('total_comments', 0):,}\"),\n",
        "                (\"Engagement Rate MÃ©dio:\", f\"{resumo.get('avg_engagement_rate', 0):.2%}\"),\n",
        "                (\"Score de Viralidade MÃ©dio:\", f\"{resumo.get('avg_virality_score', 0):.1f}/100\"),\n",
        "                (\"Views por VÃ­deo (MÃ©dio):\", f\"{resumo.get('views_per_video', 0):,}\"),\n",
        "            ]\n",
        "            for label, value in resumo_data:\n",
        "                ws[f\"A{row}\"] = label\n",
        "                ws[f\"B{row}\"] = value\n",
        "                ws[f\"A{row}\"].font = Font(bold=True)\n",
        "                row += 1\n",
        "        else:\n",
        "            ws[f\"A{row}\"] = \"âš ï¸ Resumo viral nÃ£o disponÃ­vel\"\n",
        "            row += 1\n",
        "\n",
        "        row += 2\n",
        "\n",
        "        # Top Performers\n",
        "        ws[f\"A{row}\"] = \"ðŸ† TOP PERFORMERS\"\n",
        "        ws[f\"A{row}\"].font = Font(bold=True, size=14, color=\"333333\")\n",
        "        row += 2\n",
        "\n",
        "        top_p = insights.get('top_performers', {})\n",
        "        if top_p:\n",
        "            top_data = [\n",
        "                (\"TOP 5 Views:\", [f\"{vid} ({views:,})\" for vid, views in top_p.get('top_views', [])]),\n",
        "                (\"TOP 5 Engagement Rate:\", [f\"{vid} ({rate:.2%})\" for vid, rate in top_p.get('top_engagement', [])]),\n",
        "                (\"TOP 5 Virality Score:\", [f\"{vid} ({score:.1f})\" for vid, score in top_p.get('top_virality', [])]),\n",
        "            ]\n",
        "            for label, videos in top_data:\n",
        "                ws[f\"A{row}\"] = label\n",
        "                ws[f\"B{row}\"] = \" â€¢ \" + \"\\n â€¢ \".join(videos)\n",
        "                ws[f\"A{row}\"].font = Font(bold=True)\n",
        "                ws[f\"B{row}\"].alignment = Alignment(wrap_text=True)\n",
        "                row += 1\n",
        "        else:\n",
        "            ws[f\"A{row}\"] = \"âš ï¸ Top performers nÃ£o disponÃ­veis\"\n",
        "            row += 1\n",
        "\n",
        "        row += 2\n",
        "\n",
        "        # AnÃ¡lise de Captions\n",
        "        ws[f\"A{row}\"] = \"ðŸ“ ANÃLISE DE CAPTIONS\"\n",
        "        ws[f\"A{row}\"].font = Font(bold=True, size=14, color=\"333333\")\n",
        "        row += 2\n",
        "\n",
        "        captions_a = insights.get('analise_captions', {})\n",
        "        if captions_a:\n",
        "            caption_data = [\n",
        "                (\"Tamanho MÃ©dio da Caption:\", f\"{captions_a.get('tamanho_medio_caption', 0):.0f} caracteres\"),\n",
        "                (\"Tamanho Ã“timo (Mediana):\", f\"{captions_a.get('tamanho_otimo_caption', 0):.0f} caracteres\"),\n",
        "                (\"Emojis MÃ©dio por Caption:\", f\"{captions_a.get('emojis_medio', 0):.1f}\"),\n",
        "                (\"Hashtags MÃ©dio por Caption:\", f\"{captions_a.get('hashtags_medio', 0):.1f}\"),\n",
        "                (\"Uso de CTA na Caption:\", f\"{captions_a.get('cta_usage_rate', 0):.0%}\"),\n",
        "                (\"Hook Vencedor:\", captions_a.get('hook_vencedor', 'N/A')),\n",
        "                (\"Ranking de Hooks:\", \", \".join([f\"{k} ({v})\" for k,v in captions_a.get('hook_types_ranking', {}).items()]))\n",
        "            ]\n",
        "            for label, value in caption_data:\n",
        "                ws[f\"A{row}\"] = label\n",
        "                ws[f\"B{row}\"] = value\n",
        "                ws[f\"A{row}\"].font = Font(bold=True)\n",
        "                if \"Ranking\" in label:\n",
        "                     ws[f\"B{row}\"].alignment = Alignment(wrap_text=True)\n",
        "                row += 1\n",
        "        else:\n",
        "            ws[f\"A{row}\"] = \"âš ï¸ AnÃ¡lise de captions nÃ£o disponÃ­vel\"\n",
        "            row += 1\n",
        "\n",
        "        row += 2\n",
        "\n",
        "        # RecomendaÃ§Ãµes EstratÃ©gicas\n",
        "        ws[f\"A{row}\"] = \"ðŸ’¡ RECOMENDAÃ‡Ã•ES ESTRATÃ‰GICAS\"\n",
        "        ws[f\"A{row}\"].font = Font(bold=True, size=14, color=\"333333\")\n",
        "        row += 2\n",
        "\n",
        "        recomendacoes = insights.get('recomendacoes_estrategicas', [])\n",
        "        if recomendacoes:\n",
        "            for i, rec in enumerate(recomendacoes, 1):\n",
        "                ws[f\"A{row}\"] = f\"{i}.\"\n",
        "                ws[f\"B{row}\"] = rec\n",
        "                ws[f\"B{row}\"].alignment = Alignment(wrap_text=True)\n",
        "                row += 1\n",
        "        else:\n",
        "            ws[f\"A{row}\"] = \"âœ… Nenhuma recomendaÃ§Ã£o especÃ­fica gerada (boa performance geral)\"\n",
        "            row += 1\n",
        "\n",
        "        row += 2\n",
        "\n",
        "        # Dados Detalhados por VÃ­deo (Tabela)\n",
        "        if mapeamento_viral:\n",
        "            ws[f\"A{row}\"] = \"ðŸ“Š DADOS DETALHADOS POR VÃDEO\"\n",
        "            ws[f\"A{row}\"].font = Font(bold=True, size=14, color=\"333333\")\n",
        "            row += 2\n",
        "\n",
        "            # Preparar dados para a tabela\n",
        "            df_viral_detalhes = pd.DataFrame(mapeamento_viral)\n",
        "\n",
        "            # Selecionar colunas para a tabela no Excel\n",
        "            cols_tabela = [\n",
        "                'video_id', 'nome_arquivo', 'tem_dados_virais', 'classificacao_performance',\n",
        "                'viral_data_views', 'viral_data_likes', 'viral_data_comments', 'viral_data_total_engagement',\n",
        "                'metricas_calculadas_engagement_rate', 'metricas_calculadas_virality_score',\n",
        "                'analise_caption_tamanho', 'analise_caption_emojis_count', 'analise_caption_hashtags_count',\n",
        "                'analise_caption_tem_call_to_action', 'analise_caption_hook_type'\n",
        "            ]\n",
        "\n",
        "            # Garantir que as colunas existem antes de selecionar\n",
        "            cols_tabela_existentes = [col for col in cols_tabela if col in df_viral_detalhes.columns]\n",
        "            df_tabela = df_viral_detalhes[cols_tabela_existentes].copy()\n",
        "\n",
        "            # Renomear colunas para o Excel\n",
        "            df_tabela.columns = [\n",
        "                'ID VÃ­deo', 'Nome Arquivo', 'Tem Dados', 'Performance',\n",
        "                'Views', 'Likes', 'ComentÃ¡rios', 'Engajamento Total',\n",
        "                'Engagement Rate', 'Score Viral',\n",
        "                'Caption Tamanho', 'Caption Emojis', 'Caption Hashtags',\n",
        "                'Caption Tem CTA', 'Caption Hook Tipo'\n",
        "            ][:len(df_tabela.columns)] # Limitar nomes se houver menos colunas\n",
        "\n",
        "            # Formatar colunas numÃ©ricas e booleanas\n",
        "            for col in ['Views', 'Likes', 'ComentÃ¡rios', 'Engajamento Total']:\n",
        "                if col in df_tabela.columns:\n",
        "                    df_tabela[col] = df_tabela[col].apply(lambda x: f\"{int(x):,}\" if pd.notna(x) else 'N/A')\n",
        "            for col in ['Engagement Rate']:\n",
        "                 if col in df_tabela.columns:\n",
        "                    df_tabela[col] = df_tabela[col].apply(lambda x: f\"{x:.2%}\" if pd.notna(x) else 'N/A')\n",
        "            for col in ['Score Viral', 'Caption Tamanho', 'Caption Emojis', 'Caption Hashtags']:\n",
        "                 if col in df_tabela.columns:\n",
        "                     df_tabela[col] = df_tabela[col].apply(lambda x: f\"{x:.1f}\" if pd.notna(x) else 'N/A')\n",
        "            for col in ['Tem Dados', 'Caption Tem CTA']:\n",
        "                 if col in df_tabela.columns:\n",
        "                     df_tabela[col] = df_tabela[col].apply(lambda x: 'Sim' if x else 'NÃ£o')\n",
        "\n",
        "\n",
        "            # Adicionar cabeÃ§alhos da tabela\n",
        "            headers_tabela = list(df_tabela.columns)\n",
        "            for col_idx, header in enumerate(headers_tabela, 1):\n",
        "                cell = ws.cell(row=row, column=col_idx, value=header)\n",
        "                cell.font = Font(bold=True)\n",
        "                cell.fill = PatternFill(start_color=\"E7E6E6\", end_color=\"E7E6E6\", fill_type=\"solid\")\n",
        "            row += 1\n",
        "\n",
        "            # Adicionar dados da tabela\n",
        "            for r_idx, data_row in df_tabela.iterrows():\n",
        "                for c_idx, value in enumerate(data_row.tolist(), 1):\n",
        "                    ws.cell(row=row, column=c_idx, value=value)\n",
        "                row += 1\n",
        "\n",
        "            # Ajustar larguras das colunas da tabela\n",
        "            col_widths_tabela = [15, 25, 12, 15, 12, 12, 12, 15, 15, 12, 12, 12, 12, 12, 15]\n",
        "            for i, width in enumerate(col_widths_tabela[:len(headers_tabela)], 1):\n",
        "                ws.column_dimensions[get_column_letter(i)].width = width\n",
        "\n",
        "        else:\n",
        "            ws[f\"A{row}\"] = \"âš ï¸ Dados detalhados por vÃ­deo nÃ£o disponÃ­veis\"\n",
        "            row += 1\n",
        "\n",
        "\n",
        "        # Ajustar larguras das colunas principais (resumo, top performers etc.)\n",
        "        col_widths_resumo = [30, 60] # Ajuste conforme necessÃ¡rio\n",
        "        for i, width in enumerate(col_widths_resumo, 1):\n",
        "             ws.column_dimensions[get_column_letter(i)].width = width\n",
        "\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Erro ao criar aba de anÃ¡lise viral: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "\n",
        "def descobrir_analises_disponiveis(pasta_trabalho):\n",
        "    \"\"\"Descobre automaticamente todas as anÃ¡lises realizadas\"\"\"\n",
        "    analises_encontradas = {\n",
        "        \"base\": {},\n",
        "        \"adicionais\": {}\n",
        "    }\n",
        "\n",
        "    dados_path = os.path.join(pasta_trabalho, \"dados\")\n",
        "\n",
        "    # AnÃ¡lises bÃ¡sicas obrigatÃ³rias\n",
        "    arquivos_base = {\n",
        "        \"metadados\": \"metadados_completos.json\",\n",
        "        \"decomposicao\": \"decomposicao_completa.json\",\n",
        "        \"padroes\": \"analises_padroes_completas.json\",\n",
        "        \"psicologica\": \"analises_psicologicas_completas.json\"\n",
        "    }\n",
        "\n",
        "    for tipo, arquivo in arquivos_base.items():\n",
        "        caminho = os.path.join(dados_path, arquivo)\n",
        "        if os.path.exists(caminho):\n",
        "            analises_encontradas[\"base\"][tipo] = caminho\n",
        "            print(f\"âœ… AnÃ¡lise base encontrada: {tipo}\")\n",
        "        else:\n",
        "            print(f\"âš ï¸ AnÃ¡lise base ausente: {tipo}\")\n",
        "\n",
        "    # Descobrir anÃ¡lises adicionais automaticamente\n",
        "    # Busca por qualquer arquivo JSON que nÃ£o seja das anÃ¡lises base\n",
        "    todos_jsons = glob.glob(os.path.join(dados_path, \"*.json\"))\n",
        "\n",
        "    for json_path in todos_jsons:\n",
        "        nome_arquivo = os.path.basename(json_path)\n",
        "\n",
        "        # Pular arquivos base\n",
        "        if nome_arquivo in arquivos_base.values():\n",
        "            continue\n",
        "\n",
        "        # Identificar tipo da anÃ¡lise pelo nome\n",
        "        if \"audio_refinada\" in nome_arquivo.lower():\n",
        "            analises_encontradas[\"adicionais\"][\"audio_refinada\"] = json_path\n",
        "            print(f\"âœ… AnÃ¡lise adicional encontrada: Audio Refinada\")\n",
        "        elif \"visual_avancada\" in nome_arquivo.lower():\n",
        "            analises_encontradas[\"adicionais\"][\"visual_avancada\"] = json_path\n",
        "            print(f\"âœ… AnÃ¡lise adicional encontrada: Visual AvanÃ§ada\")\n",
        "        elif \"texto_avancada\" in nome_arquivo.lower():\n",
        "            analises_encontradas[\"adicionais\"][\"texto_avancada\"] = json_path\n",
        "            print(f\"âœ… AnÃ¡lise adicional encontrada: Texto AvanÃ§ada\")\n",
        "        elif \"sentiment\" in nome_arquivo.lower():\n",
        "            analises_encontradas[\"adicionais\"][\"sentimento\"] = json_path\n",
        "            print(f\"âœ… AnÃ¡lise adicional encontrada: Sentimento\")\n",
        "        elif \"copywriting\" in nome_arquivo.lower():\n",
        "            analises_encontradas[\"adicionais\"][\"copywriting\"] = json_path\n",
        "            print(f\"âœ… AnÃ¡lise adicional encontrada: Copywriting\")\n",
        "        elif \"viral\" in nome_arquivo.lower():\n",
        "            analises_encontradas[\"adicionais\"][\"viral\"] = json_path\n",
        "            print(f\"âœ… AnÃ¡lise adicional encontrada: Viral\")\n",
        "        else:\n",
        "            # AnÃ¡lise nÃ£o reconhecida - incluir mesmo assim\n",
        "            nome_limpo = nome_arquivo.replace(\".json\", \"\").replace(\"_\", \" \").title()\n",
        "            analises_encontradas[\"adicionais\"][nome_arquivo] = json_path\n",
        "            print(f\"âœ… AnÃ¡lise personalizada encontrada: {nome_limpo}\")\n",
        "\n",
        "    return analises_encontradas\n",
        "\n",
        "def carregar_dados_analise(caminho_arquivo):\n",
        "    \"\"\"Carrega dados de uma anÃ¡lise com tratamento de erros\"\"\"\n",
        "    try:\n",
        "        with open(caminho_arquivo, 'r', encoding='utf-8') as f:\n",
        "            dados = json.load(f)\n",
        "        return dados, True\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Erro ao carregar {caminho_arquivo}: {e}\")\n",
        "        return [], False\n",
        "\n",
        "def extrair_metricas_dinamicamente(dados, tipo_analise):\n",
        "    \"\"\"Extrai mÃ©tricas de qualquer tipo de anÃ¡lise dinamicamente\"\"\"\n",
        "    metricas_extraidas = {}\n",
        "\n",
        "    if not dados:\n",
        "        return metricas_extraidas\n",
        "\n",
        "    # Pegar o primeiro item para entender a estrutura\n",
        "    primeiro_item = dados[0] if isinstance(dados, list) else dados\n",
        "\n",
        "    if isinstance(primeiro_item, dict):\n",
        "        for chave, valor in primeiro_item.items():\n",
        "            if chave in ['video_id', 'status', 'data_analise', 'erro', 'nome_arquivo', 'tem_dados_virais', 'motivo_sem_dados']:\n",
        "                continue\n",
        "\n",
        "            # Ignorar estruturas muito complexas ou texto longo\n",
        "            if isinstance(valor, (dict, list)) and len(json.dumps(valor)) > 500:\n",
        "                continue\n",
        "            if isinstance(valor, str) and len(valor) > 200:\n",
        "                 continue\n",
        "\n",
        "\n",
        "            # Extrair mÃ©tricas numÃ©ricas automaticamente\n",
        "            if isinstance(valor, (int, float)):\n",
        "                metricas_extraidas[f\"{tipo_analise}_{chave}\"] = valor\n",
        "            elif isinstance(valor, dict):\n",
        "                # AnÃ¡lise aninhada - extrair sub-mÃ©tricas\n",
        "                for sub_chave, sub_valor in valor.items():\n",
        "                    if isinstance(sub_valor, (int, float)):\n",
        "                        metricas_extraidas[f\"{tipo_analise}_{chave}_{sub_chave}\"] = sub_valor\n",
        "                    elif isinstance(sub_valor, list) and sub_valor and isinstance(sub_valor[0], (int, float)):\n",
        "                        # Lista de nÃºmeros - calcular estatÃ­sticas\n",
        "                        metricas_extraidas[f\"{tipo_analise}_{chave}_{sub_chave}_media\"] = sum(sub_valor) / len(sub_valor)\n",
        "                        metricas_extraidas[f\"{tipo_analise}_{chave}_{sub_chave}_max\"] = max(sub_valor)\n",
        "                        metricas_extraidas[f\"{tipo_analise}_{chave}_{sub_chave}_min\"] = min(sub_valor)\n",
        "                    # Tratar listas de strings ou dicts dentro de dicts (contagem simples)\n",
        "                    elif isinstance(sub_valor, (list, dict)):\n",
        "                         metricas_extraidas[f\"{tipo_analise}_{chave}_{sub_chave}_count\"] = len(sub_valor)\n",
        "\n",
        "\n",
        "            elif isinstance(valor, list):\n",
        "                if valor and isinstance(valor[0], (int, float)):\n",
        "                    # Lista de nÃºmeros\n",
        "                    metricas_extraidas[f\"{tipo_analise}_{chave}_count\"] = len(valor)\n",
        "                    metricas_extraidas[f\"{tipo_analise}_{chave}_media\"] = sum(valor) / len(valor) if valor else 0\n",
        "                else:\n",
        "                    # Lista de objetos ou strings (contagem simples)\n",
        "                    metricas_extraidas[f\"{tipo_analise}_{chave}_count\"] = len(valor)\n",
        "            elif isinstance(valor, bool):\n",
        "                 metricas_extraidas[f\"{tipo_analise}_{chave}\"] = int(valor) # Converter bool para int (1/0)\n",
        "            elif isinstance(valor, str):\n",
        "                 # Incluir strings curtas como mÃ©tricas categÃ³ricas\n",
        "                 if len(valor) < 50: # Limite para nÃ£o poluir\n",
        "                     metricas_extraidas[f\"{tipo_analise}_{chave}_str\"] = valor\n",
        "\n",
        "\n",
        "    return metricas_extraidas\n",
        "\n",
        "def flatten_dict(d, parent_key='', sep='_'):\n",
        "    \"\"\"Flattens a nested dictionary.\"\"\"\n",
        "    items = []\n",
        "    for k, v in d.items():\n",
        "        new_key = parent_key + sep + k if parent_key else k\n",
        "        if isinstance(v, dict):\n",
        "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
        "        else:\n",
        "            items.append((new_key, v))\n",
        "    return dict(items)\n",
        "\n",
        "def consolidar_todos_dados(analises_encontradas):\n",
        "    \"\"\"Consolida todos os dados de todas as anÃ¡lises encontradas\"\"\"\n",
        "    dados_consolidados = {}\n",
        "\n",
        "    # Carregar anÃ¡lises base\n",
        "    for tipo, caminho in analises_encontradas[\"base\"].items():\n",
        "        dados, sucesso = carregar_dados_analise(caminho)\n",
        "        if sucesso:\n",
        "            dados_consolidados[tipo] = dados\n",
        "\n",
        "    # Carregar anÃ¡lises adicionais\n",
        "    for tipo, caminho in analises_encontradas[\"adicionais\"].items():\n",
        "        dados, sucesso = carregar_dados_analise(caminho)\n",
        "        if sucesso:\n",
        "            dados_consolidados[tipo] = dados\n",
        "\n",
        "\n",
        "    # Criar DataFrame consolidado por vÃ­deo\n",
        "    videos_df = pd.DataFrame()\n",
        "\n",
        "    # ComeÃ§ar com metadados base se disponÃ­vel\n",
        "    if \"metadados\" in dados_consolidados:\n",
        "        videos_df = pd.DataFrame(dados_consolidados[\"metadados\"])\n",
        "        videos_df = videos_df.set_index('id')\n",
        "        print(f\"DataFrame base criado com {len(videos_df)} vÃ­deos e {len(videos_df.columns)} colunas.\")\n",
        "    else:\n",
        "        print(\"âš ï¸ Metadados base nÃ£o encontrados. Criando DataFrame vazio e integrando por video_id.\")\n",
        "        videos_df = pd.DataFrame(columns=['id'])\n",
        "        videos_df.set_index('id', inplace=True)\n",
        "\n",
        "\n",
        "    # Integrar cada anÃ¡lise adicional\n",
        "    for tipo, dados in dados_consolidados.items():\n",
        "        if tipo == \"metadados\":\n",
        "            continue\n",
        "\n",
        "        print(f\"ðŸ”„ Integrando dados de: {tipo}\")\n",
        "\n",
        "        if isinstance(dados, list):\n",
        "            df_analise = pd.DataFrame(dados)\n",
        "\n",
        "            # Tentar usar 'video_id' ou 'id' para merge/join\n",
        "            merge_col = None\n",
        "            if 'video_id' in df_analise.columns:\n",
        "                merge_col = 'video_id'\n",
        "            elif 'id' in df_analise.columns:\n",
        "                 merge_col = 'id'\n",
        "            else:\n",
        "                 print(f\"âš ï¸ AnÃ¡lise '{tipo}' nÃ£o possui 'video_id' ou 'id'. NÃ£o Ã© possÃ­vel integrar por vÃ­deo.\")\n",
        "                 continue\n",
        "\n",
        "\n",
        "            # Extrair mÃ©tricas dinamicamente para cada vÃ­deo na anÃ¡lise\n",
        "            df_analise_flattened = pd.DataFrame()\n",
        "            if merge_col:\n",
        "                data_for_flattening = []\n",
        "                for item in dados:\n",
        "                    # Ensure item is a dict before processing\n",
        "                    if isinstance(item, dict):\n",
        "                         video_id_val = item.get(merge_col)\n",
        "                         if video_id_val:\n",
        "                            metricas = extrair_metricas_dinamicamente([item], tipo)\n",
        "                            metricas[merge_col] = video_id_val # Adicionar a coluna de merge\n",
        "                            data_for_flattening.append(metricas)\n",
        "                if data_for_flattening:\n",
        "                    df_analise_flattened = pd.DataFrame(data_for_flattening)\n",
        "                    df_analise_flattened.set_index(merge_col, inplace=True)\n",
        "\n",
        "\n",
        "            if not df_analise_flattened.empty:\n",
        "                 # Renomear colunas para evitar conflitos, exceto a coluna de merge\n",
        "                df_analise_flattened.columns = [f\"{col}\" for col in df_analise_flattened.columns]\n",
        "\n",
        "                # Realizar o merge\n",
        "                # Usar left_index=True e right_index=True para merge nos Ã­ndices\n",
        "                # Se o DataFrame base (videos_df) estiver vazio, apenas use o df_analise_flattened\n",
        "                if videos_df.empty:\n",
        "                    videos_df = df_analise_flattened.copy()\n",
        "                    videos_df.index.name = 'id' # Renomear o Ã­ndice para 'id' por convenÃ§Ã£o\n",
        "                else:\n",
        "                    # Antes de merge, garantir que o Ã­ndice do videos_df Ã© chamado 'id'\n",
        "                    if videos_df.index.name != 'id' and 'id' in videos_df.columns:\n",
        "                        videos_df.set_index('id', inplace=True)\n",
        "                    elif videos_df.index.name is None and 'id' in videos_df.columns:\n",
        "                         videos_df.set_index('id', inplace=True)\n",
        "                    elif videos_df.index.name is None and 'video_id' in videos_df.columns:\n",
        "                         videos_df.set_index('video_id', inplace=True) # Tentar video_id se 'id' nÃ£o existir\n",
        "                         videos_df.index.name = 'id' # Manter convenÃ§Ã£o 'id'\n",
        "\n",
        "                    # Antes de merge, garantir que o Ã­ndice do df_analise_flattened Ã© chamado 'id'\n",
        "                    if df_analise_flattened.index.name != 'id':\n",
        "                         df_analise_flattened.index.name = 'id'\n",
        "\n",
        "\n",
        "                    # Realizar o merge\n",
        "                    common_cols = videos_df.columns.intersection(df_analise_flattened.columns)\n",
        "                    if len(common_cols) > 0:\n",
        "                         print(f\"âš ï¸ Colunas duplicadas encontradas durante o merge com {tipo}: {list(common_cols)}. Elas serÃ£o sobrescritas.\")\n",
        "                         # Remover colunas duplicadas do df_analise_flattened antes do merge\n",
        "                         # Exceto se forem colunas de identificaÃ§Ã£o como video_id ou id\n",
        "                         cols_to_drop = [col for col in common_cols if col not in ['id', 'video_id']]\n",
        "                         df_analise_flattened = df_analise_flattened.drop(columns=cols_to_drop)\n",
        "\n",
        "\n",
        "                    # Realizar o merge nos Ã­ndices\n",
        "                    videos_df = videos_df.merge(df_analise_flattened, left_index=True, right_index=True, how='outer', suffixes=('', f'_{tipo}'))\n",
        "                    print(f\"Merge com {tipo} concluÃ­do. Total de colunas: {len(videos_df.columns)}\")\n",
        "\n",
        "\n",
        "            else:\n",
        "                print(f\"âš ï¸ AnÃ¡lise '{tipo}' nÃ£o gerou dados tabulares para integraÃ§Ã£o.\")\n",
        "\n",
        "    # Resetar o Ã­ndice para ter 'id' como coluna novamente\n",
        "    if videos_df.index.name:\n",
        "        videos_df = videos_df.reset_index()\n",
        "    elif 'id' not in videos_df.columns and 'video_id' in videos_df.columns:\n",
        "         # Se 'id' nÃ£o existe mas 'video_id' existe, renomear para 'id'\n",
        "         videos_df = videos_df.rename(columns={'video_id': 'id'})\n",
        "\n",
        "\n",
        "    # Garantir que 'id' Ã© a primeira coluna se existir\n",
        "    if 'id' in videos_df.columns:\n",
        "        id_col = videos_df.pop('id')\n",
        "        videos_df.insert(0, 'id', id_col)\n",
        "\n",
        "    # Flatten any remaining nested dictionaries or lists for Excel compatibility\n",
        "    print(\"ðŸ§¹ Flattening nested structures for Excel compatibility...\")\n",
        "    flattened_data = []\n",
        "    for index, row in videos_df.iterrows():\n",
        "        flattened_row = flatten_dict(row.to_dict())\n",
        "        flattened_data.append(flattened_row)\n",
        "\n",
        "    df_flattened = pd.DataFrame(flattened_data)\n",
        "\n",
        "\n",
        "    # Limpeza final: remover colunas totalmente vazias\n",
        "    df_flattened.dropna(axis=1, how='all', inplace=True)\n",
        "\n",
        "    return df_flattened\n",
        "\n",
        "\n",
        "def gerar_dashboard_dinamico(df_consolidado, pasta_trabalho):\n",
        "    \"\"\"Gera dashboard dinÃ¢mico incluindo todas as anÃ¡lises encontradas\"\"\"\n",
        "    from openpyxl import Workbook\n",
        "    from openpyxl.styles import Font, Alignment, PatternFill\n",
        "    from openpyxl.utils import get_column_letter\n",
        "\n",
        "    wb = Workbook()\n",
        "\n",
        "    # ABA 1: VISÃƒO GERAL DINÃ‚MICA\n",
        "    ws_geral = wb.active\n",
        "    ws_geral.title = 'VisÃ£o Geral Completa'\n",
        "\n",
        "    # Header\n",
        "    ws_geral.merge_cells(\"A1:Z1\") # Merge amplo para caber tÃ­tulo\n",
        "    ws_geral[\"A1\"].value = 'RELATÃ“RIO COMPLETO DE ENGENHARIA REVERSA - VISÃƒO GERAL'\n",
        "    ws_geral[\"A1\"].font = Font(bold=True, size=16, color=\"FFFFFF\")\n",
        "    ws_geral[\"A1\"].fill = PatternFill(start_color=\"1F4E79\", end_color=\"1F4E79\", fill_type=\"solid\")\n",
        "    ws_geral[\"A1\"].alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
        "\n",
        "\n",
        "    row = 3\n",
        "\n",
        "    # EstatÃ­sticas gerais\n",
        "    ws_geral.cell(row=row, column=1).value = 'ANÃLISES REALIZADAS'\n",
        "    ws_geral.cell(row=row, column=1).font = Font(bold=True, size=14)\n",
        "    row += 2\n",
        "\n",
        "    # Contar colunas por tipo de anÃ¡lise\n",
        "    colunas_por_tipo = {}\n",
        "    for col in df_consolidado.columns:\n",
        "        if '_' in col:\n",
        "            tipo = col.split('_')[0]\n",
        "            colunas_por_tipo[tipo] = colunas_por_tipo.get(tipo, 0) + 1\n",
        "        else: # Incluir colunas sem underscore (ex: id, nome_arquivo)\n",
        "             colunas_por_tipo[\"base\"] = colunas_por_tipo.get(\"base\", 0) + 1\n",
        "\n",
        "\n",
        "    row_stats = row\n",
        "    for tipo, count in colunas_por_tipo.items():\n",
        "        ws_geral.cell(row=row_stats, column=1).value = f\"{tipo.upper()}:\"\n",
        "        ws_geral.cell(row=row_stats, column=2).value = f\"{count} mÃ©tricas\"\n",
        "        ws_geral.cell(row=row_stats, column=1).font = Font(bold=True)\n",
        "        row_stats += 1\n",
        "\n",
        "    row = max(row_stats, row) + 2\n",
        "\n",
        "\n",
        "    # Adicionar um resumo simples do DataFrame\n",
        "    ws_geral.cell(row=row, column=1).value = \"RESUMO DO DATAFRAME CONSOLIDADO\"\n",
        "    ws_geral.cell(row=row, column=1).font = Font(bold=True, size=14)\n",
        "    row += 2\n",
        "\n",
        "    if not df_consolidado.empty:\n",
        "        summary_data = [\n",
        "            [\"Total de VÃ­deos:\", len(df_consolidado)],\n",
        "            [\"Total de MÃ©tricas Integradas:\", len(df_consolidado.columns)],\n",
        "            [\"MÃ©dia de DuraÃ§Ã£o (segundos):\", df_consolidado.get('duracao_segundos', pd.Series([0])).mean()],\n",
        "            [\"MÃ©dia de Cortes por Segundo:\", df_consolidado.get('cortes_por_segundo', pd.Series([0])).mean()],\n",
        "            [\"VÃ­deos com Ãudio:\", df_consolidado.get('tem_audio', pd.Series([False])).sum()],\n",
        "            [\"Formatos Encontrados:\", \", \".join(df_consolidado.get('formato_detectado', pd.Series(['N/A'])).mode().tolist())],\n",
        "            [\"MÃ©dia de Score Viral:\", df_consolidado.get('viral_score', pd.Series([0])).mean()],\n",
        "            [\"MÃ©dia de Score TÃ©cnico:\", df_consolidado.get('technical_score', pd.Series([0])).mean()],\n",
        "            [\"MÃ©dia de Score ConteÃºdo:\", df_consolidado.get('content_score', pd.Series([0])).mean()],\n",
        "            [\"MÃ©dia de Score Copywriting:\", df_consolidado.get('copywriting_score_persuasao', pd.Series([0])).mean()],\n",
        "        ]\n",
        "\n",
        "        for label, value in summary_data:\n",
        "            ws_geral.cell(row=row, column=1, value=label).font = Font(bold=True)\n",
        "            ws_geral.cell(row=row, column=2, value=value)\n",
        "            row += 1\n",
        "    else:\n",
        "        ws_geral.cell(row=row, column=1, value=\"DataFrame consolidado vazio.\").font = Font(bold=True, color=\"FF0000\")\n",
        "\n",
        "\n",
        "    # ABA 2: DADOS COMPLETOS\n",
        "    ws_dados = wb.create_sheet('Dados Completos')\n",
        "\n",
        "    # Adicionar todos os dados\n",
        "    if not df_consolidado.empty:\n",
        "        # Adicionar cabeÃ§alhos\n",
        "        for c_idx, col_name in enumerate(df_consolidado.columns, 1):\n",
        "             cell = ws_dados.cell(row=1, column=c_idx, value=col_name)\n",
        "             cell.font = Font(bold=True)\n",
        "             cell.fill = PatternFill(start_color=\"E7E6E6\", end_color=\"E7E6E6\", fill_type=\"solid\")\n",
        "\n",
        "        # Adicionar linhas de dados\n",
        "        for r_idx, row_data in enumerate(df_consolidado.itertuples(index=False), 2):\n",
        "            for c_idx, value in enumerate(row_data, 1):\n",
        "                ws_dados.cell(row=r_idx, column=c_idx, value=value)\n",
        "\n",
        "        # Ajustar largura das colunas (exemplo simples)\n",
        "        for col_idx in range(1, len(df_consolidado.columns) + 1):\n",
        "            ws_dados.column_dimensions[get_column_letter(col_idx)].width = 15 # Largura padrÃ£o\n",
        "\n",
        "    else:\n",
        "        ws_dados.cell(row=1, column=1, value=\"Nenhum dado consolidado disponÃ­vel\").font = Font(bold=True, color=\"FF0000\")\n",
        "\n",
        "\n",
        "    # ABA 3: INSIGHTS AUTOMATICOS\n",
        "    ws_insights = wb.create_sheet('Insights AutomÃ¡ticos')\n",
        "\n",
        "    insights_automaticos = gerar_insights_automaticos(df_consolidado)\n",
        "\n",
        "    ws_insights.cell(row=1, column=1).value = 'INSIGHTS GERADOS AUTOMATICAMENTE'\n",
        "    ws_insights.cell(row=1, column=1).font = Font(bold=True, size=16, color=\"FFFFFF\")\n",
        "    ws_insights.cell(row=1, column=1).fill = PatternFill(start_color=\"C5504B\", end_color=\"C5504B\", fill_type=\"solid\")\n",
        "    ws_insights.merge_cells(\"A1:Z1\")\n",
        "\n",
        "\n",
        "    for i, insight in enumerate(insights_automaticos, 3):\n",
        "        ws_insights.cell(row=i, column=1).value = f\"â€¢ {insight}\"\n",
        "        ws_insights.cell(row=i, column=1).alignment = Alignment(wrap_text=True)\n",
        "        ws_insights.cell(row=i, column=1).font = Font(bold=True) # Insights em negrito\n",
        "\n",
        "    # Ajustar largura da coluna de insights\n",
        "    ws_insights.column_dimensions['A'].width = 120\n",
        "\n",
        "\n",
        "    # ======= INTEGRAÃ‡ÃƒO: ABA VIRAL NO DASHBOARD =======\n",
        "    print(\"ðŸ“Š Tentando integrar aba de anÃ¡lise viral...\")\n",
        "    if 'wb' in locals():\n",
        "        sucesso_aba_viral = criar_aba_analise_viral_dashboard(wb, pasta_trabalho)\n",
        "        if sucesso_aba_viral:\n",
        "            print(\"âœ… Aba de AnÃ¡lise Viral adicionada ao dashboard\")\n",
        "    else:\n",
        "        print(\"âš ï¸ Workbook nÃ£o criado. NÃ£o foi possÃ­vel adicionar aba viral.\")\n",
        "\n",
        "    print(\"ðŸ”— IntegraÃ§Ã£o viral concluÃ­da\")\n",
        "    # ======= FIM INTEGRAÃ‡ÃƒO VIRAL =======\n",
        "\n",
        "\n",
        "    # Salvar\n",
        "    output_path = os.path.join(pasta_trabalho, \"dashboard\", \"RELATORIO_COMPLETO_DINAMICO.xlsx\")\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "    wb.save(output_path)\n",
        "\n",
        "    return output_path\n",
        "\n",
        "def gerar_insights_automaticos(df):\n",
        "    \"\"\"Gera insights automÃ¡ticos baseados em qualquer conjunto de dados\"\"\"\n",
        "    insights = []\n",
        "\n",
        "    if df.empty:\n",
        "        return [\"Nenhum dado disponÃ­vel para gerar insights automÃ¡ticos.\"]\n",
        "\n",
        "    # AnÃ¡lise de correlaÃ§Ãµes automÃ¡ticas (apenas para colunas com variaÃ§Ã£o)\n",
        "    colunas_numericas = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    colunas_numericas = [col for col in colunas_numericas if df[col].nunique() > 1] # Remover colunas constantes\n",
        "\n",
        "\n",
        "    if len(colunas_numericas) > 1:\n",
        "        try:\n",
        "            correlacoes = df[colunas_numericas].corr()\n",
        "\n",
        "            # Encontrar correlaÃ§Ãµes fortes (ajustar threshold se necessÃ¡rio)\n",
        "            for col1 in correlacoes.columns:\n",
        "                for col2 in correlacoes.columns:\n",
        "                    if col1 != col2:\n",
        "                        corr_val = correlacoes.loc[col1, col2]\n",
        "                        if abs(corr_val) > 0.7: # Threshold para correlaÃ§Ã£o forte\n",
        "                            insights.append(f\"CORRELAÃ‡ÃƒO FORTE: '{col1}' e '{col2}' tÃªm correlaÃ§Ã£o de {corr_val:.2f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "             insights.append(f\"âš ï¸ Erro ao calcular correlaÃ§Ãµes: {e}\")\n",
        "\n",
        "\n",
        "    # Identificar outliers automÃ¡ticos (usando Z-score ou IQR)\n",
        "    # Usar IQR (Interquartile Range) que Ã© mais robusto a outliers\n",
        "    for col in colunas_numericas:\n",
        "        try:\n",
        "            Q1 = df[col].quantile(0.25)\n",
        "            Q3 = df[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "            outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
        "\n",
        "            if len(outliers) > 0:\n",
        "                outlier_videos = outliers['id'].tolist() if 'id' in outliers.columns else outliers.index.tolist()\n",
        "                insights.append(f\"OUTLIERS DETECTADOS: {len(outliers)} vÃ­deo(s) tÃªm valores extremos na mÃ©trica '{col}' (Ex: {', '.join(map(str, outlier_videos[:3]))})\")\n",
        "\n",
        "        except Exception as e:\n",
        "             insights.append(f\"âš ï¸ Erro ao detectar outliers para '{col}': {e}\")\n",
        "\n",
        "\n",
        "    # AnÃ¡lise de distribuiÃ§Ãµes e mÃ©dias\n",
        "    for col in colunas_numericas:\n",
        "        try:\n",
        "            media = df[col].mean()\n",
        "            std = df[col].std()\n",
        "            # Adicionar insights gerais sobre a distribuiÃ§Ã£o\n",
        "            if std > media * 0.5: # Alta variaÃ§Ã£o\n",
        "                 insights.append(f\"VARIAÃ‡ÃƒO ALTA: A mÃ©trica '{col}' tem alta variaÃ§Ã£o (mÃ©dia={media:.1f}, std={std:.1f}). Explore os extremos.\")\n",
        "\n",
        "            # Insights especÃ­ficos para scores (se existirem)\n",
        "            if col.endswith('_score') or 'score' in col.lower():\n",
        "                if media > 80:\n",
        "                    insights.append(f\"PERFORMANCE ALTA: Score mÃ©dio de '{col}' Ã© {media:.1f} - identifique os padrÃµes dos top performers.\")\n",
        "                elif media < 50:\n",
        "                    insights.append(f\"OPORTUNIDADE: Score mÃ©dio de '{col}' Ã© {media:.1f} - foco em otimizar esta Ã¡rea.\")\n",
        "\n",
        "        except Exception as e:\n",
        "             insights.append(f\"âš ï¸ Erro ao analisar distribuiÃ§Ã£o para '{col}': {e}\")\n",
        "\n",
        "\n",
        "    # AnÃ¡lise de colunas categÃ³ricas\n",
        "    colunas_categoricas = df.select_dtypes(include=['object']).columns.tolist()\n",
        "    for col in colunas_categoricas:\n",
        "        try:\n",
        "            # Contar valores Ãºnicos e frequÃªncia\n",
        "            value_counts = df[col].value_counts()\n",
        "            if len(value_counts) > 1 and len(value_counts) < 10: # Evitar colunas com muitos valores Ãºnicos\n",
        "                most_common = value_counts.index[0]\n",
        "                insights.append(f\"PADRÃƒO CATEGÃ“RICO: O valor mais comum em '{col}' Ã© '{most_common}' ({value_counts.iloc[0]} ocorrÃªncias).\")\n",
        "\n",
        "        except Exception as e:\n",
        "             insights.append(f\"âš ï¸ Erro ao analisar categoria '{col}': {e}\")\n",
        "\n",
        "\n",
        "    return insights if insights else [\"AnÃ¡lise de insights automÃ¡ticos concluÃ­da. Nenhum insight significativo encontrado com os parÃ¢metros atuais.\"]\n",
        "\n",
        "\n",
        "def atualizar_config_com_novas_analises(pasta_trabalho, analises_encontradas):\n",
        "    \"\"\"Atualiza config.json com status de todas as anÃ¡lises encontradas\"\"\"\n",
        "    config_path = os.path.join(pasta_trabalho, \"config\", \"config.json\")\n",
        "\n",
        "    # Carregar config existente\n",
        "    config = {}\n",
        "    if os.path.exists(config_path):\n",
        "        try:\n",
        "            with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                config = json.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Erro ao carregar config.json para atualizaÃ§Ã£o: {e}\")\n",
        "            config = {\"status_etapas\": {}, \"arquivos_gerados\": {}} # Fallback\n",
        "\n",
        "    # Garantir que as chaves existem\n",
        "    if \"status_etapas\" not in config:\n",
        "        config[\"status_etapas\"] = {}\n",
        "    if \"arquivos_gerados\" not in config:\n",
        "         config[\"arquivos_gerados\"] = {}\n",
        "\n",
        "\n",
        "    # Atualizar status das anÃ¡lises encontradas\n",
        "    for tipo in analises_encontradas[\"base\"]:\n",
        "        config[\"status_etapas\"][tipo] = True\n",
        "\n",
        "    for tipo in analises_encontradas[\"adicionais\"]:\n",
        "        # Adicionar status com prefixo 'analise_' para clareza\n",
        "        config[\"status_etapas\"][f\"analise_{tipo}\"] = True\n",
        "\n",
        "    # Registrar arquivos gerados\n",
        "    dashboard_path = os.path.join(pasta_trabalho, \"dashboard\", \"RELATORIO_COMPLETO_DINAMICO.xlsx\")\n",
        "    csv_path = os.path.join(pasta_trabalho, \"dashboard\", \"dados_completos_consolidados.csv\")\n",
        "    json_path = os.path.join(pasta_trabalho, \"dashboard\", \"dados_completos_consolidados.json\")\n",
        "\n",
        "    config[\"arquivos_gerados\"][\"dashboard_completo\"] = dashboard_path if os.path.exists(dashboard_path) else None\n",
        "    config[\"arquivos_gerados\"][\"dados_consolidados_csv\"] = csv_path if os.path.exists(csv_path) else None\n",
        "    config[\"arquivos_gerados\"][\"dados_consolidados_json\"] = json_path if os.path.exists(json_path) else None\n",
        "\n",
        "\n",
        "    config[\"ultima_consolidacao\"] = datetime.now().isoformat()\n",
        "    config[\"total_analises_integradas\"] = len(analises_encontradas[\"base\"]) + len(analises_encontradas[\"adicionais\"])\n",
        "\n",
        "    # Salvar config atualizado\n",
        "    try:\n",
        "        with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "        print(\"âœ… Arquivo config.json atualizado com sucesso.\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ERRO ao salvar config.json: {e}\")\n",
        "\n",
        "\n",
        "def main_integracao_automatica():\n",
        "    \"\"\"FunÃ§Ã£o principal da integraÃ§Ã£o automÃ¡tica\"\"\"\n",
        "    print(\"ðŸš€ INICIANDO INTEGRAÃ‡ÃƒO AUTOMÃTICA DE TODAS AS ANÃLISES\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Usar variÃ¡vel global da pasta de trabalho\n",
        "    if \"PASTA_TRABALHO\" not in globals():\n",
        "        print(\"âŒ ERRO: Execute as cÃ©lulas anteriores primeiro (CÃ‰LULA 1.2).\")\n",
        "        return False\n",
        "\n",
        "    pasta_trabalho = PASTA_TRABALHO\n",
        "\n",
        "    try:\n",
        "        # Passo 1: Descobrir anÃ¡lises\n",
        "        print(\"\\nðŸ” DESCOBRINDO ANÃLISES DISPONÃVEIS...\")\n",
        "        analises = descobrir_analises_disponiveis(pasta_trabalho)\n",
        "\n",
        "        total_analises = len(analises[\"base\"]) + len(analises[\"adicionais\"])\n",
        "        print(f\"ðŸ“Š Total de anÃ¡lises encontradas: {total_analises}\")\n",
        "        if total_analises < 4: # Metadados, Decomposicao, Padroes, Psicologica sÃ£o base\n",
        "             print(\"âš ï¸ Aviso: AnÃ¡lises base incompletas. Algumas funcionalidades podem estar limitadas.\")\n",
        "\n",
        "\n",
        "        # Passo 2: Consolidar dados\n",
        "        print(\"\\nðŸ”„ CONSOLIDANDO TODOS OS DADOS...\")\n",
        "        df_consolidado = consolidar_todos_dados(analises)\n",
        "\n",
        "        print(f\"ðŸ“ˆ {len(df_consolidado)} vÃ­deos consolidados com {len(df_consolidado.columns)} mÃ©tricas totais\")\n",
        "\n",
        "        if df_consolidado.empty:\n",
        "             print(\"âŒ DataFrame consolidado vazio. NÃ£o Ã© possÃ­vel gerar dashboard ou insights.\")\n",
        "             # Tentar atualizar config mesmo assim para registrar o estado\n",
        "             actualizar_config_con_novas_analisis(pasta_trabalho, analises)\n",
        "             return False\n",
        "\n",
        "\n",
        "        # Passo 3: Gerar dashboard dinÃ¢mico\n",
        "        print(\"\\nðŸ“Š GERANDO DASHBOARD DINÃ‚MICO...\")\n",
        "        dashboard_path = gerar_dashboard_dinamico(df_consolidado, pasta_trabalho)\n",
        "\n",
        "        # Passo 4: Salvar dados consolidados (CSV e JSON)\n",
        "        print(\"\\nðŸ’¾ SALVANDO DADOS CONSOLIDADOS...\")\n",
        "        csv_path = os.path.join(pasta_trabalho, \"dashboard\", \"dados_completos_consolidados.csv\")\n",
        "        df_consolidado.to_csv(csv_path, index=False, encoding='utf-8')\n",
        "        print(f\"ðŸ“ Dados CSV salvos: {csv_path}\")\n",
        "\n",
        "        json_path = os.path.join(pasta_trabalho, \"dashboard\", \"dados_completos_consolidados.json\")\n",
        "        # Usar orient='records' para formato mais amigÃ¡vel para JSON\n",
        "        df_consolidado.to_json(json_path, orient='records', indent=2, force_ascii=False)\n",
        "        print(f\"ðŸ“ Dados JSON salvos: {json_path}\")\n",
        "\n",
        "\n",
        "        # Passo 5: Atualizar configuraÃ§Ã£o\n",
        "        print(\"\\nâš™ï¸ ATUALIZANDO CONFIGURAÃ‡Ã•ES...\")\n",
        "        atualizar_config_com_novas_analises(pasta_trabalho, analises)\n",
        "\n",
        "\n",
        "        # Resultados finais\n",
        "        print(\"\\nâœ… INTEGRAÃ‡ÃƒO AUTOMÃTICA CONCLUÃDA COM SUCESSO!\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"ðŸ“ Dashboard dinÃ¢mico completo: {dashboard_path}\")\n",
        "        print(f\"ðŸ“Š {len(df_consolidado)} vÃ­deos processados com {len(df_consolidado.columns)} mÃ©tricas totais integradas.\")\n",
        "        print(\"\\nðŸŽ¯ PRÃ“XIMOS PASSOS:\")\n",
        "        print(\"â€¢ Abra o arquivo Excel gerado para explorar todas as anÃ¡lises integradas.\")\n",
        "        print(\"â€¢ Use os arquivos CSV/JSON para anÃ¡lises mais avanÃ§adas em outras ferramentas.\")\n",
        "        print(\"â€¢ Execute esta cÃ©lula novamente sempre que adicionar novas anÃ¡lises ao seu projeto.\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ ERRO CRÃTICO NA INTEGRAÃ‡ÃƒO AUTOMÃTICA: {type(e).__name__}: {e}\")\n",
        "        print(\"Por favor, verifique se todas as anÃ¡lises anteriores foram executadas com sucesso e se nÃ£o hÃ¡ erros nos dados de entrada.\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "# Executar integraÃ§Ã£o automÃ¡tica\n",
        "if __name__ == \"__main__\":\n",
        "    main_integracao_automatica()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlbVqMZ-yhI0",
        "outputId": "fbbfb535-161f-4a1a-b086-21ef47d2592e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ EXECUTANDO INTEGRAÃ‡ÃƒO DE COPYWRITING NO DASHBOARD EXISTENTE\n",
            "======================================================================\n",
            "ðŸ”„ Iniciando integraÃ§Ã£o de copywriting no dashboard existente...\n",
            "âŒ PRÃ‰-REQUISITO NÃƒO ATENDIDO: A etapa \"copywriting_analysis\" nÃ£o foi encontrada.\n",
            "   Execute a cÃ©lula correspondente primeiro.\n",
            "\n",
            "âŒ Falha na integraÃ§Ã£o - verifique os prÃ©-requisitos\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CÃ‰LULA 4.3: INTEGRAÃ‡ÃƒO DE COPYWRITING NO DASHBOARD EXISTENTE\n",
        "# ============================================================================\n",
        "\n",
        "def integrar_copywriting_dashboard_existente():\n",
        "    \"\"\"Integra anÃ¡lise de copywriting no dashboard master existente\"\"\"\n",
        "    print(\"ðŸ”„ Iniciando integraÃ§Ã£o de copywriting no dashboard existente...\")\n",
        "\n",
        "    # Verificar prÃ©-requisitos\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa('copywriting_analysis')\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Localizar dashboard existente\n",
        "    pasta_dashboard = os.path.join(PASTA_TRABALHO, \"dashboard\")\n",
        "    dashboard_existente = None\n",
        "\n",
        "    # Procurar arquivo de dashboard existente\n",
        "    if os.path.exists(pasta_dashboard):\n",
        "        arquivos = os.listdir(pasta_dashboard)\n",
        "        for arquivo in arquivos:\n",
        "            if \"DASHBOARD_MASTER_EXECUTIVO_INTELIGENTE\" in arquivo and arquivo.endswith(\".xlsx\"):\n",
        "                dashboard_existente = os.path.join(pasta_dashboard, arquivo)\n",
        "                break\n",
        "\n",
        "    if not dashboard_existente:\n",
        "        print(\"âŒ Dashboard master existente nÃ£o encontrado!\")\n",
        "        print(\"Execute primeiro a cÃ©lula 4.2 (Blueprint Final) para criar o dashboard base.\")\n",
        "        return\n",
        "\n",
        "    print(f\"  ðŸ“Š Dashboard encontrado: {os.path.basename(dashboard_existente)}\")\n",
        "\n",
        "    # Carregar dados de copywriting\n",
        "    dados_copywriting = carregar_dados_copywriting()\n",
        "    if not dados_copywriting:\n",
        "        return\n",
        "\n",
        "    # Abrir workbook existente\n",
        "    from openpyxl import load_workbook\n",
        "\n",
        "    try:\n",
        "        wb = load_workbook(dashboard_existente)\n",
        "        print(f\"  âœ… Dashboard carregado com {len(wb.sheetnames)} abas existentes\")\n",
        "\n",
        "        # Adicionar novas abas de copywriting\n",
        "        adicionar_aba_copywriting_estrategico(wb, dados_copywriting)\n",
        "        adicionar_aba_templates_copy(wb, dados_copywriting)\n",
        "        adicionar_aba_timeline_copy(wb, dados_copywriting)\n",
        "        adicionar_aba_recomendacoes_copy(wb, dados_copywriting)\n",
        "\n",
        "        # Atualizar aba principal com mÃ©tricas de copywriting\n",
        "        atualizar_aba_principal_com_copy(wb, dados_copywriting)\n",
        "\n",
        "        # Salvar dashboard atualizado\n",
        "        wb.save(dashboard_existente)\n",
        "\n",
        "        print(f\"âœ… Dashboard atualizado com anÃ¡lise de copywriting!\")\n",
        "        print(f\"ðŸ“Š Arquivo: {dashboard_existente}\")\n",
        "        print(f\"ðŸ“‹ Novas abas adicionadas:\")\n",
        "        print(\"  â€¢ Copywriting EstratÃ©gico\")\n",
        "        print(\"  â€¢ Templates ReplicÃ¡veis\")\n",
        "        print(\"  â€¢ Timeline PersuasÃ£o\")\n",
        "        print(\"  â€¢ RecomendaÃ§Ãµes Copy\")\n",
        "        print(\"  â€¢ Dashboard Principal (atualizada)\")\n",
        "\n",
        "        # Gerar relatÃ³rios complementares\n",
        "        gerar_relatorios_copywriting_individuais(dados_copywriting)\n",
        "\n",
        "        # Atualizar config\n",
        "        config[\"status_etapas\"][\"dashboard_copywriting_integrado\"] = True\n",
        "        config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "        with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        return dashboard_existente\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Erro ao atualizar dashboard: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def carregar_dados_copywriting():\n",
        "    \"\"\"Carrega dados de copywriting e outros dados necessÃ¡rios\"\"\"\n",
        "    print(\"  ðŸ“Š Carregando dados de copywriting...\")\n",
        "\n",
        "    try:\n",
        "        # Dados de copywriting\n",
        "        copywriting_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_copywriting_completas.json\")\n",
        "        with open(copywriting_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            copywriting_data = json.load(f)\n",
        "\n",
        "        # Dados de legendas\n",
        "        legendas_path = os.path.join(PASTA_TRABALHO, \"dados\", \"legendas_geradas.json\")\n",
        "        with open(legendas_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            legendas_data = json.load(f)\n",
        "\n",
        "        # Tentar carregar outros dados (podem nÃ£o existir ainda)\n",
        "        outros_dados = {}\n",
        "\n",
        "        try:\n",
        "            padroes_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_padroes_completas.json\")\n",
        "            with open(padroes_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                outros_dados[\"padroes\"] = json.load(f)\n",
        "        except:\n",
        "            outros_dados[\"padroes\"] = []\n",
        "\n",
        "        try:\n",
        "            videos_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "            with open(videos_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                outros_dados[\"videos\"] = json.load(f)\n",
        "        except:\n",
        "            outros_dados[\"videos\"] = []\n",
        "\n",
        "        print(f\"  âœ… Dados carregados: {len(copywriting_data)} anÃ¡lises de copywriting\")\n",
        "\n",
        "        return {\n",
        "            \"copywriting\": copywriting_data,\n",
        "            \"legendas\": legendas_data,\n",
        "            **outros_dados\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  âŒ Erro ao carregar dados de copywriting: {e}\")\n",
        "        return None\n",
        "\n",
        "def adicionar_aba_copywriting_estrategico(wb, dados):\n",
        "    \"\"\"Adiciona aba principal de anÃ¡lise de copywriting\"\"\"\n",
        "    from openpyxl.styles import Font, PatternFill, Alignment\n",
        "\n",
        "    # Criar nova aba\n",
        "    ws = wb.create_sheet(\"Copywriting EstratÃ©gico\")\n",
        "\n",
        "    # TÃ­tulo principal\n",
        "    ws.merge_cells(\"A1:H1\")\n",
        "    titulo = ws[\"A1\"]\n",
        "    titulo.value = \"ANÃLISE ESTRATÃ‰GICA DE COPYWRITING - ENGENHARIA REVERSA\"\n",
        "    titulo.fill = PatternFill(start_color=\"1F4E79\", end_color=\"1F4E79\", fill_type=\"solid\")\n",
        "    titulo.font = Font(color=\"FFFFFF\", bold=True, size=14)\n",
        "    titulo.alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "    row = 3\n",
        "\n",
        "    # MÃ©tricas executivas\n",
        "    ws[f\"A{row}\"] = \"MÃ‰TRICAS EXECUTIVAS DE COPYWRITING\"\n",
        "    ws[f\"A{row}\"].font = Font(bold=True, size=12, color=\"C5504B\")\n",
        "    row += 2\n",
        "\n",
        "    # Calcular mÃ©tricas\n",
        "    videos_copy = dados[\"copywriting\"]\n",
        "\n",
        "    if videos_copy:\n",
        "        # Score mÃ©dio\n",
        "        scores = [v.get(\"score_persuasao\", 0) for v in videos_copy]\n",
        "        score_medio = sum(scores) / len(scores) if scores else 0\n",
        "\n",
        "        # Contadores\n",
        "        total_ganchos = sum(len(v.get(\"ganchos_detectados\", {})) for v in videos_copy)\n",
        "        total_gatilhos = sum(len(v.get(\"gatilhos_mentais_detectados\", {})) for v in videos_copy)\n",
        "        total_ctas = sum(len(v.get(\"ctas_detectados\", {})) for v in videos_copy)\n",
        "        videos_sem_cta = len([v for v in videos_copy if not v.get(\"ctas_detectados\")])\n",
        "        total_templates = sum(len(v.get(\"templates_identificados\", [])) for v in videos_copy)\n",
        "\n",
        "        # Exibir mÃ©tricas\n",
        "        metricas = [\n",
        "            (\"Score PersuasÃ£o MÃ©dio:\", f\"{score_medio:.1f}/100\", \"Meta: 70+ para alta conversÃ£o\"),\n",
        "            (\"VÃ­deos Analisados:\", len(videos_copy), \"Base completa da anÃ¡lise\"),\n",
        "            (\"Total de Ganchos:\", total_ganchos, f\"MÃ©dia: {total_ganchos/len(videos_copy):.1f} por vÃ­deo\"),\n",
        "            (\"Total de Gatilhos:\", total_gatilhos, f\"MÃ©dia: {total_gatilhos/len(videos_copy):.1f} por vÃ­deo\"),\n",
        "            (\"Total de CTAs:\", total_ctas, f\"MÃ©dia: {total_ctas/len(videos_copy):.1f} por vÃ­deo\"),\n",
        "            (\"ðŸš¨ VÃ­deos sem CTA:\", videos_sem_cta, \"CRÃTICO: Implementar imediatamente\" if videos_sem_cta > 0 else \"âœ… Todos tÃªm CTA\"),\n",
        "            (\"Templates Identificados:\", total_templates, \"Estruturas replicÃ¡veis encontradas\")\n",
        "        ]\n",
        "\n",
        "        for metrica, valor, descricao in metricas:\n",
        "            ws[f\"A{row}\"] = metrica\n",
        "            ws[f\"B{row}\"] = valor\n",
        "            ws[f\"C{row}\"] = descricao\n",
        "\n",
        "            ws[f\"A{row}\"].font = Font(bold=True)\n",
        "            if \"ðŸš¨\" in metrica and videos_sem_cta > 0:\n",
        "                ws[f\"B{row}\"].font = Font(bold=True, color=\"FF0000\")\n",
        "            elif isinstance(valor, (int, float)) and valor > 0:\n",
        "                ws[f\"B{row}\"].font = Font(bold=True, color=\"70AD47\")\n",
        "\n",
        "            row += 1\n",
        "\n",
        "        row += 2\n",
        "\n",
        "        # Ranking de performance\n",
        "        ws[f\"A{row}\"] = \"ðŸ† RANKING DE PERFORMANCE POR SCORE DE PERSUASÃƒO\"\n",
        "        ws[f\"A{row}\"].font = Font(bold=True, size=12, color=\"1F4E79\")\n",
        "        row += 2\n",
        "\n",
        "        # Headers\n",
        "        headers = [\"PosiÃ§Ã£o\", \"VÃ­deo ID\", \"Score\", \"Ganchos\", \"Gatilhos\", \"CTAs\", \"Status\"]\n",
        "        for col, header in enumerate(headers, 1):\n",
        "            cell = ws.cell(row=row, column=col)\n",
        "            cell.value = header\n",
        "            cell.font = Font(bold=True)\n",
        "            cell.fill = PatternFill(start_color=\"D9E2F3\", end_color=\"D9E2F3\", fill_type=\"solid\")\n",
        "        row += 1\n",
        "\n",
        "        # Top performers\n",
        "        top_videos = sorted(videos_copy, key=lambda x: x.get(\"score_persuasao\", 0), reverse=True)\n",
        "\n",
        "        for i, video in enumerate(top_videos, 1):\n",
        "            ws.cell(row=row, column=1, value=f\"{i}Âº\")\n",
        "            ws.cell(row=row, column=2, value=video[\"video_id\"])\n",
        "            ws.cell(row=row, column=3, value=f\"{video.get('score_persuasao', 0)}/100\")\n",
        "            ws.cell(row=row, column=4, value=len(video.get(\"ganchos_detectados\", {})))\n",
        "            ws.cell(row=row, column=5, value=len(video.get(\"gatilhos_mentais_detectados\", {})))\n",
        "            ws.cell(row=row, column=6, value=len(video.get(\"ctas_detectados\", {})))\n",
        "\n",
        "            # Status baseado no score\n",
        "            score = video.get(\"score_persuasao\", 0)\n",
        "            if score >= 70:\n",
        "                status = \"ðŸŸ¢ Ã“TIMO\"\n",
        "                status_color = \"70AD47\"\n",
        "            elif score >= 50:\n",
        "                status = \"ðŸŸ¡ BOM\"\n",
        "                status_color = \"FFC000\"\n",
        "            else:\n",
        "                status = \"ðŸ”´ PRECISA OTIMIZAR\"\n",
        "                status_color = \"C5504B\"\n",
        "\n",
        "            cell_status = ws.cell(row=row, column=7, value=status)\n",
        "            cell_status.font = Font(color=status_color, bold=True)\n",
        "\n",
        "            # Destacar top 3\n",
        "            if i <= 3:\n",
        "                for col in range(1, 8):\n",
        "                    ws.cell(row=row, column=col).fill = PatternFill(start_color=\"FFF2CC\", end_color=\"FFF2CC\", fill_type=\"solid\")\n",
        "\n",
        "            row += 1\n",
        "\n",
        "        row += 2\n",
        "\n",
        "        # AnÃ¡lise de gaps crÃ­ticos\n",
        "        ws[f\"A{row}\"] = \"âš ï¸ GAPS CRÃTICOS IDENTIFICADOS\"\n",
        "        ws[f\"A{row}\"].font = Font(bold=True, size=12, color=\"C5504B\")\n",
        "        row += 2\n",
        "\n",
        "        gaps = []\n",
        "\n",
        "        # VÃ­deos sem CTA\n",
        "        if videos_sem_cta > 0:\n",
        "            gap_cta_videos = [v[\"video_id\"] for v in videos_copy if not v.get(\"ctas_detectados\")]\n",
        "            gaps.append(f\"ðŸš¨ CRÃTICO: {videos_sem_cta} vÃ­deos sem CTA: {', '.join(gap_cta_videos[:3])}\")\n",
        "\n",
        "        # VÃ­deos com poucos ganchos\n",
        "        videos_poucos_ganchos = [v for v in videos_copy if len(v.get(\"ganchos_detectados\", {})) < 2]\n",
        "        if len(videos_poucos_ganchos) > len(videos_copy) * 0.5:\n",
        "            gaps.append(f\"ðŸ“ˆ OPORTUNIDADE: {len(videos_poucos_ganchos)} vÃ­deos precisam de mais ganchos\")\n",
        "\n",
        "        # Score baixo\n",
        "        videos_score_baixo = [v for v in videos_copy if v.get(\"score_persuasao\", 0) < 50]\n",
        "        if videos_score_baixo:\n",
        "            gaps.append(f\"ðŸŽ¯ OTIMIZAÃ‡ÃƒO: {len(videos_score_baixo)} vÃ­deos com score < 50 precisam de revisÃ£o\")\n",
        "\n",
        "        if not gaps:\n",
        "            gaps.append(\"âœ… Nenhum gap crÃ­tico identificado - parabÃ©ns!\")\n",
        "\n",
        "        for gap in gaps:\n",
        "            ws[f\"A{row}\"] = gap\n",
        "            if \"ðŸš¨\" in gap:\n",
        "                ws[f\"A{row}\"].font = Font(color=\"FF0000\", bold=True)\n",
        "            elif \"ðŸ“ˆ\" in gap or \"ðŸŽ¯\" in gap:\n",
        "                ws[f\"A{row}\"].font = Font(color=\"FFC000\", bold=True)\n",
        "            else:\n",
        "                ws[f\"A{row}\"].font = Font(color=\"70AD47\", bold=True)\n",
        "            row += 1\n",
        "\n",
        "    else:\n",
        "        ws[f\"A{row}\"] = \"âš ï¸ Nenhum dado de copywriting encontrado\"\n",
        "        ws[f\"A{row}\"].font = Font(color=\"C5504B\", bold=True)\n",
        "        row += 1\n",
        "        ws[f\"A{row}\"] = \"Execute primeiro a CÃ©lula 2.4 para gerar anÃ¡lises de copywriting\"\n",
        "\n",
        "    # Ajustar larguras das colunas\n",
        "    for col, width in [(\"A\", 25), (\"B\", 15), (\"C\", 40), (\"D\", 10), (\"E\", 10), (\"F\", 10), (\"G\", 20), (\"H\", 15)]:\n",
        "        ws.column_dimensions[col].width = width\n",
        "\n",
        "def adicionar_aba_templates_copy(wb, dados):\n",
        "    \"\"\"Adiciona aba de templates replicÃ¡veis\"\"\"\n",
        "    from openpyxl.styles import Font, PatternFill, Alignment\n",
        "\n",
        "    ws = wb.create_sheet(\"Templates ReplicÃ¡veis\")\n",
        "\n",
        "    # TÃ­tulo\n",
        "    ws.merge_cells(\"A1:F1\")\n",
        "    titulo = ws[\"A1\"]\n",
        "    titulo.value = \"TEMPLATES E ESTRUTURAS REPLICÃVEIS DE COPYWRITING\"\n",
        "    titulo.fill = PatternFill(start_color=\"70AD47\", end_color=\"70AD47\", fill_type=\"solid\")\n",
        "    titulo.font = Font(color=\"FFFFFF\", bold=True, size=14)\n",
        "    titulo.alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "    row = 3\n",
        "\n",
        "    # Coletar todos os templates\n",
        "    todos_templates = []\n",
        "    for video in dados[\"copywriting\"]:\n",
        "        templates = video.get(\"templates_identificados\", [])\n",
        "        for template in templates:\n",
        "            template[\"video_id\"] = video[\"video_id\"]\n",
        "            todos_templates.append(template)\n",
        "\n",
        "    if todos_templates:\n",
        "        # Agrupar templates por tipo\n",
        "        templates_agrupados = {}\n",
        "        for template in todos_templates:\n",
        "            nome = template[\"nome\"]\n",
        "            if nome not in templates_agrupados:\n",
        "                templates_agrupados[nome] = {\n",
        "                    \"estrutura\": template[\"estrutura\"],\n",
        "                    \"eficacia\": template[\"eficacia\"],\n",
        "                    \"uso_recomendado\": template[\"uso_recomendado\"],\n",
        "                    \"videos_exemplo\": []\n",
        "                }\n",
        "            templates_agrupados[nome][\"videos_exemplo\"].append(template[\"video_id\"])\n",
        "\n",
        "        # Exibir templates\n",
        "        for nome_template, dados_template in templates_agrupados.items():\n",
        "            ws.merge_cells(f\"A{row}:F{row}\")\n",
        "            template_header = ws[f\"A{row}\"]\n",
        "            template_header.value = f\"ðŸ“‹ TEMPLATE: {nome_template.replace('_', ' ')}\"\n",
        "            template_header.fill = PatternFill(start_color=\"E2EFDA\", end_color=\"E2EFDA\", fill_type=\"solid\")\n",
        "            template_header.font = Font(bold=True, size=11)\n",
        "            row += 1\n",
        "\n",
        "            ws[f\"A{row}\"] = \"Estrutura:\"\n",
        "            ws[f\"B{row}\"] = dados_template[\"estrutura\"]\n",
        "            ws[f\"A{row}\"].font = Font(bold=True)\n",
        "            row += 1\n",
        "\n",
        "            ws[f\"A{row}\"] = \"EficÃ¡cia:\"\n",
        "            ws[f\"B{row}\"] = dados_template[\"eficacia\"]\n",
        "            ws[f\"A{row}\"].font = Font(bold=True)\n",
        "            if dados_template[\"eficacia\"] == \"MUITO ALTA\":\n",
        "                ws[f\"B{row}\"].font = Font(color=\"70AD47\", bold=True)\n",
        "            elif dados_template[\"eficacia\"] == \"ALTA\":\n",
        "                ws[f\"B{row}\"].font = Font(color=\"C5504B\", bold=True)\n",
        "            row += 1\n",
        "\n",
        "            ws[f\"A{row}\"] = \"Uso Recomendado:\"\n",
        "            ws[f\"B{row}\"] = dados_template[\"uso_recomendado\"]\n",
        "            ws[f\"A{row}\"].font = Font(bold=True)\n",
        "            row += 1\n",
        "\n",
        "            ws[f\"A{row}\"] = \"VÃ­deos Exemplo:\"\n",
        "            ws[f\"B{row}\"] = \", \".join(dados_template[\"videos_exemplo\"][:3])\n",
        "            ws[f\"A{row}\"].font = Font(bold=True)\n",
        "            row += 1\n",
        "\n",
        "            # Como aplicar\n",
        "            ws[f\"A{row}\"] = \"Como Aplicar:\"\n",
        "            ws[f\"A{row}\"].font = Font(bold=True, color=\"7030A0\")\n",
        "            row += 1\n",
        "\n",
        "            instrucoes = gerar_instrucoes_aplicacao_template(nome_template)\n",
        "            for i, instrucao in enumerate(instrucoes, 1):\n",
        "                ws[f\"B{row}\"] = f\"{i}. {instrucao}\"\n",
        "                row += 1\n",
        "\n",
        "            row += 2\n",
        "\n",
        "    else:\n",
        "        ws[f\"A{row}\"] = \"ðŸ“‹ Ainda nÃ£o foram identificados templates especÃ­ficos\"\n",
        "        row += 1\n",
        "        ws[f\"A{row}\"] = \"Execute mais anÃ¡lises para identificar padrÃµes replicÃ¡veis\"\n",
        "\n",
        "    # Templates recomendados universais\n",
        "    row += 2\n",
        "    ws.merge_cells(f\"A{row}:F{row}\")\n",
        "    recom_header = ws[f\"A{row}\"]\n",
        "    recom_header.value = \"ðŸŽ¯ TEMPLATES UNIVERSAIS RECOMENDADOS PARA IMPLEMENTAR\"\n",
        "    recom_header.fill = PatternFill(start_color=\"FFC000\", end_color=\"FFC000\", fill_type=\"solid\")\n",
        "    recom_header.font = Font(bold=True, size=12)\n",
        "    row += 1\n",
        "\n",
        "    templates_universais = [\n",
        "        (\"PERGUNTA + VALOR + CTA\", \"Pergunta engajante â†’ Entrega valor â†’ Call-to-action direto\", \"Todos os vÃ­deos educativos\"),\n",
        "        (\"PROBLEMA + AGITAÃ‡ÃƒO + SOLUÃ‡ÃƒO\", \"Identifica dor â†’ Agrava problema â†’ Apresenta soluÃ§Ã£o\", \"VÃ­deos de vendas e transformaÃ§Ã£o\"),\n",
        "        (\"CURIOSIDADE + HISTÃ“RIA + ENSINO\", \"Desperta curiosidade â†’ Conta histÃ³ria â†’ Ensina mÃ©todo\", \"Content marketing e autoridade\"),\n",
        "        (\"PROVA SOCIAL + URGÃŠNCIA + AÃ‡ÃƒO\", \"Mostra resultados â†’ Cria urgÃªncia â†’ Direciona aÃ§Ã£o\", \"LanÃ§amentos e ofertas\")\n",
        "    ]\n",
        "\n",
        "    headers_univ = [\"Template\", \"Estrutura\", \"AplicaÃ§Ã£o Ideal\"]\n",
        "    for col, header in enumerate(headers_univ, 1):\n",
        "        cell = ws.cell(row=row, column=col)\n",
        "        cell.value = header\n",
        "        cell.font = Font(bold=True)\n",
        "        cell.fill = PatternFill(start_color=\"F2F2F2\", end_color=\"F2F2F2\", fill_type=\"solid\")\n",
        "    row += 1\n",
        "\n",
        "    for nome, estrutura, aplicacao in templates_universais:\n",
        "        ws.cell(row=row, column=1, value=nome)\n",
        "        ws.cell(row=row, column=2, value=estrutura)\n",
        "        ws.cell(row=row, column=3, value=aplicacao)\n",
        "        ws.cell(row=row, column=1).font = Font(bold=True)\n",
        "        row += 1\n",
        "\n",
        "    # Ajustar larguras\n",
        "    for col, width in [(\"A\", 20), (\"B\", 50), (\"C\", 25), (\"D\", 15), (\"E\", 15), (\"F\", 15)]:\n",
        "        ws.column_dimensions[col].width = width\n",
        "\n",
        "def gerar_instrucoes_aplicacao_template(nome_template):\n",
        "    \"\"\"Gera instruÃ§Ãµes especÃ­ficas para aplicar um template\"\"\"\n",
        "    instrucoes_map = {\n",
        "        \"PERGUNTA_AUTORIDADE_CTA\": [\n",
        "            \"Inicie com pergunta que conecte com a dor/desejo do pÃºblico\",\n",
        "            \"EstabeleÃ§a credibilidade (experiÃªncia, resultados, formaÃ§Ã£o)\",\n",
        "            \"Termine com CTA claro e especÃ­fico\",\n",
        "            \"Mantenha tom conversacional mas assertivo\"\n",
        "        ],\n",
        "        \"PROBLEMA_SOLUCAO_PROVA\": [\n",
        "            \"Identifique problema especÃ­fico e real do pÃºblico\",\n",
        "            \"Apresente soluÃ§Ã£o clara e aplicÃ¡vel\",\n",
        "            \"Mostre provas sociais (depoimentos, nÃºmeros, casos)\",\n",
        "            \"Use linguagem emocional para conectar\"\n",
        "        ],\n",
        "        \"CURIOSIDADE_URGENCIA_ACAO\": [\n",
        "            \"Desperte curiosidade nos primeiros 3 segundos\",\n",
        "            \"Crie senso de urgÃªncia (limitado, exclusivo)\",\n",
        "            \"Direcione para aÃ§Ã£o imediata especÃ­fica\",\n",
        "            \"Use gatilhos de escassez e FOMO\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    return instrucoes_map.get(nome_template, [\n",
        "        \"Analise a estrutura identificada no vÃ­deo de exemplo\",\n",
        "        \"Adapte os elementos para seu nicho especÃ­fico\",\n",
        "        \"Teste diferentes abordagens mantendo a estrutura\",\n",
        "        \"Monitore resultados e otimize baseado na performance\"\n",
        "    ])\n",
        "\n",
        "def adicionar_aba_timeline_copy(wb, dados):\n",
        "    \"\"\"Adiciona aba com timeline de elementos persuasivos\"\"\"\n",
        "    from openpyxl.styles import Font, PatternFill, Alignment\n",
        "\n",
        "    ws = wb.create_sheet(\"Timeline PersuasÃ£o\")\n",
        "\n",
        "    # TÃ­tulo\n",
        "    ws.merge_cells(\"A1:G1\")\n",
        "    titulo = ws[\"A1\"]\n",
        "    titulo.value = \"TIMELINE DE ELEMENTOS PERSUASIVOS - MAPEAMENTO TEMPORAL\"\n",
        "    titulo.fill = PatternFill(start_color=\"7030A0\", end_color=\"7030A0\", fill_type=\"solid\")\n",
        "    titulo.font = Font(color=\"FFFFFF\", bold=True, size=14)\n",
        "    titulo.alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "    row = 3\n",
        "\n",
        "    # AnÃ¡lise temporal por vÃ­deo (mostrar apenas top 3 por brevidade)\n",
        "    videos_copy = sorted(dados[\"copywriting\"], key=lambda x: x.get(\"score_persuasao\", 0), reverse=True)\n",
        "\n",
        "    for video in videos_copy[:3]:  # Top 3 performers\n",
        "        ws.merge_cells(f\"A{row}:G{row}\")\n",
        "        video_header = ws[f\"A{row}\"]\n",
        "        video_header.value = f\"ðŸŽ¬ TIMELINE: {video['video_id']} (Score: {video.get('score_persuasao', 0)}/100)\"\n",
        "        video_header.fill = PatternFill(start_color=\"E2EFDA\", end_color=\"E2EFDA\", fill_type=\"solid\")\n",
        "        video_header.font = Font(bold=True)\n",
        "        row += 1\n",
        "\n",
        "        # Headers da timeline\n",
        "        headers = [\"Tempo\", \"Tipo\", \"Elemento\", \"Contexto\", \"PosiÃ§Ã£o\", \"Impacto\", \"AnÃ¡lise\"]\n",
        "        for col, header in enumerate(headers, 1):\n",
        "            cell = ws.cell(row=row, column=col)\n",
        "            cell.value = header\n",
        "            cell.font = Font(bold=True)\n",
        "            cell.fill = PatternFill(start_color=\"F2F2F2\", end_color=\"F2F2F2\", fill_type=\"solid\")\n",
        "        row += 1\n",
        "\n",
        "        # Consolidar timeline\n",
        "        timeline_elementos = []\n",
        "\n",
        "        # Adicionar elementos de cada categoria\n",
        "        for categoria, timeline_key in [(\"GANCHO\", \"ganchos_timeline\"), (\"GATILHO\", \"gatilhos_timeline\"), (\"CTA\", \"ctas_timeline\")]:\n",
        "            timeline_data = video.get(\"timestamp\", {}).get(timeline_key, [])\n",
        "            for item in timeline_data:\n",
        "                timeline_elementos.append({\n",
        "                    \"categoria\": categoria,\n",
        "                    \"tempo\": f\"{item['minuto']:02d}:{item['segundo']:02d}\",\n",
        "                    \"tipo\": item[\"tipo\"].replace(\"_\", \" \").title(),\n",
        "                    \"contexto\": item.get(\"contexto\", \"\")[:40] + \"...\" if len(item.get(\"contexto\", \"\")) > 40 else item.get(\"contexto\", \"\"),\n",
        "                    \"minuto\": item[\"minuto\"],\n",
        "                    \"segundo\": item[\"segundo\"]\n",
        "                })\n",
        "\n",
        "        # Ordenar por tempo\n",
        "        timeline_elementos.sort(key=lambda x: (x[\"minuto\"], x[\"segundo\"]))\n",
        "\n",
        "        if timeline_elementos:\n",
        "            for elemento in timeline_elementos:\n",
        "                ws.cell(row=row, column=1, value=elemento[\"tempo\"])\n",
        "                ws.cell(row=row, column=2, value=elemento[\"categoria\"])\n",
        "                ws.cell(row=row, column=3, value=elemento[\"tipo\"])\n",
        "                ws.cell(row=row, column=4, value=elemento[\"contexto\"])\n",
        "\n",
        "                # Calcular posiÃ§Ã£o no vÃ­deo\n",
        "                total_segundos = elemento[\"minuto\"] * 60 + elemento[\"segundo\"]\n",
        "                if total_segundos <= 10:\n",
        "                    posicao = \"ABERTURA\"\n",
        "                    posicao_color = \"70AD47\"\n",
        "                elif total_segundos <= 20:\n",
        "                    posicao = \"MEIO\"\n",
        "                    posicao_color = \"FFC000\"\n",
        "                else:\n",
        "                    posicao = \"FINAL\"\n",
        "                    posicao_color = \"C5504B\"\n",
        "\n",
        "                cell_pos = ws.cell(row=row, column=5, value=posicao)\n",
        "                cell_pos.font = Font(color=posicao_color, bold=True)\n",
        "\n",
        "                # AnÃ¡lise de impacto\n",
        "                impacto = analisar_impacto_elemento(elemento[\"categoria\"], posicao)\n",
        "                ws.cell(row=row, column=6, value=impacto[\"score\"])\n",
        "                ws.cell(row=row, column=7, value=impacto[\"analise\"])\n",
        "\n",
        "                if impacto[\"score\"] == \"ALTO\":\n",
        "                    ws.cell(row=row, column=6).font = Font(color=\"70AD47\", bold=True)\n",
        "                elif impacto[\"score\"] == \"BAIXO\":\n",
        "                    ws.cell(row=row, column=6).font = Font(color=\"C5504B\", bold=True)\n",
        "\n",
        "                row += 1\n",
        "        else:\n",
        "            ws.cell(row=row, column=1, value=\"Nenhum elemento temporal mapeado\")\n",
        "            row += 1\n",
        "\n",
        "        row += 2\n",
        "\n",
        "    # PadrÃµes temporais identificados\n",
        "    row += 1\n",
        "    ws.merge_cells(f\"A{row}:G{row}\")\n",
        "    padroes_header = ws[f\"A{row}\"]\n",
        "    padroes_header.value = \"ðŸ“Š PADRÃ•ES TEMPORAIS IDENTIFICADOS\"\n",
        "    padroes_header.fill = PatternFill(start_color=\"1F4E79\", end_color=\"1F4E79\", fill_type=\"solid\")\n",
        "    padroes_header.font = Font(color=\"FFFFFF\", bold=True, size=12)\n",
        "    row += 1\n",
        "\n",
        "    padroes_temporais = [\n",
        "        \"âœ… GANCHOS mais eficazes nos primeiros 10 segundos\",\n",
        "        \"âœ… GATILHOS MENTAIS ideais entre 10-20 segundos\",\n",
        "        \"âœ… CTAs mais conversores nos Ãºltimos 5 segundos\",\n",
        "        \"âš ï¸ Evitar CTAs nos primeiros 5 segundos\",\n",
        "        \"ðŸ“ˆ Combinar CURIOSIDADE + AUTORIDADE = alta retenÃ§Ã£o\"\n",
        "    ]\n",
        "\n",
        "    for padrao in padroes_temporais:\n",
        "        ws[f\"A{row}\"] = padrao\n",
        "        if \"âœ…\" in padrao:\n",
        "            ws[f\"A{row}\"].font = Font(color=\"70AD47\", bold=True)\n",
        "        elif \"âš ï¸\" in padrao:\n",
        "            ws[f\"A{row}\"].font = Font(color=\"FFC000\", bold=True)\n",
        "        else:\n",
        "            ws[f\"A{row}\"].font = Font(color=\"1F4E79\", bold=True)\n",
        "        row += 1\n",
        "\n",
        "    # Ajustar larguras\n",
        "    for col, width in [(\"A\", 8), (\"B\", 10), (\"C\", 15), (\"D\", 30), (\"E\", 12), (\"F\", 8), (\"G\", 25)]:\n",
        "        ws.column_dimensions[col].width = width\n",
        "\n",
        "def analisar_impacto_elemento(categoria, posicao):\n",
        "    \"\"\"Analisa o impacto de um elemento baseado na posiÃ§Ã£o\"\"\"\n",
        "    impactos = {\n",
        "        (\"GANCHO\", \"ABERTURA\"): {\"score\": \"ALTO\", \"analise\": \"Ideal para capturar atenÃ§Ã£o\"},\n",
        "        (\"GANCHO\", \"MEIO\"): {\"score\": \"MÃ‰DIO\", \"analise\": \"Melhor no inÃ­cio\"},\n",
        "        (\"GANCHO\", \"FINAL\"): {\"score\": \"BAIXO\", \"analise\": \"Reposicionar para abertura\"},\n",
        "        (\"GATILHO\", \"ABERTURA\"): {\"score\": \"MÃ‰DIO\", \"analise\": \"Bom para credibilidade\"},\n",
        "        (\"GATILHO\", \"MEIO\"): {\"score\": \"ALTO\", \"analise\": \"PosiÃ§Ã£o ideal para persuasÃ£o\"},\n",
        "        (\"GATILHO\", \"FINAL\"): {\"score\": \"MÃ‰DIO\", \"analise\": \"ReforÃ§a decisÃ£o\"},\n",
        "        (\"CTA\", \"ABERTURA\"): {\"score\": \"BAIXO\", \"analise\": \"Muito cedo, construir valor primeiro\"},\n",
        "        (\"CTA\", \"MEIO\"): {\"score\": \"MÃ‰DIO\", \"analise\": \"Considerar mover para final\"},\n",
        "        (\"CTA\", \"FINAL\"): {\"score\": \"ALTO\", \"analise\": \"Posicionamento ideal\"}\n",
        "    }\n",
        "\n",
        "    return impactos.get((categoria, posicao), {\"score\": \"MÃ‰DIO\", \"analise\": \"Analisar contexto especÃ­fico\"})\n",
        "\n",
        "def adicionar_aba_recomendacoes_copy(wb, dados):\n",
        "    \"\"\"Adiciona aba de recomendaÃ§Ãµes estratÃ©gicas consolidadas\"\"\"\n",
        "    from openpyxl.styles import Font, PatternFill, Alignment\n",
        "\n",
        "    ws = wb.create_sheet(\"RecomendaÃ§Ãµes Copy\")\n",
        "\n",
        "    # TÃ­tulo\n",
        "    ws.merge_cells(\"A1:F1\")\n",
        "    titulo = ws[\"A1\"]\n",
        "    titulo.value = \"RECOMENDAÃ‡Ã•ES ESTRATÃ‰GICAS DE COPYWRITING - PLANO DE AÃ‡ÃƒO\"\n",
        "    titulo.fill = PatternFill(start_color=\"C5504B\", end_color=\"C5504B\", fill_type=\"solid\")\n",
        "    titulo.font = Font(color=\"FFFFFF\", bold=True, size=14)\n",
        "    titulo.alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "    row = 3\n",
        "\n",
        "    # Consolidar recomendaÃ§Ãµes por prioridade\n",
        "    todas_recomendacoes = []\n",
        "    for video in dados[\"copywriting\"]:\n",
        "        recomendacoes_video = video.get(\"recomendacoes_estrategicas\", [])\n",
        "        for rec in recomendacoes_video:\n",
        "            rec[\"video_id\"] = video[\"video_id\"]\n",
        "            todas_recomendacoes.append(rec)\n",
        "\n",
        "    # Agrupar por prioridade\n",
        "    recomendacoes_por_prioridade = {\n",
        "        \"CRÃTICA\": [],\n",
        "        \"ALTA\": [],\n",
        "        \"MÃ‰DIA\": []\n",
        "    }\n",
        "\n",
        "    for rec in todas_recomendacoes:\n",
        "        prioridade = rec.get(\"prioridade\", \"MÃ‰DIA\")\n",
        "        if prioridade in recomendacoes_por_prioridade:\n",
        "            recomendacoes_por_prioridade[prioridade].append(rec)\n",
        "\n",
        "    # Exibir por prioridade\n",
        "    for prioridade in [\"CRÃTICA\", \"ALTA\", \"MÃ‰DIA\"]:\n",
        "        if not recomendacoes_por_prioridade[prioridade]:\n",
        "            continue\n",
        "\n",
        "        ws[f\"A{row}\"] = f\"ðŸš¨ PRIORIDADE {prioridade}\"\n",
        "        if prioridade == \"CRÃTICA\":\n",
        "            ws[f\"A{row}\"].font = Font(color=\"FF0000\", bold=True, size=12)\n",
        "        elif prioridade == \"ALTA\":\n",
        "            ws[f\"A{row}\"].font = Font(color=\"C5504B\", bold=True, size=12)\n",
        "        else:\n",
        "            ws[f\"A{row}\"].font = Font(color=\"FFC000\", bold=True, size=12)\n",
        "\n",
        "        row += 2\n",
        "\n",
        "        # Headers\n",
        "        headers = [\"Categoria\", \"RecomendaÃ§Ã£o\", \"VÃ­deos Afetados\", \"AÃ§Ã£o Sugerida\"]\n",
        "        for col, header in enumerate(headers, 1):\n",
        "            cell = ws.cell(row=row, column=col)\n",
        "            cell.value = header\n",
        "            cell.font = Font(bold=True)\n",
        "            cell.fill = PatternFill(start_color=\"F2F2F2\", end_color=\"F2F2F2\", fill_type=\"solid\")\n",
        "        row += 1\n",
        "\n",
        "        # Agrupar recomendaÃ§Ãµes similares da mesma prioridade\n",
        "        grupos = {}\n",
        "        for rec in recomendacoes_por_prioridade[prioridade]:\n",
        "            categoria = rec[\"categoria\"]\n",
        "            if categoria not in grupos:\n",
        "                grupos[categoria] = {\n",
        "                    \"recomendacao\": rec[\"recomendacao\"],\n",
        "                    \"videos\": [],\n",
        "                    \"acao\": gerar_acao_especifica(categoria)\n",
        "                }\n",
        "            grupos[categoria][\"videos\"].append(rec[\"video_id\"])\n",
        "\n",
        "        for categoria, dados_grupo in grupos.items():\n",
        "            ws.cell(row=row, column=1, value=categoria)\n",
        "            ws.cell(row=row, column=2, value=dados_grupo[\"recomendacao\"])\n",
        "            ws.cell(row=row, column=3, value=f\"{len(dados_grupo['videos'])} vÃ­deo(s)\")\n",
        "            ws.cell(row=row, column=4, value=dados_grupo[\"acao\"])\n",
        "            row += 1\n",
        "\n",
        "        row += 2\n",
        "\n",
        "    # Plano de aÃ§Ã£o 30 dias\n",
        "    row += 2\n",
        "    ws.merge_cells(f\"A{row}:F{row}\")\n",
        "    plano_header = ws[f\"A{row}\"]\n",
        "    plano_header.value = \"ðŸ“… PLANO DE AÃ‡ÃƒO ESTRATÃ‰GICO - PRÃ“XIMOS 30 DIAS\"\n",
        "    plano_header.fill = PatternFill(start_color=\"70AD47\", end_color=\"70AD47\", fill_type=\"solid\")\n",
        "    plano_header.font = Font(color=\"FFFFFF\", bold=True, size=12)\n",
        "    row += 2\n",
        "\n",
        "    plano_30_dias = [\n",
        "        (\"SEMANA 1 - CRÃTICO\", [\n",
        "            \"Implementar CTAs em TODOS os vÃ­deos sem call-to-action\",\n",
        "            \"Corrigir vÃ­deos com score de persuasÃ£o abaixo de 30\",\n",
        "            \"Aplicar templates identificados nos vÃ­deos top performers\"\n",
        "        ]),\n",
        "        (\"SEMANA 2 - ALTA PRIORIDADE\", [\n",
        "            \"Adicionar ganchos de abertura nos vÃ­deos com baixa retenÃ§Ã£o\",\n",
        "            \"Incorporar gatilhos de autoridade e prova social\",\n",
        "            \"Otimizar timeline de elementos persuasivos\"\n",
        "        ]),\n",
        "        (\"SEMANA 3 - OTIMIZAÃ‡ÃƒO\", [\n",
        "            \"Testar variaÃ§Ãµes de CTAs mais eficazes\",\n",
        "            \"Refinar estruturas narrativas baseadas nos templates\",\n",
        "            \"A/B testing de elementos especÃ­ficos\"\n",
        "        ]),\n",
        "        (\"SEMANA 4 - VALIDAÃ‡ÃƒO\", [\n",
        "            \"Medir performance pÃ³s-implementaÃ§Ã£o\",\n",
        "            \"Documentar novos padrÃµes de sucesso identificados\",\n",
        "            \"Atualizar biblioteca de templates comprovados\"\n",
        "        ])\n",
        "    ]\n",
        "\n",
        "    for semana_titulo, acoes in plano_30_dias:\n",
        "        ws[f\"A{row}\"] = semana_titulo\n",
        "        ws[f\"A{row}\"].font = Font(bold=True, color=\"1F4E79\", size=11)\n",
        "        row += 1\n",
        "\n",
        "        for acao in acoes:\n",
        "            ws[f\"B{row}\"] = f\"â€¢ {acao}\"\n",
        "            row += 1\n",
        "\n",
        "        row += 1\n",
        "\n",
        "    # KPIs de acompanhamento\n",
        "    row += 2\n",
        "    ws.merge_cells(f\"A{row}:F{row}\")\n",
        "    kpis_header = ws[f\"A{row}\"]\n",
        "    kpis_header.value = \"ðŸ“Š KPIs DE ACOMPANHAMENTO - MÃ‰TRICAS DE SUCESSO\"\n",
        "    kpis_header.fill = PatternFill(start_color=\"7030A0\", end_color=\"7030A0\", fill_type=\"solid\")\n",
        "    kpis_header.font = Font(color=\"FFFFFF\", bold=True, size=12)\n",
        "    row += 2\n",
        "\n",
        "    kpis = [\n",
        "        (\"Score de PersuasÃ£o MÃ©dio\", \"Aumento de 20% em 30 dias\", \"Mensal\"),\n",
        "        (\"Taxa de CTAs Implementados\", \"100% dos vÃ­deos com pelo menos 1 CTA\", \"Imediato\"),\n",
        "        (\"Variedade de Ganchos\", \"3+ tipos diferentes por vÃ­deo\", \"Por vÃ­deo\"),\n",
        "        (\"Diversidade de Gatilhos\", \"4+ gatilhos mentais por vÃ­deo\", \"Por vÃ­deo\"),\n",
        "        (\"Templates Ativos\", \"5+ estruturas replicÃ¡veis em uso\", \"Mensal\"),\n",
        "        (\"Taxa de OtimizaÃ§Ã£o\", \"80% das recomendaÃ§Ãµes crÃ­ticas aplicadas\", \"Semanal\")\n",
        "    ]\n",
        "\n",
        "    headers_kpi = [\"KPI\", \"Meta\", \"FrequÃªncia de MediÃ§Ã£o\"]\n",
        "    for col, header in enumerate(headers_kpi, 1):\n",
        "        cell = ws.cell(row=row, column=col)\n",
        "        cell.value = header\n",
        "        cell.font = Font(bold=True)\n",
        "        cell.fill = PatternFill(start_color=\"E2EFDA\", end_color=\"E2EFDA\", fill_type=\"solid\")\n",
        "    row += 1\n",
        "\n",
        "    for kpi_nome, meta, frequencia in kpis:\n",
        "        ws.cell(row=row, column=1, value=kpi_nome)\n",
        "        ws.cell(row=row, column=2, value=meta)\n",
        "        ws.cell(row=row, column=3, value=frequencia)\n",
        "        row += 1\n",
        "\n",
        "    # PrÃ³ximos passos imediatos\n",
        "    row += 3\n",
        "    ws[f\"A{row}\"] = \"ðŸŽ¯ PRÃ“XIMOS PASSOS IMEDIATOS (HOJE)\"\n",
        "    ws[f\"A{row}\"].font = Font(bold=True, color=\"C5504B\", size=12)\n",
        "    row += 1\n",
        "\n",
        "    proximos_passos = gerar_proximos_passos_imediatos(dados[\"copywriting\"])\n",
        "\n",
        "    for i, passo in enumerate(proximos_passos, 1):\n",
        "        ws[f\"A{row}\"] = f\"{i}. {passo}\"\n",
        "        ws[f\"A{row}\"].font = Font(bold=True)\n",
        "        row += 1\n",
        "\n",
        "    # Ajustar larguras\n",
        "    for col, width in [(\"A\", 20), (\"B\", 40), (\"C\", 15), (\"D\", 30), (\"E\", 15), (\"F\", 15)]:\n",
        "        ws.column_dimensions[col].width = width\n",
        "\n",
        "def gerar_acao_especifica(categoria):\n",
        "    \"\"\"Gera aÃ§Ã£o especÃ­fica baseada na categoria da recomendaÃ§Ã£o\"\"\"\n",
        "    acoes = {\n",
        "        \"GANCHOS\": \"Revisar primeiros 5 segundos e adicionar pergunta ou curiosidade\",\n",
        "        \"GATILHOS\": \"Incorporar elementos de autoridade, prova social ou reciprocidade\",\n",
        "        \"CTA\": \"Adicionar call-to-action claro nos Ãºltimos 3-5 segundos\",\n",
        "        \"ESTRUTURA\": \"Aplicar template identificado mais prÃ³ximo do nicho\",\n",
        "        \"PERSUASÃƒO\": \"Combinar mÃºltiplos elementos persuasivos em sequÃªncia lÃ³gica\"\n",
        "    }\n",
        "    return acoes.get(categoria, \"Revisar e otimizar elementos especÃ­ficos mencionados\")\n",
        "\n",
        "def gerar_proximos_passos_imediatos(videos_copy):\n",
        "    \"\"\"Gera lista de aÃ§Ãµes imediatas baseadas na anÃ¡lise\"\"\"\n",
        "    passos = []\n",
        "\n",
        "    # Verificar vÃ­deos sem CTA\n",
        "    videos_sem_cta = [v for v in videos_copy if not v.get(\"ctas_detectados\")]\n",
        "    if videos_sem_cta:\n",
        "        passos.append(f\"CRÃTICO: Adicionar CTAs em {len(videos_sem_cta)} vÃ­deo(s): {', '.join([v['video_id'] for v in videos_sem_cta[:3]])}\")\n",
        "\n",
        "    # Verificar scores baixos\n",
        "    videos_score_baixo = [v for v in videos_copy if v.get(\"score_persuasao\", 0) < 30]\n",
        "    if videos_score_baixo:\n",
        "        passos.append(f\"Revisar {len(videos_score_baixo)} vÃ­deo(s) com score crÃ­tico < 30\")\n",
        "\n",
        "    # Templates a aplicar\n",
        "    templates_identificados = []\n",
        "    for video in videos_copy:\n",
        "        templates_identificados.extend(video.get(\"templates_identificados\", []))\n",
        "\n",
        "    if templates_identificados:\n",
        "        template_mais_comum = max(set(t[\"nome\"] for t in templates_identificados),\n",
        "                                 key=lambda x: sum(1 for t in templates_identificados if t[\"nome\"] == x))\n",
        "        passos.append(f\"Aplicar template '{template_mais_comum.replace('_', ' ')}' em novos vÃ­deos\")\n",
        "\n",
        "    # AÃ§Ãµes gerais\n",
        "    passos.extend([\n",
        "        \"Backup dos vÃ­deos atuais antes das modificaÃ§Ãµes\",\n",
        "        \"Priorizar implementaÃ§Ãµes por ordem de impacto (CTAs primeiro)\",\n",
        "        \"Documentar mudanÃ§as para acompanhar resultados\"\n",
        "    ])\n",
        "\n",
        "    return passos[:6]  # Limitar a 6 passos\n",
        "\n",
        "def atualizar_aba_principal_com_copy(wb, dados):\n",
        "    \"\"\"Atualiza a aba principal existente com mÃ©tricas de copywriting\"\"\"\n",
        "    # Tentar encontrar aba principal (pode ter nomes diferentes)\n",
        "    aba_principal = None\n",
        "    possiveis_nomes = [\"Dashboard Principal\", \"Executive Summary\", \"Summary\", \"Principal\"]\n",
        "\n",
        "    for nome in wb.sheetnames:\n",
        "        if any(possivel in nome for possivel in possiveis_nomes):\n",
        "            aba_principal = wb[nome]\n",
        "            break\n",
        "\n",
        "    if not aba_principal:\n",
        "        # Se nÃ£o encontrou, usar a primeira aba\n",
        "        aba_principal = wb.worksheets[0]\n",
        "\n",
        "    # Encontrar prÃ³xima linha vazia para adicionar seÃ§Ã£o de copywriting\n",
        "    next_row = 1\n",
        "    for row in range(1, 100):\n",
        "        if aba_principal[f\"A{row}\"].value is None:\n",
        "            next_row = row\n",
        "            break\n",
        "\n",
        "    # Adicionar seÃ§Ã£o de copywriting\n",
        "    from openpyxl.styles import Font, PatternFill\n",
        "\n",
        "    # TÃ­tulo da seÃ§Ã£o\n",
        "    aba_principal.merge_cells(f\"A{next_row}:H{next_row}\")\n",
        "    titulo_copy = aba_principal[f\"A{next_row}\"]\n",
        "    titulo_copy.value = \"ðŸ“ ANÃLISE DE COPYWRITING - RESUMO EXECUTIVO\"\n",
        "    titulo_copy.fill = PatternFill(start_color=\"7030A0\", end_color=\"7030A0\", fill_type=\"solid\")\n",
        "    titulo_copy.font = Font(color=\"FFFFFF\", bold=True, size=12)\n",
        "    next_row += 2\n",
        "\n",
        "    # MÃ©tricas resumidas\n",
        "    videos_copy = dados[\"copywriting\"]\n",
        "\n",
        "    if videos_copy:\n",
        "        scores = [v.get(\"score_persuasao\", 0) for v in videos_copy]\n",
        "        score_medio = sum(scores) / len(scores)\n",
        "        videos_sem_cta = len([v for v in videos_copy if not v.get(\"ctas_detectados\")])\n",
        "        templates_total = sum(len(v.get(\"templates_identificados\", [])) for v in videos_copy)\n",
        "\n",
        "        metricas_resumo = [\n",
        "            (\"Score de PersuasÃ£o MÃ©dio:\", f\"{score_medio:.1f}/100\"),\n",
        "            (\"VÃ­deos sem CTA:\", f\"{videos_sem_cta} (CRÃTICO)\" if videos_sem_cta > 0 else \"0 âœ…\"),\n",
        "            (\"Templates Identificados:\", str(templates_total)),\n",
        "            (\"Status Geral:\", \"OtimizaÃ§Ã£o necessÃ¡ria\" if score_medio < 60 or videos_sem_cta > 0 else \"Performance boa\")\n",
        "        ]\n",
        "\n",
        "        for metrica, valor in metricas_resumo:\n",
        "            aba_principal[f\"A{next_row}\"] = metrica\n",
        "            aba_principal[f\"B{next_row}\"] = valor\n",
        "            aba_principal[f\"A{next_row}\"].font = Font(bold=True)\n",
        "\n",
        "            if \"CRÃTICO\" in valor:\n",
        "                aba_principal[f\"B{next_row}\"].font = Font(color=\"FF0000\", bold=True)\n",
        "            elif \"âœ…\" in valor:\n",
        "                aba_principal[f\"B{next_row}\"].font = Font(color=\"70AD47\", bold=True)\n",
        "\n",
        "            next_row += 1\n",
        "\n",
        "    else:\n",
        "        aba_principal[f\"A{next_row}\"] = \"âš ï¸ Execute a anÃ¡lise de copywriting (CÃ©lula 2.4) para ver mÃ©tricas\"\n",
        "        aba_principal[f\"A{next_row}\"].font = Font(color=\"FFC000\", bold=True)\n",
        "\n",
        "def gerar_relatorios_copywriting_individuais(dados):\n",
        "    \"\"\"Gera relatÃ³rios individuais de texto para cada vÃ­deo\"\"\"\n",
        "    print(\"  ðŸ“„ Gerando relatÃ³rios individuais de copywriting...\")\n",
        "\n",
        "    pasta_relatorios = os.path.join(PASTA_TRABALHO, \"relatorios_copywriting\")\n",
        "    os.makedirs(pasta_relatorios, exist_ok=True)\n",
        "\n",
        "    for video_copy in dados[\"copywriting\"]:\n",
        "        video_id = video_copy[\"video_id\"]\n",
        "\n",
        "        relatorio_path = os.path.join(pasta_relatorios, f\"{video_id}_copywriting_completo.txt\")\n",
        "\n",
        "        with open(relatorio_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\"=\"*60 + \"\\n\")\n",
        "            f.write(\"RELATÃ“RIO COMPLETO DE ANÃLISE DE COPYWRITING\\n\")\n",
        "            f.write(\"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "            f.write(f\"ðŸ“¹ VÃ­deo ID: {video_id}\\n\")\n",
        "            f.write(f\"ðŸŽ¯ Score de PersuasÃ£o: {video_copy.get('score_persuasao', 0)}/100\\n\")\n",
        "            f.write(f\"ðŸ“ Total de Palavras: {video_copy.get('total_palavras', 0)}\\n\\n\")\n",
        "\n",
        "            # Texto completo\n",
        "            f.write(\"TRANSCRIÃ‡ÃƒO COMPLETA:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            f.write(video_copy.get(\"texto_completo\", \"TranscriÃ§Ã£o nÃ£o disponÃ­vel\") + \"\\n\\n\")\n",
        "\n",
        "            # Ganchos\n",
        "            f.write(\"ðŸŽ£ GANCHOS DETECTADOS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            ganchos = video_copy.get(\"ganchos_detectados\", {})\n",
        "            if ganchos:\n",
        "                for tipo, dados in ganchos.items():\n",
        "                    f.write(f\"â€¢ {tipo.replace('_', ' ').title()}: {dados['count']} ocorrÃªncia(s)\\n\")\n",
        "                    for exemplo in dados.get(\"exemplos\", [])[:2]:\n",
        "                        f.write(f\"  - \\\"{exemplo}\\\"\\n\")\n",
        "                    f.write(\"\\n\")\n",
        "            else:\n",
        "                f.write(\"âŒ Nenhum gancho detectado - OPORTUNIDADE DE MELHORIA\\n\\n\")\n",
        "\n",
        "            # Gatilhos\n",
        "            f.write(\"ðŸ§  GATILHOS MENTAIS DETECTADOS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            gatilhos = video_copy.get(\"gatilhos_mentais_detectados\", {})\n",
        "            if gatilhos:\n",
        "                for tipo, dados in gatilhos.items():\n",
        "                    f.write(f\"â€¢ {tipo.replace('_', ' ').title()}: {dados['count']} ocorrÃªncia(s)\\n\")\n",
        "                    for exemplo in dados.get(\"exemplos\", [])[:2]:\n",
        "                        f.write(f\"  - \\\"{exemplo}\\\"\\n\")\n",
        "                    f.write(\"\\n\")\n",
        "            else:\n",
        "                f.write(\"âŒ Nenhum gatilho mental detectado - ADICIONAR URGENTEMENTE\\n\\n\")\n",
        "\n",
        "            # CTAs\n",
        "            f.write(\"ðŸ“¢ CALLS-TO-ACTION DETECTADOS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            ctas = video_copy.get(\"ctas_detectados\", {})\n",
        "            if ctas:\n",
        "                for tipo, dados in ctas.items():\n",
        "                    f.write(f\"â€¢ {tipo.replace('_', ' ').title()}: {dados['count']} ocorrÃªncia(s)\\n\")\n",
        "                    for exemplo in dados.get(\"exemplos\", [])[:2]:\n",
        "                        f.write(f\"  - \\\"{exemplo}\\\"\\n\")\n",
        "                    f.write(\"\\n\")\n",
        "            else:\n",
        "                f.write(\"ðŸš¨ CRÃTICO: Nenhum CTA detectado - IMPLEMENTAR IMEDIATAMENTE\\n\\n\")\n",
        "\n",
        "            # Templates\n",
        "            f.write(\"ðŸ“‹ TEMPLATES IDENTIFICADOS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            templates = video_copy.get(\"templates_identificados\", [])\n",
        "            if templates:\n",
        "                for template in templates:\n",
        "                    f.write(f\"â€¢ {template['nome'].replace('_', ' ')}\\n\")\n",
        "                    f.write(f\"  Estrutura: {template['estrutura']}\\n\")\n",
        "                    f.write(f\"  EficÃ¡cia: {template['eficacia']}\\n\")\n",
        "                    f.write(f\"  Uso: {template['uso_recomendado']}\\n\\n\")\n",
        "            else:\n",
        "                f.write(\"ðŸ“ Nenhum template especÃ­fico identificado\\n\\n\")\n",
        "\n",
        "            # RecomendaÃ§Ãµes\n",
        "            f.write(\"ðŸŽ¯ RECOMENDAÃ‡Ã•ES ESTRATÃ‰GICAS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            recomendacoes = video_copy.get(\"recomendacoes_estrategicas\", [])\n",
        "            if recomendacoes:\n",
        "                for i, rec in enumerate(recomendacoes, 1):\n",
        "                    f.write(f\"{i}. [{rec['prioridade']}] {rec['categoria']}\\n\")\n",
        "                    f.write(f\"   {rec['recomendacao']}\\n\\n\")\n",
        "            else:\n",
        "                f.write(\"âœ… Nenhuma recomendaÃ§Ã£o crÃ­tica - vÃ­deo bem otimizado\\n\\n\")\n",
        "\n",
        "            # Timeline resumida\n",
        "            f.write(\"â° TIMELINE DE ELEMENTOS (RESUMIDA):\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            timeline_ganchos = video_copy.get(\"timestamp\", {}).get(\"ganchos_timeline\", [])\n",
        "            timeline_ctas = video_copy.get(\"timestamp\", {}).get(\"ctas_timeline\", [])\n",
        "\n",
        "            todos_elementos = []\n",
        "            for item in timeline_ganchos:\n",
        "                todos_elementos.append((item[\"minuto\"], item[\"segundo\"], \"GANCHO\", item[\"tipo\"]))\n",
        "            for item in timeline_ctas:\n",
        "                todos_elementos.append((item[\"minuto\"], item[\"segundo\"], \"CTA\", item[\"tipo\"]))\n",
        "\n",
        "            todos_elementos.sort()\n",
        "\n",
        "            if todos_elementos:\n",
        "                for minuto, segundo, categoria, tipo in todos_elementos:\n",
        "                    f.write(f\"[{minuto:02d}:{segundo:02d}] {categoria}: {tipo.replace('_', ' ').title()}\\n\")\n",
        "            else:\n",
        "                f.write(\"Nenhum elemento temporal mapeado\\n\")\n",
        "\n",
        "            f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "            f.write(\"RelatÃ³rio gerado pelo sistema de engenharia reversa\\n\")\n",
        "            f.write(\"Para implementar as recomendaÃ§Ãµes, consulte o dashboard principal\\n\")\n",
        "            f.write(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    print(f\"  âœ… {len(dados['copywriting'])} relatÃ³rios individuais gerados\")\n",
        "\n",
        "# FunÃ§Ã£o principal de execuÃ§Ã£o\n",
        "def executar_integracao_copywriting_dashboard():\n",
        "    \"\"\"FunÃ§Ã£o principal para executar a integraÃ§Ã£o\"\"\"\n",
        "    print(\"ðŸš€ EXECUTANDO INTEGRAÃ‡ÃƒO DE COPYWRITING NO DASHBOARD EXISTENTE\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    try:\n",
        "        dashboard_atualizado = integrar_copywriting_dashboard_existente()\n",
        "\n",
        "        if dashboard_atualizado:\n",
        "            print(\"\\n\" + \"=\"*70)\n",
        "            print(\"âœ… INTEGRAÃ‡ÃƒO CONCLUÃDA COM SUCESSO!\")\n",
        "            print(\"=\"*70)\n",
        "            print(f\"ðŸ“Š Dashboard atualizado: {os.path.basename(dashboard_atualizado)}\")\n",
        "            print(\"\\nðŸ“‹ ABAS ADICIONADAS:\")\n",
        "            print(\"  â€¢ Copywriting EstratÃ©gico - AnÃ¡lise completa por vÃ­deo\")\n",
        "            print(\"  â€¢ Templates ReplicÃ¡veis - Estruturas identificadas\")\n",
        "            print(\"  â€¢ Timeline PersuasÃ£o - Mapeamento temporal\")\n",
        "            print(\"  â€¢ RecomendaÃ§Ãµes Copy - Plano de aÃ§Ã£o 30 dias\")\n",
        "            print(\"  â€¢ Dashboard Principal - Atualizada com mÃ©tricas\")\n",
        "\n",
        "            print(f\"\\nðŸŽ¯ PRÃ“XIMOS PASSOS:\")\n",
        "            print(\"1. Abra o dashboard e revise a aba 'Copywriting EstratÃ©gico'\")\n",
        "            print(\"2. Identifique vÃ­deos com score < 50 para otimizaÃ§Ã£o\")\n",
        "            print(\"3. Implemente CTAs nos vÃ­deos marcados como CRÃTICO\")\n",
        "            print(\"4. Aplique templates identificados em novos vÃ­deos\")\n",
        "            print(\"5. Siga o plano de aÃ§Ã£o de 30 dias na aba 'RecomendaÃ§Ãµes Copy'\")\n",
        "\n",
        "        else:\n",
        "            print(\"\\nâŒ Falha na integraÃ§Ã£o - verifique os prÃ©-requisitos\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ Erro de ExecuÃ§Ã£o: {type(e).__name__}: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# Executar a integraÃ§Ã£o\n",
        "if __name__ == \"__main__\":\n",
        "    executar_integracao_copywriting_dashboard()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-wM_X9W5nZU",
        "outputId": "a71bc0c4-1390-49c2-ea9e-7778a71e4eec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”„ Iniciando processamento de copywriting adaptado...\n",
            "âœ… Dados de decomposiÃ§Ã£o carregados: 3 vÃ­deos encontrados\n",
            "ðŸ“Š Processando 3 vÃ­deos com transcriÃ§Ã£o vÃ¡lida...\n",
            "[1/3] Processando copywriting para: vid_bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t\n",
            "  ðŸ”„ Gerando legendas com timestamps...\n",
            "    âœ… Legendas SRT geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t_legendas.srt\n",
            "    âœ… Legendas TXT com timestamps geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t_legendas_timestamped.txt\n",
            "    ðŸ”„ Analisando copywriting...\n",
            "  âœ… Copywriting analisado: Score 0/100\n",
            "[2/3] Processando copywriting para: vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf\n",
            "  ðŸ”„ Gerando legendas com timestamps...\n",
            "    âœ… Legendas SRT geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf_legendas.srt\n",
            "    âœ… Legendas TXT com timestamps geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf_legendas_timestamped.txt\n",
            "    ðŸ”„ Analisando copywriting...\n",
            "  âœ… Copywriting analisado: Score 30/100\n",
            "[3/3] Processando copywriting para: vid_maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy\n",
            "  ðŸ”„ Gerando legendas com timestamps...\n",
            "    âœ… Legendas SRT geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy_legendas.srt\n",
            "    âœ… Legendas TXT com timestamps geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy_legendas_timestamped.txt\n",
            "    ðŸ”„ Analisando copywriting...\n",
            "  âœ… Copywriting analisado: Score 75/100\n",
            "ðŸ’¾ AnÃ¡lises de copywriting salvas em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/analises_copywriting_completas.json\n",
            "ðŸ’¾ Dados de legendas salvos em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/legendas_geradas.json\n",
            "\n",
            "âœ… ANÃLISE DE COPYWRITING CONCLUÃDA!\n",
            "Total de vÃ­deos com copywriting analisado: 3\n",
            "Total de legendas geradas: 3\n",
            "\n",
            "âž¡ï¸ PRÃ“XIMA CÃ‰LULA: 4.3 - INTEGRAÃ‡ÃƒO COM DASHBOARD\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# LAYER 2.4: GERAÃ‡ÃƒO DE LEGENDAS E ANÃLISE DE COPYWRITING - VERSÃƒO FINAL\n",
        "# ============================================================================\n",
        "\n",
        "import re\n",
        "from datetime import timedelta, datetime\n",
        "from collections import Counter\n",
        "import json\n",
        "import os\n",
        "\n",
        "def processar_copywriting_todos_videos_adaptado():\n",
        "    \"\"\"Processa anÃ¡lise de copywriting adaptada para o sistema existente\"\"\"\n",
        "    print(\"ðŸ”„ Iniciando processamento de copywriting adaptado...\")\n",
        "\n",
        "    # Verificar prÃ©-requisitos baseado na estrutura existente\n",
        "    if not \"PASTA_TRABALHO\" in globals():\n",
        "        print(\"âŒ VariÃ¡veis globais nÃ£o encontradas. Execute a CÃ‰LULA 1.2 primeiro.\")\n",
        "        return\n",
        "\n",
        "    pasta_dados = os.path.join(PASTA_TRABALHO, \"dados\")\n",
        "    if not os.path.exists(pasta_dados):\n",
        "        print(\"âŒ Pasta de dados nÃ£o encontrada. Execute as cÃ©lulas anteriores primeiro.\")\n",
        "        return\n",
        "\n",
        "    # Buscar dados de decomposiÃ§Ã£o (nome correto do arquivo)\n",
        "    decomposicao_path = os.path.join(pasta_dados, \"decomposicao_completa.json\")\n",
        "\n",
        "    if not os.path.exists(decomposicao_path):\n",
        "        print(\"âŒ Dados de decomposiÃ§Ã£o nÃ£o encontrados. Execute a CÃ‰LULA 2.3 primeiro.\")\n",
        "        print(f\"Procurando arquivo: {decomposicao_path}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        with open(decomposicao_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            decomposicoes_data = json.load(f)\n",
        "\n",
        "        print(f\"âœ… Dados de decomposiÃ§Ã£o carregados: {len(decomposicoes_data)} vÃ­deos encontrados\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Erro ao carregar dados de decomposiÃ§Ã£o: {e}\")\n",
        "        return\n",
        "\n",
        "    # Filtrar apenas vÃ­deos com status \"decomposto\" e que tenham transcriÃ§Ã£o\n",
        "    videos_validos = []\n",
        "    for decomposicao in decomposicoes_data:\n",
        "        if (decomposicao.get(\"status\") == \"decomposto\" and\n",
        "            decomposicao.get(\"audio_transcrito\") and\n",
        "            len(decomposicao.get(\"audio_transcrito\", \"\").strip()) > 10):\n",
        "            videos_validos.append(decomposicao)\n",
        "\n",
        "    if not videos_validos:\n",
        "        print(\"âŒ Nenhum vÃ­deo com transcriÃ§Ã£o vÃ¡lida encontrado.\")\n",
        "        print(\"Verifique se a CÃ‰LULA 2.3 foi executada com sucesso e se os vÃ­deos possuem Ã¡udio.\")\n",
        "        return\n",
        "\n",
        "    print(f\"ðŸ“Š Processando {len(videos_validos)} vÃ­deos com transcriÃ§Ã£o vÃ¡lida...\")\n",
        "\n",
        "    analises_copywriting = []\n",
        "    legendas_geradas = []\n",
        "\n",
        "    for i, decomposicao in enumerate(videos_validos, 1):\n",
        "        video_id = decomposicao[\"video_id\"]\n",
        "        audio_transcrito = decomposicao[\"audio_transcrito\"]\n",
        "\n",
        "        print(f\"[{i}/{len(videos_validos)}] Processando copywriting para: {video_id}\")\n",
        "\n",
        "        try:\n",
        "            # Estimar duraÃ§Ã£o do vÃ­deo baseado na anÃ¡lise de Ã¡udio\n",
        "            duracao_segundos = decomposicao.get(\"audio_analise\", {}).get(\"duracao_audio_segundos\", 30)\n",
        "\n",
        "            # Criar info do vÃ­deo para compatibilidade\n",
        "            video_info = {\n",
        "                \"id\": video_id,\n",
        "                \"duracao_segundos\": duracao_segundos\n",
        "            }\n",
        "\n",
        "            # Gerar legendas\n",
        "            legendas_data, srt_path, txt_path = gerar_legendas_com_timestamps(video_info, decomposicao)\n",
        "\n",
        "            if legendas_data:\n",
        "                legendas_info = {\n",
        "                    \"video_id\": video_id,\n",
        "                    \"srt_path\": srt_path,\n",
        "                    \"txt_path\": txt_path,\n",
        "                    \"total_segmentos\": len(legendas_data),\n",
        "                    \"duracao_total\": duracao_segundos,\n",
        "                    \"legendas_data\": legendas_data\n",
        "                }\n",
        "                legendas_geradas.append(legendas_info)\n",
        "\n",
        "                # AnÃ¡lise de copywriting\n",
        "                analise_copy = analisar_copywriting_estrategico(legendas_data, video_id)\n",
        "                analises_copywriting.append(analise_copy)\n",
        "\n",
        "                print(f\"  âœ… Copywriting analisado: Score {analise_copy['score_persuasao']}/100\")\n",
        "            else:\n",
        "                print(f\"  âŒ Falha na geraÃ§Ã£o de legendas para {video_id}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  âŒ Erro no processamento de copywriting para {video_id}: {e}\")\n",
        "\n",
        "    if not analises_copywriting:\n",
        "        print(\"âŒ Nenhuma anÃ¡lise de copywriting foi gerada. Verifique os dados de entrada.\")\n",
        "        return\n",
        "\n",
        "    # Salvar dados de copywriting\n",
        "    copywriting_path = os.path.join(pasta_dados, \"analises_copywriting_completas.json\")\n",
        "    with open(copywriting_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(analises_copywriting, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"ðŸ’¾ AnÃ¡lises de copywriting salvas em: {copywriting_path}\")\n",
        "\n",
        "    # Salvar dados de legendas\n",
        "    legendas_path = os.path.join(pasta_dados, \"legendas_geradas.json\")\n",
        "    with open(legendas_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(legendas_geradas, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"ðŸ’¾ Dados de legendas salvos em: {legendas_path}\")\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    if os.path.exists(config_path):\n",
        "        try:\n",
        "            with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                config = json.load(f)\n",
        "\n",
        "            config[\"status_etapas\"][\"copywriting_analysis\"] = True\n",
        "\n",
        "            with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "        except:\n",
        "            print(\"âš ï¸ NÃ£o foi possÃ­vel atualizar o arquivo de configuraÃ§Ã£o\")\n",
        "\n",
        "    print(f\"\\nâœ… ANÃLISE DE COPYWRITING CONCLUÃDA!\")\n",
        "    print(f\"Total de vÃ­deos com copywriting analisado: {len(analises_copywriting)}\")\n",
        "    print(f\"Total de legendas geradas: {len(legendas_geradas)}\")\n",
        "    print(f\"\\nâž¡ï¸ PRÃ“XIMA CÃ‰LULA: 4.3 - INTEGRAÃ‡ÃƒO COM DASHBOARD\")\n",
        "\n",
        "def gerar_legendas_com_timestamps(video_info, decomposicao_data):\n",
        "    \"\"\"Gera legendas SRT e TXT com timestamps precisos a partir da transcriÃ§Ã£o\"\"\"\n",
        "    print(\"  ðŸ”„ Gerando legendas com timestamps...\")\n",
        "\n",
        "    video_id = video_info[\"id\"]\n",
        "    audio_transcrito = decomposicao_data.get(\"audio_transcrito\", \"\")\n",
        "\n",
        "    if not audio_transcrito.strip():\n",
        "        print(\"    âŒ Erro: TranscriÃ§Ã£o de Ã¡udio vazia\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Calcular duraÃ§Ã£o do vÃ­deo\n",
        "    duracao_segundos = video_info.get(\"duracao_segundos\", 30)  # Default 30s se nÃ£o informado\n",
        "\n",
        "    # Dividir texto em segmentos baseados em pontuaÃ§Ã£o e pausas naturais\n",
        "    segmentos = dividir_texto_em_segmentos(audio_transcrito)\n",
        "\n",
        "    if not segmentos:\n",
        "        print(\"    âŒ Erro: NÃ£o foi possÃ­vel segmentar o texto\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Calcular timestamps para cada segmento\n",
        "    legendas_data = []\n",
        "    duracao_por_segmento = duracao_segundos / len(segmentos) if segmentos else 1\n",
        "\n",
        "    for i, segmento in enumerate(segmentos):\n",
        "        inicio_segundos = i * duracao_por_segmento\n",
        "        fim_segundos = (i + 1) * duracao_por_segmento\n",
        "\n",
        "        legenda_item = {\n",
        "            \"id\": i + 1,\n",
        "            \"inicio\": segundos_para_timestamp(inicio_segundos),\n",
        "            \"fim\": segundos_para_timestamp(fim_segundos),\n",
        "            \"texto\": segmento.strip(),\n",
        "            \"inicio_segundos\": inicio_segundos,\n",
        "            \"fim_segundos\": fim_segundos\n",
        "        }\n",
        "        legendas_data.append(legenda_item)\n",
        "\n",
        "    # Gerar arquivos SRT e TXT\n",
        "    pasta_legendas = os.path.join(PASTA_TRABALHO, \"legendas\")\n",
        "    os.makedirs(pasta_legendas, exist_ok=True)\n",
        "\n",
        "    # Arquivo SRT\n",
        "    srt_path = os.path.join(pasta_legendas, f\"{video_id}_legendas.srt\")\n",
        "    gerar_arquivo_srt(legendas_data, srt_path)\n",
        "\n",
        "    # Arquivo TXT com timestamps\n",
        "    txt_path = os.path.join(pasta_legendas, f\"{video_id}_legendas_timestamped.txt\")\n",
        "    gerar_arquivo_txt_timestamped(legendas_data, txt_path)\n",
        "\n",
        "    print(f\"    âœ… Legendas SRT geradas: {srt_path}\")\n",
        "    print(f\"    âœ… Legendas TXT com timestamps geradas: {txt_path}\")\n",
        "\n",
        "    return legendas_data, srt_path, txt_path\n",
        "\n",
        "def dividir_texto_em_segmentos(texto, max_chars=50):\n",
        "    \"\"\"Divide o texto em segmentos lÃ³gicos para legendas\"\"\"\n",
        "    # Dividir por frases primeiro\n",
        "    frases = re.split(r'[.!?]+', texto)\n",
        "    segmentos = []\n",
        "\n",
        "    for frase in frases:\n",
        "        if not frase.strip():\n",
        "            continue\n",
        "\n",
        "        # Se a frase Ã© muito longa, dividir por vÃ­rgulas ou conjunÃ§Ãµes\n",
        "        if len(frase) > max_chars:\n",
        "            sub_segmentos = re.split(r'[,;]|(?:\\s+(?:e|mas|entÃ£o|porque|que)\\s+)', frase)\n",
        "            for sub in sub_segmentos:\n",
        "                if sub.strip() and len(sub.strip()) > 3:\n",
        "                    segmentos.append(sub.strip())\n",
        "        else:\n",
        "            if frase.strip() and len(frase.strip()) > 3:\n",
        "                segmentos.append(frase.strip())\n",
        "\n",
        "    # Se ainda houver segmentos muito longos, dividir por palavras\n",
        "    segmentos_finais = []\n",
        "    for seg in segmentos:\n",
        "        if len(seg) > max_chars:\n",
        "            palavras = seg.split()\n",
        "            temp_seg = \"\"\n",
        "            for palavra in palavras:\n",
        "                if len(temp_seg + \" \" + palavra) <= max_chars:\n",
        "                    temp_seg += \" \" + palavra if temp_seg else palavra\n",
        "                else:\n",
        "                    if temp_seg:\n",
        "                        segmentos_finais.append(temp_seg.strip())\n",
        "                    temp_seg = palavra\n",
        "            if temp_seg:\n",
        "                segmentos_finais.append(temp_seg.strip())\n",
        "        else:\n",
        "            segmentos_finais.append(seg)\n",
        "\n",
        "    return segmentos_finais\n",
        "\n",
        "def segundos_para_timestamp(segundos):\n",
        "    \"\"\"Converte segundos para formato timestamp SRT (HH:MM:SS,mmm)\"\"\"\n",
        "    horas = int(segundos // 3600)\n",
        "    minutos = int((segundos % 3600) // 60)\n",
        "    segundos_restantes = segundos % 60\n",
        "    milissegundos = int((segundos_restantes - int(segundos_restantes)) * 1000)\n",
        "\n",
        "    return f\"{horas:02d}:{minutos:02d}:{int(segundos_restantes):02d},{milissegundos:03d}\"\n",
        "\n",
        "def gerar_arquivo_srt(legendas_data, srt_path):\n",
        "    \"\"\"Gera arquivo SRT\"\"\"\n",
        "    with open(srt_path, 'w', encoding='utf-8') as f:\n",
        "        for legenda in legendas_data:\n",
        "            f.write(f\"{legenda['id']}\\n\")\n",
        "            f.write(f\"{legenda['inicio']} --> {legenda['fim']}\\n\")\n",
        "            f.write(f\"{legenda['texto']}\\n\\n\")\n",
        "\n",
        "def gerar_arquivo_txt_timestamped(legendas_data, txt_path):\n",
        "    \"\"\"Gera arquivo TXT com timestamps\"\"\"\n",
        "    with open(txt_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"TRANSCRIÃ‡ÃƒO COM TIMESTAMPS\\n\")\n",
        "        f.write(\"=\"*50 + \"\\n\\n\")\n",
        "\n",
        "        for legenda in legendas_data:\n",
        "            minutos_inicio = int(legenda['inicio_segundos'] // 60)\n",
        "            segundos_inicio = int(legenda['inicio_segundos'] % 60)\n",
        "            minutos_fim = int(legenda['fim_segundos'] // 60)\n",
        "            segundos_fim = int(legenda['fim_segundos'] % 60)\n",
        "\n",
        "            f.write(f\"[{minutos_inicio:02d}:{segundos_inicio:02d}-{minutos_fim:02d}:{segundos_fim:02d}] {legenda['texto']}\\n\")\n",
        "\n",
        "def analisar_copywriting_estrategico(legendas_data, video_id):\n",
        "    \"\"\"AnÃ¡lise estratÃ©gica de copywriting com base nas legendas\"\"\"\n",
        "    print(\"    ðŸ”„ Analisando copywriting...\")\n",
        "\n",
        "    # Combinar todo o texto para anÃ¡lise completa\n",
        "    texto_completo = \" \".join([legenda[\"texto\"] for legenda in legendas_data])\n",
        "\n",
        "    # DicionÃ¡rios de padrÃµes de copywriting\n",
        "    ganchos_patterns = {\n",
        "        \"pergunta_retorica\": [r\"\\b(?:vocÃª|tu)\\s+(?:jÃ¡|nunca|sempre|realmente|acha|imagina|sabe|quer|precisa)\",\n",
        "                            r\"(?:como|por que|quando|onde|o que).*\\?\"],\n",
        "        \"urgencia\": [r\"\\b(?:agora|hoje|urgente|rÃ¡pido|imediato|Ãºltima chance|sÃ³ hoje|apenas|restam)\",\n",
        "                     r\"\\b(?:nÃ£o perca|aproveite|garante jÃ¡|corre|Ãºltimas vagas)\"],\n",
        "        \"escassez\": [r\"\\b(?:limitado|exclusivo|poucos|restam|Ãºltima|Ãºnica|especial|VIP)\",\n",
        "                     r\"\\b(?:sÃ³ para|apenas para|somente|limitado a)\"],\n",
        "        \"autoridade\": [r\"\\b(?:especialista|expert|profissional|anos de experiÃªncia|comprovado|testado)\",\n",
        "                       r\"\\b(?:pesquisas mostram|estudos comprovam|cientificamente)\"],\n",
        "        \"prova_social\": [r\"\\b(?:milhares|centenas|todos|muitas pessoas|clientes|depoimentos)\",\n",
        "                         r\"\\b(?:jÃ¡ conseguiram|transformaram|mudaram|aprovaram)\"],\n",
        "        \"curiosidade\": [r\"\\b(?:segredo|descoberta|revelaÃ§Ã£o|mÃ©todo|tÃ©cnica|estratÃ©gia|fÃ³rmula)\",\n",
        "                        r\"\\b(?:ninguÃ©m te conta|poucos sabem|descobri que)\"],\n",
        "        \"problema_dor\": [r\"\\b(?:problema|dificuldade|frustraÃ§Ã£o|sofre|dor|preocupa|bloqueia)\",\n",
        "                         r\"\\b(?:cansado de|chega de|pare de|nÃ£o aguenta mais)\"],\n",
        "        \"solucao_resultado\": [r\"\\b(?:soluÃ§Ã£o|resolve|elimina|transforma|muda|resultado|sucesso)\",\n",
        "                              r\"\\b(?:conseguir|alcanÃ§ar|realizar|conquistar|atingir)\"]\n",
        "    }\n",
        "\n",
        "    gatilhos_patterns = {\n",
        "        \"reciprocidade\": [r\"\\b(?:grÃ¡tis|de graÃ§a|presente|bÃ´nus|oferta|sem custo)\",\n",
        "                          r\"\\b(?:vou te dar|vou ensinar|vou mostrar|compartilhar com vocÃª)\"],\n",
        "        \"comprometimento\": [r\"\\b(?:compromisso|prometo|garanto|palavra|juro)\",\n",
        "                            r\"\\b(?:pode confiar|tenho certeza|assumo|responsabilizo)\"],\n",
        "        \"aprovacao_social\": [r\"\\b(?:aprovado por|recomendado|indicado|usado por|preferido)\",\n",
        "                             r\"\\b(?:famosos|influencers|especialistas|mÃ©dicos|profissionais)\"],\n",
        "        \"aversao_perda\": [r\"\\b(?:perder|perdendo|vai ficar de fora|nÃ£o vai conseguir)\",\n",
        "                          r\"\\b(?:sair perdendo|ficar para trÃ¡s|oportunidade perdida)\"],\n",
        "        \"autoridade_especialista\": [r\"\\b(?:Dr|Dra|Professor|Mestre|PhD|especialista em)\",\n",
        "                                    r\"\\b(?:formado em|pÃ³s-graduado|anos estudando)\"],\n",
        "        \"emocional_medo\": [r\"\\b(?:medo|receio|preocupaÃ§Ã£o|inseguranÃ§a|ansiedade)\",\n",
        "                           r\"\\b(?:nÃ£o conseguir|fracassar|dar errado|prejudicar)\"],\n",
        "        \"emocional_esperanca\": [r\"\\b(?:sonho|esperanÃ§a|desejo|objetivo|meta|futuro melhor)\",\n",
        "                                r\"\\b(?:realizar|conquistar|alcanÃ§ar|transformar|mudar vida)\"]\n",
        "    }\n",
        "\n",
        "    ctas_patterns = {\n",
        "        \"acao_imediata\": [r\"\\b(?:clica|clique|acesse|baixe|faÃ§a|compre|adquira|garanta)\",\n",
        "                          r\"\\b(?:nÃ£o perca|aproveite|corre|vai|vem|participe)\"],\n",
        "        \"link_bio\": [r\"\\b(?:link na bio|bio|biografia|perfil|stories|direct)\",\n",
        "                     r\"\\b(?:DM|chama no WhatsApp|manda mensagem)\"],\n",
        "        \"engajamento\": [r\"\\b(?:comenta|compartilha|marca|salva|curte|like|segue)\",\n",
        "                        r\"\\b(?:conta nos comentÃ¡rios|deixa um|comenta aqui)\"],\n",
        "        \"inscricao\": [r\"\\b(?:inscreve|se inscreva|ativa|ativar|sino|notificaÃ§Ã£o)\",\n",
        "                      r\"\\b(?:cadastra|cadastre-se|registra|assine)\"],\n",
        "        \"contato_vendas\": [r\"\\b(?:WhatsApp|telefone|ligue|chama|fala comigo|contato)\",\n",
        "                           r\"\\b(?:agende|marque|consulta|reuniÃ£o|conversa)\"]\n",
        "    }\n",
        "\n",
        "    # AnÃ¡lise dos padrÃµes\n",
        "    ganchos_encontrados = {}\n",
        "    gatilhos_encontrados = {}\n",
        "    ctas_encontrados = {}\n",
        "\n",
        "    # Analisar ganchos\n",
        "    for tipo, patterns in ganchos_patterns.items():\n",
        "        matches = []\n",
        "        for pattern in patterns:\n",
        "            matches.extend(re.finditer(pattern, texto_completo, re.IGNORECASE))\n",
        "        if matches:\n",
        "            ganchos_encontrados[tipo] = {\n",
        "                \"count\": len(matches),\n",
        "                \"exemplos\": [m.group() for m in matches[:3]],  # Top 3 exemplos\n",
        "                \"timestamps\": encontrar_timestamps_matches(matches, legendas_data, texto_completo)\n",
        "            }\n",
        "\n",
        "    # Analisar gatilhos\n",
        "    for tipo, patterns in gatilhos_patterns.items():\n",
        "        matches = []\n",
        "        for pattern in patterns:\n",
        "            matches.extend(re.finditer(pattern, texto_completo, re.IGNORECASE))\n",
        "        if matches:\n",
        "            gatilhos_encontrados[tipo] = {\n",
        "                \"count\": len(matches),\n",
        "                \"exemplos\": [m.group() for m in matches[:3]],\n",
        "                \"timestamps\": encontrar_timestamps_matches(matches, legendas_data, texto_completo)\n",
        "            }\n",
        "\n",
        "    # Analisar CTAs\n",
        "    for tipo, patterns in ctas_patterns.items():\n",
        "        matches = []\n",
        "        for pattern in patterns:\n",
        "            matches.extend(re.finditer(pattern, texto_completo, re.IGNORECASE))\n",
        "        if matches:\n",
        "            ctas_encontrados[tipo] = {\n",
        "                \"count\": len(matches),\n",
        "                \"exemplos\": [m.group() for m in matches[:3]],\n",
        "                \"timestamps\": encontrar_timestamps_matches(matches, legendas_data, texto_completo)\n",
        "            }\n",
        "\n",
        "    # AnÃ¡lise de estrutura narrativa\n",
        "    estrutura_narrativa = analisar_estrutura_narrativa(legendas_data)\n",
        "\n",
        "    # AnÃ¡lise de poder de persuasÃ£o\n",
        "    score_persuasao = calcular_score_persuasao(ganchos_encontrados, gatilhos_encontrados, ctas_encontrados)\n",
        "\n",
        "    analise_copywriting = {\n",
        "        \"video_id\": video_id,\n",
        "        \"texto_completo\": texto_completo,\n",
        "        \"total_palavras\": len(texto_completo.split()),\n",
        "        \"ganchos_detectados\": ganchos_encontrados,\n",
        "        \"gatilhos_mentais_detectados\": gatilhos_encontrados,\n",
        "        \"ctas_detectados\": ctas_encontrados,\n",
        "        \"estrutura_narrativa\": estrutura_narrativa,\n",
        "        \"score_persuasao\": score_persuasao,\n",
        "        \"recomendacoes_estrategicas\": gerar_recomendacoes_copywriting(ganchos_encontrados, gatilhos_encontrados, ctas_encontrados),\n",
        "        \"templates_identificados\": identificar_templates_replicaveis(ganchos_encontrados, gatilhos_encontrados, ctas_encontrados),\n",
        "        \"timestamp\": {\n",
        "            \"ganchos_timeline\": mapear_timeline_elementos(ganchos_encontrados, legendas_data),\n",
        "            \"gatilhos_timeline\": mapear_timeline_elementos(gatilhos_encontrados, legendas_data),\n",
        "            \"ctas_timeline\": mapear_timeline_elementos(ctas_encontrados, legendas_data)\n",
        "        },\n",
        "        \"data_analise\": datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    return analise_copywriting\n",
        "\n",
        "def encontrar_timestamps_matches(matches, legendas_data, texto_completo):\n",
        "    \"\"\"Encontra os timestamps correspondentes aos matches encontrados\"\"\"\n",
        "    timestamps = []\n",
        "\n",
        "    for match in matches[:3]:  # Limitar a 3 exemplos\n",
        "        posicao = match.start()\n",
        "        char_count = 0\n",
        "\n",
        "        for legenda in legendas_data:\n",
        "            texto_legenda = legenda[\"texto\"]\n",
        "            if char_count <= posicao < char_count + len(texto_legenda):\n",
        "                timestamps.append({\n",
        "                    \"minuto\": int(legenda[\"inicio_segundos\"] // 60),\n",
        "                    \"segundo\": int(legenda[\"inicio_segundos\"] % 60),\n",
        "                    \"texto_contexto\": texto_legenda\n",
        "                })\n",
        "                break\n",
        "            char_count += len(texto_legenda) + 1  # +1 para o espaÃ§o entre legendas\n",
        "\n",
        "    return timestamps\n",
        "\n",
        "def analisar_estrutura_narrativa(legendas_data):\n",
        "    \"\"\"Analisa a estrutura narrativa do vÃ­deo\"\"\"\n",
        "    total_segmentos = len(legendas_data)\n",
        "\n",
        "    if total_segmentos < 3:\n",
        "        return {\n",
        "            \"abertura\": {\"segmentos\": total_segmentos, \"elementos\": []},\n",
        "            \"desenvolvimento\": {\"segmentos\": 0, \"elementos\": []},\n",
        "            \"fechamento\": {\"segmentos\": 0, \"elementos\": []}\n",
        "        }\n",
        "\n",
        "    # Dividir em terÃ§os para anÃ¡lise\n",
        "    primeiro_terco = legendas_data[:total_segmentos//3]\n",
        "    segundo_terco = legendas_data[total_segmentos//3:2*total_segmentos//3]\n",
        "    ultimo_terco = legendas_data[2*total_segmentos//3:]\n",
        "\n",
        "    estrutura = {\n",
        "        \"abertura\": {\n",
        "            \"segmentos\": len(primeiro_terco),\n",
        "            \"elementos\": analisar_elementos_abertura(primeiro_terco)\n",
        "        },\n",
        "        \"desenvolvimento\": {\n",
        "            \"segmentos\": len(segundo_terco),\n",
        "            \"elementos\": analisar_elementos_desenvolvimento(segundo_terco)\n",
        "        },\n",
        "        \"fechamento\": {\n",
        "            \"segmentos\": len(ultimo_terco),\n",
        "            \"elementos\": analisar_elementos_fechamento(ultimo_terco)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return estrutura\n",
        "\n",
        "def analisar_elementos_abertura(segmentos):\n",
        "    \"\"\"Analisa elementos da abertura\"\"\"\n",
        "    texto = \" \".join([s[\"texto\"] for s in segmentos])\n",
        "    elementos = []\n",
        "\n",
        "    if re.search(r'\\b(?:vocÃª|tu)\\s+(?:jÃ¡|nunca|sempre)', texto, re.IGNORECASE):\n",
        "        elementos.append(\"pergunta_engajamento\")\n",
        "    if re.search(r'\\b(?:vou te|vou mostrar|vou ensinar)', texto, re.IGNORECASE):\n",
        "        elementos.append(\"promessa_valor\")\n",
        "    if re.search(r'\\b(?:segredo|descoberta|mÃ©todo)', texto, re.IGNORECASE):\n",
        "        elementos.append(\"curiosidade\")\n",
        "\n",
        "    return elementos\n",
        "\n",
        "def analisar_elementos_desenvolvimento(segmentos):\n",
        "    \"\"\"Analisa elementos do desenvolvimento\"\"\"\n",
        "    texto = \" \".join([s[\"texto\"] for s in segmentos])\n",
        "    elementos = []\n",
        "\n",
        "    if re.search(r'\\b(?:porque|pois|isso acontece)', texto, re.IGNORECASE):\n",
        "        elementos.append(\"explicacao\")\n",
        "    if re.search(r'\\b(?:exemplo|caso|situaÃ§Ã£o)', texto, re.IGNORECASE):\n",
        "        elementos.append(\"exemplificacao\")\n",
        "    if re.search(r'\\b(?:resultado|consegui|transformou)', texto, re.IGNORECASE):\n",
        "        elementos.append(\"prova_resultado\")\n",
        "\n",
        "    return elementos\n",
        "\n",
        "def analisar_elementos_fechamento(segmentos):\n",
        "    \"\"\"Analisa elementos do fechamento\"\"\"\n",
        "    texto = \" \".join([s[\"texto\"] for s in segmentos])\n",
        "    elementos = []\n",
        "\n",
        "    if re.search(r'\\b(?:clica|clique|acesse|faÃ§a)', texto, re.IGNORECASE):\n",
        "        elementos.append(\"call_to_action\")\n",
        "    if re.search(r'\\b(?:link|bio|WhatsApp)', texto, re.IGNORECASE):\n",
        "        elementos.append(\"direcionamento\")\n",
        "    if re.search(r'\\b(?:comenta|compartilha|segue)', texto, re.IGNORECASE):\n",
        "        elementos.append(\"engajamento\")\n",
        "\n",
        "    return elementos\n",
        "\n",
        "def calcular_score_persuasao(ganchos, gatilhos, ctas):\n",
        "    \"\"\"Calcula score de persuasÃ£o baseado nos elementos encontrados\"\"\"\n",
        "    score = 0\n",
        "\n",
        "    # PontuaÃ§Ã£o por variedade de ganchos\n",
        "    score += len(ganchos) * 10\n",
        "\n",
        "    # PontuaÃ§Ã£o por variedade de gatilhos\n",
        "    score += len(gatilhos) * 15\n",
        "\n",
        "    # PontuaÃ§Ã£o por presenÃ§a de CTAs\n",
        "    score += len(ctas) * 20\n",
        "\n",
        "    # BÃ´nus por combinaÃ§Ãµes poderosas\n",
        "    if \"urgencia\" in ganchos and \"aversao_perda\" in gatilhos:\n",
        "        score += 25\n",
        "\n",
        "    if \"autoridade\" in ganchos and \"autoridade_especialista\" in gatilhos:\n",
        "        score += 20\n",
        "\n",
        "    if \"curiosidade\" in ganchos and any(cta in ctas for cta in [\"acao_imediata\", \"link_bio\"]):\n",
        "        score += 30\n",
        "\n",
        "    return min(score, 100)  # Limitar a 100\n",
        "\n",
        "def gerar_recomendacoes_copywriting(ganchos, gatilhos, ctas):\n",
        "    \"\"\"Gera recomendaÃ§Ãµes estratÃ©gicas baseadas na anÃ¡lise\"\"\"\n",
        "    recomendacoes = []\n",
        "\n",
        "    # RecomendaÃ§Ãµes para ganchos\n",
        "    if len(ganchos) < 2:\n",
        "        recomendacoes.append({\n",
        "            \"categoria\": \"GANCHOS\",\n",
        "            \"prioridade\": \"ALTA\",\n",
        "            \"recomendacao\": \"Adicione mais ganchos na abertura. Use perguntas retÃ³ricas ou desperte curiosidade nos primeiros 3 segundos.\"\n",
        "        })\n",
        "\n",
        "    if \"pergunta_retorica\" not in ganchos:\n",
        "        recomendacoes.append({\n",
        "            \"categoria\": \"GANCHOS\",\n",
        "            \"prioridade\": \"MÃ‰DIA\",\n",
        "            \"recomendacao\": \"Inicie com uma pergunta que faÃ§a o viewer refletir sobre sua situaÃ§Ã£o atual.\"\n",
        "        })\n",
        "\n",
        "    # RecomendaÃ§Ãµes para gatilhos\n",
        "    if len(gatilhos) < 3:\n",
        "        recomendacoes.append({\n",
        "            \"categoria\": \"GATILHOS\",\n",
        "            \"prioridade\": \"ALTA\",\n",
        "            \"recomendacao\": \"Incorpore mais gatilhos mentais. Combine autoridade + prova social para maior credibilidade.\"\n",
        "        })\n",
        "\n",
        "    if \"reciprocidade\" not in gatilhos:\n",
        "        recomendacoes.append({\n",
        "            \"categoria\": \"GATILHOS\",\n",
        "            \"prioridade\": \"MÃ‰DIA\",\n",
        "            \"recomendacao\": \"OfereÃ§a valor gratuito para ativar o gatilho da reciprocidade.\"\n",
        "        })\n",
        "\n",
        "    # RecomendaÃ§Ãµes para CTAs\n",
        "    if len(ctas) == 0:\n",
        "        recomendacoes.append({\n",
        "            \"categoria\": \"CTA\",\n",
        "            \"prioridade\": \"CRÃTICA\",\n",
        "            \"recomendacao\": \"URGENTE: Adicione pelo menos um Call-to-Action claro. Sem CTA, nÃ£o hÃ¡ conversÃ£o.\"\n",
        "        })\n",
        "\n",
        "    if \"acao_imediata\" not in ctas and \"link_bio\" not in ctas:\n",
        "        recomendacoes.append({\n",
        "            \"categoria\": \"CTA\",\n",
        "            \"prioridade\": \"ALTA\",\n",
        "            \"recomendacao\": \"Termine com um CTA direto: 'Clica no link da bio' ou 'Chama no WhatsApp'.\"\n",
        "        })\n",
        "\n",
        "    return recomendacoes\n",
        "\n",
        "def identificar_templates_replicaveis(ganchos, gatilhos, ctas):\n",
        "    \"\"\"Identifica templates e estruturas replicÃ¡veis\"\"\"\n",
        "    templates = []\n",
        "\n",
        "    # Template: Pergunta + Autoridade + CTA\n",
        "    if \"pergunta_retorica\" in ganchos and \"autoridade\" in ganchos and len(ctas) > 0:\n",
        "        templates.append({\n",
        "            \"nome\": \"PERGUNTA_AUTORIDADE_CTA\",\n",
        "            \"estrutura\": \"Pergunta RetÃ³rica â†’ Estabelecer Autoridade â†’ Call-to-Action\",\n",
        "            \"eficacia\": \"ALTA\",\n",
        "            \"uso_recomendado\": \"VÃ­deos educativos e de expertise\"\n",
        "        })\n",
        "\n",
        "    # Template: Problema + SoluÃ§Ã£o + Prova Social\n",
        "    if \"problema_dor\" in ganchos and \"solucao_resultado\" in ganchos and \"aprovacao_social\" in gatilhos:\n",
        "        templates.append({\n",
        "            \"nome\": \"PROBLEMA_SOLUCAO_PROVA\",\n",
        "            \"estrutura\": \"Identificar Problema â†’ Apresentar SoluÃ§Ã£o â†’ Mostrar Prova Social\",\n",
        "            \"eficacia\": \"MUITO ALTA\",\n",
        "            \"uso_recomendado\": \"VÃ­deos de vendas e transformaÃ§Ã£o\"\n",
        "        })\n",
        "\n",
        "    # Template: Curiosidade + UrgÃªncia + CTA\n",
        "    if \"curiosidade\" in ganchos and \"urgencia\" in ganchos and \"acao_imediata\" in ctas:\n",
        "        templates.append({\n",
        "            \"nome\": \"CURIOSIDADE_URGENCIA_ACAO\",\n",
        "            \"estrutura\": \"Despertar Curiosidade â†’ Criar UrgÃªncia â†’ AÃ§Ã£o Imediata\",\n",
        "            \"eficacia\": \"ALTA\",\n",
        "            \"uso_recomendado\": \"VÃ­deos de lanÃ§amento e ofertas limitadas\"\n",
        "        })\n",
        "\n",
        "    return templates\n",
        "\n",
        "def mapear_timeline_elementos(elementos_detectados, legendas_data):\n",
        "    \"\"\"Mapeia os elementos detectados na timeline do vÃ­deo\"\"\"\n",
        "    timeline = []\n",
        "\n",
        "    for tipo, dados in elementos_detectados.items():\n",
        "        for timestamp in dados.get(\"timestamps\", []):\n",
        "            timeline.append({\n",
        "                \"tipo\": tipo,\n",
        "                \"minuto\": timestamp[\"minuto\"],\n",
        "                \"segundo\": timestamp[\"segundo\"],\n",
        "                \"contexto\": timestamp[\"texto_contexto\"]\n",
        "            })\n",
        "\n",
        "    # Ordenar por tempo\n",
        "    timeline.sort(key=lambda x: (x[\"minuto\"], x[\"segundo\"]))\n",
        "\n",
        "    return timeline\n",
        "\n",
        "# Executar o processamento\n",
        "try:\n",
        "    processar_copywriting_todos_videos_adaptado()\n",
        "except Exception as e:\n",
        "    print(f\"âŒ ERRO de ExecuÃ§Ã£o: {type(e).__name__}: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfcEs533582H",
        "outputId": "90e6c65b-5e49-4d98-bfc6-199a6cff33dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando a Layer 2.4: GeraÃ§Ã£o de Legendas e AnÃ¡lise de Copywriting...\n",
            "ðŸ”„ Iniciando processamento de copywriting adaptado...\n",
            "âœ… Dados de decomposiÃ§Ã£o carregados: 3 vÃ­deos encontrados\n",
            "ðŸ“Š Processando 3 vÃ­deos com transcriÃ§Ã£o vÃ¡lida...\n",
            "[1/3] Processando copywriting para: vid_bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t\n",
            "  ðŸ”„ Gerando legendas com timestamps...\n",
            "    âœ… Legendas SRT geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t_legendas.srt\n",
            "    âœ… Legendas TXT com timestamps geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_bicho_naÌƒo_se_guarda_no_cofre_lixo_Se_Joga_na_lixei_vid_video_001_DLR3WYlAK5t_legendas_timestamped.txt\n",
            "    ðŸ”„ Analisando copywriting...\n",
            "  âœ… Copywriting analisado: Score 0/100\n",
            "[2/3] Processando copywriting para: vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf\n",
            "  ðŸ”„ Gerando legendas com timestamps...\n",
            "    âœ… Legendas SRT geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf_legendas.srt\n",
            "    âœ… Legendas TXT com timestamps geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_empreendedor_que_acorda_todo_dia_para_trabalhar_e__vid_video_002_DE99jlSobyf_legendas_timestamped.txt\n",
            "    ðŸ”„ Analisando copywriting...\n",
            "  âœ… Copywriting analisado: Score 30/100\n",
            "[3/3] Processando copywriting para: vid_maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy\n",
            "  ðŸ”„ Gerando legendas com timestamps...\n",
            "    âœ… Legendas SRT geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy_legendas.srt\n",
            "    âœ… Legendas TXT com timestamps geradas: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/legendas/vid_maÌquina_emocional_naÌƒo_daÌ_conta_de_converter_entaÌƒo__vid_video_003_DMxQIz5MZsy_legendas_timestamped.txt\n",
            "    ðŸ”„ Analisando copywriting...\n",
            "  âœ… Copywriting analisado: Score 75/100\n",
            "ðŸ’¾ AnÃ¡lises de copywriting salvas em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/analises_copywriting_completas.json\n",
            "ðŸ’¾ Dados de legendas salvos em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/legendas_geradas.json\n",
            "\n",
            "âœ… ANÃLISE DE COPYWRITING CONCLUÃDA!\n",
            "Total de vÃ­deos com copywriting analisado: 3\n",
            "Total de legendas geradas: 3\n",
            "\n",
            "âž¡ï¸ PRÃ“XIMA CÃ‰LULA: 4.3 - INTEGRAÃ‡ÃƒO COM DASHBOARD\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CÃ‰LULA 2.4: GERAÃ‡ÃƒO DE LEGENDAS E ANÃLISE DE COPYWRITING\n",
        "# ============================================================================\n",
        "\n",
        "# Definir a variÃ¡vel global PASTA_TRABALHO se ainda nÃ£o estiver definida\n",
        "# Certifique-se de que esta variÃ¡vel esteja definida corretamente em uma cÃ©lula anterior (ex: CÃ©lula 1.2)\n",
        "# Exemplo: PASTA_TRABALHO = \"/content/drive/MyDrive/Videos Dona Done/_engenharia_reversa\"\n",
        "\n",
        "# Executar a funÃ§Ã£o principal da Layer 2.4\n",
        "if 'PASTA_TRABALHO' in globals():\n",
        "    print(\"Iniciando a Layer 2.4: GeraÃ§Ã£o de Legendas e AnÃ¡lise de Copywriting...\")\n",
        "    processar_copywriting_todos_videos_adaptado()\n",
        "else:\n",
        "    print(\"ERRO: A variÃ¡vel PASTA_TRABALHO nÃ£o estÃ¡ definida. Certifique-se de executar a CÃ©lula 1.2 ou equivalente.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "NUeniUqRJLuo",
        "outputId": "5bb0d509-95f2-43cf-b752-5d9982043bf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:54:41] INICIANDO SISTEMA DE DASHBOARD INTELIGENTE\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'create_enhanced_dashboard_master_with_viral' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1042834839.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1042834839.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;31m# Executar criaÃ§Ã£o do dashboard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m     \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_enhanced_dashboard_master_with_viral\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'create_enhanced_dashboard_master_with_viral' is not defined"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CÃ‰LULA 4.3: DASHBOARD MASTER EXECUTIVO INTELIGENTE APRIMORADO\n",
        "# ============================================================================\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "from openpyxl.styles import Font, Alignment, PatternFill\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def log_progress(message):\n",
        "    \"\"\"Log de progresso em tempo real\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
        "    print(f\"[{timestamp}] {message}\")\n",
        "\n",
        "def calculate_viral_score(row):\n",
        "    \"\"\"Calcula score de viralidade baseado em mÃºltiplos fatores\"\"\"\n",
        "    try:\n",
        "        score = 0\n",
        "\n",
        "        # Fator 1: Ritmo (cortes por segundo) - peso 25%\n",
        "        if pd.notna(row['duracao_segundos']) and row['duracao_segundos'] > 0:\n",
        "            cortes_por_seg = row['cortes_detectados_count'] / row['duracao_segundos']\n",
        "            if cortes_por_seg > 20: score += 25\n",
        "            elif cortes_por_seg > 10: score += 20\n",
        "            elif cortes_por_seg > 5: score += 15\n",
        "            else: score += 10\n",
        "\n",
        "        # Fator 2: Complexidade Visual - peso 20%\n",
        "        if pd.notna(row['complexidade_visual_media']):\n",
        "            if row['complexidade_visual_media'] > 600: score += 20\n",
        "            elif row['complexidade_visual_media'] > 400: score += 15\n",
        "            else: score += 10\n",
        "\n",
        "        # Fator 3: PresenÃ§a de Texto (OCR) - peso 15%\n",
        "        if pd.notna(row['ocr_textos_count']):\n",
        "            if row['ocr_textos_count'] > 10: score += 15\n",
        "            elif row['ocr_textos_count'] > 5: score += 12\n",
        "            elif row['ocr_textos_count'] > 0: score += 8\n",
        "\n",
        "        # Fator 4: DuraÃ§Ã£o Ideal - peso 20%\n",
        "        if pd.notna(row['duracao_segundos']):\n",
        "            if 15 <= row['duracao_segundos'] <= 30: score += 20\n",
        "            elif 10 <= row['duracao_segundos'] <= 45: score += 15\n",
        "            else: score += 10\n",
        "\n",
        "        # Fator 5: Gatilhos PsicolÃ³gicos - peso 20%\n",
        "        gatilhos = str(row['gatilhos_psicologicos']).lower()\n",
        "        if 'urgÃªncia' in gatilhos or 'escassez' in gatilhos: score += 8\n",
        "        if 'estÃ­mulo' in gatilhos: score += 7\n",
        "        if 'atenÃ§Ã£o' in gatilhos: score += 5\n",
        "\n",
        "        return min(score, 100)\n",
        "    except:\n",
        "        return 50\n",
        "\n",
        "def calculate_technical_score(row):\n",
        "    \"\"\"Score tÃ©cnico baseado em qualidade de produÃ§Ã£o\"\"\"\n",
        "    try:\n",
        "        score = 0\n",
        "\n",
        "        if pd.notna(row['brilho_medio']):\n",
        "            if 120 <= row['brilho_medio'] <= 180: score += 25\n",
        "            elif 100 <= row['brilho_medio'] <= 200: score += 20\n",
        "            else: score += 10\n",
        "\n",
        "        formato = str(row['formato_detectado'])\n",
        "        if 'vertical_9_16' in formato: score += 25\n",
        "        elif 'horizontal_16_9' in formato: score += 20\n",
        "        else: score += 15\n",
        "\n",
        "        if row['tem_audio']: score += 25\n",
        "        else: score += 5\n",
        "\n",
        "        if pd.notna(row['total_frames']) and row['total_frames'] > 0:\n",
        "            if row['total_frames'] > 300: score += 25\n",
        "            elif row['total_frames'] > 150: score += 20\n",
        "            else: score += 15\n",
        "\n",
        "        return min(score, 100)\n",
        "    except:\n",
        "        return 50\n",
        "\n",
        "def calculate_content_score(row):\n",
        "    \"\"\"Score de conteÃºdo baseado em riqueza informacional\"\"\"\n",
        "    try:\n",
        "        score = 0\n",
        "\n",
        "        ocr_count = row['ocr_textos_count'] if pd.notna(row['ocr_textos_count']) else 0\n",
        "        audio_len = row['audio_transcrito_len'] if pd.notna(row['audio_transcrito_len']) else 0\n",
        "\n",
        "        if ocr_count > 5 or audio_len > 100: score += 30\n",
        "        elif ocr_count > 2 or audio_len > 50: score += 20\n",
        "        elif ocr_count > 0 or audio_len > 0: score += 15\n",
        "        else: score += 5\n",
        "\n",
        "        if pd.notna(row['bpm_audio']):\n",
        "            if 120 <= row['bpm_audio'] <= 140: score += 35\n",
        "            elif 100 <= row['bpm_audio'] <= 160: score += 25\n",
        "            else: score += 15\n",
        "\n",
        "        if pd.notna(row['duracao_segundos']) and row['duracao_segundos'] > 0:\n",
        "            densidade = (ocr_count + audio_len/10) / row['duracao_segundos']\n",
        "            if densidade > 2: score += 35\n",
        "            elif densidade > 1: score += 25\n",
        "            else: score += 15\n",
        "\n",
        "        return min(score, 100)\n",
        "    except:\n",
        "        return 50\n",
        "\n",
        "def generate_insights_from_data(df):\n",
        "    \"\"\"Gera insights inteligentes baseados nos dados\"\"\"\n",
        "    insights = []\n",
        "\n",
        "    try:\n",
        "        best_performing = df.nlargest(3, 'viral_score')\n",
        "        avg_duration = best_performing['duracao_segundos'].mean()\n",
        "        insights.append(f\"DURAÃ‡ÃƒO VENCEDORA: Seus top 3 vÃ­deos tÃªm duraÃ§Ã£o mÃ©dia de {avg_duration:.1f}s. Este Ã© seu sweet spot comprovado.\")\n",
        "\n",
        "        avg_cuts_per_sec = (best_performing['cortes_detectados_count'] / best_performing['duracao_segundos']).mean()\n",
        "        insights.append(f\"RITMO IDEAL: {avg_cuts_per_sec:.1f} cortes por segundo Ã© sua fÃ³rmula de ediÃ§Ã£o mais eficaz.\")\n",
        "\n",
        "        formato_winner = df['formato_detectado'].mode()[0] if not df['formato_detectado'].empty else 'N/A'\n",
        "        formato_count = df['formato_detectado'].value_counts().iloc[0] if not df['formato_detectado'].empty else 0\n",
        "        insights.append(f\"FORMATO DOMINANTE: {formato_count} vÃ­deos em {formato_winner}. Este Ã© seu formato de maior alcance.\")\n",
        "\n",
        "        high_viral = df[df['viral_score'] > 70]\n",
        "        if not high_viral.empty:\n",
        "            avg_complexity = high_viral['complexidade_visual_media'].mean()\n",
        "            insights.append(f\"COMPLEXIDADE VISUAL Ã“TIMA: VÃ­deos com score viral alto tÃªm complexidade mÃ©dia de {avg_complexity:.0f}. Use como referÃªncia.\")\n",
        "\n",
        "        text_heavy = df[df['ocr_textos_count'] > 5]\n",
        "        if not text_heavy.empty:\n",
        "            insights.append(f\"ESTRATÃ‰GIA DE TEXTO: {len(text_heavy)} vÃ­deos com muito texto tÃªm score mÃ©dio de {text_heavy['viral_score'].mean():.0f}. Texto na tela impacta performance.\")\n",
        "\n",
        "        # CORRIGIDO: bpm_audio em vez de bmp_audio\n",
        "        if df['bpm_audio'].notna().any():\n",
        "            successful_bpm = df[df['viral_score'] > 60]['bpm_audio'].mean()\n",
        "            insights.append(f\"BPM DE SUCESSO: {successful_bpm:.0f} BPM Ã© o ritmo de Ã¡udio dos seus vÃ­deos mais virais.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_progress(f\"Erro ao gerar insights: {e}\")\n",
        "        insights.append(\"Insights parciais disponÃ­veis devido a limitaÃ§Ãµes nos dados.\")\n",
        "\n",
        "    return insights\n",
        "\n",
        "def add_data_to_sheet(ws, data, start_row=1, start_col=1, headers=None):\n",
        "    \"\"\"Adiciona dados a uma planilha de forma segura\"\"\"\n",
        "    current_row = start_row\n",
        "\n",
        "    # Adicionar cabeÃ§alhos se fornecidos\n",
        "    if headers:\n",
        "        for col_idx, header in enumerate(headers):\n",
        "            cell = ws.cell(row=current_row, column=start_col + col_idx)\n",
        "            cell.value = header\n",
        "            cell.font = Font(bold=True)\n",
        "        current_row += 1\n",
        "\n",
        "    # Adicionar dados\n",
        "    for row_data in data:\n",
        "        for col_idx, value in enumerate(row_data):\n",
        "            cell = ws.cell(row=current_row, column=start_col + col_idx)\n",
        "            cell.value = value\n",
        "        current_row += 1\n",
        "\n",
        "    return current_row\n",
        "\n",
        "def create_enhanced_dashboard_master(csv_path, json_path, output_path):\n",
        "    \"\"\"Cria dashboard master executivo aprimorado\"\"\"\n",
        "\n",
        "    log_progress(\"INICIANDO CRIAÃ‡ÃƒO DO DASHBOARD MASTER EXECUTIVO INTELIGENTE\")\n",
        "\n",
        "    try:\n",
        "        # Carregar dados\n",
        "        log_progress(\"Carregando dados consolidados...\")\n",
        "        df_consolidado = pd.read_csv(csv_path, encoding='utf-8')\n",
        "\n",
        "        with open(json_path, 'r', encoding='utf-8') as f:\n",
        "            dados_detalhados = json.load(f)\n",
        "\n",
        "        log_progress(f\"Dados carregados: {len(df_consolidado)} vÃ­deos encontrados\")\n",
        "\n",
        "        # PrÃ©-processamento inteligente\n",
        "        log_progress(\"Processando inteligÃªncia artificial dos dados...\")\n",
        "\n",
        "        # Limpar e converter dados\n",
        "        try:\n",
        "            df_consolidado['emocoes_predominantes'] = df_consolidado['emocoes_predominantes'].apply(\n",
        "                lambda x: json.loads(x.replace(\"'\", '\"')) if pd.notna(x) and x != '{}' else {}\n",
        "            )\n",
        "        except:\n",
        "            df_consolidado['emocoes_predominantes'] = [{}] * len(df_consolidado)\n",
        "\n",
        "        # Calcular scores inteligentes\n",
        "        log_progress(\"Calculando scores de performance...\")\n",
        "        df_consolidado['viral_score'] = df_consolidado.apply(calculate_viral_score, axis=1)\n",
        "        df_consolidado['technical_score'] = df_consolidado.apply(calculate_technical_score, axis=1)\n",
        "        df_consolidado['content_score'] = df_consolidado.apply(calculate_content_score, axis=1)\n",
        "        df_consolidado['overall_score'] = (df_consolidado['viral_score'] + df_consolidado['technical_score'] + df_consolidado['content_score']) / 3\n",
        "\n",
        "        # Calcular mÃ©tricas avanÃ§adas\n",
        "        df_consolidado['cortes_por_segundo'] = df_consolidado['cortes_detectados_count'] / df_consolidado['duracao_segundos'].replace(0, 1)\n",
        "        df_consolidado['densidade_texto'] = df_consolidado['ocr_textos_count'] / df_consolidado['duracao_segundos'].replace(0, 1)\n",
        "        df_consolidado['eficiencia_audio'] = df_consolidado['audio_transcrito_len'] / df_consolidado['duracao_segundos'].replace(0, 1)\n",
        "\n",
        "        log_progress(\"Gerando insights estratÃ©gicos...\")\n",
        "        insights = generate_insights_from_data(df_consolidado)\n",
        "\n",
        "        # Criar workbook\n",
        "        log_progress(\"Criando estrutura do dashboard...\")\n",
        "        wb = Workbook()\n",
        "\n",
        "        # === ABA 1: EXECUTIVE SUMMARY ===\n",
        "        log_progress(\"Criando Executive Summary...\")\n",
        "        ws_summary = wb.active\n",
        "        ws_summary.title = 'Executive Summary'\n",
        "\n",
        "        # Header principal\n",
        "        header_cell = ws_summary.cell(row=1, column=1)\n",
        "        header_cell.value = 'DASHBOARD MASTER EXECUTIVO - ENGENHARIA REVERSA DE VÃDEOS'\n",
        "        header_cell.font = Font(bold=True, size=18, color='FFFFFF')\n",
        "        header_cell.fill = PatternFill(start_color='1F4E79', end_color='1F4E79', fill_type='solid')\n",
        "        header_cell.alignment = Alignment(horizontal='center', vertical='center')\n",
        "\n",
        "        # Expandir header manualmente\n",
        "        for col in range(2, 9):\n",
        "            cell = ws_summary.cell(row=1, column=col)\n",
        "            cell.fill = PatternFill(start_color='1F4E79', end_color='1F4E79', fill_type='solid')\n",
        "\n",
        "        # KPIs Principais\n",
        "        kpi_cell = ws_summary.cell(row=3, column=1)\n",
        "        kpi_cell.value = 'INDICADORES DE PERFORMANCE PRINCIPAIS'\n",
        "        kpi_cell.font = Font(bold=True, size=14)\n",
        "        kpi_cell.fill = PatternFill(start_color='E7E6E6', end_color='E7E6E6', fill_type='solid')\n",
        "\n",
        "        kpis_data = [\n",
        "            ['Total de VÃ­deos Analisados', len(df_consolidado)],\n",
        "            ['Score Viral MÃ©dio', f\"{df_consolidado['viral_score'].mean():.1f}/100\"],\n",
        "            ['Score TÃ©cnico MÃ©dio', f\"{df_consolidado['technical_score'].mean():.1f}/100\"],\n",
        "            ['Score de ConteÃºdo MÃ©dio', f\"{df_consolidado['content_score'].mean():.1f}/100\"],\n",
        "            ['DuraÃ§Ã£o MÃ©dia Otimizada', f\"{df_consolidado['duracao_segundos'].mean():.1f}s\"],\n",
        "            ['Ritmo MÃ©dio de Cortes', f\"{df_consolidado['cortes_por_segundo'].mean():.1f}/seg\"],\n",
        "        ]\n",
        "\n",
        "        add_data_to_sheet(ws_summary, kpis_data, start_row=4, start_col=1)\n",
        "\n",
        "        # Top 3 VÃ­deos\n",
        "        top3_cell = ws_summary.cell(row=3, column=4)\n",
        "        top3_cell.value = 'TOP 3 VÃDEOS POR PERFORMANCE'\n",
        "        top3_cell.font = Font(bold=True, size=14)\n",
        "        top3_cell.fill = PatternFill(start_color='E7E6E6', end_color='E7E6E6', fill_type='solid')\n",
        "\n",
        "        top3 = df_consolidado.nlargest(3, 'overall_score')[['nome_arquivo', 'overall_score', 'viral_score', 'technical_score', 'content_score']]\n",
        "\n",
        "        top3_data = []\n",
        "        for _, video in top3.iterrows():\n",
        "            nome_curto = video['nome_arquivo'][:30] + \"...\" if len(video['nome_arquivo']) > 30 else video['nome_arquivo']\n",
        "            top3_data.append([\n",
        "                nome_curto,\n",
        "                f\"{video['overall_score']:.1f}\",\n",
        "                f\"{video['viral_score']:.1f}\",\n",
        "                f\"{video['technical_score']:.1f}\",\n",
        "                f\"{video['content_score']:.1f}\"\n",
        "            ])\n",
        "\n",
        "        top3_headers = ['VÃ­deo', 'Score Geral', 'Viral', 'TÃ©cnico', 'ConteÃºdo']\n",
        "        add_data_to_sheet(ws_summary, top3_data, start_row=4, start_col=4, headers=top3_headers)\n",
        "\n",
        "        # Insights EstratÃ©gicos\n",
        "        insights_cell = ws_summary.cell(row=12, column=1)\n",
        "        insights_cell.value = 'INSIGHTS ESTRATÃ‰GICOS BASEADOS EM IA'\n",
        "        insights_cell.font = Font(bold=True, size=14, color='FFFFFF')\n",
        "        insights_cell.fill = PatternFill(start_color='C5504B', end_color='C5504B', fill_type='solid')\n",
        "        insights_cell.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Adicionar insights\n",
        "        for i, insight in enumerate(insights, 13):\n",
        "            insight_cell = ws_summary.cell(row=i, column=1)\n",
        "            insight_cell.value = f\"â€¢ {insight}\"\n",
        "            insight_cell.alignment = Alignment(wrap_text=True)\n",
        "\n",
        "        # === ABA 2: ANÃLISE DE PERFORMANCE ===\n",
        "        log_progress(\"Criando AnÃ¡lise de Performance...\")\n",
        "        ws_performance = wb.create_sheet('AnÃ¡lise de Performance')\n",
        "\n",
        "        perf_header = ws_performance.cell(row=1, column=1)\n",
        "        perf_header.value = 'ANÃLISE DETALHADA DE PERFORMANCE'\n",
        "        perf_header.font = Font(bold=True, size=16)\n",
        "        perf_header.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Ranking completo\n",
        "        ranking_data = df_consolidado[['nome_arquivo', 'overall_score', 'viral_score', 'technical_score', 'content_score',\n",
        "                                     'duracao_segundos', 'cortes_por_segundo', 'formato_detectado']].sort_values('overall_score', ascending=False)\n",
        "\n",
        "        ranking_list = []\n",
        "        for _, video in ranking_data.iterrows():\n",
        "            nome_curto = video['nome_arquivo'][:40] + \"...\" if len(video['nome_arquivo']) > 40 else video['nome_arquivo']\n",
        "            ranking_list.append([\n",
        "                nome_curto,\n",
        "                f\"{video['overall_score']:.1f}\",\n",
        "                f\"{video['viral_score']:.1f}\",\n",
        "                f\"{video['technical_score']:.1f}\",\n",
        "                f\"{video['content_score']:.1f}\",\n",
        "                f\"{video['duracao_segundos']:.1f}s\",\n",
        "                f\"{video['cortes_por_segundo']:.1f}\",\n",
        "                video['formato_detectado']\n",
        "            ])\n",
        "\n",
        "        ranking_headers = ['VÃ­deo', 'Score Geral', 'Viral', 'TÃ©cnico', 'ConteÃºdo', 'DuraÃ§Ã£o', 'Cortes/s', 'Formato']\n",
        "        add_data_to_sheet(ws_performance, ranking_list, start_row=3, start_col=1, headers=ranking_headers)\n",
        "\n",
        "        # === ABA 3: INTELIGÃŠNCIA TÃ‰CNICA ===\n",
        "        log_progress(\"Criando InteligÃªncia TÃ©cnica...\")\n",
        "        ws_tecnica = wb.create_sheet('InteligÃªncia TÃ©cnica')\n",
        "\n",
        "        tec_header = ws_tecnica.cell(row=1, column=1)\n",
        "        tec_header.value = 'ANÃLISE TÃ‰CNICA AVANÃ‡ADA'\n",
        "        tec_header.font = Font(bold=True, size=16)\n",
        "        tec_header.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # AnÃ¡lise de correlaÃ§Ãµes\n",
        "        corr_header = ws_tecnica.cell(row=3, column=1)\n",
        "        corr_header.value = 'CORRELAÃ‡Ã•ES DESCOBERTAS'\n",
        "        corr_header.font = Font(bold=True, size=12)\n",
        "\n",
        "        correlations_data = [\n",
        "            ['DuraÃ§Ã£o vs Score Viral', f\"{df_consolidado['duracao_segundos'].corr(df_consolidado['viral_score']):.3f}\", 'CORRELAÃ‡ÃƒO MODERADA'],\n",
        "            ['Cortes/s vs Score Viral', f\"{df_consolidado['cortes_por_segundo'].corr(df_consolidado['viral_score']):.3f}\", 'CORRELAÃ‡ÃƒO MODERADA'],\n",
        "            ['Complexidade Visual vs Performance', f\"{df_consolidado['complexidade_visual_media'].corr(df_consolidado['overall_score']):.3f}\", 'CORRELAÃ‡ÃƒO FRACA'],\n",
        "            ['BPM vs Engajamento', f\"{df_consolidado['bpm_audio'].corr(df_consolidado['viral_score']) if df_consolidado['bpm_audio'].notna().any() else 0:.3f}\", 'CORRELAÃ‡ÃƒO FRACA'],\n",
        "        ]\n",
        "\n",
        "        corr_headers = ['MÃ©trica', 'CorrelaÃ§Ã£o', 'ClassificaÃ§Ã£o']\n",
        "        add_data_to_sheet(ws_tecnica, correlations_data, start_row=4, start_col=1, headers=corr_headers)\n",
        "\n",
        "        # === ABA 4: BLUEPRINT DE PRODUÃ‡ÃƒO ===\n",
        "        log_progress(\"Criando Blueprint de ProduÃ§Ã£o...\")\n",
        "        ws_blueprint = wb.create_sheet('Blueprint de ProduÃ§Ã£o')\n",
        "\n",
        "        bp_header = ws_blueprint.cell(row=1, column=1)\n",
        "        bp_header.value = 'BLUEPRINT ESTRATÃ‰GICO DE PRODUÃ‡ÃƒO'\n",
        "        bp_header.font = Font(bold=True, size=16, color='FFFFFF')\n",
        "        bp_header.fill = PatternFill(start_color='1F4E79', end_color='1F4E79', fill_type='solid')\n",
        "        bp_header.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Receita de sucesso baseada nos top performers\n",
        "        top_performers = df_consolidado[df_consolidado['overall_score'] > df_consolidado['overall_score'].quantile(0.7)]\n",
        "\n",
        "        blueprint_data = [\n",
        "            ['DURAÃ‡ÃƒO IDEAL', f\"{top_performers['duracao_segundos'].mean():.1f} segundos (Â±{top_performers['duracao_segundos'].std():.1f}s)\"],\n",
        "            ['RITMO DE EDIÃ‡ÃƒO', f\"{top_performers['cortes_por_segundo'].mean():.1f} cortes por segundo\"],\n",
        "            ['FORMATO VENCEDOR', top_performers['formato_detectado'].mode()[0] if not top_performers.empty else 'N/A'],\n",
        "            ['COMPLEXIDADE VISUAL', f\"NÃ­vel {top_performers['complexidade_visual_media'].mean():.0f} (escala de estÃ­mulo)\"],\n",
        "            ['BPM RECOMENDADO', f\"{top_performers['bpm_audio'].mean():.0f} BPM\" if top_performers['bpm_audio'].notna().any() else 'N/A'],\n",
        "            ['DENSIDADE DE TEXTO', f\"{top_performers['densidade_texto'].mean():.1f} textos por segundo\"],\n",
        "        ]\n",
        "\n",
        "        bp_sub_header = ws_blueprint.cell(row=3, column=1)\n",
        "        bp_sub_header.value = 'FÃ“RMULA DE SUCESSO BASEADA EM DADOS'\n",
        "        bp_sub_header.font = Font(bold=True, size=12)\n",
        "\n",
        "        add_data_to_sheet(ws_blueprint, blueprint_data, start_row=4, start_col=1)\n",
        "\n",
        "        # === ABA 5: RECOMENDAÃ‡Ã•ES ESTRATÃ‰GICAS ===\n",
        "        log_progress(\"Criando RecomendaÃ§Ãµes EstratÃ©gicas...\")\n",
        "        ws_recomendacoes = wb.create_sheet('RecomendaÃ§Ãµes EstratÃ©gicas')\n",
        "\n",
        "        rec_header = ws_recomendacoes.cell(row=1, column=1)\n",
        "        rec_header.value = 'RECOMENDAÃ‡Ã•ES ESTRATÃ‰GICAS BASEADAS EM IA'\n",
        "        rec_header.font = Font(bold=True, size=16, color='FFFFFF')\n",
        "        rec_header.fill = PatternFill(start_color='C5504B', end_color='C5504B', fill_type='solid')\n",
        "        rec_header.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # RecomendaÃ§Ãµes inteligentes baseadas nos dados\n",
        "        recommendations = []\n",
        "\n",
        "        # AnÃ¡lise de duraÃ§Ã£o\n",
        "        if df_consolidado['duracao_segundos'].mean() > 60:\n",
        "            recommendations.append(['DURAÃ‡ÃƒO', 'REDUZA DURAÃ‡ÃƒO', 'Seus vÃ­deos estÃ£o longos demais. VÃ­deos de 15-30s tÃªm melhor performance.', 'ALTA'])\n",
        "        elif df_consolidado['duracao_segundos'].mean() < 15:\n",
        "            recommendations.append(['DURAÃ‡ÃƒO', 'AUMENTE DURAÃ‡ÃƒO', 'VÃ­deos muito curtos podem nÃ£o transmitir valor suficiente.', 'MÃ‰DIA'])\n",
        "\n",
        "        # AnÃ¡lise de ritmo\n",
        "        avg_cuts_per_sec = df_consolidado['cortes_por_segundo'].mean()\n",
        "        if avg_cuts_per_sec < 5:\n",
        "            recommendations.append(['EDIÃ‡ÃƒO', 'ACELERE O RITMO', 'Aumente o nÃºmero de cortes para manter atenÃ§Ã£o. Meta: 8-12 cortes/segundo.', 'ALTA'])\n",
        "        elif avg_cuts_per_sec > 20:\n",
        "            recommendations.append(['EDIÃ‡ÃƒO', 'DIMINUA CORTES', 'Muitos cortes podem causar fadiga visual. Encontre o equilÃ­brio.', 'MÃ‰DIA'])\n",
        "\n",
        "        # AnÃ¡lise de formato\n",
        "        formato_dominante = df_consolidado['formato_detectado'].mode()[0] if not df_consolidado['formato_detectado'].empty else 'N/A'\n",
        "        if 'horizontal' in formato_dominante.lower():\n",
        "            recommendations.append(['FORMATO', 'FOQUE EM VERTICAL', 'Formato vertical (9:16) tem melhor performance em redes sociais.', 'ALTA'])\n",
        "\n",
        "        # AnÃ¡lise de texto\n",
        "        if df_consolidado['densidade_texto'].mean() < 1:\n",
        "            recommendations.append(['CONTEÃšDO', 'ADICIONE MAIS TEXTO', 'Textos na tela aumentam retenÃ§Ã£o e acessibilidade.', 'MÃ‰DIA'])\n",
        "\n",
        "        rec_headers = ['Categoria', 'AÃ§Ã£o', 'Justificativa', 'Prioridade']\n",
        "        add_data_to_sheet(ws_recomendacoes, recommendations, start_row=3, start_col=1, headers=rec_headers)\n",
        "\n",
        "        # Salvar arquivo\n",
        "        log_progress(\"Salvando dashboard...\")\n",
        "        wb.save(output_path)\n",
        "\n",
        "        log_progress(\"DASHBOARD MASTER EXECUTIVO CRIADO COM SUCESSO!\")\n",
        "        log_progress(f\"Arquivo salvo em: {output_path}\")\n",
        "        log_progress(f\"{len(df_consolidado)} vÃ­deos analisados\")\n",
        "        log_progress(f\"{len(insights)} insights estratÃ©gicos gerados\")\n",
        "        log_progress(f\"{len(recommendations)} recomendaÃ§Ãµes criadas\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        log_progress(f\"ERRO CRÃTICO: {e}\")\n",
        "        log_progress(\"Verifique os arquivos de entrada e tente novamente\")\n",
        "        return False\n",
        "\n",
        "def main():\n",
        "    \"\"\"FunÃ§Ã£o principal de execuÃ§Ã£o\"\"\"\n",
        "    log_progress(\"INICIANDO SISTEMA DE DASHBOARD INTELIGENTE\")\n",
        "\n",
        "    # Configurar caminhos\n",
        "    BASE_PATH = \"/content/drive/MyDrive/Videos Dona Done/_engenharia_reversa\"\n",
        "    CSV_PATH = os.path.join(BASE_PATH, \"dashboard\", \"dados_consolidados.csv\")\n",
        "    JSON_PATH = os.path.join(BASE_PATH, \"dashboard\", \"dados_detalhados.json\")\n",
        "    OUTPUT_PATH = os.path.join(BASE_PATH, \"dashboard\", \"DASHBOARD_MASTER_EXECUTIVO_INTELIGENTE.xlsx\")\n",
        "\n",
        "    # Verificar se arquivos existem\n",
        "    if not os.path.exists(CSV_PATH):\n",
        "        log_progress(f\"ERRO: Arquivo CSV nÃ£o encontrado: {CSV_PATH}\")\n",
        "        return False\n",
        "\n",
        "    if not os.path.exists(JSON_PATH):\n",
        "        log_progress(f\"ERRO: Arquivo JSON nÃ£o encontrado: {JSON_PATH}\")\n",
        "        return False\n",
        "\n",
        "    # Executar criaÃ§Ã£o do dashboard\n",
        "    success = create_enhanced_dashboard_master_with_viral(csv_path, json_path, output_path)\n",
        "\n",
        "    if success:\n",
        "        log_progress(\"PROCESSO CONCLUÃDO COM SUCESSO!\")\n",
        "        log_progress(\"Dashboard inteligente pronto para uso estratÃ©gico\")\n",
        "    else:\n",
        "        log_progress(\"PROCESSO FALHOU - Verifique os logs acima\")\n",
        "\n",
        "    return success\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ixFPocx6m8Q"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# LAYER 4.3: INTEGRAÃ‡ÃƒO DE COPYWRITING NO DASHBOARD EXISTENTE\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from openpyxl import load_workbook\n",
        "from openpyxl.styles import Font, PatternFill, Alignment\n",
        "\n",
        "def integrar_copywriting_dashboard_existente():\n",
        "    \"\"\"Integra anÃ¡lise de copywriting no dashboard master existente\"\"\"\n",
        "    print(\"ðŸ”„ Iniciando integraÃ§Ã£o de copywriting no dashboard existente...\")\n",
        "\n",
        "    # Verificar prÃ©-requisitos\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa('copywriting_analysis')\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Localizar dashboard existente\n",
        "    pasta_dashboard = os.path.join(PASTA_TRABALHO, \"dashboard\")\n",
        "    dashboard_existente = None\n",
        "\n",
        "    # Procurar arquivo de dashboard existente\n",
        "    if os.path.exists(pasta_dashboard):\n",
        "        arquivos = os.listdir(pasta_dashboard)\n",
        "        for arquivo in arquivos:\n",
        "            if \"DASHBOARD_MASTER_EXECUTIVO_INTELIGENTE\" in arquivo and arquivo.endswith(\".xlsx\"):\n",
        "                dashboard_existente = os.path.join(pasta_dashboard, arquivo)\n",
        "                break\n",
        "\n",
        "    if not dashboard_existente:\n",
        "        print(\"âŒ Dashboard master existente nÃ£o encontrado!\")\n",
        "        print(\"Execute primeiro a cÃ©lula 4.2 (Blueprint Final) para criar o dashboard base.\")\n",
        "        return\n",
        "\n",
        "    print(f\"  ðŸ“Š Dashboard encontrado: {os.path.basename(dashboard_existente)}\")\n",
        "\n",
        "    # Carregar dados de copywriting\n",
        "    dados_copywriting = carregar_dados_copywriting()\n",
        "    if not dados_copywriting:\n",
        "        return\n",
        "\n",
        "    # Abrir workbook existente\n",
        "    try:\n",
        "        wb = load_workbook(dashboard_existente)\n",
        "        print(f\"  âœ… Dashboard carregado com {len(wb.sheetnames)} abas existentes\")\n",
        "\n",
        "        # Adicionar novas abas de copywriting\n",
        "        adicionar_aba_copywriting_estrategico(wb, dados_copywriting)\n",
        "        adicionar_aba_templates_copy(wb, dados_copywriting)\n",
        "        adicionar_aba_timeline_copy(wb, dados_copywriting)\n",
        "        adicionar_aba_recomendacoes_copy(wb, dados_copywriting)\n",
        "\n",
        "        # Atualizar aba principal com mÃ©tricas de copywriting\n",
        "        atualizar_aba_principal_com_copy(wb, dados_copywriting)\n",
        "\n",
        "        # Salvar dashboard atualizado\n",
        "        wb.save(dashboard_existente)\n",
        "\n",
        "        print(f\"âœ… Dashboard atualizado com anÃ¡lise de copywriting!\")\n",
        "        print(f\"ðŸ“Š Arquivo: {dashboard_existente}\")\n",
        "        print(f\"ðŸ“‹ Novas abas adicionadas:\")\n",
        "        print(\"  â€¢ Copywriting EstratÃ©gico\")\n",
        "        print(\"  â€¢ Templates ReplicÃ¡veis\")\n",
        "        print(\"  â€¢ Timeline PersuasÃ£o\")\n",
        "        print(\"  â€¢ RecomendaÃ§Ãµes Copy\")\n",
        "        print(\"  â€¢ Dashboard Principal (atualizada)\")\n",
        "\n",
        "        # Gerar relatÃ³rios complementares\n",
        "        gerar_relatorios_copywriting_individuais(dados_copywriting)\n",
        "\n",
        "        # Atualizar config\n",
        "        config[\"status_etapas\"][\"dashboard_copywriting_integrado\"] = True\n",
        "        config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "        with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        return dashboard_existente\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Erro ao atualizar dashboard: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def verificar_prerequisito_etapa(etapa):\n",
        "    \"\"\"Verifica se uma etapa foi executada\"\"\"\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "\n",
        "    if not os.path.exists(config_path):\n",
        "        print(f\"âŒ Arquivo de configuraÃ§Ã£o nÃ£o encontrado: {config_path}\")\n",
        "        return False, None\n",
        "\n",
        "    try:\n",
        "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            config = json.load(f)\n",
        "\n",
        "        if not config.get(\"status_etapas\", {}).get(etapa, False):\n",
        "            print(f\"âŒ PrÃ©-requisito nÃ£o atendido: {etapa}\")\n",
        "            print(\"Execute primeiro a cÃ©lula correspondente.\")\n",
        "            return False, None\n",
        "\n",
        "        return True, config\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Erro ao verificar prÃ©-requisitos: {e}\")\n",
        "        return False, None\n",
        "\n",
        "def carregar_dados_copywriting():\n",
        "    \"\"\"Carrega dados de copywriting e outros dados necessÃ¡rios\"\"\"\n",
        "    print(\"  ðŸ“Š Carregando dados de copywriting...\")\n",
        "\n",
        "    try:\n",
        "        # Dados de copywriting\n",
        "        copywriting_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_copywriting_completas.json\")\n",
        "        with open(copywriting_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            copywriting_data = json.load(f)\n",
        "\n",
        "        # Dados de legendas\n",
        "        legendas_path = os.path.join(PASTA_TRABALHO, \"dados\", \"legendas_geradas.json\")\n",
        "        with open(legendas_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            legendas_data = json.load(f)\n",
        "\n",
        "        # Tentar carregar outros dados (podem nÃ£o existir ainda)\n",
        "        outros_dados = {}\n",
        "\n",
        "        try:\n",
        "            padroes_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_padroes_completas.json\")\n",
        "            with open(padroes_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                outros_dados[\"padroes\"] = json.load(f)\n",
        "        except:\n",
        "            outros_dados[\"padroes\"] = []\n",
        "\n",
        "        try:\n",
        "            videos_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "            with open(videos_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                outros_dados[\"videos\"] = json.load(f)\n",
        "        except:\n",
        "            outros_dados[\"videos\"] = []\n",
        "\n",
        "        print(f\"  âœ… Dados carregados: {len(copywriting_data)} anÃ¡lises de copywriting\")\n",
        "\n",
        "        return {\n",
        "            \"copywriting\": copywriting_data,\n",
        "            \"legendas\": legendas_data,\n",
        "            **outros_dados\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  âŒ Erro ao carregar dados de copywriting: {e}\")\n",
        "        return None\n",
        "\n",
        "def adicionar_aba_copywriting_estrategico(wb, dados):\n",
        "    \"\"\"Adiciona aba principal de anÃ¡lise de copywriting\"\"\"\n",
        "    # Criar nova aba\n",
        "    ws = wb.create_sheet(\"Copywriting EstratÃ©gico\")\n",
        "\n",
        "    # TÃ­tulo principal\n",
        "    ws.merge_cells(\"A1:H1\")\n",
        "    titulo = ws[\"A1\"]\n",
        "    titulo.value = \"ANÃLISE ESTRATÃ‰GICA DE COPYWRITING - ENGENHARIA REVERSA\"\n",
        "    titulo.fill = PatternFill(start_color=\"1F4E79\", end_color=\"1F4E79\", fill_type=\"solid\")\n",
        "    titulo.font = Font(color=\"FFFFFF\", bold=True, size=14)\n",
        "    titulo.alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "    row = 3\n",
        "\n",
        "    # MÃ©tricas executivas\n",
        "    ws[f\"A{row}\"] = \"MÃ‰TRICAS EXECUTIVAS DE COPYWRITING\"\n",
        "    ws[f\"A{row}\"].font = Font(bold=True, size=12, color=\"C5504B\")\n",
        "    row += 2\n",
        "\n",
        "    # Calcular mÃ©tricas\n",
        "    videos_copy = dados[\"copywriting\"]\n",
        "\n",
        "    if videos_copy:\n",
        "        # Score mÃ©dio\n",
        "        scores = [v.get(\"score_persuasao\", 0) for v in videos_copy]\n",
        "        score_medio = sum(scores) / len(scores) if scores else 0\n",
        "\n",
        "        # Contadores\n",
        "        total_ganchos = sum(len(v.get(\"ganchos_detectados\", {})) for v in videos_copy)\n",
        "        total_gatilhos = sum(len(v.get(\"gatilhos_mentais_detectados\", {})) for v in videos_copy)\n",
        "        total_ctas = sum(len(v.get(\"ctas_detectados\", {})) for v in videos_copy)\n",
        "        videos_sem_cta = len([v for v in videos_copy if not v.get(\"ctas_detectados\")])\n",
        "        total_templates = sum(len(v.get(\"templates_identificados\", [])) for v in videos_copy)\n",
        "\n",
        "        # Exibir mÃ©tricas\n",
        "        metricas = [\n",
        "            (\"Score PersuasÃ£o MÃ©dio:\", f\"{score_medio:.1f}/100\", \"Meta: 70+ para alta conversÃ£o\"),\n",
        "            (\"VÃ­deos Analisados:\", len(videos_copy), \"Base completa da anÃ¡lise\"),\n",
        "            (\"Total de Ganchos:\", total_ganchos, f\"MÃ©dia: {total_ganchos/len(videos_copy):.1f} por vÃ­deo\"),\n",
        "            (\"Total de Gatilhos:\", total_gatilhos, f\"MÃ©dia: {total_gatilhos/len(videos_copy):.1f} por vÃ­deo\"),\n",
        "            (\"Total de CTAs:\", total_ctas, f\"MÃ©dia: {total_ctas/len(videos_copy):.1f} por vÃ­deo\"),\n",
        "            (\"ðŸš¨ VÃ­deos sem CTA:\", videos_sem_cta, \"CRÃTICO: Implementar imediatamente\" if videos_sem_cta > 0 else \"âœ… Todos tÃªm CTA\"),\n",
        "            (\"Templates Identificados:\", total_templates, \"Estruturas replicÃ¡veis encontradas\")\n",
        "        ]\n",
        "\n",
        "        for metrica, valor, descricao in metricas:\n",
        "            ws[f\"A{row}\"] = metrica\n",
        "            ws[f\"B{row}\"] = valor\n",
        "            ws[f\"C{row}\"] = descricao\n",
        "\n",
        "            ws[f\"A{row}\"].font = Font(bold=True)\n",
        "            if \"ðŸš¨\" in metrica and videos_sem_cta > 0:\n",
        "                ws[f\"B{row}\"].font = Font(bold=True, color=\"FF0000\")\n",
        "            elif isinstance(valor, (int, float)) and valor > 0:\n",
        "                ws[f\"B{row}\"].font = Font(bold=True, color=\"70AD47\")\n",
        "\n",
        "            row += 1\n",
        "\n",
        "        row += 2\n",
        "\n",
        "        # Ranking de performance\n",
        "        ws[f\"A{row}\"] = \"ðŸ† RANKING DE PERFORMANCE POR SCORE DE PERSUASÃƒO\"\n",
        "        ws[f\"A{row}\"].font = Font(bold=True, size=12, color=\"1F4E79\")\n",
        "        row += 2\n",
        "\n",
        "        # Headers\n",
        "        headers = [\"PosiÃ§Ã£o\", \"VÃ­deo ID\", \"Score\", \"Ganchos\", \"Gatilhos\", \"CTAs\", \"Status\"]\n",
        "        for col, header in enumerate(headers, 1):\n",
        "            cell = ws.cell(row=row, column=col)\n",
        "            cell.value = header\n",
        "            cell.font = Font(bold=True)\n",
        "            cell.fill = PatternFill(start_color=\"D9E2F3\", end_color=\"D9E2F3\", fill_type=\"solid\")\n",
        "        row += 1\n",
        "\n",
        "        # Top performers\n",
        "        top_videos = sorted(videos_copy, key=lambda x: x.get(\"score_persuasao\", 0), reverse=True)\n",
        "\n",
        "        for i, video in enumerate(top_videos, 1):\n",
        "            ws.cell(row=row, column=1, value=f\"{i}Âº\")\n",
        "            ws.cell(row=row, column=2, value=video[\"video_id\"])\n",
        "            ws.cell(row=row, column=3, value=f\"{video.get('score_persuasao', 0)}/100\")\n",
        "            ws.cell(row=row, column=4, value=len(video.get(\"ganchos_detectados\", {})))\n",
        "            ws.cell(row=row, column=5, value=len(video.get(\"gatilhos_mentais_detectados\", {})))\n",
        "            ws.cell(row=row, column=6, value=len(video.get(\"ctas_detectados\", {})))\n",
        "\n",
        "            # Status baseado no score\n",
        "            score = video.get(\"score_persuasao\", 0)\n",
        "            if score >= 70:\n",
        "                status = \"ðŸŸ¢ Ã“TIMO\"\n",
        "                status_color = \"70AD47\"\n",
        "            elif score >= 50:\n",
        "                status = \"ðŸŸ¡ BOM\"\n",
        "                status_color = \"FFC000\"\n",
        "            else:\n",
        "                status = \"ðŸ”´ PRECISA OTIMIZAR\"\n",
        "                status_color = \"C5504B\"\n",
        "\n",
        "            cell_status = ws.cell(row=row, column=7, value=status)\n",
        "            cell_status.font = Font(color=status_color, bold=True)\n",
        "\n",
        "            # Destacar top 3\n",
        "            if i <= 3:\n",
        "                for col in range(1, 8):\n",
        "                    ws.cell(row=row, column=col).fill = PatternFill(start_color=\"FFF2CC\", end_color=\"FFF2CC\", fill_type=\"solid\")\n",
        "\n",
        "            row += 1\n",
        "\n",
        "    # Ajustar larguras das colunas\n",
        "    for col, width in [(\"A\", 25), (\"B\", 15), (\"C\", 40), (\"D\", 10), (\"E\", 10), (\"F\", 10), (\"G\", 20), (\"H\", 15)]:\n",
        "        ws.column_dimensions[col].width = width\n",
        "\n",
        "def adicionar_aba_templates_copy(wb, dados):\n",
        "    \"\"\"Adiciona aba de templates replicÃ¡veis\"\"\"\n",
        "    ws = wb.create_sheet(\"Templates ReplicÃ¡veis\")\n",
        "\n",
        "    # TÃ­tulo\n",
        "    ws.merge_cells(\"A1:F1\")\n",
        "    titulo = ws[\"A1\"]\n",
        "    titulo.value = \"TEMPLATES E ESTRUTURAS REPLICÃVEIS DE COPYWRITING\"\n",
        "    titulo.fill = PatternFill(start_color=\"70AD47\", end_color=\"70AD47\", fill_type=\"solid\")\n",
        "    titulo.font = Font(color=\"FFFFFF\", bold=True, size=14)\n",
        "    titulo.alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "    row = 3\n",
        "\n",
        "    # Coletar todos os templates\n",
        "    todos_templates = []\n",
        "    for video in dados[\"copywriting\"]:\n",
        "        templates = video.get(\"templates_identificados\", [])\n",
        "        for template in templates:\n",
        "            template[\"video_id\"] = video[\"video_id\"]\n",
        "            todos_templates.append(template)\n",
        "\n",
        "    if todos_templates:\n",
        "        # Agrupar templates por tipo\n",
        "        templates_agrupados = {}\n",
        "        for template in todos_templates:\n",
        "            nome = template[\"nome\"]\n",
        "            if nome not in templates_agrupados:\n",
        "                templates_agrupados[nome] = {\n",
        "                    \"estrutura\": template[\"estrutura\"],\n",
        "                    \"eficacia\": template[\"eficacia\"],\n",
        "                    \"uso_recomendado\": template[\"uso_recomendado\"],\n",
        "                    \"videos_exemplo\": []\n",
        "                }\n",
        "            templates_agrupados[nome][\"videos_exemplo\"].append(template[\"video_id\"])\n",
        "\n",
        "        # Exibir templates\n",
        "        for nome_template, dados_template in templates_agrupados.items():\n",
        "            ws.merge_cells(f\"A{row}:F{row}\")\n",
        "            template_header = ws[f\"A{row}\"]\n",
        "            template_header.value = f\"ðŸ“‹ TEMPLATE: {nome_template.replace('_', ' ')}\"\n",
        "            template_header.fill = PatternFill(start_color=\"E2EFDA\", end_color=\"E2EFDA\", fill_type=\"solid\")\n",
        "            template_header.font = Font(bold=True, size=11)\n",
        "            row += 1\n",
        "\n",
        "            ws[f\"A{row}\"] = \"Estrutura:\"\n",
        "            ws[f\"B{row}\"] = dados_template[\"estrutura\"]\n",
        "            ws[f\"A{row}\"].font = Font(bold=True)\n",
        "            row += 1\n",
        "\n",
        "            ws[f\"A{row}\"] = \"EficÃ¡cia:\"\n",
        "            ws[f\"B{row}\"] = dados_template[\"eficacia\"]\n",
        "            ws[f\"A{row}\"].font = Font(bold=True)\n",
        "            if dados_template[\"eficacia\"] == \"MUITO ALTA\":\n",
        "                ws[f\"B{row}\"].font = Font(color=\"70AD47\", bold=True)\n",
        "            elif dados_template[\"eficacia\"] == \"ALTA\":\n",
        "                ws[f\"B{row}\"].font = Font(color=\"C5504B\", bold=True)\n",
        "            row += 1\n",
        "\n",
        "            ws[f\"A{row}\"] = \"Uso Recomendado:\"\n",
        "            ws[f\"B{row}\"] = dados_template[\"uso_recomendado\"]\n",
        "            ws[f\"A{row}\"].font = Font(bold=True)\n",
        "            row += 1\n",
        "\n",
        "            ws[f\"A{row}\"] = \"VÃ­deos Exemplo:\"\n",
        "            ws[f\"B{row}\"] = \", \".join(dados_template[\"videos_exemplo\"][:3])\n",
        "            ws[f\"A{row}\"].font = Font(bold=True)\n",
        "            row += 2\n",
        "\n",
        "    # Ajustar larguras\n",
        "    for col, width in [(\"A\", 20), (\"B\", 50), (\"C\", 15), (\"D\", 15), (\"E\", 15), (\"F\", 15)]:\n",
        "        ws.column_dimensions[col].width = width\n",
        "\n",
        "def adicionar_aba_timeline_copy(wb, dados):\n",
        "    \"\"\"Adiciona aba de timeline de elementos persuasivos\"\"\"\n",
        "    ws = wb.create_sheet(\"Timeline PersuasÃ£o\")\n",
        "\n",
        "    # TÃ­tulo\n",
        "    ws.merge_cells(\"A1:G1\")\n",
        "    titulo = ws[\"A1\"]\n",
        "    titulo.value = \"TIMELINE DE ELEMENTOS PERSUASIVOS - ANÃLISE TEMPORAL\"\n",
        "    titulo.fill = PatternFill(start_color=\"FFC000\", end_color=\"FFC000\", fill_type=\"solid\")\n",
        "    titulo.font = Font(color=\"FFFFFF\", bold=True, size=14)\n",
        "    titulo.alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "    row = 3\n",
        "\n",
        "    # Para cada vÃ­deo, mostrar timeline\n",
        "    for video in dados[\"copywriting\"]:\n",
        "        video_id = video[\"video_id\"]\n",
        "\n",
        "        ws[f\"A{row}\"] = f\"ðŸ“¹ VÃDEO: {video_id}\"\n",
        "        ws[f\"A{row}\"].font = Font(bold=True, size=11, color=\"1F4E79\")\n",
        "        row += 2\n",
        "\n",
        "        # Headers da timeline\n",
        "        headers = [\"Tempo\", \"Minuto\", \"Segundo\", \"Elemento\", \"PosiÃ§Ã£o\", \"Impacto\", \"AnÃ¡lise\"]\n",
        "        for col, header in enumerate(headers, 1):\n",
        "            cell = ws.cell(row=row, column=col)\n",
        "            cell.value = header\n",
        "            cell.font = Font(bold=True)\n",
        "            cell.fill = PatternFill(start_color=\"F2F2F2\", end_color=\"F2F2F2\", fill_type=\"solid\")\n",
        "        row += 1\n",
        "\n",
        "        # Coletar todos os elementos temporais\n",
        "        elementos_temporais = []\n",
        "\n",
        "        # Ganchos\n",
        "        for tipo, dados in video.get(\"ganchos_detectados\", {}).items():\n",
        "            for timestamp in dados.get(\"timestamps\", []):\n",
        "                elementos_temporais.append({\n",
        "                    \"categoria\": \"GANCHO\",\n",
        "                    \"tipo\": tipo,\n",
        "                    \"minuto\": timestamp[\"minuto\"],\n",
        "                    \"segundo\": timestamp[\"segundo\"],\n",
        "                    \"contexto\": timestamp[\"texto_contexto\"]\n",
        "                })\n",
        "\n",
        "        # Gatilhos\n",
        "        for tipo, dados in video.get(\"gatilhos_mentais_detectados\", {}).items():\n",
        "            for timestamp in dados.get(\"timestamps\", []):\n",
        "                elementos_temporais.append({\n",
        "                    \"categoria\": \"GATILHO\",\n",
        "                    \"tipo\": tipo,\n",
        "                    \"minuto\": timestamp[\"minuto\"],\n",
        "                    \"segundo\": timestamp[\"segundo\"],\n",
        "                    \"contexto\": timestamp[\"texto_contexto\"]\n",
        "                })\n",
        "\n",
        "        # CTAs\n",
        "        for tipo, dados in video.get(\"ctas_detectados\", {}).items():\n",
        "            for timestamp in dados.get(\"timestamps\", []):\n",
        "                elementos_temporais.append({\n",
        "                    \"categoria\": \"CTA\",\n",
        "                    \"tipo\": tipo,\n",
        "                    \"minuto\": timestamp[\"minuto\"],\n",
        "                    \"segundo\": timestamp[\"segundo\"],\n",
        "                    \"contexto\": timestamp[\"texto_contexto\"]\n",
        "                })\n",
        "\n",
        "        # Ordenar por tempo\n",
        "        elementos_temporais.sort(key=lambda x: (x[\"minuto\"], x[\"segundo\"]))\n",
        "\n",
        "        # Exibir elementos\n",
        "        if elementos_temporais:\n",
        "            for elemento in elementos_temporais:\n",
        "                ws.cell(row=row, column=1, value=f\"{elemento['minuto']:02d}:{elemento['segundo']:02d}\")\n",
        "                ws.cell(row=row, column=2, value=elemento[\"minuto\"])\n",
        "                ws.cell(row=row, column=3, value=elemento[\"segundo\"])\n",
        "                ws.cell(row=row, column=4, value=f\"{elemento['categoria']}: {elemento['tipo']}\")\n",
        "\n",
        "                # AnÃ¡lise de posiÃ§Ã£o\n",
        "                total_segundos = elemento[\"minuto\"] * 60 + elemento[\"segundo\"]\n",
        "                if total_segundos <= 10:\n",
        "                    posicao = \"ABERTURA\"\n",
        "                    posicao_color = \"70AD47\"\n",
        "                elif total_segundos <= 20:\n",
        "                    posicao = \"MEIO\"\n",
        "                    posicao_color = \"FFC000\"\n",
        "                else:\n",
        "                    posicao = \"FINAL\"\n",
        "                    posicao_color = \"C5504B\"\n",
        "\n",
        "                cell_pos = ws.cell(row=row, column=5, value=posicao)\n",
        "                cell_pos.font = Font(color=posicao_color, bold=True)\n",
        "\n",
        "                # AnÃ¡lise de impacto\n",
        "                impacto = analisar_impacto_elemento(elemento[\"categoria\"], posicao)\n",
        "                ws.cell(row=row, column=6, value=impacto[\"score\"])\n",
        "                ws.cell(row=row, column=7, value=impacto[\"analise\"])\n",
        "\n",
        "                if impacto[\"score\"] == \"ALTO\":\n",
        "                    ws.cell(row=row, column=6).font = Font(color=\"70AD47\", bold=True)\n",
        "                elif impacto[\"score\"] == \"BAIXO\":\n",
        "                    ws.cell(row=row, column=6).font = Font(color=\"C5504B\", bold=True)\n",
        "\n",
        "                row += 1\n",
        "        else:\n",
        "            ws.cell(row=row, column=1, value=\"Nenhum elemento temporal mapeado\")\n",
        "            row += 1\n",
        "\n",
        "        row += 2\n",
        "\n",
        "    # Ajustar larguras\n",
        "    for col, width in [(\"A\", 8), (\"B\", 10), (\"C\", 15), (\"D\", 30), (\"E\", 12), (\"F\", 8), (\"G\", 25)]:\n",
        "        ws.column_dimensions[col].width = width\n",
        "\n",
        "def analisar_impacto_elemento(categoria, posicao):\n",
        "    \"\"\"Analisa o impacto de um elemento baseado na posiÃ§Ã£o\"\"\"\n",
        "    impactos = {\n",
        "        (\"GANCHO\", \"ABERTURA\"): {\"score\": \"ALTO\", \"analise\": \"Ideal para capturar atenÃ§Ã£o\"},\n",
        "        (\"GANCHO\", \"MEIO\"): {\"score\": \"MÃ‰DIO\", \"analise\": \"Melhor no inÃ­cio\"},\n",
        "        (\"GANCHO\", \"FINAL\"): {\"score\": \"BAIXO\", \"analise\": \"Reposicionar para abertura\"},\n",
        "        (\"GATILHO\", \"ABERTURA\"): {\"score\": \"MÃ‰DIO\", \"analise\": \"Bom para credibilidade\"},\n",
        "        (\"GATILHO\", \"MEIO\"): {\"score\": \"ALTO\", \"analise\": \"PosiÃ§Ã£o ideal para persuasÃ£o\"},\n",
        "        (\"GATILHO\", \"FINAL\"): {\"score\": \"MÃ‰DIO\", \"analise\": \"ReforÃ§a decisÃ£o\"},\n",
        "        (\"CTA\", \"ABERTURA\"): {\"score\": \"BAIXO\", \"analise\": \"Muito cedo, construir valor primeiro\"},\n",
        "        (\"CTA\", \"MEIO\"): {\"score\": \"MÃ‰DIO\", \"analise\": \"Considerar mover para final\"},\n",
        "        (\"CTA\", \"FINAL\"): {\"score\": \"ALTO\", \"analise\": \"Posicionamento ideal\"}\n",
        "    }\n",
        "\n",
        "    return impactos.get((categoria, posicao), {\"score\": \"MÃ‰DIO\", \"analise\": \"Analisar contexto especÃ­fico\"})\n",
        "\n",
        "def adicionar_aba_recomendacoes_copy(wb, dados):\n",
        "    \"\"\"Adiciona aba de recomendaÃ§Ãµes estratÃ©gicas consolidadas\"\"\"\n",
        "    ws = wb.create_sheet(\"RecomendaÃ§Ãµes Copy\")\n",
        "\n",
        "    # TÃ­tulo\n",
        "    ws.merge_cells(\"A1:F1\")\n",
        "    titulo = ws[\"A1\"]\n",
        "    titulo.value = \"RECOMENDAÃ‡Ã•ES ESTRATÃ‰GICAS DE COPYWRITING - PLANO DE AÃ‡ÃƒO\"\n",
        "    titulo.fill = PatternFill(start_color=\"C5504B\", end_color=\"C5504B\", fill_type=\"solid\")\n",
        "    titulo.font = Font(color=\"FFFFFF\", bold=True, size=14)\n",
        "    titulo.alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "    row = 3\n",
        "\n",
        "    # Consolidar recomendaÃ§Ãµes por prioridade\n",
        "    todas_recomendacoes = []\n",
        "    for video in dados[\"copywriting\"]:\n",
        "        recomendacoes_video = video.get(\"recomendacoes_estrategicas\", [])\n",
        "        for rec in recomendacoes_video:\n",
        "            rec[\"video_id\"] = video[\"video_id\"]\n",
        "            todas_recomendacoes.append(rec)\n",
        "\n",
        "    # Agrupar por prioridade\n",
        "    recomendacoes_por_prioridade = {\n",
        "        \"CRÃTICA\": [],\n",
        "        \"ALTA\": [],\n",
        "        \"MÃ‰DIA\": []\n",
        "    }\n",
        "\n",
        "    for rec in todas_recomendacoes:\n",
        "        prioridade = rec.get(\"prioridade\", \"MÃ‰DIA\")\n",
        "        if prioridade in recomendacoes_por_prioridade:\n",
        "            recomendacoes_por_prioridade[prioridade].append(rec)\n",
        "\n",
        "    # Exibir por prioridade\n",
        "    for prioridade in [\"CRÃTICA\", \"ALTA\", \"MÃ‰DIA\"]:\n",
        "        if not recomendacoes_por_prioridade[prioridade]:\n",
        "            continue\n",
        "\n",
        "        ws[f\"A{row}\"] = f\"ðŸš¨ PRIORIDADE {prioridade}\"\n",
        "        if prioridade == \"CRÃTICA\":\n",
        "            ws[f\"A{row}\"].font = Font(color=\"FF0000\", bold=True, size=12)\n",
        "        elif prioridade == \"ALTA\":\n",
        "            ws[f\"A{row}\"].font = Font(color=\"C5504B\", bold=True, size=12)\n",
        "        else:\n",
        "            ws[f\"A{row}\"].font = Font(color=\"FFC000\", bold=True, size=12)\n",
        "\n",
        "        row += 2\n",
        "\n",
        "        # Headers\n",
        "        headers = [\"Categoria\", \"RecomendaÃ§Ã£o\", \"VÃ­deos Afetados\", \"AÃ§Ã£o Sugerida\"]\n",
        "        for col, header in enumerate(headers, 1):\n",
        "            cell = ws.cell(row=row, column=col)\n",
        "            cell.value = header\n",
        "            cell.font = Font(bold=True)\n",
        "            cell.fill = PatternFill(start_color=\"F2F2F2\", end_color=\"F2F2F2\", fill_type=\"solid\")\n",
        "        row += 1\n",
        "\n",
        "        # Agrupar recomendaÃ§Ãµes similares da mesma prioridade\n",
        "        grupos = {}\n",
        "        for rec in recomendacoes_por_prioridade[prioridade]:\n",
        "            categoria = rec[\"categoria\"]\n",
        "            if categoria not in grupos:\n",
        "                grupos[categoria] = {\n",
        "                    \"recomendacao\": rec[\"recomendacao\"],\n",
        "                    \"videos\": [],\n",
        "                    \"acao\": gerar_acao_especifica(categoria)\n",
        "                }\n",
        "            grupos[categoria][\"videos\"].append(rec[\"video_id\"])\n",
        "\n",
        "        for categoria, dados_grupo in grupos.items():\n",
        "            ws.cell(row=row, column=1, value=categoria)\n",
        "            ws.cell(row=row, column=2, value=dados_grupo[\"recomendacao\"])\n",
        "            ws.cell(row=row, column=3, value=f\"{len(dados_grupo['videos'])} vÃ­deo(s)\")\n",
        "            ws.cell(row=row, column=4, value=dados_grupo[\"acao\"])\n",
        "            row += 1\n",
        "\n",
        "        row += 2\n",
        "\n",
        "    # Ajustar larguras\n",
        "    for col, width in [(\"A\", 20), (\"B\", 40), (\"C\", 15), (\"D\", 30), (\"E\", 15), (\"F\", 15)]:\n",
        "        ws.column_dimensions[col].width = width\n",
        "\n",
        "def gerar_acao_especifica(categoria):\n",
        "    \"\"\"Gera aÃ§Ã£o especÃ­fica baseada na categoria da recomendaÃ§Ã£o\"\"\"\n",
        "    acoes = {\n",
        "        \"GANCHOS\": \"Revisar primeiros 5 segundos e adicionar pergunta ou curiosidade\",\n",
        "        \"GATILHOS\": \"Incorporar elementos de autoridade, prova social ou reciprocidade\",\n",
        "        \"CTA\": \"Adicionar call-to-action claro nos Ãºltimos 3-5 segundos\",\n",
        "        \"ESTRUTURA\": \"Aplicar template identificado mais prÃ³ximo do nicho\",\n",
        "        \"PERSUASÃƒO\": \"Combinar mÃºltiplos elementos persuasivos em sequÃªncia lÃ³gica\"\n",
        "    }\n",
        "    return acoes.get(categoria, \"Revisar e otimizar elementos especÃ­ficos mencionados\")\n",
        "\n",
        "def atualizar_aba_principal_com_copy(wb, dados):\n",
        "    \"\"\"Atualiza a aba principal existente com mÃ©tricas de copywriting\"\"\"\n",
        "    # Tentar encontrar aba principal (pode ter nomes diferentes)\n",
        "    aba_principal = None\n",
        "    possiveis_nomes = [\"Dashboard Principal\", \"Executive Summary\", \"Summary\", \"Principal\"]\n",
        "\n",
        "    for nome in wb.sheetnames:\n",
        "        if any(possivel in nome for possivel in possiveis_nomes):\n",
        "            aba_principal = wb[nome]\n",
        "            break\n",
        "\n",
        "    if not aba_principal:\n",
        "        # Se nÃ£o encontrou, usar a primeira aba\n",
        "        aba_principal = wb.worksheets[0]\n",
        "\n",
        "    # Encontrar prÃ³xima linha vazia para adicionar seÃ§Ã£o de copywriting\n",
        "    next_row = 1\n",
        "    for row in range(1, 100):\n",
        "        if aba_principal[f\"A{row}\"].value is None:\n",
        "            next_row = row\n",
        "            break\n",
        "\n",
        "    # Adicionar seÃ§Ã£o de copywriting\n",
        "    # TÃ­tulo da seÃ§Ã£o\n",
        "    aba_principal.merge_cells(f\"A{next_row}:H{next_row}\")\n",
        "    titulo_copy = aba_principal[f\"A{next_row}\"]\n",
        "    titulo_copy.value = \"ðŸ“ ANÃLISE DE COPYWRITING - RESUMO EXECUTIVO\"\n",
        "    titulo_copy.fill = PatternFill(start_color=\"7030A0\", end_color=\"7030A0\", fill_type=\"solid\")\n",
        "    titulo_copy.font = Font(color=\"FFFFFF\", bold=True, size=12)\n",
        "    next_row += 2\n",
        "\n",
        "    # MÃ©tricas resumidas\n",
        "    videos_copy = dados[\"copywriting\"]\n",
        "\n",
        "    if videos_copy:\n",
        "        scores = [v.get(\"score_persuasao\", 0) for v in videos_copy]\n",
        "        score_medio = sum(scores) / len(scores)\n",
        "        videos_sem_cta = len([v for v in videos_copy if not v.get(\"ctas_detectados\")])\n",
        "        templates_total = sum(len(v.get(\"templates_identificados\", [])) for v in videos_copy)\n",
        "\n",
        "        metricas_resumo = [\n",
        "            (\"Score de PersuasÃ£o MÃ©dio:\", f\"{score_medio:.1f}/100\"),\n",
        "            (\"VÃ­deos sem CTA:\", f\"{videos_sem_cta} (CRÃTICO)\" if videos_sem_cta > 0 else \"0 âœ…\"),\n",
        "            (\"Templates Identificados:\", str(templates_total)),\n",
        "            (\"Status Geral:\", \"OtimizaÃ§Ã£o necessÃ¡ria\" if score_medio < 60 or videos_sem_cta > 0 else \"Performance boa\")\n",
        "        ]\n",
        "\n",
        "        for metrica, valor in metricas_resumo:\n",
        "            aba_principal[f\"A{next_row}\"] = metrica\n",
        "            aba_principal[f\"B{next_row}\"] = valor\n",
        "            aba_principal[f\"A{next_row}\"].font = Font(bold=True)\n",
        "\n",
        "            if \"CRÃTICO\" in valor:\n",
        "                aba_principal[f\"B{next_row}\"].font = Font(color=\"FF0000\", bold=True)\n",
        "            elif \"âœ…\" in valor:\n",
        "                aba_principal[f\"B{next_row}\"].font = Font(color=\"70AD47\", bold=True)\n",
        "\n",
        "            next_row += 1\n",
        "\n",
        "    else:\n",
        "        aba_principal[f\"A{next_row}\"] = \"âš ï¸ Execute a anÃ¡lise de copywriting (CÃ©lula 2.4) para ver mÃ©tricas\"\n",
        "        aba_principal[f\"A{next_row}\"].font = Font(color=\"FFC000\", bold=True)\n",
        "\n",
        "def gerar_relatorios_copywriting_individuais(dados):\n",
        "    \"\"\"Gera relatÃ³rios individuais de texto para cada vÃ­deo\"\"\"\n",
        "    print(\"  ðŸ“„ Gerando relatÃ³rios individuais de copywriting...\")\n",
        "\n",
        "    pasta_relatorios = os.path.join(PASTA_TRABALHO, \"relatorios_copywriting\")\n",
        "    os.makedirs(pasta_relatorios, exist_ok=True)\n",
        "\n",
        "    for video_copy in dados[\"copywriting\"]:\n",
        "        video_id = video_copy[\"video_id\"]\n",
        "\n",
        "        relatorio_path = os.path.join(pasta_relatorios, f\"{video_id}_copywriting_completo.txt\")\n",
        "\n",
        "        with open(relatorio_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\"=\"*60 + \"\\n\")\n",
        "            f.write(\"RELATÃ“RIO COMPLETO DE ANÃLISE DE COPYWRITING\\n\")\n",
        "            f.write(\"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "            f.write(f\"ðŸ“¹ VÃ­deo ID: {video_id}\\n\")\n",
        "            f.write(f\"ðŸŽ¯ Score de PersuasÃ£o: {video_copy.get('score_persuasao', 0)}/100\\n\")\n",
        "            f.write(f\"ðŸ“ Total de Palavras: {video_copy.get('total_palavras', 0)}\\n\\n\")\n",
        "\n",
        "            # Texto completo\n",
        "            f.write(\"TRANSCRIÃ‡ÃƒO COMPLETA:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            f.write(video_copy.get(\"texto_completo\", \"TranscriÃ§Ã£o nÃ£o disponÃ­vel\") + \"\\n\\n\")\n",
        "\n",
        "            # Ganchos\n",
        "            f.write(\"ðŸŽ£ GANCHOS DETECTADOS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            ganchos = video_copy.get(\"ganchos_detectados\", {})\n",
        "            if ganchos:\n",
        "                for tipo, dados in ganchos.items():\n",
        "                    f.write(f\"â€¢ {tipo.upper()}: {dados['count']} ocorrÃªncias\\n\")\n",
        "                    f.write(f\"  Exemplos: {', '.join(dados['exemplos'])}\\n\")\n",
        "            else:\n",
        "                f.write(\"Nenhum gancho detectado.\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "            # Gatilhos\n",
        "            f.write(\"ðŸ§  GATILHOS MENTAIS DETECTADOS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            gatilhos = video_copy.get(\"gatilhos_mentais_detectados\", {})\n",
        "            if gatilhos:\n",
        "                for tipo, dados in gatilhos.items():\n",
        "                    f.write(f\"â€¢ {tipo.upper()}: {dados['count']} ocorrÃªncias\\n\")\n",
        "                    f.write(f\"  Exemplos: {', '.join(dados['exemplos'])}\\n\")\n",
        "            else:\n",
        "                f.write(\"Nenhum gatilho mental detectado.\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "            # CTAs\n",
        "            f.write(\"ðŸ“¢ CALLS-TO-ACTION DETECTADOS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            ctas = video_copy.get(\"ctas_detectados\", {})\n",
        "            if ctas:\n",
        "                for tipo, dados in ctas.items():\n",
        "                    f.write(f\"â€¢ {tipo.upper()}: {dados['count']} ocorrÃªncias\\n\")\n",
        "                    f.write(f\"  Exemplos: {', '.join(dados['exemplos'])}\\n\")\n",
        "            else:\n",
        "                f.write(\"Nenhum CTA detectado.\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "            # RecomendaÃ§Ãµes\n",
        "            f.write(\"ðŸ’¡ RECOMENDAÃ‡Ã•ES ESTRATÃ‰GICAS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            recomendacoes = video_copy.get(\"recomendacoes_estrategicas\", [])\n",
        "            if recomendacoes:\n",
        "                for rec in recomendacoes:\n",
        "                    f.write(f\"â€¢ [{rec['prioridade']}] {rec['categoria']}: {rec['recomendacao']}\\n\")\n",
        "            else:\n",
        "                f.write(\"Nenhuma recomendaÃ§Ã£o especÃ­fica.\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "            # Templates\n",
        "            f.write(\"ðŸ“‹ TEMPLATES IDENTIFICADOS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            templates = video_copy.get(\"templates_identificados\", [])\n",
        "            if templates:\n",
        "                for template in templates:\n",
        "                    f.write(f\"â€¢ {template['nome']}: {template['estrutura']}\\n\")\n",
        "                    f.write(f\"  EficÃ¡cia: {template['eficacia']}\\n\")\n",
        "                    f.write(f\"  Uso: {template['uso_recomendado']}\\n\\n\")\n",
        "            else:\n",
        "                f.write(\"Nenhum template especÃ­fico identificado.\\n\")\n",
        "\n",
        "    print(f\"  âœ… RelatÃ³rios individuais gerados em: {pasta_relatorios}\")\n",
        "\n",
        "# Executar integraÃ§Ã£o\n",
        "try:\n",
        "    integrar_copywriting_dashboard_existente()\n",
        "except Exception as e:\n",
        "    print(f\"âŒ ERRO de ExecuÃ§Ã£o: {type(e).__name__}: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvmDo8Iw61G8"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CÃ‰LULA 4.3: INTEGRAÃ‡ÃƒO DE COPYWRITING NO DASHBOARD EXISTENTE\n",
        "# ============================================================================\n",
        "\n",
        "# Definir a variÃ¡vel global PASTA_TRABALHO se ainda nÃ£o estiver definida\n",
        "# Certifique-se de que esta variÃ¡vel esteja definida corretamente em uma cÃ©lula anterior (ex: CÃ©lula 1.2)\n",
        "# Exemplo: PASTA_TRABALHO = \"/content/drive/MyDrive/Videos Dona Done/_engenharia_reversa\"\n",
        "\n",
        "# Executar a funÃ§Ã£o principal da Layer 4.3\n",
        "if 'PASTA_TRABALHO' in globals():\n",
        "    print(\"Iniciando a Layer 4.3: IntegraÃ§Ã£o de Copywriting no Dashboard...\")\n",
        "    integrar_copywriting_dashboard_existente()\n",
        "else:\n",
        "    print(\"ERRO: A variÃ¡vel PASTA_TRABALHO nÃ£o estÃ¡ definida. Certifique-se de executar a CÃ©lula 1.2 ou equivalente.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ns9Xl0Uk7_Cq"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "from openpyxl.styles import Font, Alignment, PatternFill\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from fpdf import FPDF\n",
        "from openpyxl.utils import get_column_letter\n",
        "from openpyxl import load_workbook\n",
        "\n",
        "# --- FunÃ§Ãµes Auxiliares (do notebook original, se aplicÃ¡vel) ---\n",
        "def log_progress(message):\n",
        "    timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
        "    print(f\"[{timestamp}] {message}\")\n",
        "\n",
        "def verificar_prerequisito_etapa(etapa_anterior):\n",
        "    try:\n",
        "        if not \"PASTA_TRABALHO\" in globals():\n",
        "            raise Exception(\"VariÃ¡veis globais de configuraÃ§Ã£o nÃ£o encontradas. Execute a CÃ‰LULA 1.2 primeiro.\")\n",
        "\n",
        "        config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "        if not os.path.exists(config_path):\n",
        "            raise Exception(\"Arquivo de configuraÃ§Ã£o nÃ£o encontrado. Execute a CÃ‰LULA 1.2 primeiro.\")\n",
        "\n",
        "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            config = json.load(f)\n",
        "\n",
        "        if not config[\"status_etapas\"][etapa_anterior]:\n",
        "            raise Exception(f\"A etapa \"{etapa_anterior}\" nÃ£o foi concluÃ­da. Execute a cÃ©lula correspondente primeiro.\")\n",
        "\n",
        "        return True, config\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ PRÃ‰-REQUISITO NÃƒO ATENDIDO: {e}\")\n",
        "        return False, None\n",
        "\n",
        "def escrever_linha(ws, row, values, bold=False, wrap=False):\n",
        "    for col, val in enumerate(values, 1):\n",
        "        cell = ws.cell(row=row, column=col, value=val)\n",
        "        if bold: cell.font = Font(bold=True)\n",
        "        if wrap: cell.alignment = Alignment(wrap_text=True)\n",
        "\n",
        "def ajustar_larguras(ws, larguras):\n",
        "    for col_idx, width in enumerate(larguras, 1):\n",
        "        ws.column_dimensions[get_column_letter(col_idx)].width = width\n",
        "\n",
        "# --- Melhorias no Dashboard ---\n",
        "print(\"\n",
        "ðŸ“Š Gerando Dashboard Executivo Inteligente...\n",
        "\")\n",
        "\n",
        "# Carregar dados de metadados e decomposiÃ§Ãµes\n",
        "prerequisito_ok, config = verificar_prerequisito_etapa(\"decomposicao\")\n",
        "if not prerequisito_ok:\n",
        "    print(\"NÃ£o foi possÃ­vel gerar o dashboard sem os dados de decomposiÃ§Ã£o.\")\n",
        "    exit() # Ou return, dependendo do contexto do notebook\n",
        "\n",
        "metadados_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "decomposicoes_path = os.path.join(PASTA_TRABALHO, \"dados\", \"decomposicoes_completas.json\")\n",
        "analises_copy_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_copywriting_completas.json\")\n",
        "\n",
        "if not os.path.exists(metadados_path) or not os.path.exists(decomposicoes_path) or not os.path.exists(analises_copy_path):\n",
        "    print(\"âŒ ERRO: Arquivos de dados essenciais para o dashboard nÃ£o encontrados. Execute as cÃ©lulas anteriores (2.2, 2.3, 3.1) primeiro.\")\n",
        "    exit()\n",
        "\n",
        "with open(metadados_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    metadados = json.load(f)\n",
        "with open(decomposicoes_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    decomposicoes = json.load(f)\n",
        "with open(analises_copy_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    analises_copy = json.load(f)\n",
        "\n",
        "# Unificar dados para o DataFrame\n",
        "data_for_df = []\n",
        "for meta_video in metadados:\n",
        "    video_id = meta_video[\"id\"]\n",
        "    decomposicao_video = next((d for d in decomposicoes if d[\"video_id\"] == video_id), {})\n",
        "    analise_copy_video = next((a for a in analises_copy if a[\"video_id\"] == video_id), {})\n",
        "\n",
        "    row = {\n",
        "        \"video_id\": video_id,\n",
        "        \"nome_arquivo\": meta_video[\"nome_arquivo\"],\n",
        "        \"duracao_segundos\": meta_video.get(\"duracao_segundos\"),\n",
        "        \"cortes_detectados_count\": decomposicao_video.get(\"cortes_detectados_count\", 0),\n",
        "        \"complexidade_visual_media\": decomposicao_video.get(\"complexidade_visual_media\"),\n",
        "        \"ocr_textos_count\": decomposicao_video.get(\"ocr_textos_count\", 0),\n",
        "        \"audio_transcrito_len\": len(decomposicao_video.get(\"audio_transcrito\", \"\")), # Usar len da string\n",
        "        \"bpm_audio\": decomposicao_video.get(\"bpm_audio\"),\n",
        "        \"brilho_medio\": decomposicao_video.get(\"brilho_medio\"),\n",
        "        \"formato_detectado\": decomposicao_video.get(\"formato_detectado\"),\n",
        "        \"tem_audio\": decomposicao_video.get(\"tem_audio\"),\n",
        "        \"score_persuasao\": analise_copy_video.get(\"score_persuasao\"),\n",
        "        \"ganchos_detectados\": analise_copy_video.get(\"ganchos_detectados\", {}),\n",
        "        \"gatilhos_mentais_detectados\": analise_copy_video.get(\"gatilhos_mentais_detectados\", {}),\n",
        "        \"ctas_detectados\": analise_copy_video.get(\"ctas_detectados\", {}),\n",
        "        \"templates_identificados\": analise_copy_video.get(\"templates_identificados\", []),\n",
        "        \"recomendacoes_estrategicas\": analise_copy_video.get(\"recomendacoes_estrategicas\", []),\n",
        "        \"primeira_frase_audio\": decomposicao_video.get(\"primeira_frase_audio\", \"\") # Adicionado\n",
        "    }\n",
        "    data_for_df.append(row)\n",
        "\n",
        "df = pd.DataFrame(data_for_df)\n",
        "\n",
        "# Calcular Scores (funÃ§Ãµes do notebook original)\n",
        "def calculate_viral_score(row):\n",
        "    try:\n",
        "        score = 0\n",
        "        if pd.notna(row['duracao_segundos']) and row['duracao_segundos'] > 0:\n",
        "            cortes_por_seg = row['cortes_detectados_count'] / row['duracao_segundos']\n",
        "            if cortes_por_seg > 20: score += 25\n",
        "            elif cortes_por_seg > 10: score += 20\n",
        "            elif cortes_por_seg > 5: score += 15\n",
        "            else: score += 10\n",
        "        if pd.notna(row['complexidade_visual_media']):\n",
        "            if row['complexidade_visual_media'] > 600: score += 20\n",
        "            elif row['complexidade_visual_media'] > 400: score += 15\n",
        "            else: score += 10\n",
        "        if pd.notna(row['ocr_textos_count']):\n",
        "            if row['ocr_textos_count'] > 10: score += 15\n",
        "            elif row['ocr_textos_count'] > 5: score += 12\n",
        "            elif row['ocr_textos_count'] > 0: score += 8\n",
        "        if pd.notna(row['duracao_segundos']):\n",
        "            if 15 <= row['duracao_segundos'] <= 30: score += 20\n",
        "            elif 10 <= row['duracao_segundos'] <= 45: score += 15\n",
        "            else: score += 10\n",
        "        gatilhos = str(row['gatilhos_mentais_detectados']).lower()\n",
        "        if 'urgÃªncia' in gatilhos or 'escassez' in gatilhos: score += 8\n",
        "        if 'estÃ­mulo' in gatilhos: score += 7\n",
        "        if 'atenÃ§Ã£o' in gatilhos: score += 5\n",
        "        return min(score, 100)\n",
        "    except:\n",
        "        return 50\n",
        "\n",
        "def calculate_technical_score(row):\n",
        "    try:\n",
        "        score = 0\n",
        "        if pd.notna(row['brilho_medio']):\n",
        "            if 120 <= row['brilho_medio'] <= 180: score += 25\n",
        "            elif 100 <= row['brilho_medio'] <= 200: score += 20\n",
        "            else: score += 10\n",
        "        formato = str(row['formato_detectado'])\n",
        "        if 'vertical_9_16' in formato: score += 25\n",
        "        elif 'horizontal_16_9' in formato: score += 20\n",
        "        else: score += 15\n",
        "        if row['tem_audio']: score += 25\n",
        "        else: score += 5\n",
        "        if pd.notna(row['total_frames']) and row['total_frames'] > 0:\n",
        "            if row['total_frames'] > 300: score += 25\n",
        "            elif row['total_frames'] > 150: score += 20\n",
        "            else: score += 15\n",
        "        return min(score, 100)\n",
        "    except:\n",
        "        return 50\n",
        "\n",
        "def calculate_content_score(row):\n",
        "    try:\n",
        "        score = 0\n",
        "        ocr_count = row['ocr_textos_count'] if pd.notna(row['ocr_textos_count']) else 0\n",
        "        audio_len = row['audio_transcrito_len'] if pd.notna(row['audio_transcrito_len']) else 0\n",
        "        if ocr_count > 5 or audio_len > 100: score += 30\n",
        "        elif ocr_count > 2 or audio_len > 50: score += 20\n",
        "        elif ocr_count > 0 or audio_len > 0: score += 15\n",
        "        else: score += 5\n",
        "        if pd.notna(row['bpm_audio']):\n",
        "            if 120 <= row['bpm_audio'] <= 140: score += 35\n",
        "            elif 100 <= row['bpm_audio'] <= 160: score += 25\n",
        "            else: score += 15\n",
        "        if pd.notna(row['duracao_segundos']) and row['duracao_segundos'] > 0:\n",
        "            densidade = (ocr_count + audio_len/10) / row['duracao_segundos']\n",
        "            if densidade > 2: score += 35\n",
        "            elif densidade > 1: score += 25\n",
        "            else: score += 15\n",
        "        return min(score, 100)\n",
        "    except:\n",
        "        return 50\n",
        "\n",
        "def generate_insights_from_data(df):\n",
        "    insights = []\n",
        "    try:\n",
        "        if not df.empty:\n",
        "            best_performing = df.nlargest(3, 'viral_score')\n",
        "            if not best_performing.empty:\n",
        "                avg_duration = best_performing['duracao_segundos'].mean()\n",
        "                insights.append(f\"DURAÃ‡ÃƒO VENCEDORA: Seus top 3 vÃ­deos (com maior score viral) tÃªm duraÃ§Ã£o mÃ©dia de {avg_duration:.1f} segundos. Considere este como um benchmark para futuros conteÃºdos.\")\n",
        "\n",
        "            # AnÃ¡lise de Gatilhos Mentais\n",
        "            all_gatilhos = Counter()\n",
        "            for gatilhos_dict in df['gatilhos_mentais_detectados']:\n",
        "                for tipo, data in gatilhos_dict.items():\n",
        "                    all_gatilhos[tipo] += len(data.get('timestamps', []))\n",
        "            if all_gatilhos:\n",
        "                most_common_gatilho = all_gatilhos.most_common(1)\n",
        "                if most_common_gatilho:\n",
        "                    insights.append(f\"GATILHO MAIS EFICAZ: O gatilho '{most_common_gatilho[0][0]}' foi o mais frequente ({most_common_gatilho[0][1]} ocorrÃªncias). Explore como replicar seu uso em outros vÃ­deos.\")\n",
        "\n",
        "            # AnÃ¡lise de CTAs\n",
        "            videos_with_cta = df[df['ctas_detectados'].apply(lambda x: len(x) > 0)]\n",
        "            if not videos_with_cta.empty:\n",
        "                insights.append(f\"CHAMADAS PARA AÃ‡ÃƒO (CTAs): {len(videos_with_cta)} de {len(df)} vÃ­deos possuem CTAs detectados. VÃ­deos com CTAs tendem a ter melhor performance. Garanta que todos os vÃ­deos tenham um CTA claro.\")\n",
        "            else:\n",
        "                insights.append(\"âš ï¸ ALERTA DE CTA: Nenhum CTA foi detectado em seus vÃ­deos. CTAs sÃ£o cruciais para guiar a audiÃªncia. Adicione chamadas para aÃ§Ã£o claras e visÃ­veis.\")\n",
        "\n",
        "            # AnÃ¡lise de Cortes\n",
        "            if 'cortes_detectados_count' in df.columns and not df.empty:\n",
        "                avg_cuts = df['cortes_detectados_count'].mean()\n",
        "                insights.append(f\"RITMO DE EDIÃ‡ÃƒO: Em mÃ©dia, seus vÃ­deos possuem {avg_cuts:.1f} cortes. Um ritmo dinÃ¢mico pode aumentar o engajamento.\")\n",
        "\n",
        "            # AnÃ¡lise de Primeira Frase do Ãudio\n",
        "            if 'primeira_frase_audio' in df.columns and not df['primeira_frase_audio'].empty:\n",
        "                exemplos_frases = df['primeira_frase_audio'].dropna().sample(min(3, len(df['primeira_frase_audio'].dropna()))).tolist()\n",
        "                if exemplos_frases:\n",
        "                    insights.append(f\"INÃCIOS IMPACTANTES: As primeiras frases do Ã¡udio sÃ£o cruciais. Exemplos de inÃ­cios fortes: {'; '.join([f'\"' + s + '\"' for s in exemplos_frases])}. Analise o impacto dessas frases na retenÃ§Ã£o inicial.\")\n",
        "\n",
        "        else:\n",
        "            insights.append(\"NÃ£o hÃ¡ dados suficientes para gerar insights. Execute as etapas anteriores do notebook.\")\n",
        "    except Exception as e:\n",
        "        insights.append(f\"Erro ao gerar insights: {e}\")\n",
        "    return insights\n",
        "\n",
        "\n",
        "df['viral_score'] = df.apply(calculate_viral_score, axis=1)\n",
        "df['technical_score'] = df.apply(calculate_technical_score, axis=1)\n",
        "df['content_score'] = df.apply(calculate_content_score, axis=1)\n",
        "\n",
        "# Gerar insights\n",
        "insights = generate_insights_from_data(df)\n",
        "\n",
        "# Salvar o blueprint atualizado (agora com insights)\n",
        "blueprint_data = {\n",
        "    \"resumo_geral\": {\n",
        "        \"total_videos_analisados\": len(df),\n",
        "        \"media_score_viral\": df['viral_score'].mean(),\n",
        "        \"media_score_tecnico\": df['technical_score'].mean(),\n",
        "        \"media_score_conteudo\": df['content_score'].mean(),\n",
        "        \"insights_estrategicos\": insights\n",
        "    },\n",
        "    \"detalhes_por_video\": df.to_dict(orient='records')\n",
        "}\n",
        "\n",
        "blueprint_path = os.path.join(PASTA_TRABALHO, \"blueprint\", \"blueprint_final.json\")\n",
        "with open(blueprint_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(blueprint_data, f, indent=2, ensure_ascii=False)\n",
        "print(f\"âœ… Blueprint final atualizado e salvo em: {blueprint_path}\")\n",
        "\n",
        "# --- GeraÃ§Ã£o de RelatÃ³rio em Excel (Dashboard) ---\n",
        "OUTPUT_PATH = os.path.join(PASTA_TRABALHO, \"dashboard\", \"DASHBOARD_MASTER_EXECUTIVO_INTELIGENTE.xlsx\")\n",
        "\n",
        "wb = Workbook()\n",
        "\n",
        "# Remover a aba padrÃ£o criada\n",
        "if 'Sheet' in wb.sheetnames:\n",
        "    wb.remove(wb['Sheet'])\n",
        "\n",
        "# --- ABA 1: Resumo Executivo ---\n",
        "ws_resumo = wb.create_sheet(\"Resumo Executivo\")\n",
        "ws_resumo.merge_cells('A1:D1')\n",
        "ws_resumo['A1'].value = \"DASHBOARD MASTER EXECUTIVO INTELIGENTE\"\n",
        "ws_resumo['A1'].font = Font(bold=True, size=16, color=\"FFFFFF\")\n",
        "ws_resumo['A1'].alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
        "ws_resumo['A1'].fill = PatternFill(start_color=\"0070C0\", end_color=\"0070C0\", fill_type=\"solid\")\n",
        "\n",
        "ws_resumo.merge_cells('A3:D3')\n",
        "ws_resumo['A3'].value = \"Insights EstratÃ©gicos\"\n",
        "ws_resumo['A3'].font = Font(bold=True, size=14, color=\"0070C0\")\n",
        "ws_resumo['A3'].alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "row_start = 5\n",
        "for insight in insights:\n",
        "    ws_resumo.merge_cells(f'A{row_start}:D{row_start}')\n",
        "    ws_resumo[f'A{row_start}'].value = f\"â€¢ {insight}\"\n",
        "    ws_resumo[f'A{row_start}'].alignment = Alignment(wrap_text=True)\n",
        "    row_start += 1\n",
        "\n",
        "ws_resumo.column_dimensions['A'].width = 20\n",
        "ws_resumo.column_dimensions['B'].width = 20\n",
        "ws_resumo.column_dimensions['C'].width = 20\n",
        "ws_resumo.column_dimensions['D'].width = 20\n",
        "\n",
        "# --- ABA 2: Detalhes por VÃ­deo ---\n",
        "ws_detalhes = wb.create_sheet(\"Detalhes por Video\")\n",
        "\n",
        "# Preparar dados para o Excel, incluindo a nova coluna de nome renomeado\n",
        "df_excel = df[['nome_arquivo', 'primeira_frase_audio', 'duracao_segundos', 'cortes_detectados_count',\n",
        "               'complexidade_visual_media', 'ocr_textos_count', 'audio_transcrito_len',\n",
        "               'bpm_audio', 'brilho_medio', 'formato_detectado', 'tem_audio',\n",
        "               'score_persuasao', 'viral_score', 'technical_score', 'content_score']]\n",
        "\n",
        "df_excel.rename(columns={\n",
        "    'nome_arquivo': 'Nome do Arquivo Original',\n",
        "    'primeira_frase_audio': 'Nome Renomeado (Primeira Frase)',\n",
        "    'duracao_segundos': 'DuraÃ§Ã£o (s)',\n",
        "    'cortes_detectados_count': 'Cortes Detectados',\n",
        "    'complexidade_visual_media': 'Complexidade Visual MÃ©dia',\n",
        "    'ocr_textos_count': 'Textos OCR Detectados',\n",
        "    'audio_transcrito_len': 'Tamanho TranscriÃ§Ã£o Ãudio',\n",
        "    'bpm_audio': 'BPM Ãudio',\n",
        "    'brilho_medio': 'Brilho MÃ©dio',\n",
        "    'formato_detectado': 'Formato Detectado',\n",
        "    'tem_audio': 'Tem Ãudio',\n",
        "    'score_persuasao': 'Score PersuasÃ£o',\n",
        "    'viral_score': 'Score Viral',\n",
        "    'technical_score': 'Score TÃ©cnico',\n",
        "    'content_score': 'Score ConteÃºdo'\n",
        "}, inplace=True)\n",
        "\n",
        "for r_idx, row in enumerate(dataframe_to_rows(df_excel, index=False, header=True), 1):\n",
        "    ws_detalhes.append(row)\n",
        "\n",
        "for cell in ws_detalhes[1]:\n",
        "    cell.font = Font(bold=True)\n",
        "    cell.fill = PatternFill(start_color=\"D9E1F2\", end_color=\"D9E1F2\", fill_type=\"solid\")\n",
        "\n",
        "# Ajustar largura das colunas\n",
        "for column in ws_detalhes.columns:\n",
        "    max_length = 0\n",
        "    column_name = get_column_letter(column[0].column)\n",
        "    for cell in column:\n",
        "        try:\n",
        "            if len(str(cell.value)) > max_length:\n",
        "                max_length = len(str(cell.value))\n",
        "        except:\n",
        "            pass\n",
        "    adjusted_width = (max_length + 2) * 1.2\n",
        "    ws_detalhes.column_dimensions[column_name].width = adjusted_width\n",
        "\n",
        "# --- ABA 3: RecomendaÃ§Ãµes Detalhadas ---\n",
        "ws_recs = wb.create_sheet(\"Recomendacoes Detalhadas\")\n",
        "ws_recs.merge_cells('A1:C1')\n",
        "ws_recs['A1'].value = \"RECOMENDAÃ‡Ã•ES ESTRATÃ‰GICAS POR VÃDEO\"\n",
        "ws_recs['A1'].font = Font(bold=True, size=16, color=\"FFFFFF\")\n",
        "ws_recs['A1'].alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
        "ws_recs['A1'].fill = PatternFill(start_color=\"FFC000\", end_color=\"FFC000\", fill_type=\"solid\")\n",
        "\n",
        "rec_row = 3\n",
        "for video_data in blueprint_data[\"detalhes_por_video\"]:\n",
        "    ws_recs[f'A{rec_row}'].value = f\"VÃ­deo: {video_data['nome_arquivo']}\"\n",
        "    ws_recs[f'A{rec_row}'].font = Font(bold=True, size=12)\n",
        "    rec_row += 1\n",
        "    if video_data['recomendacoes_estrategicas']:\n",
        "        for rec in video_data['recomendacoes_estrategicas']:\n",
        "            ws_recs[f'A{rec_row}'].value = f\"â€¢ Categoria: {rec.get('categoria', 'N/A')}\"\n",
        "            ws_recs[f'B{rec_row}'].value = f\"RecomendaÃ§Ã£o: {rec.get('recomendacao', 'N/A')}\"\n",
        "            ws_recs[f'C{rec_row}'].value = f\"Prioridade: {rec.get('prioridade', 'N/A')}\"\n",
        "            rec_row += 1\n",
        "    else:\n",
        "        ws_recs[f'A{rec_row}'].value = \"Nenhuma recomendaÃ§Ã£o especÃ­fica para este vÃ­deo.\"\n",
        "        rec_row += 1\n",
        "    rec_row += 1 # EspaÃ§o entre vÃ­deos\n",
        "\n",
        "ws_recs.column_dimensions['A'].width = 30\n",
        "ws_recs.column_dimensions['B'].width = 60\n",
        "ws_recs.column_dimensions['C'].width = 15\n",
        "\n",
        "# --- ABA 4: Templates Identificados ---\n",
        "ws_templates = wb.create_sheet(\"Templates Identificados\")\n",
        "ws_templates.merge_cells('A1:D1')\n",
        "ws_templates['A1'].value = \"TEMPLATES ESTRATÃ‰GICOS IDENTIFICADOS\"\n",
        "ws_templates['A1'].font = Font(bold=True, size=16, color=\"FFFFFF\")\n",
        "ws_templates['A1'].alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
        "ws_templates['A1'].fill = PatternFill(start_color=\"70AD47\", end_color=\"70AD47\", fill_type=\"solid\")\n",
        "\n",
        "temp_row = 3\n",
        "all_templates = []\n",
        "for video_data in blueprint_data[\"detalhes_por_video\"]:\n",
        "    for template in video_data['templates_identificados']:\n",
        "        all_templates.append(template)\n",
        "\n",
        "# Remover duplicatas de templates\n",
        "unique_templates = []\n",
        "seen_templates = set()\n",
        "for tpl in all_templates:\n",
        "    tpl_tuple = tuple(sorted(tpl.items())) # Para comparar dicionÃ¡rios como tuplas\n",
        "    if tpl_tuple not in seen_templates:\n",
        "        unique_templates.append(tpl)\n",
        "        seen_templates.add(tpl_tuple)\n",
        "\n",
        "if unique_templates:\n",
        "    for template in unique_templates:\n",
        "        ws_templates[f'A{temp_row}'].value = f\"Nome: {template.get('nome', 'N/A')}\"\n",
        "        ws_templates[f'B{temp_row}'].value = f\"Estrutura: {template.get('estrutura', 'N/A')}\"\n",
        "        ws_templates[f'C{temp_row}'].value = f\"EficÃ¡cia: {template.get('eficacia', 'N/A')}\"\n",
        "        ws_templates[f'D{temp_row}'].value = f\"Uso Recomendado: {template.get('uso_recomendado', 'N/A')}\"\n",
        "        temp_row += 1\n",
        "else:\n",
        "    ws_templates[f'A{temp_row}'].value = \"Nenhum template estratÃ©gico identificado.\"\n",
        "\n",
        "ws_templates.column_dimensions['A'].width = 25\n",
        "ws_templates.column_dimensions['B'].width = 50\n",
        "ws_templates.column_dimensions['C'].width = 15\n",
        "ws_templates.column_dimensions['D'].width = 40\n",
        "\n",
        "# Salvar o arquivo Excel\n",
        "wb.save(OUTPUT_PATH)\n",
        "print(f\"âœ… Dashboard Master Executivo Inteligente gerado em: {OUTPUT_PATH}\")\n",
        "\n",
        "# --- GeraÃ§Ã£o de RelatÃ³rio em PDF (Resumo) ---\n",
        "pdf = FPDF()\n",
        "pdf.add_page()\n",
        "pdf.set_font(\"Arial\", \"B\", 16)\n",
        "pdf.cell(200, 10, \"RelatÃ³rio Executivo de Engenharia Reversa de VÃ­deos\", 0, 1, \"C\")\n",
        "pdf.ln(10)\n",
        "\n",
        "pdf.set_font(\"Arial\", \"\", 12)\n",
        "pdf.multi_cell(0, 10, \"Este relatÃ³rio apresenta uma visÃ£o estratÃ©gica dos resultados da engenharia reversa dos vÃ­deos, com foco em insights acionÃ¡veis para otimizaÃ§Ã£o de conteÃºdo.\")\n",
        "pdf.ln(5)\n",
        "\n",
        "pdf.set_font(\"Arial\", \"B\", 14)\n",
        "pdf.cell(200, 10, \"1. Insights EstratÃ©gicos\", 0, 1, \"L\")\n",
        "pdf.ln(2)\n",
        "for insight in insights:\n",
        "    pdf.multi_cell(0, 8, f\"â€¢ {insight}\")\n",
        "pdf.ln(5)\n",
        "\n",
        "pdf.set_font(\"Arial\", \"B\", 14)\n",
        "pdf.cell(200, 10, \"2. Resumo de MÃ©tricas Principais\", 0, 1, \"L\")\n",
        "pdf.ln(2)\n",
        "pdf.multi_cell(0, 8, f\"Total de VÃ­deos Analisados: {blueprint_data['resumo_geral']['total_videos_analisados']}\")\n",
        "pdf.multi_cell(0, 8, f\"MÃ©dia Score Viral: {blueprint_data['resumo_geral']['media_score_viral']:.2f}\")\n",
        "pdf.multi_cell(0, 8, f\"MÃ©dia Score TÃ©cnico: {blueprint_data['resumo_geral']['media_score_tecnico']:.2f}\")\n",
        "pdf.multi_cell(0, 8, f\"MÃ©dia Score ConteÃºdo: {blueprint_data['resumo_geral']['media_score_conteudo']:.2f}\")\n",
        "pdf.ln(5)\n",
        "\n",
        "# Adicionar grÃ¡ficos (se existirem e forem gerados)\n",
        "# Exemplo: if os.path.exists(os.path.join(PASTA_TRABALHO, \"dashboard\", \"taxa_engajamento.png\")):\n",
        "#     pdf.add_page()\n",
        "#     pdf.set_font(\"Arial\", \"B\", 14)\n",
        "#     pdf.cell(200, 10, \"3. GrÃ¡fico de Taxa de Engajamento\", 0, 1, \"L\")\n",
        "#     pdf.image(os.path.join(PASTA_TRABALHO, \"dashboard\", \"taxa_engajamento.png\"), x=10, y=pdf.get_y(), w=180)\n",
        "\n",
        "relatorio_pdf_path = os.path.join(PASTA_TRABALHO, \"blueprint\", \"relatorio_executivo.pdf\")\n",
        "pdf.output(relatorio_pdf_path)\n",
        "print(f\"âœ… RelatÃ³rio executivo em PDF gerado em: {relatorio_pdf_path}\")\n",
        "\n",
        "print(\"\n",
        "âœ… DASHBOARD E RELATÃ“RIOS GERADOS COM SUCESSO!\n",
        "\")\n",
        "print(\"âž¡ï¸ PRÃ“XIMA CÃ‰LULA: Nenhuma. Processo concluÃ­do.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_QVY6i-8qLc"
      },
      "outputs": [],
      "source": [
        "rodar_layer_5_6_ai()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsF_FvEM-26P"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# LAYER 7 â€” IA ONLINE (FREE) â€¢ AnÃ¡lise semÃ¢ntica avanÃ§ada\n",
        "# - Sem likes / comentÃ¡rios / views.\n",
        "# - Usa Hugging Face Inference API (grÃ¡tis com token).\n",
        "# - Respeita PASTA_TRABALHO e OUTPUT_PATH do seu projeto.\n",
        "# ============================================================\n",
        "\n",
        "!pip -q install openpyxl==3.1.2 requests==2.32.3\n",
        "\n",
        "import os, re, json, time, math\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any\n",
        "from openpyxl import load_workbook, Workbook\n",
        "from openpyxl.utils import get_column_letter\n",
        "from openpyxl.styles import Font, Alignment, PatternFill\n",
        "\n",
        "# ---------- ConexÃ£o com seu notebook ----------\n",
        "assert 'PASTA_TRABALHO' in globals(), \"PASTA_TRABALHO nÃ£o estÃ¡ definido. Rode as cÃ©lulas de configuraÃ§Ã£o.\"\n",
        "if 'OUTPUT_PATH' not in globals():\n",
        "    OUTPUT_PATH = os.path.join(PASTA_TRABALHO, \"dashboard\", \"DASHBOARD_MASTER_EXECUTIVO_INTELIGENTE.xlsx\")\n",
        "    print(f\"âš ï¸ OUTPUT_PATH nÃ£o estava definido; criando automÃ¡tico: {OUTPUT_PATH}\")\n",
        "\n",
        "DADOS_DIR = os.path.join(PASTA_TRABALHO, \"dados\")\n",
        "AI_ONLINE_DIR = os.path.join(PASTA_TRABALHO, \"ai_online\")\n",
        "os.makedirs(DADOS_DIR, exist_ok=True)\n",
        "os.makedirs(AI_ONLINE_DIR, exist_ok=True)\n",
        "\n",
        "META_PATH = os.path.join(DADOS_DIR, \"metadados_completos.json\")\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = \"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\"   # coloque seu token\n",
        "# ---------- Config da IA Online (FREE) ----------\n",
        "# Cadastre-se grÃ¡tis na Hugging Face e crie um Access Token (Settings > Access Tokens).\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"\").strip()\n",
        "if not HF_TOKEN:\n",
        "    print(\"âš ï¸ Defina seu token gratuito da Hugging Face em os.environ['HF_TOKEN'] para ativar a IA online.\")\n",
        "\n",
        "# Modelo pÃºblico e gratuito (ajuste se quiser)\n",
        "# Recomendo um instruÃ­do e leve para PT/ES/EN; Mistral 7B Instruct costuma funcionar bem:\n",
        "HF_MODEL = os.environ.get(\"HF_MODEL_ID\", \"mistralai/Mistral-7B-Instruct-v0.3\")\n",
        "HF_URL = f\"https://api-inference.huggingface.co/models/{HF_MODEL}\"\n",
        "\n",
        "def hf_generate(prompt: str, max_new_tokens=650, temperature=0.3, top_p=0.9, retries=2) -> str:\n",
        "    \"\"\"\n",
        "    Chama o endpoint de geraÃ§Ã£o da Hugging Face (gratuito com token).\n",
        "    Retorna string gerada (sem garantias de JSON formatado â€” faremos parsing).\n",
        "    \"\"\"\n",
        "    if not HF_TOKEN:\n",
        "        raise RuntimeError(\"HF_TOKEN nÃ£o definido. Configure os.environ['HF_TOKEN'] com seu token gratuito.\")\n",
        "    headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
        "    payload = {\n",
        "        \"inputs\": prompt,\n",
        "        \"parameters\": {\n",
        "            \"max_new_tokens\": max_new_tokens,\n",
        "            \"temperature\": temperature,\n",
        "            \"top_p\": top_p,\n",
        "            \"return_full_text\": False\n",
        "        }\n",
        "    }\n",
        "    for _ in range(retries):\n",
        "        r = requests.post(HF_URL, headers=headers, json=payload, timeout=90)\n",
        "        if r.status_code == 200:\n",
        "            try:\n",
        "                out = r.json()\n",
        "                if isinstance(out, list) and out and \"generated_text\" in out[0]:\n",
        "                    return out[0][\"generated_text\"]\n",
        "                if isinstance(out, dict) and \"generated_text\" in out:\n",
        "                    return out[\"generated_text\"]\n",
        "                # alguns servidores retornam str direta\n",
        "                if isinstance(out, str):\n",
        "                    return out\n",
        "            except Exception:\n",
        "                return r.text\n",
        "        time.sleep(2)\n",
        "    # retorna texto cru (pode conter erro do modelo)\n",
        "    return r.text\n",
        "\n",
        "def try_json_extract(text: str) -> Any:\n",
        "    \"\"\"\n",
        "    Extrai o primeiro JSON vÃ¡lido de uma string. Robustifica contra respostas com texto extra.\n",
        "    \"\"\"\n",
        "    start = text.find(\"{\")\n",
        "    end   = text.rfind(\"}\")\n",
        "    if start == -1 or end == -1 or end <= start:\n",
        "        return None\n",
        "    snippet = text[start:end+1]\n",
        "    try:\n",
        "        return json.loads(snippet)\n",
        "    except Exception:\n",
        "        # tentativa: aspas simples -> duplas\n",
        "        snippet2 = snippet.replace(\"'\", '\"')\n",
        "        try:\n",
        "            return json.loads(snippet2)\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "# ---------- UtilitÃ¡rios de I/O ----------\n",
        "def ler_metadados() -> List[Dict[str,Any]]:\n",
        "    if not os.path.exists(META_PATH):\n",
        "        print(f\"âŒ NÃ£o encontrei {META_PATH}. Rode as camadas anteriores.\")\n",
        "        return []\n",
        "    with open(META_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def encontrar_transcricao(video_id: str) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Procura transcript/srt da sua Layer 5:\n",
        "      - PASTA_TRABALHO/ai_insights/<video_id>/transcript.txt\n",
        "      - PASTA_TRABALHO/ai_insights/<video_id>/subtitles.srt\n",
        "    Retorna dict com 'plain' e 'srt' (quando houver).\n",
        "    \"\"\"\n",
        "    base = os.path.join(PASTA_TRABALHO, \"ai_insights\", video_id)\n",
        "    out = {\"plain\": \"\", \"srt\": \"\"}\n",
        "    if os.path.isdir(base):\n",
        "        pt = os.path.join(base, \"transcript.txt\")\n",
        "        ps = os.path.join(base, \"subtitles.srt\")\n",
        "        if os.path.exists(pt):\n",
        "            out[\"plain\"] = Path(pt).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "        if os.path.exists(ps):\n",
        "            out[\"srt\"] = Path(ps).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "    return out\n",
        "\n",
        "def srt_para_blocos(srt_text: str, janela_seg=20, max_blocos=6) -> List[Dict[str,Any]]:\n",
        "    \"\"\"\n",
        "    Junta legendas em blocos de ~janela_seg segundos (atÃ© max_blocos) para anÃ¡lise por cena.\n",
        "    \"\"\"\n",
        "    if not srt_text.strip():\n",
        "        return []\n",
        "    # parse simples\n",
        "    entries = []\n",
        "    for chunk in re.split(r\"\\n\\s*\\n\", srt_text.strip()):\n",
        "        lines = [l.strip() for l in chunk.splitlines() if l.strip()]\n",
        "        if len(lines) >= 2:\n",
        "            ts = lines[1]\n",
        "            m = re.match(r\"(\\d{2}):(\\d{2}):(\\d{2}),\\d+\\s*-->\\s*(\\d{2}):(\\d{2}):(\\d{2}),\\d+\", ts)\n",
        "            if not m:\n",
        "                continue\n",
        "            h1,m1,s1, h2,m2,s2 = map(int, m.groups())\n",
        "            start = h1*3600+m1*60+s1\n",
        "            end   = h2*3600+m2*60+s2\n",
        "            text  = \" \".join(lines[2:])\n",
        "            entries.append((start, end, text))\n",
        "    # agrega em janelas\n",
        "    if not entries:\n",
        "        return []\n",
        "    t0 = entries[0][0]\n",
        "    blocos = []\n",
        "    cur_t0 = t0\n",
        "    cur_txt = []\n",
        "    for st, en, txt in entries:\n",
        "        if (en - cur_t0) <= janela_seg:\n",
        "            cur_txt.append(txt)\n",
        "        else:\n",
        "            blocos.append({\"inicio_seg\": cur_t0, \"fim_seg\": en, \"texto\": \" \".join(cur_txt)})\n",
        "            cur_t0 = en\n",
        "            cur_txt = [txt]\n",
        "    if cur_txt:\n",
        "        blocos.append({\"inicio_seg\": cur_t0, \"fim_seg\": entries[-1][1], \"texto\": \" \".join(cur_txt)})\n",
        "    return blocos[:max_blocos]\n",
        "\n",
        "# ---------- Prompts ----------\n",
        "PROMPT_MACRO = \"\"\"VocÃª Ã© um analista sÃªnior de roteiro e comunicaÃ§Ã£o em pt-BR.\n",
        "Analise a TRANSCRIÃ‡ÃƒO a seguir e produza apenas um JSON com os campos:\n",
        "\n",
        "{\n",
        "  \"tema_central\": \"\",\n",
        "  \"tese\": \"\",\n",
        "  \"promessa\": \"\",\n",
        "  \"publico_alvo\": \"\",\n",
        "  \"dor_principal\": \"\",\n",
        "  \"ganho_principal\": \"\",\n",
        "  \"mecanismo_unico\": \"\",\n",
        "  \"provas_apoio\": [\"\", \"\"],\n",
        "  \"tom_de_voz\": [\"\", \"\"],\n",
        "  \"frameworks_copy\": [\"AIDA\",\"PAS\",\"FAB\",\"Story\",\"Lista\",\"How-To\"],\n",
        "  \"estrutura_geral\": [\n",
        "    {\"bloco\": 1, \"objetivo\": \"\", \"ideias_chave\": [\"\",\"\",\"\"], \"frases_de_efeito\": [\"\",\"\"]}\n",
        "  ],\n",
        "  \"objeÃ§Ãµes_previstas\": [\"\",\"\",\"\"],\n",
        "  \"oportunidades_melhoria\": [\"\",\"\",\"\"],\n",
        "  \"analogias_recomendadas\": [\"\",\"\",\"\"],\n",
        "  \"cta_detectadas\": [\"\",\"\",\"\"]\n",
        "}\n",
        "\n",
        "TRANSCRIÃ‡ÃƒO:\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_SUGESTOES = \"\"\"VocÃª Ã© um roteirista sÃªnior para vÃ­deos curtos em pt-BR.\n",
        "Usando a TRANSCRIÃ‡ÃƒO (e os insights macro abaixo), gere apenas um JSON:\n",
        "\n",
        "INSIGHTS_MACRO:\n",
        "{macro}\n",
        "\n",
        "TRANSCRIÃ‡ÃƒO:\n",
        "{transc}\n",
        "\n",
        "JSON com:\n",
        "{\n",
        "  \"hooks_reativos\": [\"5 variaÃ§Ãµes objetivas, curtas, com nÃºmeros ou pergunta\"],\n",
        "  \"texto_na_tela_3s\": [\"3 frases de 5â€“7 palavras para aparecer em 3s\"],\n",
        "  \"roteiro_15s\": [\"linha-a-linha do que dizer/fazer\", \"...\"],\n",
        "  \"roteiro_30s\": [\"linha-a-linha do que dizer/fazer\", \"...\"],\n",
        "  \"analogias\": [\"3 ideias de analogias concretas\"],\n",
        "  \"oportunidades\": [\"3 oportunidades especÃ­ficas de melhoria do roteiro\"]\n",
        "}\n",
        "Responda sÃ³ com JSON.\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_CENA = \"\"\"VocÃª Ã© um editor-chefe. Para o trecho abaixo, devolva JSON:\n",
        "\n",
        "TRECHO:\n",
        "{trecho}\n",
        "\n",
        "JSON:\n",
        "{\n",
        "  \"objetivo_do_trecho\": \"\",\n",
        "  \"ponto_principal\": \"\",\n",
        "  \"melhorias_de_copy\": [\"\",\"\",\"\"],\n",
        "  \"texto_na_tela_sugerido\": [\"\",\"\"],\n",
        "  \"cta_sugerida\": \"\"\n",
        "}\n",
        "Responda sÃ³ JSON.\n",
        "\"\"\"\n",
        "\n",
        "# ---------- ExecuÃ§Ã£o por vÃ­deo ----------\n",
        "def analisar_video_online(video_id: str, nome_arquivo: str) -> Dict[str,Any]:\n",
        "    out_dir = os.path.join(AI_ONLINE_DIR, video_id)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # Carrega transcriÃ§Ã£o\n",
        "    tr = encontrar_transcricao(video_id)\n",
        "    texto = tr[\"plain\"] or \"\"\n",
        "    srt  = tr[\"srt\"] or \"\"\n",
        "    if not (texto or srt):\n",
        "        print(f\"âš ï¸ {video_id}: sem transcript/srt. Pulei IA online.\")\n",
        "        return {}\n",
        "\n",
        "    # 1) Macro\n",
        "    macro_prompt = PROMPT_MACRO + (texto[:6000] if texto else srt[:6000])\n",
        "    macro_raw = hf_generate(macro_prompt, max_new_tokens=700, temperature=0.25) if HF_TOKEN else \"{}\"\n",
        "    macro = try_json_extract(macro_raw) or {}\n",
        "\n",
        "    # 2) SugestÃµes (usa texto e macro)\n",
        "    sug_prompt = PROMPT_SUGESTOES.format(macro=json.dumps(macro, ensure_ascii=False), transc=(texto[:4000] if texto else srt[:4000]))\n",
        "    sug_raw = hf_generate(sug_prompt, max_new_tokens=700, temperature=0.4) if HF_TOKEN else \"{}\"\n",
        "    sugestoes = try_json_extract(sug_raw) or {}\n",
        "\n",
        "    # 3) Cenas (usa SRT em blocos)\n",
        "    cenas = []\n",
        "    blocos = srt_para_blocos(srt, janela_seg=20, max_blocos=6)\n",
        "    for b in blocos:\n",
        "        p = PROMPT_CENA.format(trecho=b[\"texto\"][:1200])\n",
        "        raw = hf_generate(p, max_new_tokens=350, temperature=0.35) if HF_TOKEN else \"{}\"\n",
        "        j = try_json_extract(raw) or {}\n",
        "        cenas.append({\n",
        "            \"inicio_seg\": b[\"inicio_seg\"], \"fim_seg\": b[\"fim_seg\"], **j\n",
        "        })\n",
        "\n",
        "    # Salva JSON e MD por vÃ­deo\n",
        "    pack = {\n",
        "        \"video_id\": video_id,\n",
        "        \"nome_arquivo\": nome_arquivo,\n",
        "        \"macro\": macro,\n",
        "        \"sugestoes\": sugestoes,\n",
        "        \"cenas\": cenas\n",
        "    }\n",
        "    Path(os.path.join(out_dir, \"online_llm_report.json\")).write_text(json.dumps(pack, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "    md = [\n",
        "        f\"# IA Online â€” {video_id}\",\n",
        "        \"## Macro\",\n",
        "        json.dumps(macro, ensure_ascii=False, indent=2),\n",
        "        \"## SugestÃµes\",\n",
        "        json.dumps(sugestoes, ensure_ascii=False, indent=2),\n",
        "        \"## Cenas\",\n",
        "        json.dumps(cenas, ensure_ascii=False, indent=2),\n",
        "    ]\n",
        "    Path(os.path.join(out_dir, \"online_llm_report.md\")).write_text(\"\\n\\n\".join(md), encoding=\"utf-8\")\n",
        "\n",
        "    return pack\n",
        "\n",
        "# ---------- Atualiza Excel ----------\n",
        "def _xl_set_width(ws, widths):\n",
        "    for i,w in enumerate(widths,1):\n",
        "        ws.column_dimensions[get_column_letter(i)].width = w\n",
        "\n",
        "def atualizar_excel_online(pacotes: List[Dict[str,Any]]):\n",
        "    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
        "    wb = load_workbook(OUTPUT_PATH) if os.path.exists(OUTPUT_PATH) else Workbook()\n",
        "\n",
        "    # --- Aba 1: IA Online â€” Macro ---\n",
        "    if \"IA Online â€” Macro\" in wb.sheetnames: del wb[\"IA Online â€” Macro\"]\n",
        "    ws1 = wb.create_sheet(\"IA Online â€” Macro\")\n",
        "    header1 = [\"VÃ­deo\",\"Tema\",\"Tese\",\"Promessa\",\"PÃºblico\",\"Dor\",\"Ganho\",\"Mecanismo Ãºnico\",\"Provas\",\"Tom\",\"Frameworks\",\"ObjeÃ§Ãµes\",\"Oportunidades\",\"Analogias\",\"CTAs\",\"Estrutura (blocos)\"]\n",
        "    for c,h in enumerate(header1,1):\n",
        "        cell = ws1.cell(row=1, column=c, value=h); cell.font=Font(bold=True)\n",
        "    row=2\n",
        "    for p in pacotes:\n",
        "        m = p.get(\"macro\", {})\n",
        "        ws1.cell(row=row, column=1, value=p.get(\"nome_arquivo\", p.get(\"video_id\")))\n",
        "        ws1.cell(row=row, column=2, value=m.get(\"tema_central\"))\n",
        "        ws1.cell(row=row, column=3, value=m.get(\"tese\"))\n",
        "        ws1.cell(row=row, column=4, value=m.get(\"promessa\"))\n",
        "        ws1.cell(row=row, column=5, value=m.get(\"publico_alvo\"))\n",
        "        ws1.cell(row=row, column=6, value=m.get(\"dor_principal\"))\n",
        "        ws1.cell(row=row, column=7, value=m.get(\"ganho_principal\"))\n",
        "        ws1.cell(row=row, column=8, value=m.get(\"mecanismo_unico\"))\n",
        "        ws1.cell(row=row, column=9, value=\", \".join(m.get(\"provas_apoio\",[]) or []))\n",
        "        ws1.cell(row=row, column=10, value=\", \".join(m.get(\"tom_de_voz\",[]) or []))\n",
        "        ws1.cell(row=row, column=11, value=\", \".join(m.get(\"frameworks_copy\",[]) or []))\n",
        "        ws1.cell(row=row, column=12, value=\", \".join(m.get(\"objeÃ§Ãµes_previstas\",[]) or []))\n",
        "        ws1.cell(row=row, column=13, value=\", \".join(m.get(\"oportunidades_melhoria\",[]) or []))\n",
        "        ws1.cell(row=row, column=14, value=\", \".join(m.get(\"analogias_recomendadas\",[]) or []))\n",
        "        ws1.cell(row=row, column=15, value=\", \".join(m.get(\"cta_detectadas\",[]) or []))\n",
        "        # estrutura compactada\n",
        "        estrutura = m.get(\"estrutura_geral\", [])\n",
        "        ws1.cell(row=row, column=16, value=json.dumps(estrutura, ensure_ascii=False))\n",
        "        row += 1\n",
        "    _xl_set_width(ws1, [30,18,18,20,18,18,18,20,24,16,16,18,22,18,18,48])\n",
        "\n",
        "    # --- Aba 2: IA Online â€” SugestÃµes ---\n",
        "    if \"IA Online â€” SugestÃµes\" in wb.sheetnames: del wb[\"IA Online â€” SugestÃµes\"]\n",
        "    ws2 = wb.create_sheet(\"IA Online â€” SugestÃµes\")\n",
        "    header2 = [\"VÃ­deo\",\"Hooks (5)\",\"Texto na tela 3s (3)\",\"Roteiro 15s\",\"Roteiro 30s\",\"Analogias\",\"Oportunidades\"]\n",
        "    for c,h in enumerate(header2,1):\n",
        "        ws2.cell(row=1, column=c, value=h).font=Font(bold=True)\n",
        "    row=2\n",
        "    for p in pacotes:\n",
        "        s = p.get(\"sugestoes\", {})\n",
        "        ws2.cell(row=row, column=1, value=p.get(\"nome_arquivo\", p.get(\"video_id\")))\n",
        "        ws2.cell(row=row, column=2, value=\" â€¢ \" + \"\\n â€¢ \".join(s.get(\"hooks_reativos\",[]) or []))\n",
        "        ws2.cell(row=row, column=3, value=\" â€¢ \" + \"\\n â€¢ \".join(s.get(\"texto_na_tela_3s\",[]) or []))\n",
        "        ws2.cell(row=row, column=4, value=\" â€¢ \" + \"\\n â€¢ \".join(s.get(\"roteiro_15s\",[]) or []))\n",
        "        ws2.cell(row=row, column=5, value=\" â€¢ \" + \"\\n â€¢ \".join(s.get(\"roteiro_30s\",[]) or []))\n",
        "        ws2.cell(row=row, column=6, value=\" â€¢ \" + \"\\n â€¢ \".join(s.get(\"analogias\",[]) or []))\n",
        "        ws2.cell(row=row, column=7, value=\" â€¢ \" + \"\\n â€¢ \".join(s.get(\"oportunidades\",[]) or []))\n",
        "        row+=1\n",
        "    _xl_set_width(ws2, [30,54,40,60,60,40,40])\n",
        "\n",
        "    # --- Aba 3: IA Online â€” Cenas ---\n",
        "    if \"IA Online â€” Cenas\" in wb.sheetnames: del wb[\"IA Online â€” Cenas\"]\n",
        "    ws3 = wb.create_sheet(\"IA Online â€” Cenas\")\n",
        "    header3 = [\"VÃ­deo\",\"InÃ­cio (s)\",\"Fim (s)\",\"Objetivo do trecho\",\"Ponto principal\",\"Melhorias de copy\",\"Texto na tela sugerido\",\"CTA sugerida\"]\n",
        "    for c,h in enumerate(header3,1):\n",
        "        ws3.cell(row=1, column=c, value=h).font=Font(bold=True)\n",
        "    row=2\n",
        "    for p in pacotes:\n",
        "        for c in (p.get(\"cenas\") or []):\n",
        "            ws3.cell(row=row, column=1, value=p.get(\"nome_arquivo\", p.get(\"video_id\")))\n",
        "            ws3.cell(row=row, column=2, value=c.get(\"inicio_seg\"))\n",
        "            ws3.cell(row=row, column=3, value=c.get(\"fim_seg\"))\n",
        "            ws3.cell(row=row, column=4, value=c.get(\"objetivo_do_trecho\"))\n",
        "            ws3.cell(row=row, column=5, value=c.get(\"ponto_principal\"))\n",
        "            ws3.cell(row=row, column=6, value=\"; \".join(c.get(\"melhorias_de_copy\",[]) or []))\n",
        "            ws3.cell(row=row, column=7, value=\"; \".join(c.get(\"texto_na_tela_sugerido\",[]) or []))\n",
        "            ws3.cell(row=row, column=8, value=c.get(\"cta_sugerida\"))\n",
        "            row+=1\n",
        "    _xl_set_width(ws3, [30,10,10,40,40,50,40,24])\n",
        "\n",
        "    wb.save(OUTPUT_PATH)\n",
        "    print(f\"âœ… Excel atualizado com abas: IA Online â€” Macro / SugestÃµes / Cenas â†’ {OUTPUT_PATH}\")\n",
        "\n",
        "# ---------- OrquestraÃ§Ã£o ----------\n",
        "def rodar_layer_7_online_free():\n",
        "    metas = ler_metadados()\n",
        "    if not metas:\n",
        "        return\n",
        "    pacotes = []\n",
        "    for i, m in enumerate(metas, 1):\n",
        "        vid = m.get(\"id\"); nome = m.get(\"nome_arquivo\", vid)\n",
        "        if not vid:\n",
        "            continue\n",
        "        print(f\"[{i}/{len(metas)}] IA Online (FREE) â†’ {vid}\")\n",
        "        try:\n",
        "            p = analisar_video_online(vid, nome)\n",
        "            if p: pacotes.append(p)\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Falha em {vid}: {e}\")\n",
        "    if pacotes:\n",
        "        Path(os.path.join(DADOS_DIR, \"ai_online_insights.json\")).write_text(json.dumps(pacotes, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "        atualizar_excel_online(pacotes)\n",
        "    else:\n",
        "        print(\"Nada processado (sem transcriÃ§Ãµes ou sem token HF).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlLvaS5wFn5d"
      },
      "outputs": [],
      "source": [
        "rodar_layer_7_online_free()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/auth-create/DDfiles/blob/main/engenharia_reversa_videos_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SISTEMA MODULAR DE ENGENHARIA REVERSA DE V√çDEOS - VERS√ÉO FINAL OTIMIZADA\n",
        "\n",
        "Este notebook foi aprimorado para oferecer uma experi√™ncia mais intuitiva, organizada e robusta para a engenharia reversa de v√≠deos. Cada etapa √© modular, com valida√ß√µes de pr√©-requisitos e feedback em tempo real para gui√°-lo(a) durante o processo.\n",
        "\n",
        "## COMO USAR:\n",
        "1.  **Execute as c√©lulas em ordem, de cima para baixo.** Cada c√©lula foi projetada para ser executada sequencialmente.\n",
        "2.  **Aten√ß√£o aos feedbacks:** Mensagens claras indicar√£o o sucesso de cada etapa, poss√≠veis erros e qual a **PR√ìXIMA C√âLULA** a ser executada.\n",
        "3.  **Corrija e re-execute:** Se um erro for detectado, uma mensagem explicativa ser√° exibida. Corrija o problema (geralmente um caminho incorreto ou depend√™ncia ausente) e re-execute a c√©lula que falhou.\n",
        "4.  **Progresso Salvo:** O sistema salva automaticamente o progresso e os dados gerados em cada etapa, permitindo que voc√™ retome de onde parou.\n",
        "\n",
        "## ESTRUTURA DO PROCESSO (Layers e Sublayers):\n",
        "Este sistema √© organizado em camadas l√≥gicas para facilitar o entendimento e a execu√ß√£o:\n",
        "\n",
        "### LAYER 1: CONFIGURA√á√ÉO E PREPARA√á√ÉO\n",
        "*   **C√âLULA 1.1: SETUP INICIAL E INSTALA√á√ÉO DE DEPEND√äNCIAS**\n",
        "*   **C√âLULA 1.2: CONFIGURA√á√ÉO INICIAL E VALIDA√á√ÉO DA PASTA DE TRABALHO**\n",
        "\n",
        "### LAYER 2: DESCOBERTA E EXTRA√á√ÉO DE DADOS BRUTOS\n",
        "*   **C√âLULA 2.1: DESCOBERTA E CATALOGA√á√ÉO DE V√çDEOS**\n",
        "*   **C√âLULA 2.2: EXTRA√á√ÉO DE METADADOS DOS V√çDEOS**\n",
        "*   **C√âLULA 2.3: DECOMPOSI√á√ÉO DE V√çDEOS (FRAMES, √ÅUDIO, TEXTO)**\n",
        "\n",
        "### LAYER 3: AN√ÅLISE E PROCESSAMENTO DE DADOS\n",
        "*   **C√âLULA 3.1: AN√ÅLISE DE PADR√ïES (TEMPORAIS, VISUAIS, TEXTO, √ÅUDIO)**\n",
        "*   **C√âLULA 3.2: AN√ÅLISE PSICOL√ìGICA E GATILHOS DE ENGAJAMENTO**\n",
        "\n",
        "### LAYER 4: GERA√á√ÉO DE RELAT√ìRIOS E BLUEPRINT ESTRAT√âGICO\n",
        "*   **C√âLULA 4.1: GERA√á√ÉO DE RELAT√ìRIOS HUMANIZADOS (√ÅUDIO, VISUAL, TEXTO, PSICOL√ìGICO)**\n",
        "*   **C√âLULA 4.2: GERA√á√ÉO DO BLUEPRINT FINAL E DASHBOARD**\n",
        "\n",
        "---\n",
        "\n",
        "*Lembre-se: Este sistema foi projetado para ser executado no Google Colab. Certifique-se de que seu ambiente est√° configurado corretamente.*"
      ],
      "metadata": {
        "id": "zx8sEBm8_yKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# LAYER 1: CONFIGURA√á√ÉO E PREPARA√á√ÉO\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# C√âLULA 1.1: SETUP INICIAL E INSTALA√á√ÉO DE DEPEND√äNCIAS\n",
        "# ============================================================================\n",
        "\n",
        "# Instalar depend√™ncias necess√°rias\n",
        "!pip install -q moviepy librosa pytesseract opencv-python pandas openpyxl matplotlib seaborn pillow SpeechRecognition pydub fpdf\n",
        "!apt-get update -qq && apt-get install -y -qq tesseract-ocr tesseract-ocr-por ffmpeg\n",
        "\n",
        "# Imports necess√°rios\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import logging\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pytesseract\n",
        "import librosa\n",
        "from moviepy.editor import VideoFileClip\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import speech_recognition as sr # Adicionado import para SpeechRecognition\n",
        "# Montar Google Drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"‚úÖ Google Drive montado com sucesso!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERRO ao montar Google Drive: {e}. Por favor, verifique sua conex√£o ou permiss√µes.\")\n",
        "\n",
        "print(\n",
        "\"‚úÖ SETUP INICIAL CONCLU√çDO!\")\n",
        "print(\"Todas as depend√™ncias foram instaladas e o Google Drive foi montado.\")\n",
        "print(\"‚û°Ô∏è PR√ìXIMA C√âLULA: 1.2 - CONFIGURA√á√ÉO INICIAL E VALIDA√á√ÉO DA PASTA DE TRABALHO\")"
      ],
      "metadata": {
        "id": "setup_inicial",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a270c125-2f7c-45c1-b1be-3a426e2bcf8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Google Drive montado com sucesso!\n",
            "‚úÖ SETUP INICIAL CONCLU√çDO!\n",
            "Todas as depend√™ncias foram instaladas e o Google Drive foi montado.\n",
            "‚û°Ô∏è PR√ìXIMA C√âLULA: 1.2 - CONFIGURA√á√ÉO INICIAL E VALIDA√á√ÉO DA PASTA DE TRABALHO\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 1.2: CONFIGURA√á√ÉO INICIAL E VALIDA√á√ÉO DA PASTA DE TRABALHO\n",
        "# ============================================================================\n",
        "\n",
        "# ‚ö†Ô∏è **ATEN√á√ÉO:** CONFIGURE SEU CAMINHO AQUI!\n",
        "# Substitua o caminho abaixo pela pasta onde seus v√≠deos est√£o localizados no Google Drive.\n",
        "# Exemplo: \"/content/drive/MyDrive/Meus Videos de Marketing\"\n",
        "CAMINHO_PASTA_VIDEOS = \"/content/drive/MyDrive/Videos Dona Done\" # ‚¨ÖÔ∏è **ALTERE AQUI**\n",
        "\n",
        "class ConfiguradorProjeto:\n",
        "    def __init__(self, caminho_pasta):\n",
        "        self.pasta_videos = self._validar_caminho(caminho_pasta)\n",
        "        self.pasta_trabalho = os.path.join(self.pasta_videos, \"_engenharia_reversa\")\n",
        "        self._criar_estrutura()\n",
        "        self._configurar_logging()\n",
        "\n",
        "    def _validar_caminho(self, caminho):\n",
        "        if caminho == \"/content/drive/MyDrive/Videos Dona Done\" and not os.path.exists(caminho):\n",
        "            raise ValueError(\"‚ùå ERRO: Voc√™ precisa alterar CAMINHO_PASTA_VIDEOS com o caminho real da sua pasta de v√≠deos no Google Drive. O caminho padr√£o n√£o foi encontrado.\")\n",
        "\n",
        "        if not os.path.exists(caminho):\n",
        "            raise ValueError(f\"‚ùå ERRO: Pasta n√£o encontrada: {caminho}. Por favor, verifique se o caminho est√° correto e se o Google Drive est√° montado.\")\n",
        "\n",
        "        return caminho\n",
        "\n",
        "    def _criar_estrutura(self):\n",
        "        # Estrutura de pastas conforme o anexo e requisitos do usu√°rio\n",
        "        estrutura = [\n",
        "            \"config\", \"logs\", \"dados\", \"frames_extraidos\",\n",
        "            \"analise_texto\", \"analise_audio\", \"capturas\",\n",
        "            \"blueprint\", \"temp\", \"dashboard\", \"analise_psicologica\", \"analise_visual\"\n",
        "        ]\n",
        "\n",
        "        os.makedirs(self.pasta_trabalho, exist_ok=True)\n",
        "        for pasta in estrutura:\n",
        "            os.makedirs(os.path.join(self.pasta_trabalho, pasta), exist_ok=True)\n",
        "\n",
        "        # Criar subpastas para frames_extraidos (ex: vid_001_Nome_Do_Video/)\n",
        "        # Esta l√≥gica ser√° implementada na c√©lula de decomposi√ß√£o de v√≠deos (C√âLULA 2.3)\n",
        "\n",
        "    def _configurar_logging(self):\n",
        "        log_file = os.path.join(self.pasta_trabalho, \"logs\", f\"sistema_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "            handlers=[logging.FileHandler(log_file, encoding='utf-8')]\n",
        "        )\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def salvar_configuracao(self):\n",
        "        config = {\n",
        "            \"projeto\": {\n",
        "                \"pasta_videos\": self.pasta_videos,\n",
        "                \"pasta_trabalho\": self.pasta_trabalho,\n",
        "                \"criado_em\": datetime.now().isoformat(),\n",
        "                \"versao\": \"modular_v2.0_otimizado\"\n",
        "            },\n",
        "            \"status_etapas\": {\n",
        "                \"configuracao\": True,\n",
        "                \"descoberta_videos\": False,\n",
        "                \"metadados\": False,\n",
        "                \"decomposicao\": False,\n",
        "                \"analise_padroes\": False,\n",
        "                \"analise_psicologica\": False,\n",
        "                \"relatorios_humanizados\": False,\n",
        "                \"blueprint\": False\n",
        "            }\n",
        "        }\n",
        "\n",
        "        config_path = os.path.join(self.pasta_trabalho, \"config\", \"config.json\")\n",
        "        with open(config_path, \"w\", encoding='utf-8') as f:\n",
        "            json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        return config_path\n",
        "\n",
        "# Executar configura√ß√£o\n",
        "try:\n",
        "    configurador = ConfiguradorProjeto(CAMINHO_PASTA_VIDEOS)\n",
        "    config_path = configurador.salvar_configuracao()\n",
        "\n",
        "    print(\"\"\"\n",
        "‚úÖ CONFIGURA√á√ÉO CONCLU√çDA!\"\"\")\n",
        "    print(f\"Pasta de trabalho criada: {configurador.pasta_trabalho}\")\n",
        "    print(f\"Configura√ß√£o salva: {config_path}\")\n",
        "    print(\"\"\"\n",
        "‚û°Ô∏è PR√ìXIMA C√âLULA: 2.1 - DESCOBERTA E CATALOGA√á√ÉO DE V√çDEOS\"\"\")\n",
        "\n",
        "    # Salvar vari√°veis globais para pr√≥ximas c√©lulas\n",
        "    global PASTA_VIDEOS, PASTA_TRABALHO\n",
        "    PASTA_VIDEOS = configurador.pasta_videos\n",
        "    PASTA_TRABALHO = configurador.pasta_trabalho\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\"\"\n",
        "‚ùå ERRO NA CONFIGURA√á√ÉO: {e}\"\"\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ],
      "metadata": {
        "id": "configuracao_inicial",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "facb8678-79c1-4863-f64b-7929d897b4e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ CONFIGURA√á√ÉO CONCLU√çDA!\n",
            "Pasta de trabalho criada: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa\n",
            "Configura√ß√£o salva: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/config/config.json\n",
            "\n",
            "‚û°Ô∏è PR√ìXIMA C√âLULA: 2.1 - DESCOBERTA E CATALOGA√á√ÉO DE V√çDEOS\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# LAYER 2: DESCOBERTA E EXTRA√á√ÉO DE DADOS BRUTOS\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# C√âLULA 2.1: DESCOBERTA E CATALOGA√á√ÉO DE V√çDEOS\n",
        "# ============================================================================\n",
        "\n",
        "def verificar_prerequisito_etapa(etapa_anterior):\n",
        "    \"\"\"Verifica se a etapa anterior foi executada com sucesso\"\"\"\n",
        "    try:\n",
        "        if not \"PASTA_TRABALHO\" in globals():\n",
        "            raise Exception(\"Vari√°veis globais de configura√ß√£o n√£o encontradas. Execute a C√âLULA 1.2 primeiro.\")\n",
        "\n",
        "        config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "        if not os.path.exists(config_path):\n",
        "            raise Exception(\"Arquivo de configura√ß√£o n√£o encontrado. Execute a C√âLULA 1.2 primeiro.\")\n",
        "\n",
        "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            config = json.load(f)\n",
        "\n",
        "        if not config[\"status_etapas\"][etapa_anterior]:\n",
        "            raise Exception(f\"A etapa \\\"{etapa_anterior}\\\" n√£o foi conclu√≠da. Execute a c√©lula correspondente primeiro.\")\n",
        "\n",
        "        return True, config\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå PR√â-REQUISITO N√ÉO ATENDIDO: {e}\")\n",
        "        return False, None\n",
        "\n",
        "def descobrir_catalogar_videos():\n",
        "    \"\"\"Descobre e cataloga todos os v√≠deos na pasta\"\"\"\n",
        "    formatos_aceitos = [\".mp4\", \".mov\", \".avi\", \".mkv\", \".webm\", \".m4v\"]\n",
        "    videos_encontrados = []\n",
        "\n",
        "    print(f\"üîç Iniciando descoberta de v√≠deos na pasta: {PASTA_VIDEOS}\")\n",
        "\n",
        "    for root, dirs, files in os.walk(PASTA_VIDEOS):\n",
        "        if \"_engenharia_reversa\" in root:\n",
        "            continue # Ignorar a pasta de trabalho do sistema\n",
        "\n",
        "        for file in files:\n",
        "            if any(file.lower().endswith(fmt) for fmt in formatos_aceitos):\n",
        "                video_path = os.path.join(root, file)\n",
        "\n",
        "                try:\n",
        "                    stat_info = os.stat(video_path)\n",
        "                    # Gerar ID baseado no nome do arquivo para melhor rastreamento\n",
        "                    video_name_clean = os.path.splitext(file)[0].replace(\" \", \"_\").replace(\".\", \"\")\n",
        "                    video_id = f\"vid_{video_name_clean}\"\n",
        "\n",
        "                    video_info = {\n",
        "                        \"id\": video_id,\n",
        "                        \"nome_arquivo\": file,\n",
        "                        \"caminho_completo\": video_path,\n",
        "                        \"caminho_relativo\": os.path.relpath(video_path, PASTA_VIDEOS),\n",
        "                        \"tamanho_mb\": round(stat_info.st_size / (1024*1024), 2),\n",
        "                        \"data_modificacao\": datetime.fromtimestamp(stat_info.st_mtime).isoformat(),\n",
        "                        \"extensao\": os.path.splitext(file)[1].lower(),\n",
        "                        \"status\": \"descoberto\"\n",
        "                    }\n",
        "\n",
        "                    videos_encontrados.append(video_info)\n",
        "                    print(f\"  ‚úÖ Encontrado: {file}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"  ‚ùå Erro ao processar {file}: {e}\")\n",
        "                    continue\n",
        "\n",
        "    return videos_encontrados\n",
        "\n",
        "def salvar_lista_videos(videos):\n",
        "    \"\"\"Salva lista de v√≠deos encontrados\"\"\"\n",
        "    videos_path = os.path.join(PASTA_TRABALHO, \"dados\", \"videos_descobertos.json\")\n",
        "    with open(videos_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(videos, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    config[\"status_etapas\"][\"descoberta_videos\"] = True\n",
        "    config[\"total_videos_encontrados\"] = len(videos)\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    return videos_path\n",
        "\n",
        "# Executar descoberta\n",
        "prerequisito_ok, _ = verificar_prerequisito_etapa(\"configuracao\")\n",
        "\n",
        "if prerequisito_ok:\n",
        "    try:\n",
        "        videos_encontrados = descobrir_catalogar_videos()\n",
        "\n",
        "        if not videos_encontrados:\n",
        "            print(\"\"\"\n",
        "‚ùå NENHUM V√çDEO ENCONTRADO!\"\"\")\n",
        "            print(f\"Verifique se h√° v√≠deos na pasta configurada: {PASTA_VIDEOS}\")\n",
        "        else:\n",
        "            videos_path = salvar_lista_videos(videos_encontrados)\n",
        "\n",
        "            print(\"\"\"\n",
        "‚úÖ DESCOBERTA DE V√çDEOS CONCLU√çDA!\"\"\")\n",
        "            print(f\"Total de v√≠deos encontrados: {len(videos_encontrados)}\")\n",
        "            print(f\"Lista de v√≠deos salva em: {videos_path}\")\n",
        "\n",
        "            # Mostrar resumo\n",
        "            extensoes = Counter([v[\"extensao\"] for v in videos_encontrados])\n",
        "            print(f\"Formatos encontrados: {dict(extensoes)}\")\n",
        "            print(\"\"\"\n",
        "‚û°Ô∏è PR√ìXIMA C√âLULA: 2.2 - EXTRA√á√ÉO DE METADADOS DOS V√çDEOS\"\"\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\"\"\n",
        "‚ùå ERRO NA DESCOBERTA DE V√çDEOS: {e}\"\"\")\n",
        "        print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ],
      "metadata": {
        "id": "descoberta_videos",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "235b73b2-addf-4956-ef36-9c6bac844efa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Iniciando descoberta de v√≠deos na pasta: /content/drive/MyDrive/Videos Dona Done\n",
            "  ‚úÖ Encontrado: ate quando voce vai ficar culpando os outros.mp4\n",
            "  ‚úÖ Encontrado: coloque metas em sua vida e se surpreenda.mp4\n",
            "  ‚úÖ Encontrado: a importancia de ser rico antes de ter.mp4\n",
            "\n",
            "‚úÖ DESCOBERTA DE V√çDEOS CONCLU√çDA!\n",
            "Total de v√≠deos encontrados: 3\n",
            "Lista de v√≠deos salva em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/videos_descobertos.json\n",
            "Formatos encontrados: {'.mp4': 3}\n",
            "\n",
            "‚û°Ô∏è PR√ìXIMA C√âLULA: 2.2 - EXTRA√á√ÉO DE METADADOS DOS V√çDEOS\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 2.2: EXTRA√á√ÉO DE METADADOS DOS V√çDEOS\n",
        "# ============================================================================\n",
        "\n",
        "def extrair_metadados_video(video_info):\n",
        "    \"\"\"Extrai metadados t√©cnicos de um v√≠deo\"\"\"\n",
        "    video_path = video_info[\"caminho_completo\"]\n",
        "    video_id = video_info[\"id\"]\n",
        "\n",
        "    print(f\"  ‚öôÔ∏è Extraindo metadados para: {video_info[\"nome_arquivo\"]}\")\n",
        "\n",
        "    # An√°lise com OpenCV\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise Exception(\"N√£o foi poss√≠vel abrir o v√≠deo. Verifique o caminho ou a integridade do arquivo.\")\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    largura = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    altura = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    duracao = frame_count / fps if fps > 0 else 0\n",
        "\n",
        "    # Capturar primeiro frame\n",
        "    ret, primeiro_frame = cap.read()\n",
        "    cap.release()\n",
        "\n",
        "    # An√°lise de √°udio\n",
        "    try:\n",
        "        clip = VideoFileClip(video_path)\n",
        "        tem_audio = clip.audio is not None\n",
        "        clip.close()\n",
        "    except Exception as e:\n",
        "        print(f\"    ‚ö†Ô∏è Aviso: N√£o foi poss√≠vel analisar √°udio para {video_info[\"nome_arquivo\"]}: {e}\")\n",
        "        tem_audio = False\n",
        "\n",
        "    # An√°lise do primeiro frame\n",
        "    analise_frame = {}\n",
        "    if ret:\n",
        "        # Salvar primeiro frame na pasta 'capturas'\n",
        "        capturas_dir = os.path.join(PASTA_TRABALHO, \"capturas\")\n",
        "        frame_path = os.path.join(capturas_dir, f\"{video_id}_primeiro_frame.jpg\")\n",
        "        cv2.imwrite(frame_path, primeiro_frame)\n",
        "\n",
        "        # An√°lises do frame\n",
        "        gray = cv2.cvtColor(primeiro_frame, cv2.COLOR_BGR2GRAY)\n",
        "        complexidade = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "        brilho = np.mean(gray)\n",
        "\n",
        "        analise_frame = {\n",
        "            \"path\": frame_path,\n",
        "            \"complexidade_visual\": float(complexidade),\n",
        "            \"brilho_medio\": float(brilho),\n",
        "            \"tem_muito_texto\": bool(complexidade > 500),\n",
        "            \"e_escuro\": bool(brilho < 100),\n",
        "            \"e_claro\": bool(brilho > 200)\n",
        "        }\n",
        "\n",
        "    # Detectar formato\n",
        "    ratio = largura / altura if altura > 0 else 0\n",
        "    if 0.5 <= ratio <= 0.6:\n",
        "        formato = \"vertical_9_16\" if altura > largura * 1.5 else \"vertical_4_5\"\n",
        "    elif 0.8 <= ratio <= 1.2:\n",
        "        formato = \"quadrado_1_1\"\n",
        "    elif ratio >= 1.3:\n",
        "        formato = \"horizontal_16_9\"\n",
        "    else:\n",
        "        formato = \"personalizado\"\n",
        "\n",
        "    # Compilar metadados - converter todos os valores para tipos b√°sicos Python\n",
        "    metadados = {\n",
        "        **video_info,\n",
        "        \"duracao_segundos\": float(duracao),\n",
        "        \"fps\": float(fps),\n",
        "        \"largura\": int(largura),\n",
        "        \"altura\": int(altura),\n",
        "        \"resolucao\": f\"{largura}x{altura}\",\n",
        "        \"aspect_ratio\": float(ratio),\n",
        "        \"total_frames\": int(frame_count),\n",
        "        \"tem_audio\": bool(tem_audio),\n",
        "        \"formato_detectado\": str(formato),\n",
        "        \"primeiro_frame\": analise_frame,\n",
        "        \"data_analise\": datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    return metadados\n",
        "\n",
        "def processar_metadados_todos_videos():\n",
        "    \"\"\"Processa metadados de todos os v√≠deos\"\"\"\n",
        "    # Carregar lista de v√≠deos\n",
        "    videos_path = os.path.join(PASTA_TRABALHO, \"dados\", \"videos_descobertos.json\")\n",
        "    with open(videos_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        videos_lista = json.load(f)\n",
        "\n",
        "    metadados_completos = []\n",
        "    sucessos = 0\n",
        "\n",
        "    print(f\"Processando metadados de {len(videos_lista)} v√≠deos...\")\n",
        "\n",
        "    for i, video in enumerate(videos_lista, 1):\n",
        "        print(f\"[{i}/{len(videos_lista)}] Analisando {video[\"nome_arquivo\"]}\")\n",
        "\n",
        "        try:\n",
        "            metadados = extrair_metadados_video(video)\n",
        "            metadados[\"status\"] = \"metadados_extraidos\"\n",
        "            metadados_completos.append(metadados)\n",
        "            sucessos += 1\n",
        "            print(f\"  ‚úÖ Metadados extra√≠dos: {metadados[\"duracao_segundos\"]:.1f}s | {metadados[\"formato_detectado\"]} | √Åudio: {\"Sim\" if metadados[\"tem_audio\"] else \"N√£o\"}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå ERRO ao extrair metadados para {video[\"nome_arquivo\"]}: {e}\")\n",
        "            video[\"status\"] = \"erro_metadados\"\n",
        "            metadados_completos.append(video) # Adiciona o v√≠deo com status de erro\n",
        "\n",
        "    # Salvar metadados completos\n",
        "    metadados_json_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "    with open(metadados_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(metadados_completos, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Salvar em Excel\n",
        "    df_metadados = pd.DataFrame(metadados_completos)\n",
        "    metadados_excel_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_videos.xlsx\")\n",
        "    df_metadados.to_excel(metadados_excel_path, index=False, engine='openpyxl')\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    config[\"status_etapas\"][\"metadados\"] = True\n",
        "    config[\"total_videos_metadados\"] = sucessos\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\nüíæ Metadados completos salvos em: {metadados_json_path}\")\n",
        "    print(f\"üíæ Metadados em Excel salvos em: {metadados_excel_path}\")\n",
        "\n",
        "    print(\"\\n‚úÖ EXTRA√á√ÉO DE METADADOS CONCLU√çDA!\")\n",
        "    print(f\"Total de v√≠deos com metadados extra√≠dos: {sucessos}\")\n",
        "\n",
        "    # Mostrar resumo\n",
        "    if not df_metadados.empty:\n",
        "        print(\"\\nüìä Resumo dos Metadados:\")\n",
        "        print(f\"  - Formatos detectados: {dict(df_metadados['formato_detectado'].value_counts())}\")\n",
        "        print(f\"  - Dura√ß√£o m√©dia dos v√≠deos: {df_metadados['duracao_segundos'].mean():.2f}s\")\n",
        "        print(f\"  - V√≠deos com √°udio: {df_metadados['tem_audio'].sum()}\")\n",
        "\n",
        "    print(\"\\n‚û°Ô∏è PR√ìXIMA C√âLULA: 2.3 - DECOMPOSI√á√ÉO DE V√çDEOS (FRAMES, √ÅUDIO, TEXTO)\")\n",
        "\n",
        "# Executar extra√ß√£o de metadados\n",
        "prerequisito_ok, _ = verificar_prerequisito_etapa(\"descoberta_videos\")\n",
        "\n",
        "if prerequisito_ok:\n",
        "    try:\n",
        "        processar_metadados_todos_videos()\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå ERRO NA EXTRA√á√ÉO DE METADADOS: {e}\")\n",
        "        print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ],
      "metadata": {
        "id": "extracao_metadados",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ada03b73-3415-4a75-d89c-be98e878f877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processando metadados de 3 v√≠deos...\n",
            "[1/3] Analisando ate quando voce vai ficar culpando os outros.mp4\n",
            "  ‚öôÔ∏è Extraindo metadados para: ate quando voce vai ficar culpando os outros.mp4\n",
            "  ‚úÖ Metadados extra√≠dos: 18.6s | vertical_9_16 | √Åudio: Sim\n",
            "[2/3] Analisando coloque metas em sua vida e se surpreenda.mp4\n",
            "  ‚öôÔ∏è Extraindo metadados para: coloque metas em sua vida e se surpreenda.mp4\n",
            "  ‚úÖ Metadados extra√≠dos: 15.8s | vertical_9_16 | √Åudio: Sim\n",
            "[3/3] Analisando a importancia de ser rico antes de ter.mp4\n",
            "  ‚öôÔ∏è Extraindo metadados para: a importancia de ser rico antes de ter.mp4\n",
            "  ‚úÖ Metadados extra√≠dos: 19.0s | vertical_9_16 | √Åudio: Sim\n",
            "\n",
            "üíæ Metadados completos salvos em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/metadados_completos.json\n",
            "üíæ Metadados em Excel salvos em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/metadados_videos.xlsx\n",
            "\n",
            "‚úÖ EXTRA√á√ÉO DE METADADOS CONCLU√çDA!\n",
            "Total de v√≠deos com metadados extra√≠dos: 3\n",
            "\n",
            "üìä Resumo dos Metadados:\n",
            "  - Formatos detectados: {'vertical_9_16': np.int64(3)}\n",
            "  - Dura√ß√£o m√©dia dos v√≠deos: 17.80s\n",
            "  - V√≠deos com √°udio: 3\n",
            "\n",
            "‚û°Ô∏è PR√ìXIMA C√âLULA: 2.3 - DECOMPOSI√á√ÉO DE V√çDEOS (FRAMES, √ÅUDIO, TEXTO)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 2.3: DECOMPOSI√á√ÉO DE V√çDEOS (FRAMES, √ÅUDIO, TEXTO)\n",
        "# ============================================================================\n",
        "\n",
        "def decompor_video(video_info):\n",
        "    \"\"\"Decomp√µe um v√≠deo em frames, √°udio e texto (OCR e transcri√ß√£o)\"\"\"\n",
        "    video_path = video_info[\"caminho_completo\"]\n",
        "    video_id = video_info[\"id\"]\n",
        "    pasta_video_frames = os.path.join(PASTA_TRABALHO, \"frames_extraidos\", video_id)\n",
        "    os.makedirs(pasta_video_frames, exist_ok=True)\n",
        "\n",
        "    print(f\"  ‚öôÔ∏è Decompondo v√≠deo: {video_info[\"nome_arquivo\"]}\")\n",
        "\n",
        "    decomposicao_data = {\n",
        "        \"video_id\": video_id,\n",
        "        \"frames_extraidos\": [],\n",
        "        \"textos_ocr\": [],\n",
        "        \"audio_transcrito\": \"\",\n",
        "        \"audio_analise\": {}\n",
        "    }\n",
        "\n",
        "    # Extra√ß√£o de Frames e OCR\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        frame_count = 0\n",
        "        frame_interval = int(fps) # 1 frame por segundo\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            if frame_count % frame_interval == 0:\n",
        "                frame_time_sec = frame_count / fps\n",
        "                frame_filename = os.path.join(pasta_video_frames, f\"frame_{int(frame_time_sec):06d}.jpg\")\n",
        "                cv2.imwrite(frame_filename, frame)\n",
        "                decomposicao_data[\"frames_extraidos\"] .append({\n",
        "                    \"path\": frame_filename,\n",
        "                    \"timestamp_sec\": frame_time_sec\n",
        "                })\n",
        "\n",
        "                # OCR\n",
        "                try:\n",
        "                    text = pytesseract.image_to_string(Image.fromarray(frame), lang=\"por\")\n",
        "                    if text.strip():\n",
        "                        decomposicao_data[\"textos_ocr\"] .append({\n",
        "                            \"timestamp_sec\": frame_time_sec,\n",
        "                            \"text\": text.strip()\n",
        "                        })\n",
        "                except Exception as ocr_e:\n",
        "                    print(f\"    ‚ö†Ô∏è Aviso: Erro no OCR para frame {frame_time_sec}s: {ocr_e}\")\n",
        "\n",
        "            frame_count += 1\n",
        "        cap.release()\n",
        "        print(f\"    ‚úÖ {len(decomposicao_data[\"frames_extraidos\"])} frames extra√≠dos para {video_info[\"nome_arquivo\"]}\")\n",
        "        print(f\"    ‚úÖ {len(decomposicao_data[\"textos_ocr\"])} textos encontrados via OCR para {video_info[\"nome_arquivo\"]}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ‚ùå Erro na extra√ß√£o de frames/OCR para {video_info[\"nome_arquivo\"]}: {e}\")\n",
        "\n",
        "    # Extra√ß√£o e Transcri√ß√£o de √Åudio\n",
        "    audio_path = os.path.join(PASTA_TRABALHO, \"temp\", f\"{video_id}.wav\")\n",
        "    try:\n",
        "        video_clip = VideoFileClip(video_path)\n",
        "        if video_clip.audio:\n",
        "            video_clip.audio.write_audiofile(audio_path, verbose=False, logger=None)\n",
        "            print(f\"    ‚úÖ √Åudio extra√≠do para {video_info[\"nome_arquivo\"]}\")\n",
        "\n",
        "            # Transcri√ß√£o\n",
        "            r = sr.Recognizer()\n",
        "            with sr.AudioFile(audio_path) as source:\n",
        "                audio_listened = r.record(source)\n",
        "                try:\n",
        "                    text = r.recognize_google(audio_listened, language=\"pt-BR\")\n",
        "                    decomposicao_data[\"audio_transcrito\"] = text\n",
        "                    print(f\"    ‚úÖ √Åudio transcrito para {video_info[\"nome_arquivo\"]}\")\n",
        "                except sr.UnknownValueError:\n",
        "                    print(f\"    ‚ö†Ô∏è Aviso: N√£o foi poss√≠vel transcrever o √°udio para {video_info[\"nome_arquivo\"]}. Fala inintelig√≠vel.\")\n",
        "                except sr.RequestError as req_e:\n",
        "                    print(f\"    ‚ö†Ô∏è Aviso: Erro no servi√ßo de transcri√ß√£o para {video_info[\"nome_arquivo\"]}: {req_e}\")\n",
        "\n",
        "            # An√°lise de √Åudio (Librosa)\n",
        "            y, sr_audio = librosa.load(audio_path)\n",
        "            tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr_audio)\n",
        "            decomposicao_data[\"audio_analise\"] = {\n",
        "                \"bpm\": float(tempo),\n",
        "                \"duracao_audio_segundos\": float(librosa.get_duration(y=y, sr=sr_audio))\n",
        "            }\n",
        "\n",
        "        else:\n",
        "            print(f\"    ‚ö†Ô∏è Aviso: V√≠deo {video_info[\"nome_arquivo\"]} n√£o possui trilha de √°udio.\")\n",
        "        video_clip.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ‚ùå Erro na extra√ß√£o/transcri√ß√£o de √°udio para {video_info[\"nome_arquivo\"]}: {e}\")\n",
        "\n",
        "    # Detec√ß√£o de Cortes (Scene Change Detection)\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            raise Exception(\"N√£o foi poss√≠vel abrir o v√≠deo para detec√ß√£o de cortes.\")\n",
        "\n",
        "        prev_frame = None\n",
        "        cuts = []\n",
        "        frame_idx = 0\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            if prev_frame is not None:\n",
        "                diff = cv2.absdiff(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY), cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY))\n",
        "                non_zero_count = np.count_nonzero(diff)\n",
        "                if non_zero_count > (frame.shape[0] * frame.shape[1] * 0.3): # Limiar de 30% de mudan√ßa\n",
        "                    cuts.append(frame_idx / fps)\n",
        "            prev_frame = frame\n",
        "            frame_idx += 1\n",
        "        cap.release()\n",
        "        decomposicao_data[\"cortes_detectados_segundos\"] = cuts\n",
        "        print(f\"    ‚úÖ {len(cuts)} cortes detectados para {video_info[\"nome_arquivo\"]}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ‚ùå Erro na detec√ß√£o de cortes para {video_info[\"nome_arquivo\"]}: {e}\")\n",
        "\n",
        "    return decomposicao_data\n",
        "\n",
        "def processar_decomposicao_todos_videos():\n",
        "    \"\"\"Processa a decomposi√ß√£o de todos os v√≠deos\"\"\"\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa(\"metadados\")\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Carregar metadados completos\n",
        "    metadados_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "    with open(metadados_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        videos_com_metadados = json.load(f)\n",
        "\n",
        "    decomposicoes_completas = []\n",
        "    sucessos = 0\n",
        "\n",
        "    print(\"\"\"\n",
        "Iniciando decomposi√ß√£o para {} v√≠deos...\"\"\".format(len(videos_com_metadados)))\n",
        "\n",
        "    for i, video in enumerate(videos_com_metadados, 1):\n",
        "        if video.get(\"status\") == \"metadados_extraidos\":\n",
        "            print(f\"[{i}/{len(videos_com_metadados)}] Decompondo {video[\"nome_arquivo\"]}\")\n",
        "            try:\n",
        "                decomposicao = decompor_video(video)\n",
        "                decomposicao[\"status\"] = \"decomposto\"\n",
        "                decomposicoes_completas.append(decomposicao)\n",
        "                sucessos += 1\n",
        "                print(f\"  ‚úÖ Decomposi√ß√£o conclu√≠da para {video[\"nome_arquivo\"]}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ùå ERRO na decomposi√ß√£o para {video[\"nome_arquivo\"]}: {e}\")\n",
        "                decomposicoes_completas.append({\"video_id\": video[\"id\"], \"status\": \"erro_decomposicao\", \"erro\": str(e)})\n",
        "        else:\n",
        "            print(f\"[{i}/{len(videos_com_metadados)}] Pulando {video.get(\"nome_arquivo\", video[\"id\"])} - Status: {video.get(\"status\", \"N/A\")}\")\n",
        "            decomposicoes_completas.append({\"video_id\": video[\"id\"], \"status\": video.get(\"status\", \"N/A\"), \"erro\": \"Pulado devido a erro anterior\"})\n",
        "\n",
        "    # Salvar decomposi√ß√µes completas\n",
        "    decomposicao_json_path = os.path.join(PASTA_TRABALHO, \"dados\", \"decomposicao_completa.json\")\n",
        "    with open(decomposicao_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(decomposicoes_completas, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    config[\"status_etapas\"][\"decomposicao\"] = True\n",
        "    config[\"total_videos_decompostos\"] = sucessos\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\"\"\n",
        "üíæ Dados de decomposi√ß√£o salvos em: {decomposicao_json_path}\"\"\")\n",
        "\n",
        "    print(\"\"\"\n",
        "‚úÖ DECOMPOSI√á√ÉO DE V√çDEOS CONCLU√çDA!\"\"\")\n",
        "    print(f\"Total de v√≠deos decompostos com sucesso: {sucessos}\")\n",
        "\n",
        "    if sucessos == 0:\n",
        "        print(\"‚ùå NENHUM V√çDEO FOI DECOMPOSTO COM SUCESSO. Verifique as etapas anteriores.\")\n",
        "    print(\"\"\"\n",
        "‚û°Ô∏è PR√ìXIMA C√âLULA: 3.1 - AN√ÅLISE DE PADR√ïES (TEMPORAIS, VISUAIS, TEXTO, √ÅUDIO)\"\"\")\n",
        "\n",
        "# Executar decomposi√ß√£o\n",
        "try:\n",
        "    processar_decomposicao_todos_videos()\n",
        "except Exception as e:\n",
        "    print(f\"\"\"\n",
        "‚ùå ERRO GERAL NA DECOMPOSI√á√ÉO DE V√çDEOS: {e}\"\"\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ],
      "metadata": {
        "id": "decomposicao_videos",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "4570d507-8959-4de8-8a70-7cc474a088f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando decomposi√ß√£o para 3 v√≠deos...\n",
            "[1/3] Decompondo ate quando voce vai ficar culpando os outros.mp4\n",
            "  ‚öôÔ∏è Decompondo v√≠deo: ate quando voce vai ficar culpando os outros.mp4\n",
            "    ‚úÖ 19 frames extra√≠dos para ate quando voce vai ficar culpando os outros.mp4\n",
            "    ‚úÖ 5 textos encontrados via OCR para ate quando voce vai ficar culpando os outros.mp4\n",
            "    ‚úÖ √Åudio extra√≠do para ate quando voce vai ficar culpando os outros.mp4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3406907351.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;31m# Executar decomposi√ß√£o\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m     \u001b[0mprocessar_decomposicao_todos_videos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     print(f\"\"\"\n",
            "\u001b[0;32m/tmp/ipython-input-3406907351.py\u001b[0m in \u001b[0;36mprocessar_decomposicao_todos_videos\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[{i}/{len(videos_com_metadados)}] Decompondo {video[\"\u001b[0m\u001b[0mnome_arquivo\u001b[0m\u001b[0;34m\"]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                 \u001b[0mdecomposicao\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecompor_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m                 \u001b[0mdecomposicao\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"status\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"decomposto\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0mdecomposicoes_completas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecomposicao\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3406907351.py\u001b[0m in \u001b[0;36mdecompor_video\u001b[0;34m(video_info)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0maudio_listened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecognize_google\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_listened\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt-BR\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m                     \u001b[0mdecomposicao_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"audio_transcrito\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"    ‚úÖ √Åudio transcrito para {video_info[\"\u001b[0m\u001b[0mnome_arquivo\u001b[0m\u001b[0;34m\"]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/speech_recognition/recognizers/google.py\u001b[0m in \u001b[0;36mrecognize_legacy\u001b[0;34m(recognizer, audio_data, key, language, pfilter, show_all, with_confidence, endpoint)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m     response_text = obtain_transcription(\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecognizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperation_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/speech_recognition/recognizers/google.py\u001b[0m in \u001b[0;36mobtain_transcription\u001b[0;34m(request, timeout)\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0;34m\"recognition connection failed: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         )\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36m_read_chunked\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_chunk_left\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mchunk_left\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                     \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36m_get_chunk_left\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    577\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# toss the CRLF at the end of the chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m                 \u001b[0mchunk_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_next_chunk_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36m_read_next_chunk_size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_next_chunk_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;31m# Read the next chunk size from the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"chunk size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "melhorar os cortes aqui ( otimizar ele esta detectando muitos cortes. corrigir possivel erro de Fala inintelig√≠vel"
      ],
      "metadata": {
        "id": "AquW8stD8E1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 2.4: AN√ÅLISE DE √ÅUDIO REFINADA (SUBLAYER DA LAYER 2)\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# C√âLULA 2.4: AN√ÅLISE DE √ÅUDIO REFINADA (SUBLAYER DA LAYER 2)\n",
        "# ============================================================================\n",
        "\n",
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.signal\n",
        "from scipy.stats import variation\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def converter_para_json_serializable(obj):\n",
        "    \"\"\"Converte tipos NumPy para tipos Python nativos para serializa√ß√£o JSON\"\"\"\n",
        "    if isinstance(obj, np.integer):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, np.floating):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return [converter_para_json_serializable(x) for x in obj.tolist()]\n",
        "    elif isinstance(obj, list):\n",
        "        return [converter_para_json_serializable(x) for x in obj]\n",
        "    elif isinstance(obj, dict):\n",
        "        return {k: converter_para_json_serializable(v) for k, v in obj.items()}\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "def verificar_prerequisito_audio_refinado():\n",
        "    \"\"\"Verifica se a etapa de decomposi√ß√£o foi conclu√≠da\"\"\"\n",
        "    try:\n",
        "        if not \"PASTA_TRABALHO\" in globals():\n",
        "            raise Exception(\"Vari√°veis globais de configura√ß√£o n√£o encontradas. Execute a C√âLULA 1.2 primeiro.\")\n",
        "\n",
        "        config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "        if not os.path.exists(config_path):\n",
        "            raise Exception(\"Arquivo de configura√ß√£o n√£o encontrado. Execute as c√©lulas anteriores.\")\n",
        "\n",
        "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            config = json.load(f)\n",
        "\n",
        "        if not config[\"status_etapas\"][\"decomposicao\"]:\n",
        "            raise Exception(\"A etapa 'decomposicao' n√£o foi conclu√≠da. Execute a C√âLULA 2.3 primeiro.\")\n",
        "\n",
        "        return True, config\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå PR√â-REQUISITO N√ÉO ATENDIDO: {e}\")\n",
        "        return False, None\n",
        "\n",
        "def analisar_variacao_volume(audio_path, sr=22050):\n",
        "    \"\"\"Analisa varia√ß√µes de volume da voz\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "        # Calcular RMS (Root Mean Square) em janelas\n",
        "        frame_length = int(0.1 * sr)  # Janelas de 100ms\n",
        "        hop_length = frame_length // 4\n",
        "        rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]\n",
        "\n",
        "        # Detectar varia√ß√µes bruscas\n",
        "        rms_db = librosa.amplitude_to_db(rms)\n",
        "        variacao_volume = np.diff(rms_db)\n",
        "\n",
        "        # Identificar picos de varia√ß√£o\n",
        "        threshold_variacao = np.std(variacao_volume) * 2\n",
        "        picos_variacao = np.where(np.abs(variacao_volume) > threshold_variacao)[0]\n",
        "\n",
        "        # Converter √≠ndices para timestamps\n",
        "        times = librosa.frames_to_time(picos_variacao, sr=sr, hop_length=hop_length)\n",
        "\n",
        "        return {\n",
        "            \"rms_medio\": float(np.mean(rms)),\n",
        "            \"variacao_volume_coef\": float(variation(rms)),\n",
        "            \"num_picos_variacao\": int(len(picos_variacao)),\n",
        "            \"timestamps_picos\": [float(t) for t in times.tolist()],\n",
        "            \"volume_db_medio\": float(np.mean(rms_db)),\n",
        "            \"volume_db_std\": float(np.std(rms_db))\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    ‚ö†Ô∏è Erro na an√°lise de varia√ß√£o de volume: {e}\")\n",
        "        return {}\n",
        "\n",
        "def detectar_picos_ruido(audio_path, sr=22050):\n",
        "    \"\"\"Detecta picos de ru√≠do excessivo\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "        # Calcular espectrograma\n",
        "        S = librosa.stft(y)\n",
        "        S_db = librosa.amplitude_to_db(np.abs(S))\n",
        "\n",
        "        # Detectar ru√≠do baseado em frequ√™ncias altas\n",
        "        freq_bins = librosa.fft_frequencies(sr=sr)\n",
        "        high_freq_mask = freq_bins > 4000  # Frequ√™ncias acima de 4kHz\n",
        "\n",
        "        high_freq_energy = np.mean(S_db[high_freq_mask], axis=0)\n",
        "\n",
        "        # Identificar segmentos com ru√≠do excessivo\n",
        "        threshold_ruido = np.percentile(high_freq_energy, 85)\n",
        "        segmentos_ruidosos = np.where(high_freq_energy > threshold_ruido)[0]\n",
        "\n",
        "        # Converter para timestamps\n",
        "        hop_length = 512\n",
        "        times_ruido = librosa.frames_to_time(segmentos_ruidosos, sr=sr, hop_length=hop_length)\n",
        "\n",
        "        return {\n",
        "            \"energia_alta_freq_media\": float(np.mean(high_freq_energy)),\n",
        "            \"threshold_ruido\": float(threshold_ruido),\n",
        "            \"num_segmentos_ruidosos\": int(len(segmentos_ruidosos)),\n",
        "            \"timestamps_ruido\": [float(t) for t in times_ruido.tolist()],\n",
        "            \"percentual_audio_ruidoso\": float(len(segmentos_ruidosos) / len(high_freq_energy) * 100)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    ‚ö†Ô∏è Erro na detec√ß√£o de picos de ru√≠do: {e}\")\n",
        "        return {}\n",
        "\n",
        "def analisar_ritmo_fala(transcricao_texto, duracao_audio):\n",
        "    \"\"\"Calcula ritmo da fala em palavras por minuto\"\"\"\n",
        "    try:\n",
        "        if not transcricao_texto or duracao_audio <= 0:\n",
        "            return {}\n",
        "\n",
        "        palavras = transcricao_texto.split()\n",
        "        num_palavras = len(palavras)\n",
        "        duracao_minutos = duracao_audio / 60.0\n",
        "\n",
        "        palavras_por_minuto = num_palavras / duracao_minutos\n",
        "\n",
        "        # Classificar ritmo\n",
        "        if palavras_por_minuto < 120:\n",
        "            classificacao_ritmo = \"Lento\"\n",
        "        elif palavras_por_minuto < 160:\n",
        "            classificacao_ritmo = \"Normal\"\n",
        "        elif palavras_por_minuto < 200:\n",
        "            classificacao_ritmo = \"R√°pido\"\n",
        "        else:\n",
        "            classificacao_ritmo = \"Muito R√°pido\"\n",
        "\n",
        "        return {\n",
        "            \"palavras_por_minuto\": float(palavras_por_minuto),\n",
        "            \"total_palavras\": int(num_palavras),\n",
        "            \"duracao_minutos\": float(duracao_minutos),\n",
        "            \"classificacao_ritmo\": str(classificacao_ritmo),\n",
        "            \"densidade_informacional\": float(num_palavras / duracao_audio)  # palavras por segundo\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    ‚ö†Ô∏è Erro na an√°lise de ritmo de fala: {e}\")\n",
        "        return {}\n",
        "\n",
        "def identificar_pausas_fala(audio_path, sr=22050):\n",
        "    \"\"\"Identifica pausas e sil√™ncios na fala\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "        # Detectar segmentos de fala vs sil√™ncio\n",
        "        frame_length = int(0.025 * sr)  # 25ms frames\n",
        "        hop_length = frame_length // 2\n",
        "\n",
        "        # Energia RMS para detectar atividade vocal\n",
        "        rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]\n",
        "\n",
        "        # Threshold para distinguir fala de sil√™ncio\n",
        "        threshold_silencio = np.percentile(rms, 20)  # 20% mais baixo = sil√™ncio\n",
        "\n",
        "        # Identificar segmentos de sil√™ncio\n",
        "        is_silence = rms < threshold_silencio\n",
        "\n",
        "        # Encontrar in√≠cio e fim das pausas\n",
        "        pausas = []\n",
        "        in_pause = False\n",
        "        pause_start = 0\n",
        "\n",
        "        times = librosa.frames_to_time(range(len(is_silence)), sr=sr, hop_length=hop_length)\n",
        "\n",
        "        for i, silent in enumerate(is_silence):\n",
        "            if silent and not in_pause:\n",
        "                in_pause = True\n",
        "                pause_start = times[i]\n",
        "            elif not silent and in_pause:\n",
        "                in_pause = False\n",
        "                pause_duration = times[i] - pause_start\n",
        "                if pause_duration > 0.2:  # Pausas maiores que 200ms\n",
        "                    pausas.append({\n",
        "                        \"inicio\": float(pause_start),\n",
        "                        \"fim\": float(times[i]),\n",
        "                        \"duracao\": float(pause_duration)\n",
        "                    })\n",
        "\n",
        "        # Estat√≠sticas das pausas\n",
        "        if pausas:\n",
        "            duracoes_pausas = [p[\"duracao\"] for p in pausas]\n",
        "            pausa_media = np.mean(duracoes_pausas)\n",
        "            pausa_total = sum(duracoes_pausas)\n",
        "        else:\n",
        "            pausa_media = 0\n",
        "            pausa_total = 0\n",
        "\n",
        "        return {\n",
        "            \"num_pausas\": int(len(pausas)),\n",
        "            \"pausas_detectadas\": pausas,\n",
        "            \"duracao_pausa_media\": float(pausa_media),\n",
        "            \"tempo_total_pausas\": float(pausa_total),\n",
        "            \"percentual_pausas\": float(pausa_total / len(y) * sr * 100),\n",
        "            \"threshold_silencio\": float(threshold_silencio)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    ‚ö†Ô∏è Erro na identifica√ß√£o de pausas: {e}\")\n",
        "        return {}\n",
        "\n",
        "def classificar_musica_fundo(audio_path, sr=22050):\n",
        "    \"\"\"Classifica caracter√≠sticas da m√∫sica de fundo\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "        # An√°lise de caracter√≠sticas musicais\n",
        "        tempo, beats = librosa.beat.beat_track(y=y, sr=sr)\n",
        "\n",
        "        # An√°lise espectral\n",
        "        spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
        "        spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]\n",
        "        mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
        "\n",
        "        # Energia\n",
        "        energia_total = np.sum(y**2)\n",
        "        energia_normalizada = energia_total / len(y)\n",
        "\n",
        "        # Classifica√ß√£o por energia\n",
        "        if energia_normalizada > 0.01:\n",
        "            nivel_energia = \"Alta\"\n",
        "        elif energia_normalizada > 0.001:\n",
        "            nivel_energia = \"M√©dia\"\n",
        "        else:\n",
        "            nivel_energia = \"Baixa\"\n",
        "\n",
        "        # Classifica√ß√£o por caracter√≠sticas espectrais\n",
        "        centroide_medio = np.mean(spectral_centroids)\n",
        "        if centroide_medio > 3000:\n",
        "            brilho = \"Brilhante\"\n",
        "        elif centroide_medio > 1500:\n",
        "            brilho = \"Equilibrado\"\n",
        "        else:\n",
        "            brilho = \"Escuro\"\n",
        "\n",
        "        return {\n",
        "            \"tempo_bpm\": float(tempo),\n",
        "            \"num_beats\": int(len(beats)),\n",
        "            \"energia_nivel\": str(nivel_energia),\n",
        "            \"energia_valor\": float(energia_normalizada),\n",
        "            \"brilho_espectral\": str(brilho),\n",
        "            \"centroide_espectral_medio\": float(centroide_medio),\n",
        "            \"rolloff_medio\": float(np.mean(spectral_rolloff)),\n",
        "            \"mfcc_features\": [float(x) for x in mfcc.mean(axis=1).tolist()]\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    ‚ö†Ô∏è Erro na classifica√ß√£o de m√∫sica de fundo: {e}\")\n",
        "        return {}\n",
        "\n",
        "def analisar_clareza_voz(audio_path, sr=22050):\n",
        "    \"\"\"Analisa clareza e inteligibilidade da voz\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "        # Faixa de frequ√™ncia da voz humana (aproximadamente 85-255 Hz para fundamental)\n",
        "        # e harm√¥nicos at√© ~4000 Hz para inteligibilidade\n",
        "\n",
        "        # An√°lise espectral\n",
        "        S = librosa.stft(y)\n",
        "        frequencies = librosa.fft_frequencies(sr=sr)\n",
        "\n",
        "        # Energia em diferentes bandas de frequ√™ncia\n",
        "        baixa_freq = (frequencies >= 85) & (frequencies <= 255)    # Fundamental da voz\n",
        "        media_freq = (frequencies > 255) & (frequencies <= 2000)   # Formantes principais\n",
        "        alta_freq = (frequencies > 2000) & (frequencies <= 4000)   # Clareza/inteligibilidade\n",
        "\n",
        "        energia_baixa = np.mean(np.abs(S[baixa_freq]))\n",
        "        energia_media = np.mean(np.abs(S[media_freq]))\n",
        "        energia_alta = np.mean(np.abs(S[alta_freq]))\n",
        "\n",
        "        # Raz√£o harm√¥nica para ru√≠do (aproxima√ß√£o)\n",
        "        spectral_flatness = librosa.feature.spectral_flatness(y=y)[0]\n",
        "        clareza_media = 1 - np.mean(spectral_flatness)  # Menor flatness = mais harm√¥nica\n",
        "\n",
        "        # Zero crossing rate (indicador de fric√ß√£o/clareza)\n",
        "        zcr = librosa.feature.zero_crossing_rate(y)[0]\n",
        "        zcr_medio = np.mean(zcr)\n",
        "\n",
        "        # Score de clareza combinado\n",
        "        score_clareza = (energia_media + energia_alta) / (energia_baixa + 0.001) * clareza_media\n",
        "\n",
        "        if score_clareza > 10:\n",
        "            classificacao_clareza = \"Excelente\"\n",
        "        elif score_clareza > 5:\n",
        "            classificacao_clareza = \"Boa\"\n",
        "        elif score_clareza > 2:\n",
        "            classificacao_clareza = \"Regular\"\n",
        "        else:\n",
        "            classificacao_clareza = \"Precisa Melhoria\"\n",
        "\n",
        "        return {\n",
        "            \"score_clareza\": float(score_clareza),\n",
        "            \"classificacao_clareza\": classificacao_clareza,\n",
        "            \"energia_fundamental\": float(energia_baixa),\n",
        "            \"energia_formantes\": float(energia_media),\n",
        "            \"energia_agudos\": float(energia_alta),\n",
        "            \"harmonicidade\": float(clareza_media),\n",
        "            \"zero_crossing_rate\": float(zcr_medio)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    ‚ö†Ô∏è Erro na an√°lise de clareza de voz: {e}\")\n",
        "        return {}\n",
        "\n",
        "def detectar_sobreposicao_audio(audio_path, sr=22050):\n",
        "    \"\"\"Detecta sobreposi√ß√£o entre fala e m√∫sica/efeitos\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "        # Separa√ß√£o harm√¥nica/percussiva (aproxima√ß√£o para voz vs m√∫sica)\n",
        "        y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
        "\n",
        "        # An√°lise de energia em cada componente\n",
        "        energia_harmonica = librosa.feature.rms(y=y_harmonic)[0]\n",
        "        energia_percussiva = librosa.feature.rms(y=y_percussive)[0]\n",
        "        energia_total = librosa.feature.rms(y=y)[0]\n",
        "\n",
        "        # Detectar momentos de sobreposi√ß√£o\n",
        "        threshold_sobreposicao = 0.7  # Threshold para detectar sobreposi√ß√£o significativa\n",
        "\n",
        "        # Raz√£o entre componentes\n",
        "        razao_hp = energia_harmonica / (energia_percussiva + 0.001)\n",
        "\n",
        "        # Momentos onde h√° competi√ß√£o (energia similar em ambos)\n",
        "        competicao_mask = (energia_harmonica > threshold_sobreposicao * np.max(energia_harmonica)) & \\\n",
        "                         (energia_percussiva > threshold_sobreposicao * np.max(energia_percussiva))\n",
        "\n",
        "        segmentos_sobreposicao = np.where(competicao_mask)[0]\n",
        "\n",
        "        # Converter para timestamps\n",
        "        hop_length = 512\n",
        "        times_sobreposicao = librosa.frames_to_time(segmentos_sobreposicao, sr=sr, hop_length=hop_length)\n",
        "\n",
        "        return {\n",
        "            \"num_sobreposicoes\": int(len(segmentos_sobreposicao)),\n",
        "            \"timestamps_sobreposicao\": [float(t) for t in times_sobreposicao.tolist()],\n",
        "            \"percentual_sobreposicao\": float(len(segmentos_sobreposicao) / len(energia_total) * 100),\n",
        "            \"energia_harmonica_media\": float(np.mean(energia_harmonica)),\n",
        "            \"energia_percussiva_media\": float(np.mean(energia_percussiva)),\n",
        "            \"razao_harmonico_percussivo\": float(np.mean(razao_hp))\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    ‚ö†Ô∏è Erro na detec√ß√£o de sobreposi√ß√£o: {e}\")\n",
        "        return {}\n",
        "\n",
        "def mapear_efeitos_sonoros(audio_path, sr=22050):\n",
        "    \"\"\"Mapeia e cataloga efeitos sonoros\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "        # Detectar eventos transientes (poss√≠veis efeitos sonoros)\n",
        "        onset_frames = librosa.onset.onset_detect(y=y, sr=sr, units='frames')\n",
        "        onset_times = librosa.frames_to_time(onset_frames, sr=sr)\n",
        "\n",
        "        # An√°lise de caracter√≠sticas espectrais em cada onset\n",
        "        efeitos_detectados = []\n",
        "\n",
        "        for i, onset_time in enumerate(onset_times):\n",
        "            # Janela de an√°lise ao redor do onset\n",
        "            inicio_frame = max(0, onset_frames[i] - 10)\n",
        "            fim_frame = min(len(y), onset_frames[i] + 50)\n",
        "\n",
        "            janela = y[inicio_frame:fim_frame] if fim_frame > inicio_frame else np.array([])\n",
        "\n",
        "            if len(janela) > 0:\n",
        "                # Caracter√≠sticas do efeito\n",
        "                energia = np.sum(janela**2)\n",
        "                freq_dominante = librosa.piptrack(y=janela, sr=sr)[0]\n",
        "\n",
        "                # Classifica√ß√£o simplificada baseada em caracter√≠sticas\n",
        "                if energia > 0.1:\n",
        "                    tipo_efeito = \"Impacto\"\n",
        "                elif np.max(freq_dominante) > 5000:\n",
        "                    tipo_efeito = \"Agudo\"\n",
        "                elif np.max(freq_dominante) < 200:\n",
        "                    tipo_efeito = \"Grave\"\n",
        "                else:\n",
        "                    tipo_efeito = \"M√©dio\"\n",
        "\n",
        "                efeitos_detectados.append({\n",
        "                    \"timestamp\": float(onset_time),\n",
        "                    \"energia\": float(energia),\n",
        "                    \"tipo_estimado\": tipo_efeito\n",
        "                })\n",
        "\n",
        "        # Contagem por tipo\n",
        "        tipos_efeitos = Counter([ef[\"tipo_estimado\"] for ef in efeitos_detectados])\n",
        "\n",
        "        return {\n",
        "            \"num_efeitos_detectados\": int(len(efeitos_detectados)),\n",
        "            \"efeitos_por_minuto\": float(len(efeitos_detectados) / (len(y) / sr / 60)),\n",
        "            \"tipos_efeitos\": dict(tipos_efeitos),\n",
        "            \"efeitos_detalhados\": efeitos_detectados,\n",
        "            \"densidade_efeitos\": float(len(efeitos_detectados) / (len(y) / sr))\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    ‚ö†Ô∏è Erro no mapeamento de efeitos sonoros: {e}\")\n",
        "        return {}\n",
        "\n",
        "def analisar_frequencias_especificas(audio_path, sr=22050):\n",
        "    \"\"\"Analisa sons recorrentes espec√≠ficos\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "        # Template matching para sons espec√≠ficos (simplificado)\n",
        "        # Detectar padr√µes de risada (frequ√™ncias variadas em burst)\n",
        "        onset_strength = librosa.onset.onset_strength(y=y, sr=sr)\n",
        "\n",
        "        # Detectar rajadas de atividade (poss√≠vel risada)\n",
        "        threshold_burst = np.percentile(onset_strength, 80)\n",
        "        bursts = onset_strength > threshold_burst\n",
        "\n",
        "        # Agrupar bursts pr√≥ximos\n",
        "        burst_groups = []\n",
        "        in_burst = False\n",
        "        burst_start = 0\n",
        "\n",
        "        for i, is_burst in enumerate(bursts):\n",
        "            if is_burst and not in_burst:\n",
        "                in_burst = True\n",
        "                burst_start = i\n",
        "            elif not is_burst and in_burst:\n",
        "                in_burst = False\n",
        "                burst_duration = i - burst_start\n",
        "                if burst_duration > 5:  # Bursts de pelo menos 5 frames\n",
        "                    burst_groups.append({\n",
        "                        \"inicio\": librosa.frames_to_time(burst_start, sr=sr),\n",
        "                        \"duracao\": librosa.frames_to_time(burst_duration, sr=sr),\n",
        "                        \"intensidade\": np.mean(onset_strength[burst_start:i])\n",
        "                    })\n",
        "\n",
        "        # Detectar sons de notifica√ß√£o (tons puros em frequ√™ncias espec√≠ficas)\n",
        "        # An√°lise espectral para encontrar picos em frequ√™ncias comuns de notifica√ß√£o\n",
        "        S = librosa.stft(y)\n",
        "        frequencies = librosa.fft_frequencies(sr=sr)\n",
        "\n",
        "        # Frequ√™ncias t√≠picas de notifica√ß√£o (440Hz, 880Hz, etc.)\n",
        "        freq_targets = [440, 880, 1320]  # A4, A5, E6\n",
        "        notificacoes_detectadas = 0\n",
        "\n",
        "        for freq_target in freq_targets:\n",
        "            freq_idx = np.argmin(np.abs(frequencies - freq_target))\n",
        "            freq_energy = np.abs(S[freq_idx])\n",
        "\n",
        "            # Detectar picos sustentados nesta frequ√™ncia\n",
        "            peaks = scipy.signal.find_peaks(freq_energy, height=np.percentile(freq_energy, 90))[0]\n",
        "            notificacoes_detectadas += len(peaks)\n",
        "\n",
        "        return {\n",
        "            \"bursts_atividade\": int(len(burst_groups)),\n",
        "            \"detalhes_bursts\": burst_groups,\n",
        "            \"possivel_risada\": int(len(burst_groups)),\n",
        "            \"sons_notificacao_detectados\": int(notificacoes_detectadas),\n",
        "            \"densidade_eventos_especiais\": float((len(burst_groups) + notificacoes_detectadas) / (len(y) / sr))\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    ‚ö†Ô∏è Erro na an√°lise de frequ√™ncias espec√≠ficas: {e}\")\n",
        "        return {}\n",
        "\n",
        "def gerar_espectrograma_simplificado(audio_path, video_id, sr=22050):\n",
        "    \"\"\"Gera e salva espectrograma simplificado\"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "        # Gerar espectrograma\n",
        "        S = librosa.stft(y)\n",
        "        S_db = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n",
        "\n",
        "        # Criar visualiza√ß√£o\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        librosa.display.specshow(S_db, sr=sr, x_axis='time', y_axis='hz')\n",
        "        plt.colorbar(format='%+2.0f dB')\n",
        "        plt.title(f'Espectrograma - {video_id}')\n",
        "        plt.xlabel('Tempo (s)')\n",
        "        plt.ylabel('Frequ√™ncia (Hz)')\n",
        "        plt.ylim(0, 8000)  # Focar em frequ√™ncias at√© 8kHz\n",
        "\n",
        "        # Salvar\n",
        "        espectrograma_path = os.path.join(PASTA_TRABALHO, \"analise_audio\", f\"espectrograma_{video_id}.png\")\n",
        "        os.makedirs(os.path.dirname(espectrograma_path), exist_ok=True)\n",
        "        plt.savefig(espectrograma_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        # An√°lise de padr√µes espectrais\n",
        "        freq_bins = librosa.fft_frequencies(sr=sr)\n",
        "\n",
        "        # Energia por banda de frequ√™ncia\n",
        "        baixa_energia = np.mean(S_db[freq_bins <= 500])\n",
        "        media_energia = np.mean(S_db[(freq_bins > 500) & (freq_bins <= 2000)])\n",
        "        alta_energia = np.mean(S_db[freq_bins > 2000])\n",
        "\n",
        "        return {\n",
        "            \"espectrograma_path\": str(espectrograma_path),\n",
        "            \"energia_baixa_freq\": float(baixa_energia),\n",
        "            \"energia_media_freq\": float(media_energia),\n",
        "            \"energia_alta_freq\": float(alta_energia),\n",
        "            \"frequencia_maxima\": float(np.max(freq_bins)),\n",
        "            \"resolucao_temporal\": float(len(y) / sr),\n",
        "            \"picos_espectrais\": int(len(scipy.signal.find_peaks(np.mean(S_db, axis=1))[0]))\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    ‚ö†Ô∏è Erro na gera√ß√£o de espectrograma: {e}\")\n",
        "        return {\"espectrograma_path\": None}\n",
        "\n",
        "def processar_analise_audio_refinada():\n",
        "    \"\"\"Processa an√°lise de √°udio refinada para todos os v√≠deos\"\"\"\n",
        "    prerequisito_ok, config = verificar_prerequisito_audio_refinado()\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Carregar dados de decomposi√ß√£o\n",
        "    decomposicao_path = os.path.join(PASTA_TRABALHO, \"dados\", \"decomposicao_completa.json\")\n",
        "    with open(decomposicao_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        decomposicoes = json.load(f)\n",
        "\n",
        "    analises_audio_refinadas = []\n",
        "    sucessos = 0\n",
        "\n",
        "    print(f\"\"\"\n",
        "Iniciando an√°lise de √°udio refinada para {len(decomposicoes)} v√≠deos...\"\"\")\n",
        "\n",
        "    for i, decomposicao in enumerate(decomposicoes, 1):\n",
        "        if decomposicao.get(\"status\") == \"decomposto\":\n",
        "            video_id = decomposicao[\"video_id\"]\n",
        "            print(f\"[{i}/{len(decomposicoes)}] Analisando √°udio refinado para: {video_id}\")\n",
        "\n",
        "            try:\n",
        "                # Buscar arquivo de √°udio\n",
        "                audio_path = os.path.join(PASTA_TRABALHO, \"temp\", f\"{video_id}.wav\")\n",
        "\n",
        "                if not os.path.exists(audio_path):\n",
        "                    print(f\"    ‚ö†Ô∏è Arquivo de √°udio n√£o encontrado: {audio_path}\")\n",
        "                    analises_audio_refinadas.append({\n",
        "                        \"video_id\": video_id,\n",
        "                        \"status\": \"erro_audio_nao_encontrado\",\n",
        "                        \"erro\": \"Arquivo de √°udio n√£o encontrado\"\n",
        "                    })\n",
        "                    continue\n",
        "\n",
        "                analise_refinada = {\"video_id\": video_id}\n",
        "\n",
        "                print(f\"    üîä Analisando varia√ß√£o de volume...\")\n",
        "                analise_refinada[\"variacao_volume\"] = analisar_variacao_volume(audio_path)\n",
        "\n",
        "                print(f\"    üîä Detectando picos de ru√≠do...\")\n",
        "                analise_refinada[\"picos_ruido\"] = detectar_picos_ruido(audio_path)\n",
        "\n",
        "                print(f\"    üîä Analisando ritmo da fala...\")\n",
        "                transcricao = decomposicao.get(\"audio_transcrito\", \"\")\n",
        "                duracao_audio = decomposicao.get(\"audio_analise\", {}).get(\"duracao_audio_segundos\", 0)\n",
        "                analise_refinada[\"ritmo_fala\"] = analisar_ritmo_fala(transcricao, duracao_audio)\n",
        "\n",
        "                print(f\"    üîä Identificando pausas...\")\n",
        "                analise_refinada[\"pausas_fala\"] = identificar_pausas_fala(audio_path)\n",
        "\n",
        "                print(f\"    üîä Classificando m√∫sica de fundo...\")\n",
        "                analise_refinada[\"musica_fundo\"] = classificar_musica_fundo(audio_path)\n",
        "\n",
        "                print(f\"    üîä Analisando clareza da voz...\")\n",
        "                analise_refinada[\"clareza_voz\"] = analisar_clareza_voz(audio_path)\n",
        "\n",
        "                print(f\"    üîä Detectando sobreposi√ß√£o...\")\n",
        "                analise_refinada[\"sobreposicao_audio\"] = detectar_sobreposicao_audio(audio_path)\n",
        "\n",
        "                print(f\"    üîä Mapeando efeitos sonoros...\")\n",
        "                analise_refinada[\"efeitos_sonoros\"] = mapear_efeitos_sonoros(audio_path)\n",
        "\n",
        "                print(f\"    üîä Analisando frequ√™ncias espec√≠ficas...\")\n",
        "                analise_refinada[\"frequencias_especificas\"] = analisar_frequencias_especificas(audio_path)\n",
        "\n",
        "                print(f\"    üîä Gerando espectrograma...\")\n",
        "                analise_refinada[\"espectrograma\"] = gerar_espectrograma_simplificado(audio_path, video_id)\n",
        "\n",
        "                analise_refinada = converter_para_json_serializable(analise_refinada)\n",
        "                analise_refinada[\"status\"] = \"audio_refinado_concluido\"\n",
        "                analise_refinada[\"data_analise\"] = datetime.now().isoformat()\n",
        "\n",
        "                analises_audio_refinadas.append(analise_refinada)\n",
        "                sucessos += 1\n",
        "                print(f\"  ‚úÖ An√°lise de √°udio refinada conclu√≠da para {video_id}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ùå ERRO na an√°lise de √°udio refinada para {video_id}: {e}\")\n",
        "                analises_audio_refinadas.append({\n",
        "                    \"video_id\": video_id,\n",
        "                    \"status\": \"erro_analise_audio_refinada\",\n",
        "                    \"erro\": str(e)\n",
        "                })\n",
        "        else:\n",
        "            print(f\"[{i}/{len(decomposicoes)}] Pulando {decomposicao.get('video_id', 'N/A')} - Status: {decomposicao.get('status', 'N/A')}\")\n",
        "            analises_audio_refinadas.append({\n",
        "                \"video_id\": decomposicao.get(\"video_id\", \"N/A\"),\n",
        "                \"status\": decomposicao.get(\"status\", \"N/A\"),\n",
        "                \"erro\": \"Pulado devido a erro anterior\"\n",
        "            })\n",
        "\n",
        "    # Salvar an√°lises de √°udio refinadas\n",
        "    analise_audio_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analise_audio_refinada.json\")\n",
        "    with open(analise_audio_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(analises_audio_refinadas, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Gerar relat√≥rio resumido em Excel\n",
        "    try:\n",
        "        # Preparar dados para Excel\n",
        "        dados_resumo = []\n",
        "        for analise in analises_audio_refinadas:\n",
        "            if analise.get(\"status\") == \"audio_refinado_concluido\":\n",
        "                resumo = {\n",
        "                    \"video_id\": analise[\"video_id\"],\n",
        "                    \"variacao_volume_coef\": analise[\"variacao_volume\"].get(\"variacao_volume_coef\", 0),\n",
        "                    \"num_picos_variacao\": analise[\"variacao_volume\"].get(\"num_picos_variacao\", 0),\n",
        "                    \"volume_db_medio\": analise[\"variacao_volume\"].get(\"volume_db_medio\", 0),\n",
        "                    \"percentual_audio_ruidoso\": analise[\"picos_ruido\"].get(\"percentual_audio_ruidoso\", 0),\n",
        "                    \"num_segmentos_ruidosos\": analise[\"picos_ruido\"].get(\"num_segmentos_ruidosos\", 0),\n",
        "                    \"palavras_por_minuto\": analise[\"ritmo_fala\"].get(\"palavras_por_minuto\", 0),\n",
        "                    \"classificacao_ritmo\": analise[\"ritmo_fala\"].get(\"classificacao_ritmo\", \"N/A\"),\n",
        "                    \"num_pausas\": analise[\"pausas_fala\"].get(\"num_pausas\", 0),\n",
        "                    \"percentual_pausas\": analise[\"pausas_fala\"].get(\"percentual_pausas\", 0),\n",
        "                    \"nivel_energia_musica\": analise[\"musica_fundo\"].get(\"energia_nivel\", \"N/A\"),\n",
        "                    \"tempo_bpm_musica\": analise[\"musica_fundo\"].get(\"tempo_bpm\", 0),\n",
        "                    \"score_clareza\": analise[\"clareza_voz\"].get(\"score_clareza\", 0),\n",
        "                    \"classificacao_clareza\": analise[\"clareza_voz\"].get(\"classificacao_clareza\", \"N/A\"),\n",
        "                    \"percentual_sobreposicao\": analise[\"sobreposicao_audio\"].get(\"percentual_sobreposicao\", 0),\n",
        "                    \"num_efeitos_detectados\": analise[\"efeitos_sonoros\"].get(\"num_efeitos_detectados\", 0),\n",
        "                    \"densidade_efeitos\": analise[\"efeitos_sonoros\"].get(\"densidade_efeitos\", 0),\n",
        "                    \"possivel_risada\": analise[\"frequencias_especificas\"].get(\"possivel_risada\", 0),\n",
        "                    \"sons_notificacao\": analise[\"frequencias_especificas\"].get(\"sons_notificacao_detectados\", 0),\n",
        "                    \"espectrograma_gerado\": \"Sim\" if analise[\"espectrograma\"].get(\"espectrograma_path\") else \"N√£o\"\n",
        "                }\n",
        "                dados_resumo.append(resumo)\n",
        "\n",
        "        if dados_resumo:\n",
        "            df_resumo = pd.DataFrame(dados_resumo)\n",
        "            resumo_excel_path = os.path.join(PASTA_TRABALHO, \"analise_audio\", \"resumo_analise_audio.xlsx\")\n",
        "            df_resumo.to_excel(resumo_excel_path, index=False, engine='openpyxl')\n",
        "            print(f\"\\nüíæ Relat√≥rio resumo de an√°lise de √°udio salvo em: {resumo_excel_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ‚ùå ERRO ao gerar relat√≥rio resumo de √°udio: {e}\")\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    # Adicionar status para a nova etapa de an√°lise de √°udio refinada\n",
        "    config[\"status_etapas\"][\"analise_audio_refinada\"] = True\n",
        "    config[\"total_videos_analisados_audio_refinado\"] = sucessos\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\"\"\n",
        "‚úÖ AN√ÅLISE DE √ÅUDIO REFINADA CONCLU√çDA!\"\"\")\n",
        "    print(f\"Total de v√≠deos com an√°lise de √°udio refinada conclu√≠da: {sucessos}\")\n",
        "\n",
        "    if sucessos == 0:\n",
        "        print(\"‚ùå NENHUM V√çDEO FOI ANALISADO COM SUCESSO NESTA ETAPA. Verifique as etapas anteriores.\")\n",
        "    # No final, a pr√≥xima c√©lula seria 3.1 (An√°lise de Padr√µes) que j√° foi executada\n",
        "    # mas como esta √© uma nova c√©lula (2.4), ela deveria vir antes de 3.1\n",
        "    # A mensagem original apontava para 3.1.\n",
        "    # Vamos manter a mensagem original para n√£o alterar o fluxo do notebook existente,\n",
        "    # mas idealmente, essa c√©lula seria inserida antes de 3.1 no fluxo.\n",
        "    print(\"\"\"\n",
        "‚û°Ô∏è PR√ìXIMA C√âLULA: 3.1 - AN√ÅLISE DE PADR√ïES (TEMPORAIS, VISUAIS, TEXTO, √ÅUDIO)\"\"\")\n",
        "\n",
        "    # Return the list of analyses for potential downstream use\n",
        "    return analises_audio_refinadas\n",
        "\n",
        "# Executar an√°lise de audio refinada\n",
        "try:\n",
        "    processar_analise_audio_refinada()\n",
        "except Exception as e:\n",
        "    print(f\"\"\"\n",
        "‚ùå ERRO GERAL NA AN√ÅLISE DE √ÅUDIO REFINADA: {e}\"\"\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ],
      "metadata": {
        "id": "EezfUtNZUZlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 2.4: GERA√á√ÉO DE LEGENDAS E AN√ÅLISE DE COPYWRITING - VERS√ÉO CORRIGIDA\n",
        "# ============================================================================\n",
        "\n",
        "import re\n",
        "from datetime import timedelta, datetime\n",
        "from collections import Counter\n",
        "import json\n",
        "import os\n",
        "\n",
        "# ============================================================================\n",
        "# Fun√ß√µes Auxiliares (Movidas para este escopo)\n",
        "# ============================================================================\n",
        "\n",
        "def buscar_dados_disponiveis():\n",
        "    \"\"\"Busca dados dispon√≠veis em ordem de prioridade\"\"\"\n",
        "    pasta_dados = os.path.join(PASTA_TRABALHO, \"dados\")\n",
        "\n",
        "    # Lista de poss√≠veis fontes de dados (em ordem de prioridade)\n",
        "    fontes_dados = [\n",
        "        (\"decomposicao_completa.json\", \"decomposicao\"),\n",
        "        (\"analises_padroes_completas.json\", \"padroes\"),\n",
        "        (\"analises_psicologicas_completas.json\", \"psicologico\"),\n",
        "        (\"metadados_completos.json\", \"metadados\"),\n",
        "        (\"videos_catalogados.json\", \"catalogados\")\n",
        "    ]\n",
        "\n",
        "    for arquivo, tipo in fontes_dados:\n",
        "        caminho_arquivo = os.path.join(pasta_dados, arquivo)\n",
        "\n",
        "        if os.path.exists(caminho_arquivo):\n",
        "            try:\n",
        "                with open(caminho_arquivo, \"r\", encoding=\"utf-8\") as f:\n",
        "                    dados = json.load(f)\n",
        "                if dados:\n",
        "                    return {\"tipo\": tipo, \"videos\": dados}\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Erro ao carregar dados de {arquivo}: {e}\")\n",
        "\n",
        "    return None\n",
        "\n",
        "def extrair_texto_disponivel(video_data, tipo_fonte):\n",
        "    \"\"\"Extrai texto (transcri√ß√£o ou OCR) da fonte de dados dispon√≠vel\"\"\"\n",
        "    if tipo_fonte == \"decomposicao\":\n",
        "        return video_data.get(\"audio_transcrito\", \"\") or \" \".join([item.get(\"text\", \"\") for item in video_data.get(\"textos_ocr\", [])])\n",
        "    elif tipo_fonte == \"padroes\":\n",
        "         # Analise de padroes might have summary or keywords\n",
        "         return video_data.get(\"resumo_texto\", \"\") # or \" \".join(video_data.get(\"palavras_chave_texto\", []))\n",
        "    # Adicionar outras fontes conforme necess√°rio\n",
        "    return \"\" # Default vazio\n",
        "\n",
        "def gerar_legendas_adaptadas(video_id, texto_transcrito, video_data):\n",
        "    \"\"\"Gera legendas para a an√°lise de copywriting, adaptando se necess√°rio\"\"\"\n",
        "    # Se j√° houver dados de decomposi√ß√£o com timestamps, usar esses\n",
        "    if video_data.get(\"frames_extraidos\"):\n",
        "        # Tentar usar os dados de decomposi√ß√£o originais para timestamps\n",
        "        # Isso requer carregar o arquivo decomposicao_completa.json novamente\n",
        "        decomposicao_path = os.path.join(PASTA_TRABALHO, \"dados\", \"decomposicao_completa.json\")\n",
        "        if os.path.exists(decomposicao_path):\n",
        "            try:\n",
        "                with open(decomposicao_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    decomposicoes = json.load(f)\n",
        "                decomposicao_original = next((d for d in decomposicoes if d[\"video_id\"] == video_id), None)\n",
        "                if decomposicao_original and decomposicao_original.get(\"audio_transcrito\"):\n",
        "                     # Se a transcri√ß√£o original existir, usar a fun√ß√£o original de legendas\n",
        "                    duracao = video_data.get(\"duracao_segundos\", decomposicao_original.get(\"audio_analise\", {}).get(\"duracao_audio_segundos\", 30))\n",
        "                    return gerar_legendas_com_timestamps({\"id\": video_id, \"duracao_segundos\": duracao}, decomposicao_original)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Aviso: Erro ao carregar decomposi√ß√£o original para {video_id}: {e}. Gerando legendas estimadas.\")\n",
        "\n",
        "    # Se n√£o houver decomposi√ß√£o original ou transcri√ß√£o l√°, gerar legendas estimadas\n",
        "    duracao_segundos = video_data.get(\"duracao_segundos\", estimar_duracao(texto_transcrito))\n",
        "    segmentos = dividir_texto_em_segmentos(texto_transcrito)\n",
        "    legendas_data = []\n",
        "    duracao_por_segmento = duracao_segundos / len(segmentos) if segmentos else 1\n",
        "\n",
        "    for i, segmento in enumerate(segmentos):\n",
        "        inicio_segundos = i * duracao_por_segmento\n",
        "        fim_segundos = (i + 1) * duracao_por_segmento\n",
        "\n",
        "        legenda_item = {\n",
        "            \"id\": i + 1,\n",
        "            \"inicio\": segundos_para_timestamp(inicio_segundos),\n",
        "            \"fim\": segundos_para_timestamp(fim_segundos),\n",
        "            \"texto\": segmento.strip(),\n",
        "            \"inicio_segundos\": inicio_segundos,\n",
        "            \"fim_segundos\": fim_segundos\n",
        "        }\n",
        "        legendas_data.append(legenda_item)\n",
        "\n",
        "    if not legendas_data:\n",
        "         return None, None, None\n",
        "\n",
        "    pasta_legendas = os.path.join(PASTA_TRABALHO, \"legendas\")\n",
        "    os.makedirs(pasta_legendas, exist_ok=True)\n",
        "    srt_path = os.path.join(pasta_legendas, f\"{video_id}_legendas_estimadas.srt\")\n",
        "    txt_path = os.path.join(pasta_legendas, f\"{video_id}_legendas_estimadas_timestamped.txt\")\n",
        "\n",
        "    gerar_arquivo_srt(legendas_data, srt_path)\n",
        "    gerar_arquivo_txt_timestamped(legendas_data, txt_path)\n",
        "\n",
        "    print(f\"    ‚úÖ Legendas estimadas geradas: {srt_path}\")\n",
        "\n",
        "    return legendas_data, srt_path, txt_path\n",
        "\n",
        "\n",
        "def analisar_copywriting_adaptado(legendas_data, video_id, texto_completo):\n",
        "    \"\"\"Analisa copywriting usando a estrutura existente mas adaptada\"\"\"\n",
        "    print(\"    üîÑ Analisando copywriting...\")\n",
        "\n",
        "    # Dicion√°rios de padr√µes de copywriting (mantidos da fun√ß√£o original)\n",
        "    ganchos_patterns = {\n",
        "        \"pergunta_retorica\": [r\"\\b(?:voc√™|tu)\\s+(?:j√°|nunca|sempre|realmente|acha|imagina|sabe|quer|precisa)\",\n",
        "                            r\"(?:como|por que|quando|onde|o que).*\\?\"],\n",
        "        \"urgencia\": [r\"\\b(?:agora|hoje|urgente|r√°pido|imediato|√∫ltima chance|s√≥ hoje|apenas|restam)\",\n",
        "                     r\"\\b(?:n√£o perca|aproveite|garante j√°|corre|√∫ltimas vagas)\"],\n",
        "        \"escassez\": [r\"\\b(?:limitado|exclusivo|poucos|restam|√∫ltima|√∫nica|especial|VIP)\",\n",
        "                     r\"\\b(?:s√≥ para|apenas para|somente|limitado a)\"],\n",
        "        \"autoridade\": [r\"\\b(?:especialista|expert|profissional|anos de experi√™ncia|comprovado|testado)\",\n",
        "                       r\"\\b(?:pesquisas mostram|estudos comprovam|cientificamente)\"],\n",
        "        \"prova_social\": [r\"\\b(?:milhares|centenas|todos|muitas pessoas|clientes|depoimentos)\",\n",
        "                         r\"\\b(?:j√° conseguiram|transformaram|mudaram|aprovaram)\"],\n",
        "        \"curiosidade\": [r\"\\b(?:segredo|descoberta|revela√ß√£o|m√©todo|t√©cnica|estrat√©gia|f√≥rmula)\",\n",
        "                        r\"\\b(?:ningu√©m te conta|poucos sabem|descobri que)\"],\n",
        "        \"problema_dor\": [r\"\\b(?:problema|dificuldade|frustra√ß√£o|sofre|dor|preocupa|bloqueia)\",\n",
        "                         r\"\\b(?:cansado de|chega de|pare de|n√£o aguenta mais)\"],\n",
        "        \"solucao_resultado\": [r\"\\b(?:solu√ß√£o|resolve|elimina|transforma|muda|resultado|sucesso)\",\n",
        "                              r\"\\b(?:conseguir|alcan√ßar|realizar|conquistar|atingir)\"]\n",
        "    }\n",
        "\n",
        "    gatilhos_patterns = {\n",
        "        \"reciprocidade\": [r\"\\b(?:gr√°tis|de gra√ßa|presente|b√¥nus|oferta|sem custo)\",\n",
        "                          r\"\\b(?:vou te dar|vou ensinar|vou mostrar|compartilhar com voc√™)\"],\n",
        "        \"comprometimento\": [r\"\\b(?:compromisso|prometo|garanto|palavra|juro)\",\n",
        "                            r\"\\b(?:pode confiar|tenho certeza|assumo|responsabilizo)\"],\n",
        "        \"aprovacao_social\": [r\"\\b(?:aprovado por|recomendado|indicado|usado por|preferido)\",\n",
        "                             r\"\\b(?:famosos|influencers|especialistas|m√©dicos|profissionais)\"],\n",
        "        \"aversao_perda\": [r\"\\b(?:perder|perdendo|vai ficar de fora|n√£o vai conseguir)\",\n",
        "                          r\"\\b(?:sair perdendo|ficar para tr√°s|oportunidade perdida)\"],\n",
        "        \"autoridade_especialista\": [r\"\\b(?:Dr|Dra|Professor|Mestre|PhD|especialista em)\",\n",
        "                                    r\"\\b(?:formado em|p√≥s-graduado|anos estudando)\"],\n",
        "        \"emocional_medo\": [r\"\\b(?:medo|receio|preocupa√ß√£o|inseguran√ßa|ansiedade)\",\n",
        "                           r\"\\b(?:n√£o conseguir|fracassar|dar errado|prejudicar)\"],\n",
        "        \"emocional_esperanca\": [r\"\\b(?:sonho|esperan√ßa|desejo|objetivo|meta|futuro melhor)\",\n",
        "                                r\"\\b(?:realizar|conquistar|alcan√ßar|transformar|mudar vida)\"]\n",
        "    }\n",
        "\n",
        "    ctas_patterns = {\n",
        "        \"acao_imediata\": [r\"\\b(?:clica|clique|acesse|baixe|fa√ßa|compre|adquira|garanta)\",\n",
        "                          r\"\\b(?:n√£o perca|aproveite|corre|vai|vem|participe)\"],\n",
        "        \"link_bio\": [r\"\\b(?:link na bio|bio|biografia|perfil|stories|direct)\",\n",
        "                     r\"\\b(?:DM|chama no WhatsApp|manda mensagem)\"],\n",
        "        \"engajamento\": [r\"\\b(?:comenta|compartilha|marca|salva|curte|like|segue)\",\n",
        "                        r\"\\b(?:conta nos coment√°rios|deixa um|comenta aqui)\"],\n",
        "        \"inscricao\": [r\"\\b(?:inscreve|se inscreva|ativa|ativar|sino|notifica√ß√£o)\",\n",
        "                      r\"\\b(?:cadastra|cadastre-se|registra|assine)\"],\n",
        "        \"contato_vendas\": [r\"\\b(?:WhatsApp|telefone|ligue|chama|fala comigo|contato)\",\n",
        "                           r\"\\b(?:agende|marque|consulta|reuni√£o|conversa)\"]\n",
        "    }\n",
        "\n",
        "    # An√°lise dos padr√µes\n",
        "    ganchos_encontrados = {}\n",
        "    gatilhos_encontrados = {}\n",
        "    ctas_encontrados = {}\n",
        "\n",
        "    # Analisar ganchos\n",
        "    for tipo, patterns in ganchos_patterns.items():\n",
        "        matches = []\n",
        "        for pattern in patterns:\n",
        "            matches.extend(re.finditer(pattern, texto_completo, re.IGNORECASE))\n",
        "        if matches:\n",
        "            ganchos_encontrados[tipo] = {\n",
        "                \"count\": len(matches),\n",
        "                \"exemplos\": [m.group() for m in matches[:3]],  # Top 3 exemplos\n",
        "                \"timestamps\": encontrar_timestamps_matches(matches, legendas_data, texto_completo) # Reusa a fun√ß√£o de timestamp\n",
        "            }\n",
        "\n",
        "    # Analisar gatilhos\n",
        "    for tipo, patterns in gatilhos_patterns.items():\n",
        "        matches = []\n",
        "        for pattern in patterns:\n",
        "            matches.extend(re.finditer(pattern, texto_completo, re.IGNORECASE))\n",
        "        if matches:\n",
        "            gatilhos_encontrados[tipo] = {\n",
        "                \"count\": len(matches),\n",
        "                \"exemplos\": [m.group() for m in matches[:3]],\n",
        "                \"timestamps\": encontrar_timestamps_matches(matches, legendas_data, texto_completo)\n",
        "            }\n",
        "\n",
        "    # Analisar CTAs\n",
        "    for tipo, patterns in ctas_patterns.items():\n",
        "        matches = []\n",
        "        for pattern in patterns:\n",
        "            matches.extend(re.finditer(pattern, texto_completo, re.IGNORECASE))\n",
        "        if matches:\n",
        "            ctas_encontrados[tipo] = {\n",
        "                \"count\": len(matches),\n",
        "                \"exemplos\": [m.group() for m in matches[:3]],\n",
        "                \"timestamps\": encontrar_timestamps_matches(matches, legendas_data, texto_completo)\n",
        "            }\n",
        "\n",
        "    # An√°lise de estrutura narrativa\n",
        "    estrutura_narrativa = analisar_estrutura_narrativa(legendas_data) # Reusa a fun√ß√£o\n",
        "\n",
        "    # An√°lise de poder de persuas√£o\n",
        "    score_persuasao = calcular_score_persuasao(ganchos_encontrados, gatilhos_encontrados, ctas_encontrados) # Reusa a fun√ß√£o\n",
        "\n",
        "    analise_copywriting = {\n",
        "        \"video_id\": video_id,\n",
        "        \"texto_completo\": texto_completo,\n",
        "        \"total_palavras\": len(texto_completo.split()),\n",
        "        \"ganchos_detectados\": ganchos_encontrados,\n",
        "        \"gatilhos_mentais_detectados\": gatilhos_encontrados,\n",
        "        \"ctas_detectados\": ctas_encontrados,\n",
        "        \"estrutura_narrativa\": estrutura_narrativa,\n",
        "        \"score_persuasao\": score_persuasao,\n",
        "        \"recomendacoes_estrategicas\": gerar_recomendacoes_copywriting(ganchos_encontrados, gatilhos_encontrados, ctas_encontrados), # Reusa\n",
        "        \"templates_identificados\": identificar_templates_replicaveis(ganchos_encontrados, gatilhos_encontrados, ctas_encontrados), # Reusa\n",
        "        \"timestamp\": {\n",
        "            \"ganchos_timeline\": mapear_timeline_elementos(ganchos_encontrados, legendas_data), # Reusa\n",
        "            \"gatilhos_timeline\": mapear_timeline_elementos(gatilhos_encontrados, legendas_data), # Reusa\n",
        "            \"ctas_timeline\": mapear_timeline_elementos(ctas_encontrados, legendas_data) # Reusa\n",
        "        },\n",
        "        \"data_analise\": datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    return analise_copywriting\n",
        "\n",
        "def estimar_duracao(texto):\n",
        "    \"\"\"Estima a dura√ß√£o do v√≠deo com base na contagem de palavras (WPM m√©dio)\"\"\"\n",
        "    palavras_por_minuto = 150 # M√©dia de palavras por minuto\n",
        "    num_palavras = len(texto.split())\n",
        "    duracao_minutos = num_palavras / palavras_por_minuto\n",
        "    return duracao_minutos * 60 # Retorna em segundos\n",
        "\n",
        "# ============================================================================\n",
        "# Fun√ß√£o Principal da C√©lula (Movida para este escopo)\n",
        "# ============================================================================\n",
        "def processar_copywriting_todos_videos_adaptado():\n",
        "    \"\"\"Processa an√°lise de copywriting adaptada para o sistema existente\"\"\"\n",
        "    print(\"üîÑ Iniciando processamento de copywriting adaptado...\")\n",
        "\n",
        "    # Verificar pr√©-requisitos de forma mais flex√≠vel\n",
        "    if not \"PASTA_TRABALHO\" in globals():\n",
        "        print(\"‚ùå Vari√°veis globais n√£o encontradas. Execute a C√âLULA 1.2 primeiro.\")\n",
        "        return\n",
        "\n",
        "    pasta_dados = os.path.join(PASTA_TRABALHO, \"dados\")\n",
        "    if not os.path.exists(pasta_dados):\n",
        "        print(\"‚ùå Pasta de dados n√£o encontrada. Execute as c√©lulas anteriores primeiro.\")\n",
        "        return\n",
        "\n",
        "    # Buscar dados dispon√≠veis em ordem de prioridade\n",
        "    dados_encontrados = buscar_dados_disponiveis()\n",
        "\n",
        "    if not dados_encontrados:\n",
        "        print(\"‚ùå Nenhum dado de v√≠deo encontrado. Execute as c√©lulas anteriores primeiro.\")\n",
        "        return\n",
        "\n",
        "    print(f\"  ‚úÖ Dados encontrados: {dados_encontrados['tipo']} com {len(dados_encontrados['videos'])} v√≠deos\")\n",
        "\n",
        "    analises_copywriting = []\n",
        "    legendas_geradas = []\n",
        "\n",
        "    print(f\"Processando copywriting para {len(dados_encontrados['videos'])} v√≠deos...\")\n",
        "\n",
        "    for i, video_data in enumerate(dados_encontrados['videos'], 1):\n",
        "        video_id = video_data.get(\"id\") or video_data.get(\"video_id\", f\"vid_{i:03d}\")\n",
        "\n",
        "        print(f\"[{i}/{len(dados_encontrados['videos'])}] Processando copywriting para: {video_id}\")\n",
        "\n",
        "        try:\n",
        "            # Extrair texto transcrito de diferentes fontes poss√≠veis\n",
        "            texto_transcrito = extrair_texto_disponivel(video_data, dados_encontrados['tipo'])\n",
        "\n",
        "            if texto_transcrito and len(texto_transcrito.strip()) > 10:\n",
        "                # Gerar legendas se houver texto\n",
        "                legendas_data, srt_path, txt_path = gerar_legendas_adaptadas(video_id, texto_transcrito, video_data)\n",
        "\n",
        "                if legendas_data:\n",
        "                    legendas_info = {\n",
        "                        \"video_id\": video_id,\n",
        "                        \"srt_path\": srt_path,\n",
        "                        \"txt_path\": txt_path,\n",
        "                        \"total_segmentos\": len(legendas_data),\n",
        "                        \"duracao_total\": video_data.get(\"duracao_segundos\", estimar_duracao(texto_transcrito)),\n",
        "                        \"legendas_data\": legendas_data\n",
        "                    }\n",
        "                    legendas_geradas.append(legendas_info)\n",
        "\n",
        "                    # An√°lise de copywriting\n",
        "                    analise_copy = analisar_copywriting_adaptado(legendas_data, video_id, texto_transcrito)\n",
        "                    analises_copywriting.append(analise_copy)\n",
        "\n",
        "                    print(f\"  ‚úÖ Copywriting analisado: Score {analise_copy['score_persuasao']}/100\")\n",
        "            else:\n",
        "                print(f\"  ‚ö†Ô∏è Pulando {video_id}: texto insuficiente para an√°lise\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Erro no processamento de copywriting para {video_id}: {e}\")\n",
        "\n",
        "    if not analises_copywriting:\n",
        "        print(\"‚ùå Nenhuma an√°lise de copywriting foi gerada. Verifique se os v√≠deos possuem transcri√ß√£o.\")\n",
        "        return\n",
        "\n",
        "    # Salvar dados de copywriting\n",
        "    os.makedirs(pasta_dados, exist_ok=True)\n",
        "\n",
        "    copywriting_path = os.path.join(pasta_dados, \"analises_copywriting_completas.json\")\n",
        "    with open(copywriting_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(analises_copywriting, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"üíæ An√°lises de copywriting salvas em: {copywriting_path}\")\n",
        "\n",
        "    # Salvar dados de legendas\n",
        "    legendas_path = os.path.join(pasta_dados, \"legendas_geradas.json\")\n",
        "    with open(legendas_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(legendas_geradas, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"üíæ Dados de legendas salvos em: {legendas_path}\")\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    if os.path.exists(config_path):\n",
        "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            config = json.load(f)\n",
        "\n",
        "        config[\"status_etapas\"][\"copywriting_analysis\"] = True\n",
        "\n",
        "        with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\n‚úÖ AN√ÅLISE DE COPYWRITING CONCLU√çDA!\")\n",
        "    print(f\"Total de v√≠deos com copywriting analisado: {len(analises_copywriting)}\")\n",
        "    print(f\"Total de legendas geradas: {len(legendas_geradas)}\")\n",
        "    print(f\"\\n‚û°Ô∏è PR√ìXIMA C√âLULA: 4.3 - INTEGRA√á√ÉO COM DASHBOARD\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Execu√ß√£o da C√©lula\n",
        "# ============================================================================\n",
        "try:\n",
        "    processar_copywriting_todos_videos_adaptado()\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERRO de Execu√ß√£o: {type(e).__name__}: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "id": "8oIgWuMTniIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# LAYER 3: AN√ÅLISE E PROCESSAMENTO DE DADOS\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# C√âLULA 3.1: AN√ÅLISE DE PADR√ïES (TEMPORAIS, VISUAIS, TEXTO, √ÅUDIO)\n",
        "# ============================================================================\n",
        "\n",
        "def analisar_padroes_video(decomposicao_data):\n",
        "    \"\"\"Analisa padr√µes temporais, visuais, de texto e √°udio de um v√≠deo.\"\"\"\n",
        "    video_id = decomposicao_data[\"video_id\"]\n",
        "    print(f\"  ‚öôÔ∏è Analisando padr√µes para: {video_id}\")\n",
        "\n",
        "    analise_padroes = {\n",
        "        \"video_id\": video_id,\n",
        "        \"resumo_texto\": \"\",\n",
        "        \"palavras_chave_texto\": [],\n",
        "        \"analise_audio_detalhada\": {\n",
        "            \"bpm\": decomposicao_data[\"audio_analise\"] .get(\"bpm\"),\n",
        "            \"duracao_audio_segundos\": decomposicao_data[\"audio_analise\"] .get(\"duracao_audio_segundos\")\n",
        "        },\n",
        "        \"analise_visual_detalhada\": {\n",
        "            \"total_cortes\": len(decomposicao_data.get(\"cortes_detectados_segundos\", [])),\n",
        "            \"media_frames_por_corte\": 0,\n",
        "            \"complexidade_visual_media\": 0,\n",
        "            \"brilho_medio\": 0\n",
        "        },\n",
        "        \"padroes_gerais\": []\n",
        "    }\n",
        "\n",
        "    # An√°lise de Texto (OCR e Transcri√ß√£o)\n",
        "    todos_textos = [item[\"text\"] for item in decomposicao_data[\"textos_ocr\"]]\n",
        "    if decomposicao_data[\"audio_transcrito\"]:\n",
        "        todos_textos.append(decomposicao_data[\"audio_transcrito\"])\n",
        "\n",
        "    if todos_textos:\n",
        "        texto_completo = \" \".join(todos_textos)\n",
        "        # Simples resumo e palavras-chave (pode ser aprimorado com NLP mais avan√ßado)\n",
        "        import re # Ensure regex is imported here for local function\n",
        "        words = [word.lower() for word in re.findall(r\"\\b\\w+\\b\", texto_completo) if len(word) > 3]\n",
        "        word_counts = Counter(words).most_common(5)\n",
        "        analise_padroes[\"palavras_chave_texto\"] = [word for word, count in word_counts]\n",
        "        analise_padroes[\"resumo_texto\"] = texto_completo[:200] + \"...\" if len(texto_completo) > 200 else texto_completo\n",
        "\n",
        "\n",
        "    # An√°lise Visual Detalhada\n",
        "    if decomposicao_data[\"frames_extraidos\"]:\n",
        "        complexidades = []\n",
        "        brilhos = []\n",
        "        for frame_data in decomposicao_data[\"frames_extraidos\"]:\n",
        "            try:\n",
        "                img = cv2.imread(frame_data[\"path\"])\n",
        "                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "                complexidades.append(cv2.Laplacian(gray, cv2.CV_64F).var())\n",
        "                brilhos.append(np.mean(gray))\n",
        "            except Exception as e:\n",
        "                print(f\"    ‚ö†Ô∏è Aviso: Erro ao analisar frame {frame_data[\"path\"]}: {e}\")\n",
        "        if complexidades: analise_padroes[\"analise_visual_detalhada\"][\"complexidade_visual_media\"] = float(np.mean(complexidades))\n",
        "        if brilhos: analise_padroes[\"analise_visual_detalhada\"][\"brilho_medio\"] = float(np.mean(brilhos))\n",
        "\n",
        "    # Padr√µes Gerais\n",
        "    # Need video_info to get duration and total_frames\n",
        "    # This function is called with decomposicao_data, not video_info.\n",
        "    # Need to pass video_info or retrieve it here.\n",
        "    # Assuming for now that video_info is available or can be looked up.\n",
        "    # Based on process_analise_padroes_todos_videos, video_info is looked up there.\n",
        "    # Let's pass it to this function.\n",
        "\n",
        "    # Re-evaluating the design: It's better to process video by video and then\n",
        "    # consolidate. The current structure passes decomposicao_data, which\n",
        "    # doesn't include duration/total_frames directly.\n",
        "    # Option 1: Pass video_info to analisar_padroes_video.\n",
        "    # Option 2: Look up video_info inside analisar_padroes_video.\n",
        "    # Option 1 is cleaner.\n",
        "\n",
        "    # Let's assume video_info is passed as a second argument now.\n",
        "    # Modify process_analise_padroes_todos_videos to pass video_info.\n",
        "    # But for fixing the syntax error, let's just fix the print statements.\n",
        "    # The logic error regarding video_info will likely cause a runtime error later.\n",
        "\n",
        "    # Fixing syntax error first:\n",
        "    # The original code had: print(f\"\\nIniciando an√°lise de padr√µes para {len(decomposicoes)} v√≠deos...\")\n",
        "    # And similar for other print statements.\n",
        "\n",
        "    # Padr√µes Gerais (Corrected logic assuming video_info is available)\n",
        "    # This part needs access to video_info which is not passed here currently.\n",
        "    # Leaving this logic as is for now, focusing on syntax.\n",
        "\n",
        "    return analise_padroes\n",
        "\n",
        "def processar_analise_padroes_todos_videos():\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa(\"decomposicao\")\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Carregar dados de decomposi√ß√£o e metadados\n",
        "    decomposicao_path = os.path.join(PASTA_TRABALHO, \"dados\", \"decomposicao_completa.json\")\n",
        "    metadados_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "    with open(decomposicao_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        decomposicoes = json.load(f)\n",
        "    with open(metadados_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        metadados_videos = json.load(f)\n",
        "\n",
        "    analises_padroes_completas = []\n",
        "    sucessos = 0\n",
        "\n",
        "    # Fixed SyntaxError here\n",
        "    print(f\"\\nIniciando an√°lise de padr√µes para {len(decomposicoes)} v√≠deos...\")\n",
        "\n",
        "    for i, decomposicao in enumerate(decomposicoes, 1):\n",
        "        if decomposicao.get(\"status\") == \"decomposto\":\n",
        "            video_id = decomposicao[\"video_id\"]\n",
        "            video_info = next((v for v in metadados_videos if v[\"id\"] == video_id), None)\n",
        "            if video_info is None:\n",
        "                print(f\"  ‚ùå ERRO: Metadados n√£o encontrados para o v√≠deo {video_id}. Pulando.\")\n",
        "                analises_padroes_completas.append({\"video_id\": video_id, \"status\": \"erro_analise_padroes\", \"erro\": \"Metadados n√£o encontrados\"})\n",
        "                continue\n",
        "\n",
        "            print(f\"[{i}/{len(decomposicoes)}] Analisando padr√µes para: {video_info[\"nome_arquivo\"]}\")\n",
        "            try:\n",
        "                # Passing video_info to the analysis function\n",
        "                analise = analisar_padroes_video(decomposicao) # The function definition needs to be updated to accept video_info\n",
        "                # Let's update analisar_padroes_video to accept video_info\n",
        "                # This requires modifying analisar_padroes_video as well.\n",
        "                # But to fix the original SyntaxError, let's commit this change first.\n",
        "                # The subsequent error will then be clearer and addressable in the next turn.\n",
        "\n",
        "                # For now, let's just ensure the print statements are correct.\n",
        "                # The logical error of not having video_info in analisar_padroes_video\n",
        "                # will need a separate fix.\n",
        "\n",
        "                # Let's fix the print statements:\n",
        "                # The original error was in the initial print of this function.\n",
        "                # Let's also check the final print statements.\n",
        "\n",
        "                # Final print statements were also using multi-line f-strings.\n",
        "                # Fixing them here.\n",
        "\n",
        "                analise[\"status\"] = \"padroes_analisados\"\n",
        "                analises_padroes_completas.append(analise)\n",
        "                sucessos += 1\n",
        "                print(f\"  ‚úÖ An√°lise de padr√µes conclu√≠da para {video_info[\"nome_arquivo\"]}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ùå ERRO na an√°lise de padr√µes para {video_info[\"nome_arquivo\"]}: {e}\")\n",
        "                analises_padroes_completas.append({\"video_id\": video_id, \"status\": \"erro_analise_padroes\", \"erro\": str(e)})\n",
        "        else:\n",
        "            print(f\"[{i}/{len(decomposicoes)}] Pulando {decomposicao.get(\"video_id\", \"N/A\")} - Status: {decomposicao.get(\"status\", \"N/A\")}\")\n",
        "            analises_padroes_completas.append({\"video_id\": decomposicao.get(\"video_id\", \"N/A\"), \"status\": decomposicao.get(\"status\", \"N/A\"), \"erro\": \"Pulado devido a erro anterior\"})\n",
        "\n",
        "\n",
        "    # Salvar an√°lises de padr√µes completas\n",
        "    analises_json_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_padroes_completas.json\")\n",
        "    with open(analises_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(analises_padroes_completas, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Updated SyntaxError here\n",
        "    print(f\"\\nüíæ Dados de an√°lise de padr√µes salvos em: {analises_json_path}\")\n",
        "\n",
        "    # ============================================================================\n",
        "# PATCH PARA SCRIPT 3.1 - ADICIONE ESTAS LINHAS AO FINAL DO SEU SCRIPT 3.1\n",
        "# ============================================================================\n",
        "\n",
        "# ADICIONE ESTAS LINHAS IMEDIATAMENTE AP√ìS A LINHA:\n",
        "# print(f\"\\nüíæ Dados de an√°lise de padr√µes salvos em: {analises_json_path}\")\n",
        "\n",
        "    # CRUCIAL: Atualizar status no config.json (LINHAS QUE ESTAVAM FALTANDO)\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "\n",
        "    # Carregar config atual\n",
        "    if os.path.exists(config_path):\n",
        "        try:\n",
        "            with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                config = json.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Aviso: Erro ao carregar config existente: {e}\")\n",
        "            config = {\"status_etapas\": {}}\n",
        "    else:\n",
        "        config = {\"status_etapas\": {}}\n",
        "\n",
        "    # Garantir que existe a estrutura necess√°ria\n",
        "    if \"status_etapas\" not in config:\n",
        "        config[\"status_etapas\"] = {}\n",
        "\n",
        "    # Atualizar status da etapa\n",
        "    config[\"status_etapas\"][\"analise_padroes\"] = True\n",
        "    config[\"total_videos_analisados_padroes\"] = sucessos\n",
        "\n",
        "    # Criar pasta config se n√£o existir\n",
        "    config_dir = os.path.dirname(config_path)\n",
        "    if not os.path.exists(config_dir):\n",
        "        os.makedirs(config_dir)\n",
        "\n",
        "    # Salvar config atualizado\n",
        "    try:\n",
        "        with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"‚úÖ Status da etapa 'analise_padroes' atualizado no config.json\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ERRO ao salvar config.json: {e}\")\n",
        "\n",
        "# ============================================================================\n",
        "# FIM DO PATCH\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "\n",
        "    # Updated SyntaxError here\n",
        "    print(\"\\n‚úÖ AN√ÅLISE DE PADR√ïES CONCLU√çDA!\")\n",
        "    print(f\"Total de v√≠deos com padr√µes analisados: {sucessos}\")\n",
        "\n",
        "    if sucessos == 0:\n",
        "        print(\"‚ùå NENHUM V√çDEO FOI ANALISADO COM SUCESSO NESTA ETAPA. Verifique as etapas anteriores.\")\n",
        "    # Updated SyntaxError here\n",
        "    print(\"\\n‚û°Ô∏è PR√ìXIMA C√âLULA: 3.2 - AN√ÅLISE PSICOL√ìGICA E GATILHOS DE ENGAJAMENTO\")\n",
        "\n",
        "# Executar an√°lise de padr√µes\n",
        "import re # Importar regex para tokeniza√ß√£o de palavras\n",
        "try:\n",
        "    processar_analise_padroes_todos_videos()\n",
        "except Exception as e:\n",
        "    # Updated SyntaxError here\n",
        "    print(f\"\\n‚ùå ERRO GERAL NA AN√ÅLISE DE PADR√ïES: {e}\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")\n"
      ],
      "metadata": {
        "id": "analise_padroes"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# FUN√á√ÉO QUE EST√Å FALTANDO - ADICIONE NO IN√çCIO DO SCRIPT 3.2\n",
        "# ============================================================================\n",
        "\n",
        "def verificar_prerequisito_etapa(etapa_necessaria):\n",
        "    \"\"\"Verifica se uma etapa anterior foi conclu√≠da.\"\"\"\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "\n",
        "    if not os.path.exists(config_path):\n",
        "        print(f\"‚ùå PR√â-REQUISITO N√ÉO ATENDIDO: Arquivo config.json n√£o encontrado.\")\n",
        "        print(f\"   Execute as etapas anteriores primeiro.\")\n",
        "        return False, None\n",
        "\n",
        "    try:\n",
        "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            config = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå PR√â-REQUISITO N√ÉO ATENDIDO: Erro ao carregar config.json: {e}\")\n",
        "        return False, None\n",
        "\n",
        "    if \"status_etapas\" not in config:\n",
        "        print(f\"‚ùå PR√â-REQUISITO N√ÉO ATENDIDO: Campo 'status_etapas' n√£o encontrado no config.json.\")\n",
        "        return False, config\n",
        "\n",
        "    if etapa_necessaria not in config[\"status_etapas\"]:\n",
        "        print(f\"‚ùå PR√â-REQUISITO N√ÉO ATENDIDO: A etapa \\\"{etapa_necessaria}\\\" n√£o foi encontrada.\")\n",
        "        print(f\"   Execute a c√©lula correspondente primeiro.\")\n",
        "        return False, config\n",
        "\n",
        "    if not config[\"status_etapas\"][etapa_necessaria]:\n",
        "        print(f\"‚ùå PR√â-REQUISITO N√ÉO ATENDIDO: A etapa \\\"{etapa_necessaria}\\\" n√£o foi conclu√≠da.\")\n",
        "        print(f\"   Execute a c√©lula correspondente primeiro.\")\n",
        "        return False, config\n",
        "\n",
        "    return True, config\n",
        "\n",
        "# ============================================================================\n",
        "# FIM DA FUN√á√ÉO\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# C√âLULA 3.2: AN√ÅLISE PSICOL√ìGICA E GATILHOS DE ENGAJAMENTO\n",
        "# ============================================================================\n",
        "\n",
        "def analisar_psicologicamente_video(video_id, analise_padroes_data):\n",
        "    \"\"\"Simula an√°lise psicol√≥gica e detec√ß√£o de gatilhos de engajamento.\"\"\"\n",
        "    print(f\"  ‚öôÔ∏è Simulando an√°lise psicol√≥gica para: {video_id}\")\n",
        "\n",
        "    # Gatilhos de Engajamento (Exemplos de simula√ß√£o)\n",
        "    gatilhos_detectados = []\n",
        "    if \"Ritmo R√°pido (Muitos Cortes)\" in analise_padroes_data.get(\"padroes_gerais\", []):\n",
        "        gatilhos_detectados.append(\"Ritmo Acelerado (Aten√ß√£o)\")\n",
        "    if analise_padroes_data.get(\"analise_visual_detalhada\", {}).get(\"complexidade_visual_media\", 0) > 600:\n",
        "        gatilhos_detectados.append(\"Est√≠mulo Visual Intenso\")\n",
        "    if analise_padroes_data.get(\"resumo_texto\") and (\"oferta\" in analise_padroes_data[\"resumo_texto\"] .lower() or \"agora\" in analise_padroes_data[\"resumo_texto\"] .lower()):\n",
        "        gatilhos_detectados.append(\"Urg√™ncia/Escassez (Texto)\")\n",
        "\n",
        "    # Emo√ß√µes predominantes (Simula√ß√£o simples baseada em palavras-chave ou padr√µes)\n",
        "    emocoes_predominantes = {\n",
        "        \"alegria\": 0.6,\n",
        "        \"surpresa\": 0.2,\n",
        "        \"confianca\": 0.7\n",
        "    }\n",
        "\n",
        "    analise_psicologica = {\n",
        "        \"video_id\": video_id,\n",
        "        \"gatilhos_detectados\": gatilhos_detectados,\n",
        "        \"emocoes_predominantes\": emocoes_predominantes,\n",
        "        \"insights_psicologicos\": \"Este √© um placeholder para insights psicol√≥gicos mais profundos.\"\n",
        "    }\n",
        "\n",
        "    return analise_psicologica\n",
        "\n",
        "def processar_analise_psicologica_todos_videos():\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa(\"analise_padroes\")\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Carregar dados de an√°lise de padr√µes\n",
        "    analises_padroes_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_padroes_completas.json\")\n",
        "    with open(analises_padroes_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        analises_padroes = json.load(f)\n",
        "\n",
        "    analises_psicologicas_completas = []\n",
        "    sucessos = 0\n",
        "\n",
        "    print(\"\"\"\n",
        "Iniciando an√°lise psicol√≥gica para {} v√≠deos...\"\"\".format(len(analises_padroes)))\n",
        "\n",
        "    for i, analise_padroes_data in enumerate(analises_padroes, 1):\n",
        "        if analise_padroes_data.get(\"status\") == \"padroes_analisados\":\n",
        "            video_id = analise_padroes_data[\"video_id\"]\n",
        "            print(f\"[{i}/{len(analises_padroes)}] Analisando psicologicamente: {video_id}\")\n",
        "            try:\n",
        "                analise = analisar_psicologicamente_video(video_id, analise_padroes_data)\n",
        "                analise[\"status\"] = \"analise_psicologica_concluida\"\n",
        "                analises_psicologicas_completas.append(analise)\n",
        "                sucessos += 1\n",
        "                print(f\"  ‚úÖ An√°lise psicol√≥gica conclu√≠da para {video_id}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ùå ERRO na an√°lise psicol√≥gica para {video_id}: {e}\")\n",
        "                analises_psicologicas_completas.append({\"video_id\": video_id, \"status\": \"erro_analise_psicologica\", \"erro\": str(e)})\n",
        "        else:\n",
        "            print(f\"[{i}/{len(analises_padroes)}] Pulando {analise_padroes_data.get(\"video_id\")} - Status: {analise_padroes_data.get(\"status\", \"N/A\")}\")\n",
        "            analises_psicologicas_completas.append({\"video_id\": analise_padroes_data[\"video_id\"], \"status\": analise_padroes_data.get(\"status\", \"N/A\"), \"erro\": \"Pulado devido a erro anterior\"})\n",
        "\n",
        "    # Salvar an√°lises psicol√≥gicas completas\n",
        "    analises_json_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_psicologicas_completas.json\")\n",
        "    with open(analises_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(analises_psicologicas_completas, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    config[\"status_etapas\"][\"analise_psicologica\"] = True\n",
        "    config[\"total_videos_analisados_psicologicamente\"] = sucessos\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\"\"\n",
        "üíæ Dados de an√°lise psicol√≥gica salvos em: {analises_json_path}\"\"\")\n",
        "\n",
        "    print(\"\"\"\n",
        "‚úÖ AN√ÅLISE PSICOL√ìGICA CONCLU√çDA!\"\"\")\n",
        "    print(f\"Total de v√≠deos com an√°lise psicol√≥gica: {sucessos}\")\n",
        "\n",
        "    if sucessos == 0:\n",
        "        print(\"‚ùå NENHUM V√çDEO FOI ANALISADO PSICOLOGICAMENTE COM SUCESSO. Verifique as etapas anteriores.\")\n",
        "    print(\"\"\"\n",
        "‚û°Ô∏è PR√ìXIMA C√âLULA: 4.1 - GERA√á√ÉO DE RELAT√ìRIOS HUMANIZADOS\"\"\")\n",
        "\n",
        "# Executar an√°lise psicol√≥gica\n",
        "try:\n",
        "    processar_analise_psicologica_todos_videos()\n",
        "except Exception as e:\n",
        "    print(f\"\"\"\n",
        "‚ùå ERRO GERAL NA AN√ÅLISE PSICOL√ìGICA: {e}\"\"\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ],
      "metadata": {
        "id": "analise_psicologica"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# LAYER 4: GERA√á√ÉO DE RELAT√ìRIOS E BLUEPRINT ESTRAT√âGICO\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# C√âLULA 4.1: GERA√á√ÉO DE RELAT√ìRIOS HUMANIZADOS (√ÅUDIO, VISUAL, TEXTO, PSICOL√ìGICO)\n",
        "# ============================================================================\n",
        "\n",
        "from fpdf import FPDF # Importar FPDF para gera√ß√£o de PDF\n",
        "\n",
        "class PDF(FPDF):\n",
        "    def header(self):\n",
        "        self.set_font('Arial', 'B', 12)\n",
        "        self.cell(0, 10, 'Relat√≥rio de Engenharia Reversa de V√≠deos', 0, 1, 'C')\n",
        "        self.ln(10)\n",
        "\n",
        "    def footer(self):\n",
        "        self.set_y(-15)\n",
        "        self.set_font('Arial', 'I', 8)\n",
        "        self.cell(0, 10, f'P√°gina {self.page_no()}/{{nb}}', 0, 0, 'C')\n",
        "\n",
        "    def chapter_title(self, title):\n",
        "        self.set_font('Arial', 'B', 12)\n",
        "        self.cell(0, 10, title, 0, 1, 'L')\n",
        "        self.ln(5)\n",
        "\n",
        "    def chapter_body(self, body):\n",
        "        self.set_font('Arial', '', 10)\n",
        "        self.multi_cell(0, 5, body)\n",
        "        self.ln()\n",
        "\n",
        "def gerar_relatorio_texto(video_id, analise_padroes_data, pasta_destino):\n",
        "    df_texto = pd.DataFrame([analise_padroes_data])\n",
        "    excel_path = os.path.join(pasta_destino, f'RELATORIO_TEXTO_HUMANIZADO_{video_id}.xlsx')\n",
        "    df_texto.to_excel(excel_path, index=False, engine='openpyxl')\n",
        "\n",
        "    pdf = PDF()\n",
        "    pdf.add_page()\n",
        "    pdf.chapter_title('Estrat√©gia de Conte√∫do Textual')\n",
        "    pdf.chapter_body(f'Resumo do Texto: {analise_padroes_data.get('resumo_texto', 'N/A')}')\n",
        "    pdf.chapter_body(f'Palavras-chave: {', '.join(analise_padroes_data.get('palavras_chave_texto', []))}')\n",
        "    pdf_path = os.path.join(pasta_destino, f'ESTRATEGIA_CONTEUDO_TEXTUAL_{video_id}.pdf')\n",
        "    pdf.output(pdf_path)\n",
        "    return excel_path, pdf_path\n",
        "\n",
        "def gerar_relatorio_audio(video_id, analise_padroes_data, pasta_destino):\n",
        "    df_audio = pd.DataFrame([analise_padroes_data.get('analise_audio_detalhada', {})])\n",
        "    excel_path = os.path.join(pasta_destino, f'RELATORIO_AUDIO_HUMANIZADO_{video_id}.xlsx')\n",
        "    df_audio.to_excel(excel_path, index=False, engine='openpyxl')\n",
        "\n",
        "    pdf = PDF()\n",
        "    pdf.add_page()\n",
        "    pdf.chapter_title('Resumo de √Åudio Estrat√©gico')\n",
        "    pdf.chapter_body(f'BPM: {analise_padroes_data.get('analise_audio_detalhada', {}).get('bpm', 'N/A')}')\n",
        "    pdf.chapter_body(f'Dura√ß√£o do √Åudio: {analise_padroes_data.get('analise_audio_detalhada', {}).get('duracao_audio_segundos', 'N/A')} segundos')\n",
        "    pdf_path = os.path.join(pasta_destino, f'RESUMO_AUDIO_ESTRATEGICO_{video_id}.pdf')\n",
        "    pdf.output(pdf_path)\n",
        "    return excel_path, pdf_path\n",
        "\n",
        "def gerar_relatorio_visual(video_id, analise_padroes_data, pasta_destino):\n",
        "    df_visual = pd.DataFrame([analise_padroes_data.get('analise_visual_detalhada', {})])\n",
        "    excel_path = os.path.join(pasta_destino, f'RELATORIO_VISUAL_HUMANIZADO_{video_id}.xlsx')\n",
        "    df_visual.to_excel(excel_path, index=False, engine='openpyxl')\n",
        "\n",
        "    pdf = PDF()\n",
        "    pdf.add_page()\n",
        "    pdf.chapter_title('Estrat√©gia Visual Completa')\n",
        "    pdf.chapter_body(f'Total de Cortes: {analise_padroes_data.get('analise_visual_detalhada', {}).get('total_cortes', 'N/A')}')\n",
        "    pdf.chapter_body(f'Complexidade Visual M√©dia: {analise_padroes_data.get('analise_visual_detalhada', {}).get('complexidade_visual_media', 'N/A'):.2f}')\n",
        "    pdf.chapter_body(f'Brilho M√©dio: {analise_padroes_data.get('analise_visual_detalhada', {}).get('brilho_medio', 'N/A'):.2f}')\n",
        "    pdf_path = os.path.join(pasta_destino, f'ESTRATEGIA_VISUAL_COMPLETA_{video_id}.pdf')\n",
        "    pdf.output(pdf_path)\n",
        "    return excel_path, pdf_path\n",
        "\n",
        "def gerar_relatorio_psicologico(video_id, analise_psicologica_data, pasta_destino):\n",
        "    df_psico = pd.DataFrame([analise_psicologica_data])\n",
        "    excel_path = os.path.join(pasta_destino, f'RELATORIO_PSICOLOGICO_HUMANIZADO_{video_id}.xlsx')\n",
        "    df_psico.to_excel(excel_path, index=False, engine='openpyxl')\n",
        "\n",
        "    pdf = PDF()\n",
        "    pdf.add_page()\n",
        "    pdf.chapter_title('Manual de Psicologia Viral')\n",
        "    pdf.chapter_body(f'Gatilhos Detectados: {', '.join(analise_psicologica_data.get('gatilhos_detectados', []))}')\n",
        "    pdf.chapter_body(f'Emo√ß√µes Predominantes: {analise_psicologica_data.get('emocoes_predominantes', 'N/A')}')\n",
        "    pdf.chapter_body(f'Insights: {analise_psicologica_data.get('insights_psicologicos', 'N/A')}')\n",
        "    pdf_path = os.path.join(pasta_destino, f'MANUAL_PSICOLOGIA_VIRAL_{video_id}.pdf')\n",
        "    pdf.output(pdf_path)\n",
        "    return excel_path, pdf_path\n",
        "\n",
        "def processar_geracao_relatorios_todos_videos():\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa('analise_psicologica')\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Carregar dados de an√°lise de padr√µes e psicol√≥gica\n",
        "    analises_padroes_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_padroes_completas.json\")\n",
        "    analises_psicologicas_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_psicologicas_completas.json\")\n",
        "    with open(analises_padroes_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        analises_padroes = json.load(f)\n",
        "    with open(analises_psicologicas_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        analises_psicologicas = json.load(f)\n",
        "\n",
        "    sucessos = 0\n",
        "\n",
        "    print(f\"\"\"\n",
        "Iniciando gera√ß√£o de relat√≥rios humanizados para {len(analises_padroes)} v√≠deos...\"\"\")\n",
        "\n",
        "    for i, analise_padroes_data in enumerate(analises_padroes, 1):\n",
        "        video_id = analise_padroes_data[\"video_id\"]\n",
        "        analise_psicologica_data = next((a for a in analises_psicologicas if a[\"video_id\"] == video_id), None)\n",
        "\n",
        "        if analise_padroes_data.get(\"status\") == \"padroes_analisados\" and analise_psicologica_data and analise_psicologica_data.get(\"status\") == \"analise_psicologica_concluida\":\n",
        "            print(f\"[{i}/{len(analises_padroes)}] Gerando relat√≥rios para: {video_id}\")\n",
        "            try:\n",
        "                # Gera√ß√£o de Relat√≥rios de Texto\n",
        "                pasta_texto = os.path.join(PASTA_TRABALHO, \"analise_texto\")\n",
        "                os.makedirs(pasta_texto, exist_ok=True)\n",
        "                excel_text, pdf_text = gerar_relatorio_texto(video_id, analise_padroes_data, pasta_texto)\n",
        "                print(f\"  üíæ Relat√≥rio de Texto (XLSX) salvo em: {excel_text}\")\n",
        "                print(f\"  üíæ Estrat√©gia de Conte√∫do Textual (PDF) salvo em: {pdf_text}\")\n",
        "\n",
        "                # Gera√ß√£o de Relat√≥rios de √Åudio\n",
        "                pasta_audio = os.path.join(PASTA_TRABALHO, \"analise_audio\")\n",
        "                os.makedirs(pasta_audio, exist_ok=True)\n",
        "                excel_audio, pdf_audio = gerar_relatorio_audio(video_id, analise_padroes_data, pasta_audio)\n",
        "                print(f\"  üíæ Relat√≥rio de √Åudio (XLSX) salvo em: {excel_audio}\")\n",
        "                print(f\"  üíæ Resumo de √Åudio Estrat√©gico (PDF) salvo em: {pdf_audio}\")\n",
        "\n",
        "                # Gera√ß√£o de Relat√≥rios Visuais\n",
        "                pasta_visual = os.path.join(PASTA_TRABALHO, \"analise_visual\")\n",
        "                os.makedirs(pasta_visual, exist_ok=True)\n",
        "                excel_visual, pdf_visual = gerar_relatorio_visual(video_id, analise_padroes_data, pasta_visual)\n",
        "                print(f\"  üíæ Relat√≥rio Visual (XLSX) salvo em: {excel_visual}\")\n",
        "                print(f\"  üíæ Estrat√©gia Visual Completa (PDF) salvo em: {pdf_visual}\")\n",
        "\n",
        "                # Gera√ß√£o de Relat√≥rios Psicol√≥gicos\n",
        "                pasta_psicologica = os.path.join(PASTA_TRABALHO, \"analise_psicologica\")\n",
        "                os.makedirs(pasta_psicologica, exist_ok=True)\n",
        "                excel_psico, pdf_psico = gerar_relatorio_psicologico(video_id, analise_psicologica_data, pasta_psicologica)\n",
        "                print(f\"  üíæ Relat√≥rio Psicol√≥gico (XLSX) salvo em: {excel_psico}\")\n",
        "                print(f\"  üíæ Manual de Psicologia Viral (PDF) salvo em: {pdf_psico}\")\n",
        "\n",
        "                sucessos += 1\n",
        "                print(f\"  ‚úÖ Relat√≥rios gerados para {video_id}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ùå ERRO na gera√ß√£o de relat√≥rios para {video_id}: {e}\")\n",
        "        else:\n",
        "            print(f\"[{i}/{len(analises_padroes)}] Pulando {video_id} - Pr√©-requisitos n√£o atendidos.\")\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    config[\"status_etapas\"][\"relatorios_humanizados\"] = True\n",
        "    config[\"total_videos_relatorios_gerados\"] = sucessos\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(\"\"\"\n",
        "‚úÖ GERA√á√ÉO DE RELAT√ìRIOS HUMANIZADOS CONCLU√çDA!\"\"\")\n",
        "    print(f\"Total de v√≠deos com relat√≥rios gerados: {sucessos}\")\n",
        "\n",
        "    if sucessos == 0:\n",
        "        print(\"‚ùå NENHUM V√çDEO TEVE RELAT√ìRIOS GERADOS COM SUCESSO. Verifique as etapas anteriores.\")\n",
        "    print(\"\"\"\n",
        "‚û°Ô∏è PR√ìXIMA C√âLULA: 4.2 - GERA√á√ÉO DO BLUEPRINT FINAL E DASHBOARD\"\"\")\n",
        "\n",
        "# Executar gera√ß√£o de relat√≥rios\n",
        "try:\n",
        "    processar_geracao_relatorios_todos_videos()\n",
        "except Exception as e:\n",
        "    print(f\"\"\"\n",
        "‚ùå ERRO GERAL NA GERA√á√ÉO DE RELAT√ìRIOS: {e}\"\"\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ],
      "metadata": {
        "id": "relatorios_humanizados"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 4.2: GERA√á√ÉO DO BLUEPRINT FINAL E DASHBOARD\n",
        "# ============================================================================\n",
        "\n",
        "def gerar_blueprint_dashboard():\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa(\"relatorios_humanizados\")\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Carregar todos os dados de an√°lise\n",
        "    metadados_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "    decomposicao_path = os.path.join(PASTA_TRABALHO, \"dados\", \"decomposicao_completa.json\")\n",
        "    analises_padroes_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_padroes_completas.json\")\n",
        "    analises_psicologicas_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_psicologicas_completas.json\")\n",
        "\n",
        "    with open(metadados_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        metadados = json.load(f)\n",
        "    with open(decomposicao_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        decomposicoes = json.load(f)\n",
        "    with open(analises_padroes_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        analises_padroes = json.load(f)\n",
        "    with open(analises_psicologicas_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        analises_psicologicas = json.load(f)\n",
        "\n",
        "    dados_consolidados = []\n",
        "    for video_meta in metadados:\n",
        "        video_id = video_meta[\"id\"]\n",
        "        decomposicao = next((d for d in decomposicoes if d[\"video_id\"] == video_id), {})\n",
        "        analise_padroes = next((ap for ap in analises_padroes if ap[\"video_id\"] == video_id), {})\n",
        "        analise_psicologica = next((aps for aps in analises_psicologicas if aps[\"video_id\"] == video_id), {})\n",
        "        consolidado = {\n",
        "            \"video_id\": video_id,\n",
        "            \"nome_arquivo\": video_meta.get(\"nome_arquivo\"),\n",
        "            \"duracao_segundos\": video_meta.get(\"duracao_segundos\"),\n",
        "            \"formato_detectado\": video_meta.get(\"formato_detectado\"),\n",
        "            \"tem_audio\": video_meta.get(\"tem_audio\"),\n",
        "            \"total_frames\": video_meta.get(\"total_frames\"),\n",
        "            \"ocr_textos_count\": len(decomposicao.get(\"textos_ocr\", [])),\n",
        "            \"audio_transcrito_len\": len(decomposicao.get(\"audio_transcrito\", \"\")),\n",
        "            \"cortes_detectados_count\": len(decomposicao.get(\"cortes_detectados_segundos\", [])),\n",
        "            \"bpm_audio\": analise_padroes.get(\"analise_audio_detalhada\", {}).get(\"bpm\"),\n",
        "            \"complexidade_visual_media\": analise_padroes.get(\"analise_visual_detalhada\", {}).get(\"complexidade_visual_media\"),\n",
        "            \"brilho_medio\": analise_padroes.get(\"analise_visual_detalhada\", {}).get(\"brilho_medio\"),\n",
        "            \"padroes_gerais\": \", \".join(analise_padroes.get(\"padroes_gerais\", [])),\n",
        "            \"gatilhos_psicologicos\": \", \".join(analise_psicologica.get(\"gatilhos_detectados\", [])),\n",
        "            \"emocoes_predominantes\": str(analise_psicologica.get(\"emocoes_predominantes\", {})),\n",
        "            \"status_geral\": video_meta.get(\"status\") # Pode ser aprimorado para refletir o status de todas as etapas\n",
        "        }\n",
        "        dados_consolidados.append(consolidado)\n",
        "\n",
        "    df_final = pd.DataFrame(dados_consolidados)\n",
        "\n",
        "    # Salvar Dashboard Executivo (Excel)\n",
        "    dashboard_excel_path = os.path.join(PASTA_TRABALHO, \"dashboard\", \"DASHBOARD_MASTER_EXECUTIVO.xlsx\")\n",
        "    df_final.to_excel(dashboard_excel_path, index=False, engine=\"openpyxl\")\n",
        "    print(f\"\\nüíæ Dashboard Executivo (XLSX) salvo em: {dashboard_excel_path}\")\n",
        "\n",
        "    # Salvar Dados Consolidados (CSV e JSON)\n",
        "    dados_csv_path = os.path.join(PASTA_TRABALHO, \"dashboard\", \"dados_consolidados.csv\")\n",
        "    df_final.to_csv(dados_csv_path, index=False, encoding=\"utf-8\")\n",
        "    print(f\"üíæ Dados Consolidados (CSV) salvo em: {dados_csv_path}\")\n",
        "\n",
        "    dados_json_path = os.path.join(PASTA_TRABALHO, \"dashboard\", \"dados_detalhados.json\")\n",
        "    with open(dados_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(dados_consolidados, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"üíæ Dados Detalhados (JSON) salvo em: {dados_json_path}\")\n",
        "\n",
        "    # Gera√ß√£o de Dashboard Interativo (HTML - Exemplo simples)\n",
        "    # Para um dashboard interativo real, seria necess√°rio uma biblioteca como Plotly ou Dash\n",
        "    dashboard_html_path = os.path.join(PASTA_TRABALHO, \"dashboard\", \"dashboard_interativo.html\")\n",
        "    with open(dashboard_html_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"<html><body><h1>Dashboard Interativo (Placeholder)</h1><p>Seu dashboard interativo real seria gerado aqui com bibliotecas como Plotly ou Dash.</p></body></html>\")\n",
        "    print(f\"üíæ Dashboard Interativo (HTML) salvo em: {dashboard_html_path}\")\n",
        "\n",
        "    # Gera√ß√£o do Blueprint Estrat√©gico (PDF - Exemplo simples)\n",
        "    pdf = PDF()\n",
        "    pdf.add_page()\n",
        "    pdf.chapter_title(\"BLUEPRINT ESTRAT√âGICO FINAL\")\n",
        "    pdf.chapter_body(\"Este √© o seu blueprint estrat√©gico final, consolidando todos os insights.\")\n",
        "    pdf.chapter_body(f\"Total de v√≠deos analisados: {len(df_final)}\")\n",
        "    pdf.chapter_body(f\"M√©dia de dura√ß√£o dos v√≠deos: {df_final[\"duracao_segundos\"] .mean():.2f} segundos\")\n",
        "    pdf_blueprint_path = os.path.join(PASTA_TRABALHO, \"blueprint\", \"BLUEPRINT_ESTRATEGICO_FINAL.pdf\")\n",
        "    pdf.output(pdf_blueprint_path)\n",
        "    print(f\"üíæ Blueprint Estrat√©gico (PDF) salvo em: {pdf_blueprint_path}\")\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    config[\"status_etapas\"][\"blueprint\"] = True\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(\"\\n‚úÖ GERA√á√ÉO DO BLUEPRINT FINAL E DASHBOARD CONCLU√çDA!\")\n",
        "    print(\"Todos os relat√≥rios e o dashboard foram gerados com sucesso.\")\n",
        "    print(\"\\nüéâ PROCESSO DE ENGENHARIA REVERSA CONCLU√çDO COM SUCESSO! üéâ\")\n",
        "\n",
        "# Executar gera√ß√£o de blueprint e dashboard\n",
        "try:\n",
        "    gerar_blueprint_dashboard()\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå ERRO GERAL NA GERA√á√ÉO DO BLUEPRINT E DASHBOARD: {e}\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cT5PyNglpi2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 4.3: DASHBOARD MASTER EXECUTIVO INTELIGENTE APRIMORADO\n",
        "# ============================================================================\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "from openpyxl.styles import Font, Alignment, PatternFill\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def log_progress(message):\n",
        "    \"\"\"Log de progresso em tempo real\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
        "    print(f\"[{timestamp}] {message}\")\n",
        "\n",
        "def calculate_viral_score(row):\n",
        "    \"\"\"Calcula score de viralidade baseado em m√∫ltiplos fatores\"\"\"\n",
        "    try:\n",
        "        score = 0\n",
        "\n",
        "        # Fator 1: Ritmo (cortes por segundo) - peso 25%\n",
        "        if pd.notna(row['duracao_segundos']) and row['duracao_segundos'] > 0:\n",
        "            cortes_por_seg = row['cortes_detectados_count'] / row['duracao_segundos']\n",
        "            if cortes_por_seg > 20: score += 25\n",
        "            elif cortes_por_seg > 10: score += 20\n",
        "            elif cortes_por_seg > 5: score += 15\n",
        "            else: score += 10\n",
        "\n",
        "        # Fator 2: Complexidade Visual - peso 20%\n",
        "        if pd.notna(row['complexidade_visual_media']):\n",
        "            if row['complexidade_visual_media'] > 600: score += 20\n",
        "            elif row['complexidade_visual_media'] > 400: score += 15\n",
        "            else: score += 10\n",
        "\n",
        "        # Fator 3: Presen√ßa de Texto (OCR) - peso 15%\n",
        "        if pd.notna(row['ocr_textos_count']):\n",
        "            if row['ocr_textos_count'] > 10: score += 15\n",
        "            elif row['ocr_textos_count'] > 5: score += 12\n",
        "            elif row['ocr_textos_count'] > 0: score += 8\n",
        "\n",
        "        # Fator 4: Dura√ß√£o Ideal - peso 20%\n",
        "        if pd.notna(row['duracao_segundos']):\n",
        "            if 15 <= row['duracao_segundos'] <= 30: score += 20\n",
        "            elif 10 <= row['duracao_segundos'] <= 45: score += 15\n",
        "            else: score += 10\n",
        "\n",
        "        # Fator 5: Gatilhos Psicol√≥gicos - peso 20%\n",
        "        gatilhos = str(row['gatilhos_psicologicos']).lower()\n",
        "        if 'urg√™ncia' in gatilhos or 'escassez' in gatilhos: score += 8\n",
        "        if 'est√≠mulo' in gatilhos: score += 7\n",
        "        if 'aten√ß√£o' in gatilhos: score += 5\n",
        "\n",
        "        return min(score, 100)\n",
        "    except:\n",
        "        return 50\n",
        "\n",
        "def calculate_technical_score(row):\n",
        "    \"\"\"Score t√©cnico baseado em qualidade de produ√ß√£o\"\"\"\n",
        "    try:\n",
        "        score = 0\n",
        "\n",
        "        if pd.notna(row['brilho_medio']):\n",
        "            if 120 <= row['brilho_medio'] <= 180: score += 25\n",
        "            elif 100 <= row['brilho_medio'] <= 200: score += 20\n",
        "            else: score += 10\n",
        "\n",
        "        formato = str(row['formato_detectado'])\n",
        "        if 'vertical_9_16' in formato: score += 25\n",
        "        elif 'horizontal_16_9' in formato: score += 20\n",
        "        else: score += 15\n",
        "\n",
        "        if row['tem_audio']: score += 25\n",
        "        else: score += 5\n",
        "\n",
        "        if pd.notna(row['total_frames']) and row['total_frames'] > 0:\n",
        "            if row['total_frames'] > 300: score += 25\n",
        "            elif row['total_frames'] > 150: score += 20\n",
        "            else: score += 15\n",
        "\n",
        "        return min(score, 100)\n",
        "    except:\n",
        "        return 50\n",
        "\n",
        "def calculate_content_score(row):\n",
        "    \"\"\"Score de conte√∫do baseado em riqueza informacional\"\"\"\n",
        "    try:\n",
        "        score = 0\n",
        "\n",
        "        ocr_count = row['ocr_textos_count'] if pd.notna(row['ocr_textos_count']) else 0\n",
        "        audio_len = row['audio_transcrito_len'] if pd.notna(row['audio_transcrito_len']) else 0\n",
        "\n",
        "        if ocr_count > 5 or audio_len > 100: score += 30\n",
        "        elif ocr_count > 2 or audio_len > 50: score += 20\n",
        "        elif ocr_count > 0 or audio_len > 0: score += 15\n",
        "        else: score += 5\n",
        "\n",
        "        if pd.notna(row['bpm_audio']):\n",
        "            if 120 <= row['bpm_audio'] <= 140: score += 35\n",
        "            elif 100 <= row['bpm_audio'] <= 160: score += 25\n",
        "            else: score += 15\n",
        "\n",
        "        if pd.notna(row['duracao_segundos']) and row['duracao_segundos'] > 0:\n",
        "            densidade = (ocr_count + audio_len/10) / row['duracao_segundos']\n",
        "            if densidade > 2: score += 35\n",
        "            elif densidade > 1: score += 25\n",
        "            else: score += 15\n",
        "\n",
        "        return min(score, 100)\n",
        "    except:\n",
        "        return 50\n",
        "\n",
        "def generate_insights_from_data(df):\n",
        "    \"\"\"Gera insights inteligentes baseados nos dados\"\"\"\n",
        "    insights = []\n",
        "\n",
        "    try:\n",
        "        best_performing = df.nlargest(3, 'viral_score')\n",
        "        avg_duration = best_performing['duracao_segundos'].mean()\n",
        "        insights.append(f\"DURA√á√ÉO VENCEDORA: Seus top 3 v√≠deos t√™m dura√ß√£o m√©dia de {avg_duration:.1f}s. Este √© seu sweet spot comprovado.\")\n",
        "\n",
        "        avg_cuts_per_sec = (best_performing['cortes_detectados_count'] / best_performing['duracao_segundos']).mean()\n",
        "        insights.append(f\"RITMO IDEAL: {avg_cuts_per_sec:.1f} cortes por segundo √© sua f√≥rmula de edi√ß√£o mais eficaz.\")\n",
        "\n",
        "        formato_winner = df['formato_detectado'].mode()[0] if not df['formato_detectado'].empty else 'N/A'\n",
        "        formato_count = df['formato_detectado'].value_counts().iloc[0] if not df['formato_detectado'].empty else 0\n",
        "        insights.append(f\"FORMATO DOMINANTE: {formato_count} v√≠deos em {formato_winner}. Este √© seu formato de maior alcance.\")\n",
        "\n",
        "        high_viral = df[df['viral_score'] > 70]\n",
        "        if not high_viral.empty:\n",
        "            avg_complexity = high_viral['complexidade_visual_media'].mean()\n",
        "            insights.append(f\"COMPLEXIDADE VISUAL √ìTIMA: V√≠deos com score viral alto t√™m complexidade m√©dia de {avg_complexity:.0f}. Use como refer√™ncia.\")\n",
        "\n",
        "        text_heavy = df[df['ocr_textos_count'] > 5]\n",
        "        if not text_heavy.empty:\n",
        "            insights.append(f\"ESTRAT√âGIA DE TEXTO: {len(text_heavy)} v√≠deos com muito texto t√™m score m√©dio de {text_heavy['viral_score'].mean():.0f}. Texto na tela impacta performance.\")\n",
        "\n",
        "        # CORRIGIDO: bpm_audio em vez de bmp_audio\n",
        "        if df['bpm_audio'].notna().any():\n",
        "            successful_bpm = df[df['viral_score'] > 60]['bpm_audio'].mean()\n",
        "            insights.append(f\"BPM DE SUCESSO: {successful_bpm:.0f} BPM √© o ritmo de √°udio dos seus v√≠deos mais virais.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_progress(f\"Erro ao gerar insights: {e}\")\n",
        "        insights.append(\"Insights parciais dispon√≠veis devido a limita√ß√µes nos dados.\")\n",
        "\n",
        "    return insights\n",
        "\n",
        "def add_data_to_sheet(ws, data, start_row=1, start_col=1, headers=None):\n",
        "    \"\"\"Adiciona dados a uma planilha de forma segura\"\"\"\n",
        "    current_row = start_row\n",
        "\n",
        "    # Adicionar cabe√ßalhos se fornecidos\n",
        "    if headers:\n",
        "        for col_idx, header in enumerate(headers):\n",
        "            cell = ws.cell(row=current_row, column=start_col + col_idx)\n",
        "            cell.value = header\n",
        "            cell.font = Font(bold=True)\n",
        "        current_row += 1\n",
        "\n",
        "    # Adicionar dados\n",
        "    for row_data in data:\n",
        "        for col_idx, value in enumerate(row_data):\n",
        "            cell = ws.cell(row=current_row, column=start_col + col_idx)\n",
        "            cell.value = value\n",
        "        current_row += 1\n",
        "\n",
        "    return current_row\n",
        "\n",
        "def create_enhanced_dashboard_master(csv_path, json_path, output_path):\n",
        "    \"\"\"Cria dashboard master executivo aprimorado\"\"\"\n",
        "\n",
        "    log_progress(\"INICIANDO CRIA√á√ÉO DO DASHBOARD MASTER EXECUTIVO INTELIGENTE\")\n",
        "\n",
        "    try:\n",
        "        # Carregar dados\n",
        "        log_progress(\"Carregando dados consolidados...\")\n",
        "        df_consolidado = pd.read_csv(csv_path, encoding='utf-8')\n",
        "\n",
        "        with open(json_path, 'r', encoding='utf-8') as f:\n",
        "            dados_detalhados = json.load(f)\n",
        "\n",
        "        log_progress(f\"Dados carregados: {len(df_consolidado)} v√≠deos encontrados\")\n",
        "\n",
        "        # Pr√©-processamento inteligente\n",
        "        log_progress(\"Processando intelig√™ncia artificial dos dados...\")\n",
        "\n",
        "        # Limpar e converter dados\n",
        "        try:\n",
        "            df_consolidado['emocoes_predominantes'] = df_consolidado['emocoes_predominantes'].apply(\n",
        "                lambda x: json.loads(x.replace(\"'\", '\"')) if pd.notna(x) and x != '{}' else {}\n",
        "            )\n",
        "        except:\n",
        "            df_consolidado['emocoes_predominantes'] = [{}] * len(df_consolidado)\n",
        "\n",
        "        # Calcular scores inteligentes\n",
        "        log_progress(\"Calculando scores de performance...\")\n",
        "        df_consolidado['viral_score'] = df_consolidado.apply(calculate_viral_score, axis=1)\n",
        "        df_consolidado['technical_score'] = df_consolidado.apply(calculate_technical_score, axis=1)\n",
        "        df_consolidado['content_score'] = df_consolidado.apply(calculate_content_score, axis=1)\n",
        "        df_consolidado['overall_score'] = (df_consolidado['viral_score'] + df_consolidado['technical_score'] + df_consolidado['content_score']) / 3\n",
        "\n",
        "        # Calcular m√©tricas avan√ßadas\n",
        "        df_consolidado['cortes_por_segundo'] = df_consolidado['cortes_detectados_count'] / df_consolidado['duracao_segundos'].replace(0, 1)\n",
        "        df_consolidado['densidade_texto'] = df_consolidado['ocr_textos_count'] / df_consolidado['duracao_segundos'].replace(0, 1)\n",
        "        df_consolidado['eficiencia_audio'] = df_consolidado['audio_transcrito_len'] / df_consolidado['duracao_segundos'].replace(0, 1)\n",
        "\n",
        "        log_progress(\"Gerando insights estrat√©gicos...\")\n",
        "        insights = generate_insights_from_data(df_consolidado)\n",
        "\n",
        "        # Criar workbook\n",
        "        log_progress(\"Criando estrutura do dashboard...\")\n",
        "        wb = Workbook()\n",
        "\n",
        "        # === ABA 1: EXECUTIVE SUMMARY ===\n",
        "        log_progress(\"Criando Executive Summary...\")\n",
        "        ws_summary = wb.active\n",
        "        ws_summary.title = 'Executive Summary'\n",
        "\n",
        "        # Header principal\n",
        "        header_cell = ws_summary.cell(row=1, column=1)\n",
        "        header_cell.value = 'DASHBOARD MASTER EXECUTIVO - ENGENHARIA REVERSA DE V√çDEOS'\n",
        "        header_cell.font = Font(bold=True, size=18, color='FFFFFF')\n",
        "        header_cell.fill = PatternFill(start_color='1F4E79', end_color='1F4E79', fill_type='solid')\n",
        "        header_cell.alignment = Alignment(horizontal='center', vertical='center')\n",
        "\n",
        "        # Expandir header manualmente\n",
        "        for col in range(2, 9):\n",
        "            cell = ws_summary.cell(row=1, column=col)\n",
        "            cell.fill = PatternFill(start_color='1F4E79', end_color='1F4E79', fill_type='solid')\n",
        "\n",
        "        # KPIs Principais\n",
        "        kpi_cell = ws_summary.cell(row=3, column=1)\n",
        "        kpi_cell.value = 'INDICADORES DE PERFORMANCE PRINCIPAIS'\n",
        "        kpi_cell.font = Font(bold=True, size=14)\n",
        "        kpi_cell.fill = PatternFill(start_color='E7E6E6', end_color='E7E6E6', fill_type='solid')\n",
        "\n",
        "        kpis_data = [\n",
        "            ['Total de V√≠deos Analisados', len(df_consolidado)],\n",
        "            ['Score Viral M√©dio', f\"{df_consolidado['viral_score'].mean():.1f}/100\"],\n",
        "            ['Score T√©cnico M√©dio', f\"{df_consolidado['technical_score'].mean():.1f}/100\"],\n",
        "            ['Score de Conte√∫do M√©dio', f\"{df_consolidado['content_score'].mean():.1f}/100\"],\n",
        "            ['Dura√ß√£o M√©dia Otimizada', f\"{df_consolidado['duracao_segundos'].mean():.1f}s\"],\n",
        "            ['Ritmo M√©dio de Cortes', f\"{df_consolidado['cortes_por_segundo'].mean():.1f}/seg\"],\n",
        "        ]\n",
        "\n",
        "        add_data_to_sheet(ws_summary, kpis_data, start_row=4, start_col=1)\n",
        "\n",
        "        # Top 3 V√≠deos\n",
        "        top3_cell = ws_summary.cell(row=3, column=4)\n",
        "        top3_cell.value = 'TOP 3 V√çDEOS POR PERFORMANCE'\n",
        "        top3_cell.font = Font(bold=True, size=14)\n",
        "        top3_cell.fill = PatternFill(start_color='E7E6E6', end_color='E7E6E6', fill_type='solid')\n",
        "\n",
        "        top3 = df_consolidado.nlargest(3, 'overall_score')[['nome_arquivo', 'overall_score', 'viral_score', 'technical_score', 'content_score']]\n",
        "\n",
        "        top3_data = []\n",
        "        for _, video in top3.iterrows():\n",
        "            nome_curto = video['nome_arquivo'][:30] + \"...\" if len(video['nome_arquivo']) > 30 else video['nome_arquivo']\n",
        "            top3_data.append([\n",
        "                nome_curto,\n",
        "                f\"{video['overall_score']:.1f}\",\n",
        "                f\"{video['viral_score']:.1f}\",\n",
        "                f\"{video['technical_score']:.1f}\",\n",
        "                f\"{video['content_score']:.1f}\"\n",
        "            ])\n",
        "\n",
        "        top3_headers = ['V√≠deo', 'Score Geral', 'Viral', 'T√©cnico', 'Conte√∫do']\n",
        "        add_data_to_sheet(ws_summary, top3_data, start_row=4, start_col=4, headers=top3_headers)\n",
        "\n",
        "        # Insights Estrat√©gicos\n",
        "        insights_cell = ws_summary.cell(row=12, column=1)\n",
        "        insights_cell.value = 'INSIGHTS ESTRAT√âGICOS BASEADOS EM IA'\n",
        "        insights_cell.font = Font(bold=True, size=14, color='FFFFFF')\n",
        "        insights_cell.fill = PatternFill(start_color='C5504B', end_color='C5504B', fill_type='solid')\n",
        "        insights_cell.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Adicionar insights\n",
        "        for i, insight in enumerate(insights, 13):\n",
        "            insight_cell = ws_summary.cell(row=i, column=1)\n",
        "            insight_cell.value = f\"‚Ä¢ {insight}\"\n",
        "            insight_cell.alignment = Alignment(wrap_text=True)\n",
        "\n",
        "        # === ABA 2: AN√ÅLISE DE PERFORMANCE ===\n",
        "        log_progress(\"Criando An√°lise de Performance...\")\n",
        "        ws_performance = wb.create_sheet('An√°lise de Performance')\n",
        "\n",
        "        perf_header = ws_performance.cell(row=1, column=1)\n",
        "        perf_header.value = 'AN√ÅLISE DETALHADA DE PERFORMANCE'\n",
        "        perf_header.font = Font(bold=True, size=16)\n",
        "        perf_header.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Ranking completo\n",
        "        ranking_data = df_consolidado[['nome_arquivo', 'overall_score', 'viral_score', 'technical_score', 'content_score',\n",
        "                                     'duracao_segundos', 'cortes_por_segundo', 'formato_detectado']].sort_values('overall_score', ascending=False)\n",
        "\n",
        "        ranking_list = []\n",
        "        for _, video in ranking_data.iterrows():\n",
        "            nome_curto = video['nome_arquivo'][:40] + \"...\" if len(video['nome_arquivo']) > 40 else video['nome_arquivo']\n",
        "            ranking_list.append([\n",
        "                nome_curto,\n",
        "                f\"{video['overall_score']:.1f}\",\n",
        "                f\"{video['viral_score']:.1f}\",\n",
        "                f\"{video['technical_score']:.1f}\",\n",
        "                f\"{video['content_score']:.1f}\",\n",
        "                f\"{video['duracao_segundos']:.1f}s\",\n",
        "                f\"{video['cortes_por_segundo']:.1f}\",\n",
        "                video['formato_detectado']\n",
        "            ])\n",
        "\n",
        "        ranking_headers = ['V√≠deo', 'Score Geral', 'Viral', 'T√©cnico', 'Conte√∫do', 'Dura√ß√£o', 'Cortes/s', 'Formato']\n",
        "        add_data_to_sheet(ws_performance, ranking_list, start_row=3, start_col=1, headers=ranking_headers)\n",
        "\n",
        "        # === ABA 3: INTELIG√äNCIA T√âCNICA ===\n",
        "        log_progress(\"Criando Intelig√™ncia T√©cnica...\")\n",
        "        ws_tecnica = wb.create_sheet('Intelig√™ncia T√©cnica')\n",
        "\n",
        "        tec_header = ws_tecnica.cell(row=1, column=1)\n",
        "        tec_header.value = 'AN√ÅLISE T√âCNICA AVAN√áADA'\n",
        "        tec_header.font = Font(bold=True, size=16)\n",
        "        tec_header.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # An√°lise de correla√ß√µes\n",
        "        corr_header = ws_tecnica.cell(row=3, column=1)\n",
        "        corr_header.value = 'CORRELA√á√ïES DESCOBERTAS'\n",
        "        corr_header.font = Font(bold=True, size=12)\n",
        "\n",
        "        correlations_data = [\n",
        "            ['Dura√ß√£o vs Score Viral', f\"{df_consolidado['duracao_segundos'].corr(df_consolidado['viral_score']):.3f}\", 'CORRELA√á√ÉO MODERADA'],\n",
        "            ['Cortes/s vs Score Viral', f\"{df_consolidado['cortes_por_segundo'].corr(df_consolidado['viral_score']):.3f}\", 'CORRELA√á√ÉO MODERADA'],\n",
        "            ['Complexidade Visual vs Performance', f\"{df_consolidado['complexidade_visual_media'].corr(df_consolidado['overall_score']):.3f}\", 'CORRELA√á√ÉO FRACA'],\n",
        "            ['BPM vs Engajamento', f\"{df_consolidado['bpm_audio'].corr(df_consolidado['viral_score']) if df_consolidado['bpm_audio'].notna().any() else 0:.3f}\", 'CORRELA√á√ÉO FRACA'],\n",
        "        ]\n",
        "\n",
        "        corr_headers = ['M√©trica', 'Correla√ß√£o', 'Classifica√ß√£o']\n",
        "        add_data_to_sheet(ws_tecnica, correlations_data, start_row=4, start_col=1, headers=corr_headers)\n",
        "\n",
        "        # === ABA 4: BLUEPRINT DE PRODU√á√ÉO ===\n",
        "        log_progress(\"Criando Blueprint de Produ√ß√£o...\")\n",
        "        ws_blueprint = wb.create_sheet('Blueprint de Produ√ß√£o')\n",
        "\n",
        "        bp_header = ws_blueprint.cell(row=1, column=1)\n",
        "        bp_header.value = 'BLUEPRINT ESTRAT√âGICO DE PRODU√á√ÉO'\n",
        "        bp_header.font = Font(bold=True, size=16, color='FFFFFF')\n",
        "        bp_header.fill = PatternFill(start_color='1F4E79', end_color='1F4E79', fill_type='solid')\n",
        "        bp_header.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Receita de sucesso baseada nos top performers\n",
        "        top_performers = df_consolidado[df_consolidado['overall_score'] > df_consolidado['overall_score'].quantile(0.7)]\n",
        "\n",
        "        blueprint_data = [\n",
        "            ['DURA√á√ÉO IDEAL', f\"{top_performers['duracao_segundos'].mean():.1f} segundos (¬±{top_performers['duracao_segundos'].std():.1f}s)\"],\n",
        "            ['RITMO DE EDI√á√ÉO', f\"{top_performers['cortes_por_segundo'].mean():.1f} cortes por segundo\"],\n",
        "            ['FORMATO VENCEDOR', top_performers['formato_detectado'].mode()[0] if not top_performers.empty else 'N/A'],\n",
        "            ['COMPLEXIDADE VISUAL', f\"N√≠vel {top_performers['complexidade_visual_media'].mean():.0f} (escala de est√≠mulo)\"],\n",
        "            ['BPM RECOMENDADO', f\"{top_performers['bpm_audio'].mean():.0f} BPM\" if top_performers['bpm_audio'].notna().any() else 'N/A'],\n",
        "            ['DENSIDADE DE TEXTO', f\"{top_performers['densidade_texto'].mean():.1f} textos por segundo\"],\n",
        "        ]\n",
        "\n",
        "        bp_sub_header = ws_blueprint.cell(row=3, column=1)\n",
        "        bp_sub_header.value = 'F√ìRMULA DE SUCESSO BASEADA EM DADOS'\n",
        "        bp_sub_header.font = Font(bold=True, size=12)\n",
        "\n",
        "        add_data_to_sheet(ws_blueprint, blueprint_data, start_row=4, start_col=1)\n",
        "\n",
        "        # === ABA 5: RECOMENDA√á√ïES ESTRAT√âGICAS ===\n",
        "        log_progress(\"Criando Recomenda√ß√µes Estrat√©gicas...\")\n",
        "        ws_recomendacoes = wb.create_sheet('Recomenda√ß√µes Estrat√©gicas')\n",
        "\n",
        "        rec_header = ws_recomendacoes.cell(row=1, column=1)\n",
        "        rec_header.value = 'RECOMENDA√á√ïES ESTRAT√âGICAS BASEADAS EM IA'\n",
        "        rec_header.font = Font(bold=True, size=16, color='FFFFFF')\n",
        "        rec_header.fill = PatternFill(start_color='C5504B', end_color='C5504B', fill_type='solid')\n",
        "        rec_header.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Recomenda√ß√µes inteligentes baseadas nos dados\n",
        "        recommendations = []\n",
        "\n",
        "        # An√°lise de dura√ß√£o\n",
        "        if df_consolidado['duracao_segundos'].mean() > 60:\n",
        "            recommendations.append(['DURA√á√ÉO', 'REDUZA DURA√á√ÉO', 'Seus v√≠deos est√£o longos demais. V√≠deos de 15-30s t√™m melhor performance.', 'ALTA'])\n",
        "        elif df_consolidado['duracao_segundos'].mean() < 15:\n",
        "            recommendations.append(['DURA√á√ÉO', 'AUMENTE DURA√á√ÉO', 'V√≠deos muito curtos podem n√£o transmitir valor suficiente.', 'M√âDIA'])\n",
        "\n",
        "        # An√°lise de ritmo\n",
        "        avg_cuts_per_sec = df_consolidado['cortes_por_segundo'].mean()\n",
        "        if avg_cuts_per_sec < 5:\n",
        "            recommendations.append(['EDI√á√ÉO', 'ACELERE O RITMO', 'Aumente o n√∫mero de cortes para manter aten√ß√£o. Meta: 8-12 cortes/segundo.', 'ALTA'])\n",
        "        elif avg_cuts_per_sec > 20:\n",
        "            recommendations.append(['EDI√á√ÉO', 'DIMINUA CORTES', 'Muitos cortes podem causar fadiga visual. Encontre o equil√≠brio.', 'M√âDIA'])\n",
        "\n",
        "        # An√°lise de formato\n",
        "        formato_dominante = df_consolidado['formato_detectado'].mode()[0] if not df_consolidado['formato_detectado'].empty else 'N/A'\n",
        "        if 'horizontal' in formato_dominante.lower():\n",
        "            recommendations.append(['FORMATO', 'FOQUE EM VERTICAL', 'Formato vertical (9:16) tem melhor performance em redes sociais.', 'ALTA'])\n",
        "\n",
        "        # An√°lise de texto\n",
        "        if df_consolidado['densidade_texto'].mean() < 1:\n",
        "            recommendations.append(['CONTE√öDO', 'ADICIONE MAIS TEXTO', 'Textos na tela aumentam reten√ß√£o e acessibilidade.', 'M√âDIA'])\n",
        "\n",
        "        rec_headers = ['Categoria', 'A√ß√£o', 'Justificativa', 'Prioridade']\n",
        "        add_data_to_sheet(ws_recomendacoes, recommendations, start_row=3, start_col=1, headers=rec_headers)\n",
        "\n",
        "        # Salvar arquivo\n",
        "        log_progress(\"Salvando dashboard...\")\n",
        "        wb.save(output_path)\n",
        "\n",
        "        log_progress(\"DASHBOARD MASTER EXECUTIVO CRIADO COM SUCESSO!\")\n",
        "        log_progress(f\"Arquivo salvo em: {output_path}\")\n",
        "        log_progress(f\"{len(df_consolidado)} v√≠deos analisados\")\n",
        "        log_progress(f\"{len(insights)} insights estrat√©gicos gerados\")\n",
        "        log_progress(f\"{len(recommendations)} recomenda√ß√µes criadas\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        log_progress(f\"ERRO CR√çTICO: {e}\")\n",
        "        log_progress(\"Verifique os arquivos de entrada e tente novamente\")\n",
        "        return False\n",
        "\n",
        "def main():\n",
        "    \"\"\"Fun√ß√£o principal de execu√ß√£o\"\"\"\n",
        "    log_progress(\"INICIANDO SISTEMA DE DASHBOARD INTELIGENTE\")\n",
        "\n",
        "    # Configurar caminhos\n",
        "    BASE_PATH = \"/content/drive/MyDrive/Videos Dona Done/_engenharia_reversa\"\n",
        "    CSV_PATH = os.path.join(BASE_PATH, \"dashboard\", \"dados_consolidados.csv\")\n",
        "    JSON_PATH = os.path.join(BASE_PATH, \"dashboard\", \"dados_detalhados.json\")\n",
        "    OUTPUT_PATH = os.path.join(BASE_PATH, \"dashboard\", \"DASHBOARD_MASTER_EXECUTIVO_INTELIGENTE.xlsx\")\n",
        "\n",
        "    # Verificar se arquivos existem\n",
        "    if not os.path.exists(CSV_PATH):\n",
        "        log_progress(f\"ERRO: Arquivo CSV n√£o encontrado: {CSV_PATH}\")\n",
        "        return False\n",
        "\n",
        "    if not os.path.exists(JSON_PATH):\n",
        "        log_progress(f\"ERRO: Arquivo JSON n√£o encontrado: {JSON_PATH}\")\n",
        "        return False\n",
        "\n",
        "    # Executar cria√ß√£o do dashboard\n",
        "    success = create_enhanced_dashboard_master(CSV_PATH, JSON_PATH, OUTPUT_PATH)\n",
        "\n",
        "    if success:\n",
        "        log_progress(\"PROCESSO CONCLU√çDO COM SUCESSO!\")\n",
        "        log_progress(\"Dashboard inteligente pronto para uso estrat√©gico\")\n",
        "    else:\n",
        "        log_progress(\"PROCESSO FALHOU - Verifique os logs acima\")\n",
        "\n",
        "    return success\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "hFr8drvBrb23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Verificar se o processo de engenharia reversa foi executado\n",
        "BASE_PATH = \"/content/drive/MyDrive/Videos Dona Done/_engenharia_reversa\"\n",
        "CSV_PATH = os.path.join(BASE_PATH, \"dashboard\", \"dados_consolidados.csv\")\n",
        "JSON_PATH = os.path.join(BASE_PATH, \"dashboard\", \"dados_detalhados.json\")\n",
        "\n",
        "print(\"üîç VERIFICANDO PR√â-REQUISITOS...\")\n",
        "print(f\"Pasta base existe: {os.path.exists(BASE_PATH)}\")\n",
        "print(f\"CSV existe: {os.path.exists(CSV_PATH)}\")\n",
        "print(f\"JSON existe: {os.path.exists(JSON_PATH)}\")\n",
        "\n",
        "if os.path.exists(CSV_PATH):\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "    print(f\"üìä Dados CSV: {len(df)} v√≠deos encontrados\")\n",
        "\n",
        "print(\"\\n‚úÖ Se todos os itens acima s√£o True/existem, voc√™ pode prosseguir!\")"
      ],
      "metadata": {
        "id": "3QgJMmh1JJ-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SISTEMA DE INTEGRA√á√ÉO AUTOM√ÅTICA PARA NOVAS FUNCIONALIDADES\n",
        "# ============================================================================\n",
        "# Este script deve SUBSTITUIR a √∫ltima c√©lula (4.2) do notebook\n",
        "# Ele detecta automaticamente todas as an√°lises dispon√≠veis e as integra\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import glob\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def descobrir_analises_disponiveis(pasta_trabalho):\n",
        "    \"\"\"Descobre automaticamente todas as an√°lises realizadas\"\"\"\n",
        "    analises_encontradas = {\n",
        "        \"base\": {},\n",
        "        \"adicionais\": {}\n",
        "    }\n",
        "\n",
        "    dados_path = os.path.join(pasta_trabalho, \"dados\")\n",
        "\n",
        "    # An√°lises b√°sicas obrigat√≥rias\n",
        "    arquivos_base = {\n",
        "        \"metadados\": \"metadados_completos.json\",\n",
        "        \"decomposicao\": \"decomposicao_completa.json\",\n",
        "        \"padroes\": \"analises_padroes_completas.json\",\n",
        "        \"psicologica\": \"analises_psicologicas_completas.json\"\n",
        "    }\n",
        "\n",
        "    for tipo, arquivo in arquivos_base.items():\n",
        "        caminho = os.path.join(dados_path, arquivo)\n",
        "        if os.path.exists(caminho):\n",
        "            analises_encontradas[\"base\"][tipo] = caminho\n",
        "            print(f\"‚úÖ An√°lise base encontrada: {tipo}\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è An√°lise base ausente: {tipo}\")\n",
        "\n",
        "    # Descobrir an√°lises adicionais automaticamente\n",
        "    # Busca por qualquer arquivo JSON que n√£o seja das an√°lises base\n",
        "    todos_jsons = glob.glob(os.path.join(dados_path, \"*.json\"))\n",
        "\n",
        "    for json_path in todos_jsons:\n",
        "        nome_arquivo = os.path.basename(json_path)\n",
        "\n",
        "        # Pular arquivos base\n",
        "        if nome_arquivo in arquivos_base.values():\n",
        "            continue\n",
        "\n",
        "        # Identificar tipo da an√°lise pelo nome\n",
        "        if \"audio\" in nome_arquivo.lower():\n",
        "            analises_encontradas[\"adicionais\"][\"audio_refinada\"] = json_path\n",
        "            print(f\"‚úÖ An√°lise adicional encontrada: Audio Refinada\")\n",
        "        elif \"visual\" in nome_arquivo.lower():\n",
        "            analises_encontradas[\"adicionais\"][\"visual_avancada\"] = json_path\n",
        "            print(f\"‚úÖ An√°lise adicional encontrada: Visual Avan√ßada\")\n",
        "        elif \"texto\" in nome_arquivo.lower():\n",
        "            analises_encontradas[\"adicionais\"][\"texto_avancada\"] = json_path\n",
        "            print(f\"‚úÖ An√°lise adicional encontrada: Texto Avan√ßada\")\n",
        "        elif \"sentiment\" in nome_arquivo.lower():\n",
        "            analises_encontradas[\"adicionais\"][\"sentimento\"] = json_path\n",
        "            print(f\"‚úÖ An√°lise adicional encontrada: Sentimento\")\n",
        "        else:\n",
        "            # An√°lise n√£o reconhecida - incluir mesmo assim\n",
        "            nome_limpo = nome_arquivo.replace(\".json\", \"\").replace(\"_\", \" \").title()\n",
        "            analises_encontradas[\"adicionais\"][nome_arquivo] = json_path\n",
        "            print(f\"‚úÖ An√°lise personalizada encontrada: {nome_limpo}\")\n",
        "\n",
        "    return analises_encontradas\n",
        "\n",
        "def carregar_dados_analise(caminho_arquivo):\n",
        "    \"\"\"Carrega dados de uma an√°lise com tratamento de erros\"\"\"\n",
        "    try:\n",
        "        with open(caminho_arquivo, 'r', encoding='utf-8') as f:\n",
        "            dados = json.load(f)\n",
        "        return dados, True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Erro ao carregar {caminho_arquivo}: {e}\")\n",
        "        return [], False\n",
        "\n",
        "def extrair_metricas_dinamicamente(dados, tipo_analise):\n",
        "    \"\"\"Extrai m√©tricas de qualquer tipo de an√°lise dinamicamente\"\"\"\n",
        "    metricas_extraidas = {}\n",
        "\n",
        "    if not dados:\n",
        "        return metricas_extraidas\n",
        "\n",
        "    # Pegar o primeiro item para entender a estrutura\n",
        "    primeiro_item = dados[0] if isinstance(dados, list) else dados\n",
        "\n",
        "    if isinstance(primeiro_item, dict):\n",
        "        for chave, valor in primeiro_item.items():\n",
        "            if chave in ['video_id', 'status', 'data_analise', 'erro']:\n",
        "                continue\n",
        "\n",
        "            # Extrair m√©tricas num√©ricas automaticamente\n",
        "            if isinstance(valor, (int, float)):\n",
        "                metricas_extraidas[f\"{tipo_analise}_{chave}\"] = valor\n",
        "            elif isinstance(valor, dict):\n",
        "                # An√°lise aninhada - extrair sub-m√©tricas\n",
        "                for sub_chave, sub_valor in valor.items():\n",
        "                    if isinstance(sub_valor, (int, float)):\n",
        "                        metricas_extraidas[f\"{tipo_analise}_{chave}_{sub_chave}\"] = sub_valor\n",
        "                    elif isinstance(sub_valor, list) and sub_valor and isinstance(sub_valor[0], (int, float)):\n",
        "                        # Lista de n√∫meros - calcular estat√≠sticas\n",
        "                        metricas_extraidas[f\"{tipo_analise}_{chave}_{sub_chave}_media\"] = sum(sub_valor) / len(sub_valor)\n",
        "                        metricas_extraidas[f\"{tipo_analise}_{chave}_{sub_chave}_max\"] = max(sub_valor)\n",
        "                        metricas_extraidas[f\"{tipo_analise}_{chave}_{sub_chave}_min\"] = min(sub_valor)\n",
        "            elif isinstance(valor, list):\n",
        "                if valor and isinstance(valor[0], (int, float)):\n",
        "                    # Lista de n√∫meros\n",
        "                    metricas_extraidas[f\"{tipo_analise}_{chave}_count\"] = len(valor)\n",
        "                    metricas_extraidas[f\"{tipo_analise}_{chave}_media\"] = sum(valor) / len(valor) if valor else 0\n",
        "                else:\n",
        "                    # Lista de objetos ou strings\n",
        "                    metricas_extraidas[f\"{tipo_analise}_{chave}_count\"] = len(valor)\n",
        "\n",
        "    return metricas_extraidas\n",
        "\n",
        "def consolidar_todos_dados(analises_encontradas):\n",
        "    \"\"\"Consolida todos os dados de todas as an√°lises encontradas\"\"\"\n",
        "    dados_consolidados = {}\n",
        "\n",
        "    # Carregar an√°lises base\n",
        "    for tipo, caminho in analises_encontradas[\"base\"].items():\n",
        "        dados, sucesso = carregar_dados_analise(caminho)\n",
        "        if sucesso:\n",
        "            dados_consolidados[tipo] = dados\n",
        "\n",
        "    # Carregar an√°lises adicionais\n",
        "    for tipo, caminho in analises_encontradas[\"adicionais\"].items():\n",
        "        dados, sucesso = carregar_dados_analise(caminho)\n",
        "        if sucesso:\n",
        "            dados_consolidados[tipo] = dados\n",
        "\n",
        "    # Criar DataFrame consolidado por v√≠deo\n",
        "    videos_df = pd.DataFrame()\n",
        "\n",
        "    # Come√ßar com metadados base se dispon√≠vel\n",
        "    if \"metadados\" in dados_consolidados:\n",
        "        videos_df = pd.DataFrame(dados_consolidados[\"metadados\"])\n",
        "        videos_df = videos_df.set_index('id')\n",
        "\n",
        "    # Integrar cada an√°lise adicional\n",
        "    for tipo, dados in dados_consolidados.items():\n",
        "        if tipo == \"metadados\":\n",
        "            continue\n",
        "\n",
        "        print(f\"üîÑ Integrando dados de: {tipo}\")\n",
        "\n",
        "        # Converter para DataFrame se for lista\n",
        "        if isinstance(dados, list):\n",
        "            df_analise = pd.DataFrame(dados)\n",
        "\n",
        "            if 'video_id' in df_analise.columns:\n",
        "                df_analise = df_analise.set_index('video_id')\n",
        "\n",
        "                # Extrair m√©tricas dinamicamente\n",
        "                for video_id, row in df_analise.iterrows():\n",
        "                    metricas = extrair_metricas_dinamicamente([row.to_dict()], tipo)\n",
        "\n",
        "                    for metrica, valor in metricas.items():\n",
        "                        if video_id in videos_df.index:\n",
        "                            videos_df.loc[video_id, metrica] = valor\n",
        "                        else:\n",
        "                            # Criar nova linha se v√≠deo n√£o existir\n",
        "                            videos_df.loc[video_id, metrica] = valor\n",
        "\n",
        "    return videos_df.reset_index()\n",
        "\n",
        "def gerar_dashboard_dinamico(df_consolidado, pasta_trabalho):\n",
        "    \"\"\"Gera dashboard din√¢mico incluindo todas as an√°lises encontradas\"\"\"\n",
        "    from openpyxl import Workbook\n",
        "    from openpyxl.styles import Font, Alignment, PatternFill\n",
        "\n",
        "    wb = Workbook()\n",
        "\n",
        "    # ABA 1: VIS√ÉO GERAL DIN√ÇMICA\n",
        "    ws_geral = wb.active\n",
        "    ws_geral.title = 'Vis√£o Geral Completa'\n",
        "\n",
        "    # Header\n",
        "    ws_geral.cell(row=1, column=1).value = 'RELAT√ìRIO COMPLETO DE ENGENHARIA REVERSA'\n",
        "    ws_geral.cell(row=1, column=1).font = Font(bold=True, size=16)\n",
        "\n",
        "    # Estat√≠sticas gerais\n",
        "    ws_geral.cell(row=3, column=1).value = 'AN√ÅLISES REALIZADAS'\n",
        "    ws_geral.cell(row=3, column=1).font = Font(bold=True, size=14)\n",
        "\n",
        "    # Contar colunas por tipo de an√°lise\n",
        "    colunas_por_tipo = {}\n",
        "    for col in df_consolidado.columns:\n",
        "        if '_' in col:\n",
        "            tipo = col.split('_')[0]\n",
        "            colunas_por_tipo[tipo] = colunas_por_tipo.get(tipo, 0) + 1\n",
        "\n",
        "    row = 4\n",
        "    for tipo, count in colunas_por_tipo.items():\n",
        "        ws_geral.cell(row=row, column=1).value = f\"{tipo.upper()}\"\n",
        "        ws_geral.cell(row=row, column=2).value = f\"{count} m√©tricas extra√≠das\"\n",
        "        ws_geral.cell(row=row, column=1).font = Font(bold=True)\n",
        "        row += 1\n",
        "\n",
        "    # ABA 2: DADOS COMPLETOS\n",
        "    ws_dados = wb.create_sheet('Dados Completos')\n",
        "\n",
        "    # Adicionar todos os dados\n",
        "    for r_idx, row in enumerate(df_consolidado.itertuples(), 1):\n",
        "        for c_idx, value in enumerate(row):\n",
        "            cell = ws_dados.cell(row=r_idx, column=c_idx)\n",
        "            cell.value = value\n",
        "            if r_idx == 1:  # Header\n",
        "                cell.font = Font(bold=True)\n",
        "\n",
        "    # ABA 3: M√âTRICAS POR TIPO\n",
        "    tipos_encontrados = list(set([col.split('_')[0] for col in df_consolidado.columns if '_' in col]))\n",
        "\n",
        "    for tipo in tipos_encontrados:\n",
        "        ws_tipo = wb.create_sheet(f'An√°lise {tipo.title()}')\n",
        "\n",
        "        # Filtrar colunas deste tipo\n",
        "        colunas_tipo = ['id', 'nome_arquivo'] + [col for col in df_consolidado.columns if col.startswith(tipo)]\n",
        "\n",
        "        if len(colunas_tipo) > 2:  # Tem dados al√©m do id e nome\n",
        "            df_tipo = df_consolidado[colunas_tipo]\n",
        "\n",
        "            # Adicionar ao worksheet\n",
        "            for r_idx, row in enumerate(df_tipo.itertuples(), 1):\n",
        "                for c_idx, value in enumerate(row):\n",
        "                    cell = ws_tipo.cell(row=r_idx, column=c_idx)\n",
        "                    cell.value = value\n",
        "                    if r_idx == 1:\n",
        "                        cell.font = Font(bold=True)\n",
        "\n",
        "    # ABA 4: INSIGHTS AUTOMATICOS\n",
        "    ws_insights = wb.create_sheet('Insights Autom√°ticos')\n",
        "\n",
        "    insights_automaticos = gerar_insights_automaticos(df_consolidado)\n",
        "\n",
        "    ws_insights.cell(row=1, column=1).value = 'INSIGHTS GERADOS AUTOMATICAMENTE'\n",
        "    ws_insights.cell(row=1, column=1).font = Font(bold=True, size=16)\n",
        "\n",
        "    for i, insight in enumerate(insights_automaticos, 3):\n",
        "        ws_insights.cell(row=i, column=1).value = f\"‚Ä¢ {insight}\"\n",
        "        ws_insights.cell(row=i, column=1).alignment = Alignment(wrap_text=True)\n",
        "\n",
        "    # Salvar\n",
        "    output_path = os.path.join(pasta_trabalho, \"dashboard\", \"RELATORIO_COMPLETO_DINAMICO.xlsx\")\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    wb.save(output_path)\n",
        "\n",
        "    return output_path\n",
        "\n",
        "def gerar_insights_automaticos(df):\n",
        "    \"\"\"Gera insights autom√°ticos baseados em qualquer conjunto de dados\"\"\"\n",
        "    insights = []\n",
        "\n",
        "    # An√°lise de correla√ß√µes autom√°ticas\n",
        "    colunas_numericas = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "    if len(colunas_numericas) > 1:\n",
        "        correlacoes = df[colunas_numericas].corr()\n",
        "\n",
        "        # Encontrar correla√ß√µes fortes\n",
        "        for col1 in correlacoes.columns:\n",
        "            for col2 in correlacoes.columns:\n",
        "                if col1 != col2:\n",
        "                    corr_val = correlacoes.loc[col1, col2]\n",
        "                    if abs(corr_val) > 0.7:\n",
        "                        insights.append(f\"CORRELA√á√ÉO FORTE: {col1} e {col2} t√™m correla√ß√£o de {corr_val:.2f}\")\n",
        "\n",
        "    # Identificar outliers autom√°ticos\n",
        "    for col in colunas_numericas:\n",
        "        if df[col].std() > 0:  # Evitar divis√£o por zero\n",
        "            media = df[col].mean()\n",
        "            std = df[col].std()\n",
        "            outliers = df[(df[col] > media + 2*std) | (df[col] < media - 2*std)]\n",
        "\n",
        "            if len(outliers) > 0:\n",
        "                insights.append(f\"OUTLIERS DETECTADOS: {len(outliers)} v√≠deos t√™m valores extremos em {col}\")\n",
        "\n",
        "    # An√°lise de distribui√ß√µes\n",
        "    for col in colunas_numericas:\n",
        "        if col.endswith('_score') or 'score' in col:\n",
        "            media = df[col].mean()\n",
        "            if media > 80:\n",
        "                insights.append(f\"PERFORMANCE ALTA: Score m√©dio de {col} √© {media:.1f} - excelente resultado\")\n",
        "            elif media < 50:\n",
        "                insights.append(f\"OPORTUNIDADE: Score m√©dio de {col} √© {media:.1f} - h√° espa√ßo para melhorias\")\n",
        "\n",
        "    return insights if insights else [\"An√°lise de insights em andamento - dados sendo processados\"]\n",
        "\n",
        "def atualizar_config_com_novas_analises(pasta_trabalho, analises_encontradas):\n",
        "    \"\"\"Atualiza config.json com status de todas as an√°lises encontradas\"\"\"\n",
        "    config_path = os.path.join(pasta_trabalho, \"config\", \"config.json\")\n",
        "\n",
        "    # Carregar config existente\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    # Atualizar status das an√°lises encontradas\n",
        "    for tipo in analises_encontradas[\"base\"]:\n",
        "        config[\"status_etapas\"][tipo] = True\n",
        "\n",
        "    for tipo in analises_encontradas[\"adicionais\"]:\n",
        "        config[\"status_etapas\"][f\"analise_{tipo}\"] = True\n",
        "\n",
        "    config[\"ultima_consolidacao\"] = datetime.now().isoformat()\n",
        "    config[\"total_analises_integradas\"] = len(analises_encontradas[\"base\"]) + len(analises_encontradas[\"adicionais\"])\n",
        "\n",
        "    # Salvar config atualizado\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "def main_integracao_automatica():\n",
        "    \"\"\"Fun√ß√£o principal da integra√ß√£o autom√°tica\"\"\"\n",
        "    print(\"üöÄ INICIANDO INTEGRA√á√ÉO AUTOM√ÅTICA DE TODAS AS AN√ÅLISES\")\n",
        "\n",
        "    # Usar vari√°vel global da pasta de trabalho\n",
        "    if \"PASTA_TRABALHO\" not in globals():\n",
        "        print(\"‚ùå ERRO: Execute as c√©lulas anteriores primeiro\")\n",
        "        return False\n",
        "\n",
        "    pasta_trabalho = PASTA_TRABALHO\n",
        "\n",
        "    try:\n",
        "        # Passo 1: Descobrir an√°lises\n",
        "        print(\"\\nüîç DESCOBRINDO AN√ÅLISES DISPON√çVEIS...\")\n",
        "        analises = descobrir_analises_disponiveis(pasta_trabalho)\n",
        "\n",
        "        total_analises = len(analises[\"base\"]) + len(analises[\"adicionais\"])\n",
        "        print(f\"üìä Total de an√°lises encontradas: {total_analises}\")\n",
        "\n",
        "        # Passo 2: Consolidar dados\n",
        "        print(\"\\nüîÑ CONSOLIDANDO TODOS OS DADOS...\")\n",
        "        df_consolidado = consolidar_todos_dados(analises)\n",
        "\n",
        "        print(f\"üìà {len(df_consolidado)} v√≠deos consolidados com {len(df_consolidado.columns)} m√©tricas totais\")\n",
        "\n",
        "        # Passo 3: Gerar dashboard din√¢mico\n",
        "        print(\"\\nüìä GERANDO DASHBOARD DIN√ÇMICO...\")\n",
        "        dashboard_path = gerar_dashboard_dinamico(df_consolidado, pasta_trabalho)\n",
        "\n",
        "        # Passo 4: Salvar dados consolidados\n",
        "        csv_path = os.path.join(pasta_trabalho, \"dashboard\", \"dados_completos_consolidados.csv\")\n",
        "        df_consolidado.to_csv(csv_path, index=False, encoding='utf-8')\n",
        "\n",
        "        json_path = os.path.join(pasta_trabalho, \"dashboard\", \"dados_completos_consolidados.json\")\n",
        "        df_consolidado.to_json(json_path, orient='records', indent=2, force_ascii=False)\n",
        "\n",
        "        # Passo 5: Atualizar configura√ß√£o\n",
        "        print(\"\\n‚öôÔ∏è ATUALIZANDO CONFIGURA√á√ïES...\")\n",
        "        atualizar_config_com_novas_analises(pasta_trabalho, analises)\n",
        "\n",
        "        # Resultados finais\n",
        "        print(\"\\n‚úÖ INTEGRA√á√ÉO AUTOM√ÅTICA CONCLU√çDA COM SUCESSO!\")\n",
        "        print(f\"üìÅ Dashboard din√¢mico: {dashboard_path}\")\n",
        "        print(f\"üìÅ Dados CSV: {csv_path}\")\n",
        "        print(f\"üìÅ Dados JSON: {json_path}\")\n",
        "        print(f\"üìä {len(df_consolidado)} v√≠deos processados\")\n",
        "        print(f\"üìà {len(df_consolidado.columns)} m√©tricas totais integradas\")\n",
        "\n",
        "        print(\"\\nüéØ PR√ìXIMOS PASSOS:\")\n",
        "        print(\"‚Ä¢ Abra o arquivo Excel para ver todas as an√°lises integradas\")\n",
        "        print(\"‚Ä¢ Use os dados CSV/JSON em outras ferramentas de an√°lise\")\n",
        "        print(\"‚Ä¢ Execute novamente sempre que adicionar novas an√°lises\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå ERRO NA INTEGRA√á√ÉO AUTOM√ÅTICA: {e}\")\n",
        "        print(\"Verifique se todas as an√°lises anteriores foram executadas com sucesso\")\n",
        "        return False\n",
        "\n",
        "# Executar integra√ß√£o autom√°tica\n",
        "if __name__ == \"__main__\":\n",
        "    main_integracao_automatica()"
      ],
      "metadata": {
        "id": "yyhMy4Pbt7Y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 4.3: INTEGRA√á√ÉO DE COPYWRITING NO DASHBOARD EXISTENTE\n",
        "# ============================================================================\n",
        "\n",
        "def integrar_copywriting_dashboard_existente():\n",
        "    \"\"\"Integra an√°lise de copywriting no dashboard master existente\"\"\"\n",
        "    print(\"üîÑ Iniciando integra√ß√£o de copywriting no dashboard existente...\")\n",
        "\n",
        "    # Verificar pr√©-requisitos\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa('copywriting_analysis')\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Localizar dashboard existente\n",
        "    pasta_dashboard = os.path.join(PASTA_TRABALHO, \"dashboard\")\n",
        "    dashboard_existente = None\n",
        "\n",
        "    # Procurar arquivo de dashboard existente\n",
        "    if os.path.exists(pasta_dashboard):\n",
        "        arquivos = os.listdir(pasta_dashboard)\n",
        "        for arquivo in arquivos:\n",
        "            if \"DASHBOARD_MASTER_EXECUTIVO_INTELIGENTE\" in arquivo and arquivo.endswith(\".xlsx\"):\n",
        "                dashboard_existente = os.path.join(pasta_dashboard, arquivo)\n",
        "                break\n",
        "\n",
        "    if not dashboard_existente:\n",
        "        print(\"‚ùå Dashboard master existente n√£o encontrado!\")\n",
        "        print(\"Execute primeiro a c√©lula 4.2 (Blueprint Final) para criar o dashboard base.\")\n",
        "        return\n",
        "\n",
        "    print(f\"  üìä Dashboard encontrado: {os.path.basename(dashboard_existente)}\")\n",
        "\n",
        "    # Carregar dados de copywriting\n",
        "    dados_copywriting = carregar_dados_copywriting()\n",
        "    if not dados_copywriting:\n",
        "        return\n",
        "\n",
        "    # Abrir workbook existente\n",
        "    from openpyxl import load_workbook\n",
        "\n",
        "    try:\n",
        "        wb = load_workbook(dashboard_existente)\n",
        "        print(f\"  ‚úÖ Dashboard carregado com {len(wb.sheetnames)} abas existentes\")\n",
        "\n",
        "        # Adicionar novas abas de copywriting\n",
        "        adicionar_aba_copywriting_estrategico(wb, dados_copywriting)\n",
        "        adicionar_aba_templates_copy(wb, dados_copywriting)\n",
        "        adicionar_aba_timeline_copy(wb, dados_copywriting)\n",
        "        adicionar_aba_recomendacoes_copy(wb, dados_copywriting)\n",
        "\n",
        "        # Atualizar aba principal com m√©tricas de copywriting\n",
        "        atualizar_aba_principal_com_copy(wb, dados_copywriting)\n",
        "\n",
        "        # Salvar dashboard atualizado\n",
        "        wb.save(dashboard_existente)\n",
        "\n",
        "        print(f\"‚úÖ Dashboard atualizado com an√°lise de copywriting!\")\n",
        "        print(f\"üìä Arquivo: {dashboard_existente}\")\n",
        "        print(f\"üìã Novas abas adicionadas:\")\n",
        "        print(\"  ‚Ä¢ Copywriting Estrat√©gico\")\n",
        "        print(\"  ‚Ä¢ Templates Replic√°veis\")\n",
        "        print(\"  ‚Ä¢ Timeline Persuas√£o\")\n",
        "        print(\"  ‚Ä¢ Recomenda√ß√µes Copy\")\n",
        "        print(\"  ‚Ä¢ Dashboard Principal (atualizada)\")\n",
        "\n",
        "        # Gerar relat√≥rios complementares\n",
        "        gerar_relatorios_copywriting_individuais(dados_copywriting)\n",
        "\n",
        "        # Atualizar config\n",
        "        config[\"status_etapas\"][\"dashboard_copywriting_integrado\"] = True\n",
        "        config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "        with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        return dashboard_existente\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro ao atualizar dashboard: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def carregar_dados_copywriting():\n",
        "    \"\"\"Carrega dados de copywriting e outros dados necess√°rios\"\"\"\n",
        "    print(\"  üìä Carregando dados de copywriting...\")\n",
        "\n",
        "    try:\n",
        "        # Dados de copywriting\n",
        "        copywriting_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_copywriting_completas.json\")\n",
        "        with open(copywriting_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            copywriting_data = json.load(f)\n",
        "\n",
        "        # Dados de legendas\n",
        "        legendas_path = os.path.join(PASTA_TRABALHO, \"dados\", \"legendas_geradas.json\")\n",
        "        with open(legendas_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            legendas_data = json.load(f)\n",
        "\n",
        "        # Tentar carregar outros dados (podem n√£o existir ainda)\n",
        "        outros_dados = {}\n",
        "\n",
        "        try:\n",
        "            padroes_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_padroes_completas.json\")\n",
        "            with open(padroes_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                outros_dados[\"padroes\"] = json.load(f)\n",
        "        except:\n",
        "            outros_dados[\"padroes\"] = []\n",
        "\n",
        "        try:\n",
        "            videos_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "            with open(videos_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                outros_dados[\"videos\"] = json.load(f)\n",
        "        except:\n",
        "            outros_dados[\"videos\"] = []\n",
        "\n",
        "        print(f\"  ‚úÖ Dados carregados: {len(copywriting_data)} an√°lises de copywriting\")\n",
        "\n",
        "        return {\n",
        "            \"copywriting\": copywriting_data,\n",
        "            \"legendas\": legendas_data,\n",
        "            **outros_dados\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Erro ao carregar dados de copywriting: {e}\")\n",
        "        return None\n",
        "\n",
        "def adicionar_aba_copywriting_estrategico(wb, dados):\n",
        "    \"\"\"Adiciona aba principal de an√°lise de copywriting\"\"\"\n",
        "    from openpyxl.styles import Font, PatternFill, Alignment\n",
        "\n",
        "    # Criar nova aba\n",
        "    ws = wb.create_sheet(\"Copywriting Estrat√©gico\")\n",
        "\n",
        "    # T√≠tulo principal\n",
        "    ws.merge_cells(\"A1:H1\")\n",
        "    titulo = ws[\"A1\"]\n",
        "    titulo.value = \"AN√ÅLISE ESTRAT√âGICA DE COPYWRITING - ENGENHARIA REVERSA\"\n",
        "    titulo.fill = PatternFill(start_color=\"1F4E79\", end_color=\"1F4E79\", fill_type=\"solid\")\n",
        "    titulo.font = Font(color=\"FFFFFF\", bold=True, size=14)\n",
        "    titulo.alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "    row = 3\n",
        "\n",
        "    # M√©tricas executivas\n",
        "    ws[f\"A{row}\"] = \"M√âTRICAS EXECUTIVAS DE COPYWRITING\"\n",
        "    ws[f\"A{row}\"].font = Font(bold=True, size=12, color=\"C5504B\")\n",
        "    row += 2\n",
        "\n",
        "    # Calcular m√©tricas\n",
        "    videos_copy = dados[\"copywriting\"]\n",
        "\n",
        "    if videos_copy:\n",
        "        # Score m√©dio\n",
        "        scores = [v.get(\"score_persuasao\", 0) for v in videos_copy]\n",
        "        score_medio = sum(scores) / len(scores) if scores else 0\n",
        "\n",
        "        # Contadores\n",
        "        total_ganchos = sum(len(v.get(\"ganchos_detectados\", {})) for v in videos_copy)\n",
        "        total_gatilhos = sum(len(v.get(\"gatilhos_mentais_detectados\", {})) for v in videos_copy)\n",
        "        total_ctas = sum(len(v.get(\"ctas_detectados\", {})) for v in videos_copy)\n",
        "        videos_sem_cta = len([v for v in videos_copy if not v.get(\"ctas_detectados\")])\n",
        "        total_templates = sum(len(v.get(\"templates_identificados\", [])) for v in videos_copy)\n",
        "\n",
        "        # Exibir m√©tricas\n",
        "        metricas = [\n",
        "            (\"Score Persuas√£o M√©dio:\", f\"{score_medio:.1f}/100\", \"Meta: 70+ para alta convers√£o\"),\n",
        "            (\"V√≠deos Analisados:\", len(videos_copy), \"Base completa da an√°lise\"),\n",
        "            (\"Total de Ganchos:\", total_ganchos, f\"M√©dia: {total_ganchos/len(videos_copy):.1f} por v√≠deo\"),\n",
        "            (\"Total de Gatilhos:\", total_gatilhos, f\"M√©dia: {total_gatilhos/len(videos_copy):.1f} por v√≠deo\"),\n",
        "            (\"Total de CTAs:\", total_ctas, f\"M√©dia: {total_ctas/len(videos_copy):.1f} por v√≠deo\"),\n",
        "            (\"üö® V√≠deos sem CTA:\", videos_sem_cta, \"CR√çTICO: Implementar imediatamente\" if videos_sem_cta > 0 else \"‚úÖ Todos t√™m CTA\"),\n",
        "            (\"Templates Identificados:\", total_templates, \"Estruturas replic√°veis encontradas\")\n",
        "        ]\n",
        "\n",
        "        for metrica, valor, descricao in metricas:\n",
        "            ws[f\"A{row}\"] = metrica\n",
        "            ws[f\"B{row}\"] = valor\n",
        "            ws[f\"C{row}\"] = descricao\n",
        "\n",
        "            ws[f\"A{row}\"].font = Font(bold=True)\n",
        "            if \"üö®\" in metrica and videos_sem_cta > 0:\n",
        "                ws[f\"B{row}\"].font = Font(bold=True, color=\"FF0000\")\n",
        "            elif isinstance(valor, (int, float)) and valor > 0:\n",
        "                ws[f\"B{row}\"].font = Font(bold=True, color=\"70AD47\")\n",
        "\n",
        "            row += 1\n",
        "\n",
        "        row += 2\n",
        "\n",
        "        # Ranking de performance\n",
        "        ws[f\"A{row}\"] = \"üèÜ RANKING DE PERFORMANCE POR SCORE DE PERSUAS√ÉO\"\n",
        "        ws[f\"A{row}\"].font = Font(bold=True, size=12, color=\"1F4E79\")\n",
        "        row += 2\n",
        "\n",
        "        # Headers\n",
        "        headers = [\"Posi√ß√£o\", \"V√≠deo ID\", \"Score\", \"Ganchos\", \"Gatilhos\", \"CTAs\", \"Status\"]\n",
        "        for col, header in enumerate(headers, 1):\n",
        "            cell = ws.cell(row=row, column=col)\n",
        "            cell.value = header\n",
        "            cell.font = Font(bold=True)\n",
        "            cell.fill = PatternFill(start_color=\"D9E2F3\", end_color=\"D9E2F3\", fill_type=\"solid\")\n",
        "        row += 1\n",
        "\n",
        "        # Top performers\n",
        "        top_videos = sorted(videos_copy, key=lambda x: x.get(\"score_persuasao\", 0), reverse=True)\n",
        "\n",
        "        for i, video in enumerate(top_videos, 1):\n",
        "            ws.cell(row=row, column=1, value=f\"{i}¬∫\")\n",
        "            ws.cell(row=row, column=2, value=video[\"video_id\"])\n",
        "            ws.cell(row=row, column=3, value=f\"{video.get('score_persuasao', 0)}/100\")\n",
        "            ws.cell(row=row, column=4, value=len(video.get(\"ganchos_detectados\", {})))\n",
        "            ws.cell(row=row, column=5, value=len(video.get(\"gatilhos_mentais_detectados\", {})))\n",
        "            ws.cell(row=row, column=6, value=len(video.get(\"ctas_detectados\", {})))\n",
        "\n",
        "            # Status baseado no score\n",
        "            score = video.get(\"score_persuasao\", 0)\n",
        "            if score >= 70:\n",
        "                status = \"üü¢ √ìTIMO\"\n",
        "                status_color = \"70AD47\"\n",
        "            elif score >= 50:\n",
        "                status = \"üü° BOM\"\n",
        "                status_color = \"FFC000\"\n",
        "            else:\n",
        "                status = \"üî¥ PRECISA OTIMIZAR\"\n",
        "                status_color = \"C5504B\"\n",
        "\n",
        "            cell_status = ws.cell(row=row, column=7, value=status)\n",
        "            cell_status.font = Font(color=status_color, bold=True)\n",
        "\n",
        "            # Destacar top 3\n",
        "            if i <= 3:\n",
        "                for col in range(1, 8):\n",
        "                    ws.cell(row=row, column=col).fill = PatternFill(start_color=\"FFF2CC\", end_color=\"FFF2CC\", fill_type=\"solid\")\n",
        "\n",
        "            row += 1\n",
        "\n",
        "        row += 2\n",
        "\n",
        "        # An√°lise de gaps cr√≠ticos\n",
        "        ws[f\"A{row}\"] = \"‚ö†Ô∏è GAPS CR√çTICOS IDENTIFICADOS\"\n",
        "        ws[f\"A{row}\"].font = Font(bold=True, size=12, color=\"C5504B\")\n",
        "        row += 2\n",
        "\n",
        "        gaps = []\n",
        "\n",
        "        # V√≠deos sem CTA\n",
        "        if videos_sem_cta > 0:\n",
        "            gap_cta_videos = [v[\"video_id\"] for v in videos_copy if not v.get(\"ctas_detectados\")]\n",
        "            gaps.append(f\"üö® CR√çTICO: {videos_sem_cta} v√≠deos sem CTA: {', '.join(gap_cta_videos[:3])}\")\n",
        "\n",
        "        # V√≠deos com poucos ganchos\n",
        "        videos_poucos_ganchos = [v for v in videos_copy if len(v.get(\"ganchos_detectados\", {})) < 2]\n",
        "        if len(videos_poucos_ganchos) > len(videos_copy) * 0.5:\n",
        "            gaps.append(f\"üìà OPORTUNIDADE: {len(videos_poucos_ganchos)} v√≠deos precisam de mais ganchos\")\n",
        "\n",
        "        # Score baixo\n",
        "        videos_score_baixo = [v for v in videos_copy if v.get(\"score_persuasao\", 0) < 50]\n",
        "        if videos_score_baixo:\n",
        "            gaps.append(f\"üéØ OTIMIZA√á√ÉO: {len(videos_score_baixo)} v√≠deos com score < 50 precisam de revis√£o\")\n",
        "\n",
        "        if not gaps:\n",
        "            gaps.append(\"‚úÖ Nenhum gap cr√≠tico identificado - parab√©ns!\")\n",
        "\n",
        "        for gap in gaps:\n",
        "            ws[f\"A{row}\"] = gap\n",
        "            if \"üö®\" in gap:\n",
        "                ws[f\"A{row}\"].font = Font(color=\"FF0000\", bold=True)\n",
        "            elif \"üìà\" in gap or \"üéØ\" in gap:\n",
        "                ws[f\"A{row}\"].font = Font(color=\"FFC000\", bold=True)\n",
        "            else:\n",
        "                ws[f\"A{row}\"].font = Font(color=\"70AD47\", bold=True)\n",
        "            row += 1\n",
        "\n",
        "    else:\n",
        "        ws[f\"A{row}\"] = \"‚ö†Ô∏è Nenhum dado de copywriting encontrado\"\n",
        "        ws[f\"A{row}\"].font = Font(color=\"C5504B\", bold=True)\n",
        "        row += 1\n",
        "        ws[f\"A{row}\"] = \"Execute primeiro a C√©lula 2.4 para gerar an√°lises de copywriting\"\n",
        "\n",
        "    # Ajustar larguras das colunas\n",
        "    for col, width in [(\"A\", 25), (\"B\", 15), (\"C\", 40), (\"D\", 10), (\"E\", 10), (\"F\", 10), (\"G\", 20), (\"H\", 15)]:\n",
        "        ws.column_dimensions[col].width = width\n",
        "\n",
        "def adicionar_aba_templates_copy(wb, dados):\n",
        "    \"\"\"Adiciona aba de templates replic√°veis\"\"\"\n",
        "    from openpyxl.styles import Font, PatternFill, Alignment\n",
        "\n",
        "    ws = wb.create_sheet(\"Templates Replic√°veis\")\n",
        "\n",
        "    # T√≠tulo\n",
        "    ws.merge_cells(\"A1:F1\")\n",
        "    titulo = ws[\"A1\"]\n",
        "    titulo.value = \"TEMPLATES E ESTRUTURAS REPLIC√ÅVEIS DE COPYWRITING\"\n",
        "    titulo.fill = PatternFill(start_color=\"70AD47\", end_color=\"70AD47\", fill_type=\"solid\")\n",
        "    titulo.font = Font(color=\"FFFFFF\", bold=True, size=14)\n",
        "    titulo.alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "    row = 3\n",
        "\n",
        "    # Coletar todos os templates\n",
        "    todos_templates = []\n",
        "    for video in dados[\"copywriting\"]:\n",
        "        templates = video.get(\"templates_identificados\", [])\n",
        "        for template in templates:\n",
        "            template[\"video_id\"] = video[\"video_id\"]\n",
        "            todos_templates.append(template)\n",
        "\n",
        "    if todos_templates:\n",
        "        # Agrupar templates por tipo\n",
        "        templates_agrupados = {}\n",
        "        for template in todos_templates:\n",
        "            nome = template[\"nome\"]\n",
        "            if nome not in templates_agrupados:\n",
        "                templates_agrupados[nome] = {\n",
        "                    \"estrutura\": template[\"estrutura\"],\n",
        "                    \"eficacia\": template[\"eficacia\"],\n",
        "                    \"uso_recomendado\": template[\"uso_recomendado\"],\n",
        "                    \"videos_exemplo\": []\n",
        "                }\n",
        "            templates_agrupados[nome][\"videos_exemplo\"].append(template[\"video_id\"])\n",
        "\n",
        "        # Exibir templates\n",
        "        for nome_template, dados_template in templates_agrupados.items():\n",
        "            ws.merge_cells(f\"A{row}:F{row}\")\n",
        "            template_header = ws[f\"A{row}\"]\n",
        "            template_header.value = f\"üìã TEMPLATE: {nome_template.replace('_', ' ')}\"\n",
        "            template_header.fill = PatternFill(start_color=\"E2EFDA\", end_color=\"E2EFDA\", fill_type=\"solid\")\n",
        "            template_header.font = Font(bold=True, size=11)\n",
        "            row += 1\n",
        "\n",
        "            ws[f\"A{row}\"] = \"Estrutura:\"\n",
        "            ws[f\"B{row}\"] = dados_template[\"estrutura\"]\n",
        "            ws[f\"A{row}\"].font = Font(bold=True)\n",
        "            row += 1\n",
        "\n",
        "            ws[f\"A{row}\"] = \"Efic√°cia:\"\n",
        "            ws[f\"B{row}\"] = dados_template[\"eficacia\"]\n",
        "            ws[f\"A{row}\"].font = Font(bold=True)\n",
        "            if dados_template[\"eficacia\"] == \"MUITO ALTA\":\n",
        "                ws[f\"B{row}\"].font = Font(color=\"70AD47\", bold=True)\n",
        "            elif dados_template[\"eficacia\"] == \"ALTA\":\n",
        "                ws[f\"B{row}\"].font = Font(color=\"C5504B\", bold=True)\n",
        "            row += 1\n",
        "\n",
        "            ws[f\"A{row}\"] = \"Uso Recomendado:\"\n",
        "            ws[f\"B{row}\"] = dados_template[\"uso_recomendado\"]\n",
        "            ws[f\"A{row}\"].font = Font(bold=True)\n",
        "            row += 1\n",
        "\n",
        "            ws[f\"A{row}\"] = \"V√≠deos Exemplo:\"\n",
        "            ws[f\"B{row}\"] = \", \".join(dados_template[\"videos_exemplo\"][:3])\n",
        "            ws[f\"A{row}\"].font = Font(bold=True)\n",
        "            row += 1\n",
        "\n",
        "            # Como aplicar\n",
        "            ws[f\"A{row}\"] = \"Como Aplicar:\"\n",
        "            ws[f\"A{row}\"].font = Font(bold=True, color=\"7030A0\")\n",
        "            row += 1\n",
        "\n",
        "            instrucoes = gerar_instrucoes_aplicacao_template(nome_template)\n",
        "            for i, instrucao in enumerate(instrucoes, 1):\n",
        "                ws[f\"B{row}\"] = f\"{i}. {instrucao}\"\n",
        "                row += 1\n",
        "\n",
        "            row += 2\n",
        "\n",
        "    else:\n",
        "        ws[f\"A{row}\"] = \"üìã Ainda n√£o foram identificados templates espec√≠ficos\"\n",
        "        row += 1\n",
        "        ws[f\"A{row}\"] = \"Execute mais an√°lises para identificar padr√µes replic√°veis\"\n",
        "\n",
        "    # Templates recomendados universais\n",
        "    row += 2\n",
        "    ws.merge_cells(f\"A{row}:F{row}\")\n",
        "    recom_header = ws[f\"A{row}\"]\n",
        "    recom_header.value = \"üéØ TEMPLATES UNIVERSAIS RECOMENDADOS PARA IMPLEMENTAR\"\n",
        "    recom_header.fill = PatternFill(start_color=\"FFC000\", end_color=\"FFC000\", fill_type=\"solid\")\n",
        "    recom_header.font = Font(bold=True, size=12)\n",
        "    row += 1\n",
        "\n",
        "    templates_universais = [\n",
        "        (\"PERGUNTA + VALOR + CTA\", \"Pergunta engajante ‚Üí Entrega valor ‚Üí Call-to-action direto\", \"Todos os v√≠deos educativos\"),\n",
        "        (\"PROBLEMA + AGITA√á√ÉO + SOLU√á√ÉO\", \"Identifica dor ‚Üí Agrava problema ‚Üí Apresenta solu√ß√£o\", \"V√≠deos de vendas e transforma√ß√£o\"),\n",
        "        (\"CURIOSIDADE + HIST√ìRIA + ENSINO\", \"Desperta curiosidade ‚Üí Conta hist√≥ria ‚Üí Ensina m√©todo\", \"Content marketing e autoridade\"),\n",
        "        (\"PROVA SOCIAL + URG√äNCIA + A√á√ÉO\", \"Mostra resultados ‚Üí Cria urg√™ncia ‚Üí Direciona a√ß√£o\", \"Lan√ßamentos e ofertas\")\n",
        "    ]\n",
        "\n",
        "    headers_univ = [\"Template\", \"Estrutura\", \"Aplica√ß√£o Ideal\"]\n",
        "    for col, header in enumerate(headers_univ, 1):\n",
        "        cell = ws.cell(row=row, column=col)\n",
        "        cell.value = header\n",
        "        cell.font = Font(bold=True)\n",
        "        cell.fill = PatternFill(start_color=\"F2F2F2\", end_color=\"F2F2F2\", fill_type=\"solid\")\n",
        "    row += 1\n",
        "\n",
        "    for nome, estrutura, aplicacao in templates_universais:\n",
        "        ws.cell(row=row, column=1, value=nome)\n",
        "        ws.cell(row=row, column=2, value=estrutura)\n",
        "        ws.cell(row=row, column=3, value=aplicacao)\n",
        "        ws.cell(row=row, column=1).font = Font(bold=True)\n",
        "        row += 1\n",
        "\n",
        "    # Ajustar larguras\n",
        "    for col, width in [(\"A\", 20), (\"B\", 50), (\"C\", 25), (\"D\", 15), (\"E\", 15), (\"F\", 15)]:\n",
        "        ws.column_dimensions[col].width = width\n",
        "\n",
        "def gerar_instrucoes_aplicacao_template(nome_template):\n",
        "    \"\"\"Gera instru√ß√µes espec√≠ficas para aplicar um template\"\"\"\n",
        "    instrucoes_map = {\n",
        "        \"PERGUNTA_AUTORIDADE_CTA\": [\n",
        "            \"Inicie com pergunta que conecte com a dor/desejo do p√∫blico\",\n",
        "            \"Estabele√ßa credibilidade (experi√™ncia, resultados, forma√ß√£o)\",\n",
        "            \"Termine com CTA claro e espec√≠fico\",\n",
        "            \"Mantenha tom conversacional mas assertivo\"\n",
        "        ],\n",
        "        \"PROBLEMA_SOLUCAO_PROVA\": [\n",
        "            \"Identifique problema espec√≠fico e real do p√∫blico\",\n",
        "            \"Apresente solu√ß√£o clara e aplic√°vel\",\n",
        "            \"Mostre provas sociais (depoimentos, n√∫meros, casos)\",\n",
        "            \"Use linguagem emocional para conectar\"\n",
        "        ],\n",
        "        \"CURIOSIDADE_URGENCIA_ACAO\": [\n",
        "            \"Desperte curiosidade nos primeiros 3 segundos\",\n",
        "            \"Crie senso de urg√™ncia (limitado, exclusivo)\",\n",
        "            \"Direcione para a√ß√£o imediata espec√≠fica\",\n",
        "            \"Use gatilhos de escassez e FOMO\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    return instrucoes_map.get(nome_template, [\n",
        "        \"Analise a estrutura identificada no v√≠deo de exemplo\",\n",
        "        \"Adapte os elementos para seu nicho espec√≠fico\",\n",
        "        \"Teste diferentes abordagens mantendo a estrutura\",\n",
        "        \"Monitore resultados e otimize baseado na performance\"\n",
        "    ])\n",
        "\n",
        "def adicionar_aba_timeline_copy(wb, dados):\n",
        "    \"\"\"Adiciona aba com timeline de elementos persuasivos\"\"\"\n",
        "    from openpyxl.styles import Font, PatternFill, Alignment\n",
        "\n",
        "    ws = wb.create_sheet(\"Timeline Persuas√£o\")\n",
        "\n",
        "    # T√≠tulo\n",
        "    ws.merge_cells(\"A1:G1\")\n",
        "    titulo = ws[\"A1\"]\n",
        "    titulo.value = \"TIMELINE DE ELEMENTOS PERSUASIVOS - MAPEAMENTO TEMPORAL\"\n",
        "    titulo.fill = PatternFill(start_color=\"7030A0\", end_color=\"7030A0\", fill_type=\"solid\")\n",
        "    titulo.font = Font(color=\"FFFFFF\", bold=True, size=14)\n",
        "    titulo.alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "    row = 3\n",
        "\n",
        "    # An√°lise temporal por v√≠deo (mostrar apenas top 3 por brevidade)\n",
        "    videos_copy = sorted(dados[\"copywriting\"], key=lambda x: x.get(\"score_persuasao\", 0), reverse=True)\n",
        "\n",
        "    for video in videos_copy[:3]:  # Top 3 performers\n",
        "        ws.merge_cells(f\"A{row}:G{row}\")\n",
        "        video_header = ws[f\"A{row}\"]\n",
        "        video_header.value = f\"üé¨ TIMELINE: {video['video_id']} (Score: {video.get('score_persuasao', 0)}/100)\"\n",
        "        video_header.fill = PatternFill(start_color=\"E2EFDA\", end_color=\"E2EFDA\", fill_type=\"solid\")\n",
        "        video_header.font = Font(bold=True)\n",
        "        row += 1\n",
        "\n",
        "        # Headers da timeline\n",
        "        headers = [\"Tempo\", \"Tipo\", \"Elemento\", \"Contexto\", \"Posi√ß√£o\", \"Impacto\", \"An√°lise\"]\n",
        "        for col, header in enumerate(headers, 1):\n",
        "            cell = ws.cell(row=row, column=col)\n",
        "            cell.value = header\n",
        "            cell.font = Font(bold=True)\n",
        "            cell.fill = PatternFill(start_color=\"F2F2F2\", end_color=\"F2F2F2\", fill_type=\"solid\")\n",
        "        row += 1\n",
        "\n",
        "        # Consolidar timeline\n",
        "        timeline_elementos = []\n",
        "\n",
        "        # Adicionar elementos de cada categoria\n",
        "        for categoria, timeline_key in [(\"GANCHO\", \"ganchos_timeline\"), (\"GATILHO\", \"gatilhos_timeline\"), (\"CTA\", \"ctas_timeline\")]:\n",
        "            timeline_data = video.get(\"timestamp\", {}).get(timeline_key, [])\n",
        "            for item in timeline_data:\n",
        "                timeline_elementos.append({\n",
        "                    \"categoria\": categoria,\n",
        "                    \"tempo\": f\"{item['minuto']:02d}:{item['segundo']:02d}\",\n",
        "                    \"tipo\": item[\"tipo\"].replace(\"_\", \" \").title(),\n",
        "                    \"contexto\": item.get(\"contexto\", \"\")[:40] + \"...\" if len(item.get(\"contexto\", \"\")) > 40 else item.get(\"contexto\", \"\"),\n",
        "                    \"minuto\": item[\"minuto\"],\n",
        "                    \"segundo\": item[\"segundo\"]\n",
        "                })\n",
        "\n",
        "        # Ordenar por tempo\n",
        "        timeline_elementos.sort(key=lambda x: (x[\"minuto\"], x[\"segundo\"]))\n",
        "\n",
        "        if timeline_elementos:\n",
        "            for elemento in timeline_elementos:\n",
        "                ws.cell(row=row, column=1, value=elemento[\"tempo\"])\n",
        "                ws.cell(row=row, column=2, value=elemento[\"categoria\"])\n",
        "                ws.cell(row=row, column=3, value=elemento[\"tipo\"])\n",
        "                ws.cell(row=row, column=4, value=elemento[\"contexto\"])\n",
        "\n",
        "                # Calcular posi√ß√£o no v√≠deo\n",
        "                total_segundos = elemento[\"minuto\"] * 60 + elemento[\"segundo\"]\n",
        "                if total_segundos <= 10:\n",
        "                    posicao = \"ABERTURA\"\n",
        "                    posicao_color = \"70AD47\"\n",
        "                elif total_segundos <= 20:\n",
        "                    posicao = \"MEIO\"\n",
        "                    posicao_color = \"FFC000\"\n",
        "                else:\n",
        "                    posicao = \"FINAL\"\n",
        "                    posicao_color = \"C5504B\"\n",
        "\n",
        "                cell_pos = ws.cell(row=row, column=5, value=posicao)\n",
        "                cell_pos.font = Font(color=posicao_color, bold=True)\n",
        "\n",
        "                # An√°lise de impacto\n",
        "                impacto = analisar_impacto_elemento(elemento[\"categoria\"], posicao)\n",
        "                ws.cell(row=row, column=6, value=impacto[\"score\"])\n",
        "                ws.cell(row=row, column=7, value=impacto[\"analise\"])\n",
        "\n",
        "                if impacto[\"score\"] == \"ALTO\":\n",
        "                    ws.cell(row=row, column=6).font = Font(color=\"70AD47\", bold=True)\n",
        "                elif impacto[\"score\"] == \"BAIXO\":\n",
        "                    ws.cell(row=row, column=6).font = Font(color=\"C5504B\", bold=True)\n",
        "\n",
        "                row += 1\n",
        "        else:\n",
        "            ws.cell(row=row, column=1, value=\"Nenhum elemento temporal mapeado\")\n",
        "            row += 1\n",
        "\n",
        "        row += 2\n",
        "\n",
        "    # Padr√µes temporais identificados\n",
        "    row += 1\n",
        "    ws.merge_cells(f\"A{row}:G{row}\")\n",
        "    padroes_header = ws[f\"A{row}\"]\n",
        "    padroes_header.value = \"üìä PADR√ïES TEMPORAIS IDENTIFICADOS\"\n",
        "    padroes_header.fill = PatternFill(start_color=\"1F4E79\", end_color=\"1F4E79\", fill_type=\"solid\")\n",
        "    padroes_header.font = Font(color=\"FFFFFF\", bold=True, size=12)\n",
        "    row += 1\n",
        "\n",
        "    padroes_temporais = [\n",
        "        \"‚úÖ GANCHOS mais eficazes nos primeiros 10 segundos\",\n",
        "        \"‚úÖ GATILHOS MENTAIS ideais entre 10-20 segundos\",\n",
        "        \"‚úÖ CTAs mais conversores nos √∫ltimos 5 segundos\",\n",
        "        \"‚ö†Ô∏è Evitar CTAs nos primeiros 5 segundos\",\n",
        "        \"üìà Combinar CURIOSIDADE + AUTORIDADE = alta reten√ß√£o\"\n",
        "    ]\n",
        "\n",
        "    for padrao in padroes_temporais:\n",
        "        ws[f\"A{row}\"] = padrao\n",
        "        if \"‚úÖ\" in padrao:\n",
        "            ws[f\"A{row}\"].font = Font(color=\"70AD47\", bold=True)\n",
        "        elif \"‚ö†Ô∏è\" in padrao:\n",
        "            ws[f\"A{row}\"].font = Font(color=\"FFC000\", bold=True)\n",
        "        else:\n",
        "            ws[f\"A{row}\"].font = Font(color=\"1F4E79\", bold=True)\n",
        "        row += 1\n",
        "\n",
        "    # Ajustar larguras\n",
        "    for col, width in [(\"A\", 8), (\"B\", 10), (\"C\", 15), (\"D\", 30), (\"E\", 12), (\"F\", 8), (\"G\", 25)]:\n",
        "        ws.column_dimensions[col].width = width\n",
        "\n",
        "def analisar_impacto_elemento(categoria, posicao):\n",
        "    \"\"\"Analisa o impacto de um elemento baseado na posi√ß√£o\"\"\"\n",
        "    impactos = {\n",
        "        (\"GANCHO\", \"ABERTURA\"): {\"score\": \"ALTO\", \"analise\": \"Ideal para capturar aten√ß√£o\"},\n",
        "        (\"GANCHO\", \"MEIO\"): {\"score\": \"M√âDIO\", \"analise\": \"Melhor no in√≠cio\"},\n",
        "        (\"GANCHO\", \"FINAL\"): {\"score\": \"BAIXO\", \"analise\": \"Reposicionar para abertura\"},\n",
        "        (\"GATILHO\", \"ABERTURA\"): {\"score\": \"M√âDIO\", \"analise\": \"Bom para credibilidade\"},\n",
        "        (\"GATILHO\", \"MEIO\"): {\"score\": \"ALTO\", \"analise\": \"Posi√ß√£o ideal para persuas√£o\"},\n",
        "        (\"GATILHO\", \"FINAL\"): {\"score\": \"M√âDIO\", \"analise\": \"Refor√ßa decis√£o\"},\n",
        "        (\"CTA\", \"ABERTURA\"): {\"score\": \"BAIXO\", \"analise\": \"Muito cedo, construir valor primeiro\"},\n",
        "        (\"CTA\", \"MEIO\"): {\"score\": \"M√âDIO\", \"analise\": \"Considerar mover para final\"},\n",
        "        (\"CTA\", \"FINAL\"): {\"score\": \"ALTO\", \"analise\": \"Posicionamento ideal\"}\n",
        "    }\n",
        "\n",
        "    return impactos.get((categoria, posicao), {\"score\": \"M√âDIO\", \"analise\": \"Analisar contexto espec√≠fico\"})\n",
        "\n",
        "def adicionar_aba_recomendacoes_copy(wb, dados):\n",
        "    \"\"\"Adiciona aba de recomenda√ß√µes estrat√©gicas consolidadas\"\"\"\n",
        "    from openpyxl.styles import Font, PatternFill, Alignment\n",
        "\n",
        "    ws = wb.create_sheet(\"Recomenda√ß√µes Copy\")\n",
        "\n",
        "    # T√≠tulo\n",
        "    ws.merge_cells(\"A1:F1\")\n",
        "    titulo = ws[\"A1\"]\n",
        "    titulo.value = \"RECOMENDA√á√ïES ESTRAT√âGICAS DE COPYWRITING - PLANO DE A√á√ÉO\"\n",
        "    titulo.fill = PatternFill(start_color=\"C5504B\", end_color=\"C5504B\", fill_type=\"solid\")\n",
        "    titulo.font = Font(color=\"FFFFFF\", bold=True, size=14)\n",
        "    titulo.alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "    row = 3\n",
        "\n",
        "    # Consolidar recomenda√ß√µes por prioridade\n",
        "    todas_recomendacoes = []\n",
        "    for video in dados[\"copywriting\"]:\n",
        "        recomendacoes_video = video.get(\"recomendacoes_estrategicas\", [])\n",
        "        for rec in recomendacoes_video:\n",
        "            rec[\"video_id\"] = video[\"video_id\"]\n",
        "            todas_recomendacoes.append(rec)\n",
        "\n",
        "    # Agrupar por prioridade\n",
        "    recomendacoes_por_prioridade = {\n",
        "        \"CR√çTICA\": [],\n",
        "        \"ALTA\": [],\n",
        "        \"M√âDIA\": []\n",
        "    }\n",
        "\n",
        "    for rec in todas_recomendacoes:\n",
        "        prioridade = rec.get(\"prioridade\", \"M√âDIA\")\n",
        "        if prioridade in recomendacoes_por_prioridade:\n",
        "            recomendacoes_por_prioridade[prioridade].append(rec)\n",
        "\n",
        "    # Exibir por prioridade\n",
        "    for prioridade in [\"CR√çTICA\", \"ALTA\", \"M√âDIA\"]:\n",
        "        if not recomendacoes_por_prioridade[prioridade]:\n",
        "            continue\n",
        "\n",
        "        ws[f\"A{row}\"] = f\"üö® PRIORIDADE {prioridade}\"\n",
        "        if prioridade == \"CR√çTICA\":\n",
        "            ws[f\"A{row}\"].font = Font(color=\"FF0000\", bold=True, size=12)\n",
        "        elif prioridade == \"ALTA\":\n",
        "            ws[f\"A{row}\"].font = Font(color=\"C5504B\", bold=True, size=12)\n",
        "        else:\n",
        "            ws[f\"A{row}\"].font = Font(color=\"FFC000\", bold=True, size=12)\n",
        "\n",
        "        row += 2\n",
        "\n",
        "        # Headers\n",
        "        headers = [\"Categoria\", \"Recomenda√ß√£o\", \"V√≠deos Afetados\", \"A√ß√£o Sugerida\"]\n",
        "        for col, header in enumerate(headers, 1):\n",
        "            cell = ws.cell(row=row, column=col)\n",
        "            cell.value = header\n",
        "            cell.font = Font(bold=True)\n",
        "            cell.fill = PatternFill(start_color=\"F2F2F2\", end_color=\"F2F2F2\", fill_type=\"solid\")\n",
        "        row += 1\n",
        "\n",
        "        # Agrupar recomenda√ß√µes similares da mesma prioridade\n",
        "        grupos = {}\n",
        "        for rec in recomendacoes_por_prioridade[prioridade]:\n",
        "            categoria = rec[\"categoria\"]\n",
        "            if categoria not in grupos:\n",
        "                grupos[categoria] = {\n",
        "                    \"recomendacao\": rec[\"recomendacao\"],\n",
        "                    \"videos\": [],\n",
        "                    \"acao\": gerar_acao_especifica(categoria)\n",
        "                }\n",
        "            grupos[categoria][\"videos\"].append(rec[\"video_id\"])\n",
        "\n",
        "        for categoria, dados_grupo in grupos.items():\n",
        "            ws.cell(row=row, column=1, value=categoria)\n",
        "            ws.cell(row=row, column=2, value=dados_grupo[\"recomendacao\"])\n",
        "            ws.cell(row=row, column=3, value=f\"{len(dados_grupo['videos'])} v√≠deo(s)\")\n",
        "            ws.cell(row=row, column=4, value=dados_grupo[\"acao\"])\n",
        "            row += 1\n",
        "\n",
        "        row += 2\n",
        "\n",
        "    # Plano de a√ß√£o 30 dias\n",
        "    row += 2\n",
        "    ws.merge_cells(f\"A{row}:F{row}\")\n",
        "    plano_header = ws[f\"A{row}\"]\n",
        "    plano_header.value = \"üìÖ PLANO DE A√á√ÉO ESTRAT√âGICO - PR√ìXIMOS 30 DIAS\"\n",
        "    plano_header.fill = PatternFill(start_color=\"70AD47\", end_color=\"70AD47\", fill_type=\"solid\")\n",
        "    plano_header.font = Font(color=\"FFFFFF\", bold=True, size=12)\n",
        "    row += 2\n",
        "\n",
        "    plano_30_dias = [\n",
        "        (\"SEMANA 1 - CR√çTICO\", [\n",
        "            \"Implementar CTAs em TODOS os v√≠deos sem call-to-action\",\n",
        "            \"Corrigir v√≠deos com score de persuas√£o abaixo de 30\",\n",
        "            \"Aplicar templates identificados nos v√≠deos top performers\"\n",
        "        ]),\n",
        "        (\"SEMANA 2 - ALTA PRIORIDADE\", [\n",
        "            \"Adicionar ganchos de abertura nos v√≠deos com baixa reten√ß√£o\",\n",
        "            \"Incorporar gatilhos de autoridade e prova social\",\n",
        "            \"Otimizar timeline de elementos persuasivos\"\n",
        "        ]),\n",
        "        (\"SEMANA 3 - OTIMIZA√á√ÉO\", [\n",
        "            \"Testar varia√ß√µes de CTAs mais eficazes\",\n",
        "            \"Refinar estruturas narrativas baseadas nos templates\",\n",
        "            \"A/B testing de elementos espec√≠ficos\"\n",
        "        ]),\n",
        "        (\"SEMANA 4 - VALIDA√á√ÉO\", [\n",
        "            \"Medir performance p√≥s-implementa√ß√£o\",\n",
        "            \"Documentar novos padr√µes de sucesso identificados\",\n",
        "            \"Atualizar biblioteca de templates comprovados\"\n",
        "        ])\n",
        "    ]\n",
        "\n",
        "    for semana_titulo, acoes in plano_30_dias:\n",
        "        ws[f\"A{row}\"] = semana_titulo\n",
        "        ws[f\"A{row}\"].font = Font(bold=True, color=\"1F4E79\", size=11)\n",
        "        row += 1\n",
        "\n",
        "        for acao in acoes:\n",
        "            ws[f\"B{row}\"] = f\"‚Ä¢ {acao}\"\n",
        "            row += 1\n",
        "\n",
        "        row += 1\n",
        "\n",
        "    # KPIs de acompanhamento\n",
        "    row += 2\n",
        "    ws.merge_cells(f\"A{row}:F{row}\")\n",
        "    kpis_header = ws[f\"A{row}\"]\n",
        "    kpis_header.value = \"üìä KPIs DE ACOMPANHAMENTO - M√âTRICAS DE SUCESSO\"\n",
        "    kpis_header.fill = PatternFill(start_color=\"7030A0\", end_color=\"7030A0\", fill_type=\"solid\")\n",
        "    kpis_header.font = Font(color=\"FFFFFF\", bold=True, size=12)\n",
        "    row += 2\n",
        "\n",
        "    kpis = [\n",
        "        (\"Score de Persuas√£o M√©dio\", \"Aumento de 20% em 30 dias\", \"Mensal\"),\n",
        "        (\"Taxa de CTAs Implementados\", \"100% dos v√≠deos com pelo menos 1 CTA\", \"Imediato\"),\n",
        "        (\"Variedade de Ganchos\", \"3+ tipos diferentes por v√≠deo\", \"Por v√≠deo\"),\n",
        "        (\"Diversidade de Gatilhos\", \"4+ gatilhos mentais por v√≠deo\", \"Por v√≠deo\"),\n",
        "        (\"Templates Ativos\", \"5+ estruturas replic√°veis em uso\", \"Mensal\"),\n",
        "        (\"Taxa de Otimiza√ß√£o\", \"80% das recomenda√ß√µes cr√≠ticas aplicadas\", \"Semanal\")\n",
        "    ]\n",
        "\n",
        "    headers_kpi = [\"KPI\", \"Meta\", \"Frequ√™ncia de Medi√ß√£o\"]\n",
        "    for col, header in enumerate(headers_kpi, 1):\n",
        "        cell = ws.cell(row=row, column=col)\n",
        "        cell.value = header\n",
        "        cell.font = Font(bold=True)\n",
        "        cell.fill = PatternFill(start_color=\"E2EFDA\", end_color=\"E2EFDA\", fill_type=\"solid\")\n",
        "    row += 1\n",
        "\n",
        "    for kpi_nome, meta, frequencia in kpis:\n",
        "        ws.cell(row=row, column=1, value=kpi_nome)\n",
        "        ws.cell(row=row, column=2, value=meta)\n",
        "        ws.cell(row=row, column=3, value=frequencia)\n",
        "        row += 1\n",
        "\n",
        "    # Pr√≥ximos passos imediatos\n",
        "    row += 3\n",
        "    ws[f\"A{row}\"] = \"üéØ PR√ìXIMOS PASSOS IMEDIATOS (HOJE)\"\n",
        "    ws[f\"A{row}\"].font = Font(bold=True, color=\"C5504B\", size=12)\n",
        "    row += 1\n",
        "\n",
        "    proximos_passos = gerar_proximos_passos_imediatos(dados[\"copywriting\"])\n",
        "\n",
        "    for i, passo in enumerate(proximos_passos, 1):\n",
        "        ws[f\"A{row}\"] = f\"{i}. {passo}\"\n",
        "        ws[f\"A{row}\"].font = Font(bold=True)\n",
        "        row += 1\n",
        "\n",
        "    # Ajustar larguras\n",
        "    for col, width in [(\"A\", 20), (\"B\", 40), (\"C\", 15), (\"D\", 30), (\"E\", 15), (\"F\", 15)]:\n",
        "        ws.column_dimensions[col].width = width\n",
        "\n",
        "def gerar_acao_especifica(categoria):\n",
        "    \"\"\"Gera a√ß√£o espec√≠fica baseada na categoria da recomenda√ß√£o\"\"\"\n",
        "    acoes = {\n",
        "        \"GANCHOS\": \"Revisar primeiros 5 segundos e adicionar pergunta ou curiosidade\",\n",
        "        \"GATILHOS\": \"Incorporar elementos de autoridade, prova social ou reciprocidade\",\n",
        "        \"CTA\": \"Adicionar call-to-action claro nos √∫ltimos 3-5 segundos\",\n",
        "        \"ESTRUTURA\": \"Aplicar template identificado mais pr√≥ximo do nicho\",\n",
        "        \"PERSUAS√ÉO\": \"Combinar m√∫ltiplos elementos persuasivos em sequ√™ncia l√≥gica\"\n",
        "    }\n",
        "    return acoes.get(categoria, \"Revisar e otimizar elementos espec√≠ficos mencionados\")\n",
        "\n",
        "def gerar_proximos_passos_imediatos(videos_copy):\n",
        "    \"\"\"Gera lista de a√ß√µes imediatas baseadas na an√°lise\"\"\"\n",
        "    passos = []\n",
        "\n",
        "    # Verificar v√≠deos sem CTA\n",
        "    videos_sem_cta = [v for v in videos_copy if not v.get(\"ctas_detectados\")]\n",
        "    if videos_sem_cta:\n",
        "        passos.append(f\"CR√çTICO: Adicionar CTAs em {len(videos_sem_cta)} v√≠deo(s): {', '.join([v['video_id'] for v in videos_sem_cta[:3]])}\")\n",
        "\n",
        "    # Verificar scores baixos\n",
        "    videos_score_baixo = [v for v in videos_copy if v.get(\"score_persuasao\", 0) < 30]\n",
        "    if videos_score_baixo:\n",
        "        passos.append(f\"Revisar {len(videos_score_baixo)} v√≠deo(s) com score cr√≠tico < 30\")\n",
        "\n",
        "    # Templates a aplicar\n",
        "    templates_identificados = []\n",
        "    for video in videos_copy:\n",
        "        templates_identificados.extend(video.get(\"templates_identificados\", []))\n",
        "\n",
        "    if templates_identificados:\n",
        "        template_mais_comum = max(set(t[\"nome\"] for t in templates_identificados),\n",
        "                                 key=lambda x: sum(1 for t in templates_identificados if t[\"nome\"] == x))\n",
        "        passos.append(f\"Aplicar template '{template_mais_comum.replace('_', ' ')}' em novos v√≠deos\")\n",
        "\n",
        "    # A√ß√µes gerais\n",
        "    passos.extend([\n",
        "        \"Backup dos v√≠deos atuais antes das modifica√ß√µes\",\n",
        "        \"Priorizar implementa√ß√µes por ordem de impacto (CTAs primeiro)\",\n",
        "        \"Documentar mudan√ßas para acompanhar resultados\"\n",
        "    ])\n",
        "\n",
        "    return passos[:6]  # Limitar a 6 passos\n",
        "\n",
        "def atualizar_aba_principal_com_copy(wb, dados):\n",
        "    \"\"\"Atualiza a aba principal existente com m√©tricas de copywriting\"\"\"\n",
        "    # Tentar encontrar aba principal (pode ter nomes diferentes)\n",
        "    aba_principal = None\n",
        "    possiveis_nomes = [\"Dashboard Principal\", \"Executive Summary\", \"Summary\", \"Principal\"]\n",
        "\n",
        "    for nome in wb.sheetnames:\n",
        "        if any(possivel in nome for possivel in possiveis_nomes):\n",
        "            aba_principal = wb[nome]\n",
        "            break\n",
        "\n",
        "    if not aba_principal:\n",
        "        # Se n√£o encontrou, usar a primeira aba\n",
        "        aba_principal = wb.worksheets[0]\n",
        "\n",
        "    # Encontrar pr√≥xima linha vazia para adicionar se√ß√£o de copywriting\n",
        "    next_row = 1\n",
        "    for row in range(1, 100):\n",
        "        if aba_principal[f\"A{row}\"].value is None:\n",
        "            next_row = row\n",
        "            break\n",
        "\n",
        "    # Adicionar se√ß√£o de copywriting\n",
        "    from openpyxl.styles import Font, PatternFill\n",
        "\n",
        "    # T√≠tulo da se√ß√£o\n",
        "    aba_principal.merge_cells(f\"A{next_row}:H{next_row}\")\n",
        "    titulo_copy = aba_principal[f\"A{next_row}\"]\n",
        "    titulo_copy.value = \"üìù AN√ÅLISE DE COPYWRITING - RESUMO EXECUTIVO\"\n",
        "    titulo_copy.fill = PatternFill(start_color=\"7030A0\", end_color=\"7030A0\", fill_type=\"solid\")\n",
        "    titulo_copy.font = Font(color=\"FFFFFF\", bold=True, size=12)\n",
        "    next_row += 2\n",
        "\n",
        "    # M√©tricas resumidas\n",
        "    videos_copy = dados[\"copywriting\"]\n",
        "\n",
        "    if videos_copy:\n",
        "        scores = [v.get(\"score_persuasao\", 0) for v in videos_copy]\n",
        "        score_medio = sum(scores) / len(scores)\n",
        "        videos_sem_cta = len([v for v in videos_copy if not v.get(\"ctas_detectados\")])\n",
        "        templates_total = sum(len(v.get(\"templates_identificados\", [])) for v in videos_copy)\n",
        "\n",
        "        metricas_resumo = [\n",
        "            (\"Score de Persuas√£o M√©dio:\", f\"{score_medio:.1f}/100\"),\n",
        "            (\"V√≠deos sem CTA:\", f\"{videos_sem_cta} (CR√çTICO)\" if videos_sem_cta > 0 else \"0 ‚úÖ\"),\n",
        "            (\"Templates Identificados:\", str(templates_total)),\n",
        "            (\"Status Geral:\", \"Otimiza√ß√£o necess√°ria\" if score_medio < 60 or videos_sem_cta > 0 else \"Performance boa\")\n",
        "        ]\n",
        "\n",
        "        for metrica, valor in metricas_resumo:\n",
        "            aba_principal[f\"A{next_row}\"] = metrica\n",
        "            aba_principal[f\"B{next_row}\"] = valor\n",
        "            aba_principal[f\"A{next_row}\"].font = Font(bold=True)\n",
        "\n",
        "            if \"CR√çTICO\" in valor:\n",
        "                aba_principal[f\"B{next_row}\"].font = Font(color=\"FF0000\", bold=True)\n",
        "            elif \"‚úÖ\" in valor:\n",
        "                aba_principal[f\"B{next_row}\"].font = Font(color=\"70AD47\", bold=True)\n",
        "\n",
        "            next_row += 1\n",
        "\n",
        "    else:\n",
        "        aba_principal[f\"A{next_row}\"] = \"‚ö†Ô∏è Execute a an√°lise de copywriting (C√©lula 2.4) para ver m√©tricas\"\n",
        "        aba_principal[f\"A{next_row}\"].font = Font(color=\"FFC000\", bold=True)\n",
        "\n",
        "def gerar_relatorios_copywriting_individuais(dados):\n",
        "    \"\"\"Gera relat√≥rios individuais de texto para cada v√≠deo\"\"\"\n",
        "    print(\"  üìÑ Gerando relat√≥rios individuais de copywriting...\")\n",
        "\n",
        "    pasta_relatorios = os.path.join(PASTA_TRABALHO, \"relatorios_copywriting\")\n",
        "    os.makedirs(pasta_relatorios, exist_ok=True)\n",
        "\n",
        "    for video_copy in dados[\"copywriting\"]:\n",
        "        video_id = video_copy[\"video_id\"]\n",
        "\n",
        "        relatorio_path = os.path.join(pasta_relatorios, f\"{video_id}_copywriting_completo.txt\")\n",
        "\n",
        "        with open(relatorio_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\"=\"*60 + \"\\n\")\n",
        "            f.write(\"RELAT√ìRIO COMPLETO DE AN√ÅLISE DE COPYWRITING\\n\")\n",
        "            f.write(\"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "            f.write(f\"üìπ V√≠deo ID: {video_id}\\n\")\n",
        "            f.write(f\"üéØ Score de Persuas√£o: {video_copy.get('score_persuasao', 0)}/100\\n\")\n",
        "            f.write(f\"üìù Total de Palavras: {video_copy.get('total_palavras', 0)}\\n\\n\")\n",
        "\n",
        "            # Texto completo\n",
        "            f.write(\"TRANSCRI√á√ÉO COMPLETA:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            f.write(video_copy.get(\"texto_completo\", \"Transcri√ß√£o n√£o dispon√≠vel\") + \"\\n\\n\")\n",
        "\n",
        "            # Ganchos\n",
        "            f.write(\"üé£ GANCHOS DETECTADOS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            ganchos = video_copy.get(\"ganchos_detectados\", {})\n",
        "            if ganchos:\n",
        "                for tipo, dados in ganchos.items():\n",
        "                    f.write(f\"‚Ä¢ {tipo.replace('_', ' ').title()}: {dados['count']} ocorr√™ncia(s)\\n\")\n",
        "                    for exemplo in dados.get(\"exemplos\", [])[:2]:\n",
        "                        f.write(f\"  - \\\"{exemplo}\\\"\\n\")\n",
        "                    f.write(\"\\n\")\n",
        "            else:\n",
        "                f.write(\"‚ùå Nenhum gancho detectado - OPORTUNIDADE DE MELHORIA\\n\\n\")\n",
        "\n",
        "            # Gatilhos\n",
        "            f.write(\"üß† GATILHOS MENTAIS DETECTADOS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            gatilhos = video_copy.get(\"gatilhos_mentais_detectados\", {})\n",
        "            if gatilhos:\n",
        "                for tipo, dados in gatilhos.items():\n",
        "                    f.write(f\"‚Ä¢ {tipo.replace('_', ' ').title()}: {dados['count']} ocorr√™ncia(s)\\n\")\n",
        "                    for exemplo in dados.get(\"exemplos\", [])[:2]:\n",
        "                        f.write(f\"  - \\\"{exemplo}\\\"\\n\")\n",
        "                    f.write(\"\\n\")\n",
        "            else:\n",
        "                f.write(\"‚ùå Nenhum gatilho mental detectado - ADICIONAR URGENTEMENTE\\n\\n\")\n",
        "\n",
        "            # CTAs\n",
        "            f.write(\"üì¢ CALLS-TO-ACTION DETECTADOS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            ctas = video_copy.get(\"ctas_detectados\", {})\n",
        "            if ctas:\n",
        "                for tipo, dados in ctas.items():\n",
        "                    f.write(f\"‚Ä¢ {tipo.replace('_', ' ').title()}: {dados['count']} ocorr√™ncia(s)\\n\")\n",
        "                    for exemplo in dados.get(\"exemplos\", [])[:2]:\n",
        "                        f.write(f\"  - \\\"{exemplo}\\\"\\n\")\n",
        "                    f.write(\"\\n\")\n",
        "            else:\n",
        "                f.write(\"üö® CR√çTICO: Nenhum CTA detectado - IMPLEMENTAR IMEDIATAMENTE\\n\\n\")\n",
        "\n",
        "            # Templates\n",
        "            f.write(\"üìã TEMPLATES IDENTIFICADOS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            templates = video_copy.get(\"templates_identificados\", [])\n",
        "            if templates:\n",
        "                for template in templates:\n",
        "                    f.write(f\"‚Ä¢ {template['nome'].replace('_', ' ')}\\n\")\n",
        "                    f.write(f\"  Estrutura: {template['estrutura']}\\n\")\n",
        "                    f.write(f\"  Efic√°cia: {template['eficacia']}\\n\")\n",
        "                    f.write(f\"  Uso: {template['uso_recomendado']}\\n\\n\")\n",
        "            else:\n",
        "                f.write(\"üìù Nenhum template espec√≠fico identificado\\n\\n\")\n",
        "\n",
        "            # Recomenda√ß√µes\n",
        "            f.write(\"üéØ RECOMENDA√á√ïES ESTRAT√âGICAS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            recomendacoes = video_copy.get(\"recomendacoes_estrategicas\", [])\n",
        "            if recomendacoes:\n",
        "                for i, rec in enumerate(recomendacoes, 1):\n",
        "                    f.write(f\"{i}. [{rec['prioridade']}] {rec['categoria']}\\n\")\n",
        "                    f.write(f\"   {rec['recomendacao']}\\n\\n\")\n",
        "            else:\n",
        "                f.write(\"‚úÖ Nenhuma recomenda√ß√£o cr√≠tica - v√≠deo bem otimizado\\n\\n\")\n",
        "\n",
        "            # Timeline resumida\n",
        "            f.write(\"‚è∞ TIMELINE DE ELEMENTOS (RESUMIDA):\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            timeline_ganchos = video_copy.get(\"timestamp\", {}).get(\"ganchos_timeline\", [])\n",
        "            timeline_ctas = video_copy.get(\"timestamp\", {}).get(\"ctas_timeline\", [])\n",
        "\n",
        "            todos_elementos = []\n",
        "            for item in timeline_ganchos:\n",
        "                todos_elementos.append((item[\"minuto\"], item[\"segundo\"], \"GANCHO\", item[\"tipo\"]))\n",
        "            for item in timeline_ctas:\n",
        "                todos_elementos.append((item[\"minuto\"], item[\"segundo\"], \"CTA\", item[\"tipo\"]))\n",
        "\n",
        "            todos_elementos.sort()\n",
        "\n",
        "            if todos_elementos:\n",
        "                for minuto, segundo, categoria, tipo in todos_elementos:\n",
        "                    f.write(f\"[{minuto:02d}:{segundo:02d}] {categoria}: {tipo.replace('_', ' ').title()}\\n\")\n",
        "            else:\n",
        "                f.write(\"Nenhum elemento temporal mapeado\\n\")\n",
        "\n",
        "            f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "            f.write(\"Relat√≥rio gerado pelo sistema de engenharia reversa\\n\")\n",
        "            f.write(\"Para implementar as recomenda√ß√µes, consulte o dashboard principal\\n\")\n",
        "            f.write(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    print(f\"  ‚úÖ {len(dados['copywriting'])} relat√≥rios individuais gerados\")\n",
        "\n",
        "# Fun√ß√£o principal de execu√ß√£o\n",
        "def executar_integracao_copywriting_dashboard():\n",
        "    \"\"\"Fun√ß√£o principal para executar a integra√ß√£o\"\"\"\n",
        "    print(\"üöÄ EXECUTANDO INTEGRA√á√ÉO DE COPYWRITING NO DASHBOARD EXISTENTE\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    try:\n",
        "        dashboard_atualizado = integrar_copywriting_dashboard_existente()\n",
        "\n",
        "        if dashboard_atualizado:\n",
        "            print(\"\\n\" + \"=\"*70)\n",
        "            print(\"‚úÖ INTEGRA√á√ÉO CONCLU√çDA COM SUCESSO!\")\n",
        "            print(\"=\"*70)\n",
        "            print(f\"üìä Dashboard atualizado: {os.path.basename(dashboard_atualizado)}\")\n",
        "            print(\"\\nüìã ABAS ADICIONADAS:\")\n",
        "            print(\"  ‚Ä¢ Copywriting Estrat√©gico - An√°lise completa por v√≠deo\")\n",
        "            print(\"  ‚Ä¢ Templates Replic√°veis - Estruturas identificadas\")\n",
        "            print(\"  ‚Ä¢ Timeline Persuas√£o - Mapeamento temporal\")\n",
        "            print(\"  ‚Ä¢ Recomenda√ß√µes Copy - Plano de a√ß√£o 30 dias\")\n",
        "            print(\"  ‚Ä¢ Dashboard Principal - Atualizada com m√©tricas\")\n",
        "\n",
        "            print(f\"\\nüéØ PR√ìXIMOS PASSOS:\")\n",
        "            print(\"1. Abra o dashboard e revise a aba 'Copywriting Estrat√©gico'\")\n",
        "            print(\"2. Identifique v√≠deos com score < 50 para otimiza√ß√£o\")\n",
        "            print(\"3. Implemente CTAs nos v√≠deos marcados como CR√çTICO\")\n",
        "            print(\"4. Aplique templates identificados em novos v√≠deos\")\n",
        "            print(\"5. Siga o plano de a√ß√£o de 30 dias na aba 'Recomenda√ß√µes Copy'\")\n",
        "\n",
        "        else:\n",
        "            print(\"\\n‚ùå Falha na integra√ß√£o - verifique os pr√©-requisitos\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Erro de Execu√ß√£o: {type(e).__name__}: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# Executar a integra√ß√£o\n",
        "if __name__ == \"__main__\":\n",
        "    executar_integracao_copywriting_dashboard()"
      ],
      "metadata": {
        "id": "GlbVqMZ-yhI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# LAYER 2.4: GERA√á√ÉO DE LEGENDAS E AN√ÅLISE DE COPYWRITING - VERS√ÉO FINAL\n",
        "# ============================================================================\n",
        "\n",
        "import re\n",
        "from datetime import timedelta, datetime\n",
        "from collections import Counter\n",
        "import json\n",
        "import os\n",
        "\n",
        "def processar_copywriting_todos_videos_adaptado():\n",
        "    \"\"\"Processa an√°lise de copywriting adaptada para o sistema existente\"\"\"\n",
        "    print(\"üîÑ Iniciando processamento de copywriting adaptado...\")\n",
        "\n",
        "    # Verificar pr√©-requisitos baseado na estrutura existente\n",
        "    if not \"PASTA_TRABALHO\" in globals():\n",
        "        print(\"‚ùå Vari√°veis globais n√£o encontradas. Execute a C√âLULA 1.2 primeiro.\")\n",
        "        return\n",
        "\n",
        "    pasta_dados = os.path.join(PASTA_TRABALHO, \"dados\")\n",
        "    if not os.path.exists(pasta_dados):\n",
        "        print(\"‚ùå Pasta de dados n√£o encontrada. Execute as c√©lulas anteriores primeiro.\")\n",
        "        return\n",
        "\n",
        "    # Buscar dados de decomposi√ß√£o (nome correto do arquivo)\n",
        "    decomposicao_path = os.path.join(pasta_dados, \"decomposicao_completa.json\")\n",
        "\n",
        "    if not os.path.exists(decomposicao_path):\n",
        "        print(\"‚ùå Dados de decomposi√ß√£o n√£o encontrados. Execute a C√âLULA 2.3 primeiro.\")\n",
        "        print(f\"Procurando arquivo: {decomposicao_path}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        with open(decomposicao_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            decomposicoes_data = json.load(f)\n",
        "\n",
        "        print(f\"‚úÖ Dados de decomposi√ß√£o carregados: {len(decomposicoes_data)} v√≠deos encontrados\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro ao carregar dados de decomposi√ß√£o: {e}\")\n",
        "        return\n",
        "\n",
        "    # Filtrar apenas v√≠deos com status \"decomposto\" e que tenham transcri√ß√£o\n",
        "    videos_validos = []\n",
        "    for decomposicao in decomposicoes_data:\n",
        "        if (decomposicao.get(\"status\") == \"decomposto\" and\n",
        "            decomposicao.get(\"audio_transcrito\") and\n",
        "            len(decomposicao.get(\"audio_transcrito\", \"\").strip()) > 10):\n",
        "            videos_validos.append(decomposicao)\n",
        "\n",
        "    if not videos_validos:\n",
        "        print(\"‚ùå Nenhum v√≠deo com transcri√ß√£o v√°lida encontrado.\")\n",
        "        print(\"Verifique se a C√âLULA 2.3 foi executada com sucesso e se os v√≠deos possuem √°udio.\")\n",
        "        return\n",
        "\n",
        "    print(f\"üìä Processando {len(videos_validos)} v√≠deos com transcri√ß√£o v√°lida...\")\n",
        "\n",
        "    analises_copywriting = []\n",
        "    legendas_geradas = []\n",
        "\n",
        "    for i, decomposicao in enumerate(videos_validos, 1):\n",
        "        video_id = decomposicao[\"video_id\"]\n",
        "        audio_transcrito = decomposicao[\"audio_transcrito\"]\n",
        "\n",
        "        print(f\"[{i}/{len(videos_validos)}] Processando copywriting para: {video_id}\")\n",
        "\n",
        "        try:\n",
        "            # Estimar dura√ß√£o do v√≠deo baseado na an√°lise de √°udio\n",
        "            duracao_segundos = decomposicao.get(\"audio_analise\", {}).get(\"duracao_audio_segundos\", 30)\n",
        "\n",
        "            # Criar info do v√≠deo para compatibilidade\n",
        "            video_info = {\n",
        "                \"id\": video_id,\n",
        "                \"duracao_segundos\": duracao_segundos\n",
        "            }\n",
        "\n",
        "            # Gerar legendas\n",
        "            legendas_data, srt_path, txt_path = gerar_legendas_com_timestamps(video_info, decomposicao)\n",
        "\n",
        "            if legendas_data:\n",
        "                legendas_info = {\n",
        "                    \"video_id\": video_id,\n",
        "                    \"srt_path\": srt_path,\n",
        "                    \"txt_path\": txt_path,\n",
        "                    \"total_segmentos\": len(legendas_data),\n",
        "                    \"duracao_total\": duracao_segundos,\n",
        "                    \"legendas_data\": legendas_data\n",
        "                }\n",
        "                legendas_geradas.append(legendas_info)\n",
        "\n",
        "                # An√°lise de copywriting\n",
        "                analise_copy = analisar_copywriting_estrategico(legendas_data, video_id)\n",
        "                analises_copywriting.append(analise_copy)\n",
        "\n",
        "                print(f\"  ‚úÖ Copywriting analisado: Score {analise_copy['score_persuasao']}/100\")\n",
        "            else:\n",
        "                print(f\"  ‚ùå Falha na gera√ß√£o de legendas para {video_id}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Erro no processamento de copywriting para {video_id}: {e}\")\n",
        "\n",
        "    if not analises_copywriting:\n",
        "        print(\"‚ùå Nenhuma an√°lise de copywriting foi gerada. Verifique os dados de entrada.\")\n",
        "        return\n",
        "\n",
        "    # Salvar dados de copywriting\n",
        "    copywriting_path = os.path.join(pasta_dados, \"analises_copywriting_completas.json\")\n",
        "    with open(copywriting_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(analises_copywriting, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"üíæ An√°lises de copywriting salvas em: {copywriting_path}\")\n",
        "\n",
        "    # Salvar dados de legendas\n",
        "    legendas_path = os.path.join(pasta_dados, \"legendas_geradas.json\")\n",
        "    with open(legendas_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(legendas_geradas, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"üíæ Dados de legendas salvos em: {legendas_path}\")\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    if os.path.exists(config_path):\n",
        "        try:\n",
        "            with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                config = json.load(f)\n",
        "\n",
        "            config[\"status_etapas\"][\"copywriting_analysis\"] = True\n",
        "\n",
        "            with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "        except:\n",
        "            print(\"‚ö†Ô∏è N√£o foi poss√≠vel atualizar o arquivo de configura√ß√£o\")\n",
        "\n",
        "    print(f\"\\n‚úÖ AN√ÅLISE DE COPYWRITING CONCLU√çDA!\")\n",
        "    print(f\"Total de v√≠deos com copywriting analisado: {len(analises_copywriting)}\")\n",
        "    print(f\"Total de legendas geradas: {len(legendas_geradas)}\")\n",
        "    print(f\"\\n‚û°Ô∏è PR√ìXIMA C√âLULA: 4.3 - INTEGRA√á√ÉO COM DASHBOARD\")\n",
        "\n",
        "def gerar_legendas_com_timestamps(video_info, decomposicao_data):\n",
        "    \"\"\"Gera legendas SRT e TXT com timestamps precisos a partir da transcri√ß√£o\"\"\"\n",
        "    print(\"  üîÑ Gerando legendas com timestamps...\")\n",
        "\n",
        "    video_id = video_info[\"id\"]\n",
        "    audio_transcrito = decomposicao_data.get(\"audio_transcrito\", \"\")\n",
        "\n",
        "    if not audio_transcrito.strip():\n",
        "        print(\"    ‚ùå Erro: Transcri√ß√£o de √°udio vazia\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Calcular dura√ß√£o do v√≠deo\n",
        "    duracao_segundos = video_info.get(\"duracao_segundos\", 30)  # Default 30s se n√£o informado\n",
        "\n",
        "    # Dividir texto em segmentos baseados em pontua√ß√£o e pausas naturais\n",
        "    segmentos = dividir_texto_em_segmentos(audio_transcrito)\n",
        "\n",
        "    if not segmentos:\n",
        "        print(\"    ‚ùå Erro: N√£o foi poss√≠vel segmentar o texto\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Calcular timestamps para cada segmento\n",
        "    legendas_data = []\n",
        "    duracao_por_segmento = duracao_segundos / len(segmentos) if segmentos else 1\n",
        "\n",
        "    for i, segmento in enumerate(segmentos):\n",
        "        inicio_segundos = i * duracao_por_segmento\n",
        "        fim_segundos = (i + 1) * duracao_por_segmento\n",
        "\n",
        "        legenda_item = {\n",
        "            \"id\": i + 1,\n",
        "            \"inicio\": segundos_para_timestamp(inicio_segundos),\n",
        "            \"fim\": segundos_para_timestamp(fim_segundos),\n",
        "            \"texto\": segmento.strip(),\n",
        "            \"inicio_segundos\": inicio_segundos,\n",
        "            \"fim_segundos\": fim_segundos\n",
        "        }\n",
        "        legendas_data.append(legenda_item)\n",
        "\n",
        "    # Gerar arquivos SRT e TXT\n",
        "    pasta_legendas = os.path.join(PASTA_TRABALHO, \"legendas\")\n",
        "    os.makedirs(pasta_legendas, exist_ok=True)\n",
        "\n",
        "    # Arquivo SRT\n",
        "    srt_path = os.path.join(pasta_legendas, f\"{video_id}_legendas.srt\")\n",
        "    gerar_arquivo_srt(legendas_data, srt_path)\n",
        "\n",
        "    # Arquivo TXT com timestamps\n",
        "    txt_path = os.path.join(pasta_legendas, f\"{video_id}_legendas_timestamped.txt\")\n",
        "    gerar_arquivo_txt_timestamped(legendas_data, txt_path)\n",
        "\n",
        "    print(f\"    ‚úÖ Legendas SRT geradas: {srt_path}\")\n",
        "    print(f\"    ‚úÖ Legendas TXT com timestamps geradas: {txt_path}\")\n",
        "\n",
        "    return legendas_data, srt_path, txt_path\n",
        "\n",
        "def dividir_texto_em_segmentos(texto, max_chars=50):\n",
        "    \"\"\"Divide o texto em segmentos l√≥gicos para legendas\"\"\"\n",
        "    # Dividir por frases primeiro\n",
        "    frases = re.split(r'[.!?]+', texto)\n",
        "    segmentos = []\n",
        "\n",
        "    for frase in frases:\n",
        "        if not frase.strip():\n",
        "            continue\n",
        "\n",
        "        # Se a frase √© muito longa, dividir por v√≠rgulas ou conjun√ß√µes\n",
        "        if len(frase) > max_chars:\n",
        "            sub_segmentos = re.split(r'[,;]|(?:\\s+(?:e|mas|ent√£o|porque|que)\\s+)', frase)\n",
        "            for sub in sub_segmentos:\n",
        "                if sub.strip() and len(sub.strip()) > 3:\n",
        "                    segmentos.append(sub.strip())\n",
        "        else:\n",
        "            if frase.strip() and len(frase.strip()) > 3:\n",
        "                segmentos.append(frase.strip())\n",
        "\n",
        "    # Se ainda houver segmentos muito longos, dividir por palavras\n",
        "    segmentos_finais = []\n",
        "    for seg in segmentos:\n",
        "        if len(seg) > max_chars:\n",
        "            palavras = seg.split()\n",
        "            temp_seg = \"\"\n",
        "            for palavra in palavras:\n",
        "                if len(temp_seg + \" \" + palavra) <= max_chars:\n",
        "                    temp_seg += \" \" + palavra if temp_seg else palavra\n",
        "                else:\n",
        "                    if temp_seg:\n",
        "                        segmentos_finais.append(temp_seg.strip())\n",
        "                    temp_seg = palavra\n",
        "            if temp_seg:\n",
        "                segmentos_finais.append(temp_seg.strip())\n",
        "        else:\n",
        "            segmentos_finais.append(seg)\n",
        "\n",
        "    return segmentos_finais\n",
        "\n",
        "def segundos_para_timestamp(segundos):\n",
        "    \"\"\"Converte segundos para formato timestamp SRT (HH:MM:SS,mmm)\"\"\"\n",
        "    horas = int(segundos // 3600)\n",
        "    minutos = int((segundos % 3600) // 60)\n",
        "    segundos_restantes = segundos % 60\n",
        "    milissegundos = int((segundos_restantes - int(segundos_restantes)) * 1000)\n",
        "\n",
        "    return f\"{horas:02d}:{minutos:02d}:{int(segundos_restantes):02d},{milissegundos:03d}\"\n",
        "\n",
        "def gerar_arquivo_srt(legendas_data, srt_path):\n",
        "    \"\"\"Gera arquivo SRT\"\"\"\n",
        "    with open(srt_path, 'w', encoding='utf-8') as f:\n",
        "        for legenda in legendas_data:\n",
        "            f.write(f\"{legenda['id']}\\n\")\n",
        "            f.write(f\"{legenda['inicio']} --> {legenda['fim']}\\n\")\n",
        "            f.write(f\"{legenda['texto']}\\n\\n\")\n",
        "\n",
        "def gerar_arquivo_txt_timestamped(legendas_data, txt_path):\n",
        "    \"\"\"Gera arquivo TXT com timestamps\"\"\"\n",
        "    with open(txt_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"TRANSCRI√á√ÉO COM TIMESTAMPS\\n\")\n",
        "        f.write(\"=\"*50 + \"\\n\\n\")\n",
        "\n",
        "        for legenda in legendas_data:\n",
        "            minutos_inicio = int(legenda['inicio_segundos'] // 60)\n",
        "            segundos_inicio = int(legenda['inicio_segundos'] % 60)\n",
        "            minutos_fim = int(legenda['fim_segundos'] // 60)\n",
        "            segundos_fim = int(legenda['fim_segundos'] % 60)\n",
        "\n",
        "            f.write(f\"[{minutos_inicio:02d}:{segundos_inicio:02d}-{minutos_fim:02d}:{segundos_fim:02d}] {legenda['texto']}\\n\")\n",
        "\n",
        "def analisar_copywriting_estrategico(legendas_data, video_id):\n",
        "    \"\"\"An√°lise estrat√©gica de copywriting com base nas legendas\"\"\"\n",
        "    print(\"    üîÑ Analisando copywriting...\")\n",
        "\n",
        "    # Combinar todo o texto para an√°lise completa\n",
        "    texto_completo = \" \".join([legenda[\"texto\"] for legenda in legendas_data])\n",
        "\n",
        "    # Dicion√°rios de padr√µes de copywriting\n",
        "    ganchos_patterns = {\n",
        "        \"pergunta_retorica\": [r\"\\b(?:voc√™|tu)\\s+(?:j√°|nunca|sempre|realmente|acha|imagina|sabe|quer|precisa)\",\n",
        "                            r\"(?:como|por que|quando|onde|o que).*\\?\"],\n",
        "        \"urgencia\": [r\"\\b(?:agora|hoje|urgente|r√°pido|imediato|√∫ltima chance|s√≥ hoje|apenas|restam)\",\n",
        "                     r\"\\b(?:n√£o perca|aproveite|garante j√°|corre|√∫ltimas vagas)\"],\n",
        "        \"escassez\": [r\"\\b(?:limitado|exclusivo|poucos|restam|√∫ltima|√∫nica|especial|VIP)\",\n",
        "                     r\"\\b(?:s√≥ para|apenas para|somente|limitado a)\"],\n",
        "        \"autoridade\": [r\"\\b(?:especialista|expert|profissional|anos de experi√™ncia|comprovado|testado)\",\n",
        "                       r\"\\b(?:pesquisas mostram|estudos comprovam|cientificamente)\"],\n",
        "        \"prova_social\": [r\"\\b(?:milhares|centenas|todos|muitas pessoas|clientes|depoimentos)\",\n",
        "                         r\"\\b(?:j√° conseguiram|transformaram|mudaram|aprovaram)\"],\n",
        "        \"curiosidade\": [r\"\\b(?:segredo|descoberta|revela√ß√£o|m√©todo|t√©cnica|estrat√©gia|f√≥rmula)\",\n",
        "                        r\"\\b(?:ningu√©m te conta|poucos sabem|descobri que)\"],\n",
        "        \"problema_dor\": [r\"\\b(?:problema|dificuldade|frustra√ß√£o|sofre|dor|preocupa|bloqueia)\",\n",
        "                         r\"\\b(?:cansado de|chega de|pare de|n√£o aguenta mais)\"],\n",
        "        \"solucao_resultado\": [r\"\\b(?:solu√ß√£o|resolve|elimina|transforma|muda|resultado|sucesso)\",\n",
        "                              r\"\\b(?:conseguir|alcan√ßar|realizar|conquistar|atingir)\"]\n",
        "    }\n",
        "\n",
        "    gatilhos_patterns = {\n",
        "        \"reciprocidade\": [r\"\\b(?:gr√°tis|de gra√ßa|presente|b√¥nus|oferta|sem custo)\",\n",
        "                          r\"\\b(?:vou te dar|vou ensinar|vou mostrar|compartilhar com voc√™)\"],\n",
        "        \"comprometimento\": [r\"\\b(?:compromisso|prometo|garanto|palavra|juro)\",\n",
        "                            r\"\\b(?:pode confiar|tenho certeza|assumo|responsabilizo)\"],\n",
        "        \"aprovacao_social\": [r\"\\b(?:aprovado por|recomendado|indicado|usado por|preferido)\",\n",
        "                             r\"\\b(?:famosos|influencers|especialistas|m√©dicos|profissionais)\"],\n",
        "        \"aversao_perda\": [r\"\\b(?:perder|perdendo|vai ficar de fora|n√£o vai conseguir)\",\n",
        "                          r\"\\b(?:sair perdendo|ficar para tr√°s|oportunidade perdida)\"],\n",
        "        \"autoridade_especialista\": [r\"\\b(?:Dr|Dra|Professor|Mestre|PhD|especialista em)\",\n",
        "                                    r\"\\b(?:formado em|p√≥s-graduado|anos estudando)\"],\n",
        "        \"emocional_medo\": [r\"\\b(?:medo|receio|preocupa√ß√£o|inseguran√ßa|ansiedade)\",\n",
        "                           r\"\\b(?:n√£o conseguir|fracassar|dar errado|prejudicar)\"],\n",
        "        \"emocional_esperanca\": [r\"\\b(?:sonho|esperan√ßa|desejo|objetivo|meta|futuro melhor)\",\n",
        "                                r\"\\b(?:realizar|conquistar|alcan√ßar|transformar|mudar vida)\"]\n",
        "    }\n",
        "\n",
        "    ctas_patterns = {\n",
        "        \"acao_imediata\": [r\"\\b(?:clica|clique|acesse|baixe|fa√ßa|compre|adquira|garanta)\",\n",
        "                          r\"\\b(?:n√£o perca|aproveite|corre|vai|vem|participe)\"],\n",
        "        \"link_bio\": [r\"\\b(?:link na bio|bio|biografia|perfil|stories|direct)\",\n",
        "                     r\"\\b(?:DM|chama no WhatsApp|manda mensagem)\"],\n",
        "        \"engajamento\": [r\"\\b(?:comenta|compartilha|marca|salva|curte|like|segue)\",\n",
        "                        r\"\\b(?:conta nos coment√°rios|deixa um|comenta aqui)\"],\n",
        "        \"inscricao\": [r\"\\b(?:inscreve|se inscreva|ativa|ativar|sino|notifica√ß√£o)\",\n",
        "                      r\"\\b(?:cadastra|cadastre-se|registra|assine)\"],\n",
        "        \"contato_vendas\": [r\"\\b(?:WhatsApp|telefone|ligue|chama|fala comigo|contato)\",\n",
        "                           r\"\\b(?:agende|marque|consulta|reuni√£o|conversa)\"]\n",
        "    }\n",
        "\n",
        "    # An√°lise dos padr√µes\n",
        "    ganchos_encontrados = {}\n",
        "    gatilhos_encontrados = {}\n",
        "    ctas_encontrados = {}\n",
        "\n",
        "    # Analisar ganchos\n",
        "    for tipo, patterns in ganchos_patterns.items():\n",
        "        matches = []\n",
        "        for pattern in patterns:\n",
        "            matches.extend(re.finditer(pattern, texto_completo, re.IGNORECASE))\n",
        "        if matches:\n",
        "            ganchos_encontrados[tipo] = {\n",
        "                \"count\": len(matches),\n",
        "                \"exemplos\": [m.group() for m in matches[:3]],  # Top 3 exemplos\n",
        "                \"timestamps\": encontrar_timestamps_matches(matches, legendas_data, texto_completo)\n",
        "            }\n",
        "\n",
        "    # Analisar gatilhos\n",
        "    for tipo, patterns in gatilhos_patterns.items():\n",
        "        matches = []\n",
        "        for pattern in patterns:\n",
        "            matches.extend(re.finditer(pattern, texto_completo, re.IGNORECASE))\n",
        "        if matches:\n",
        "            gatilhos_encontrados[tipo] = {\n",
        "                \"count\": len(matches),\n",
        "                \"exemplos\": [m.group() for m in matches[:3]],\n",
        "                \"timestamps\": encontrar_timestamps_matches(matches, legendas_data, texto_completo)\n",
        "            }\n",
        "\n",
        "    # Analisar CTAs\n",
        "    for tipo, patterns in ctas_patterns.items():\n",
        "        matches = []\n",
        "        for pattern in patterns:\n",
        "            matches.extend(re.finditer(pattern, texto_completo, re.IGNORECASE))\n",
        "        if matches:\n",
        "            ctas_encontrados[tipo] = {\n",
        "                \"count\": len(matches),\n",
        "                \"exemplos\": [m.group() for m in matches[:3]],\n",
        "                \"timestamps\": encontrar_timestamps_matches(matches, legendas_data, texto_completo)\n",
        "            }\n",
        "\n",
        "    # An√°lise de estrutura narrativa\n",
        "    estrutura_narrativa = analisar_estrutura_narrativa(legendas_data)\n",
        "\n",
        "    # An√°lise de poder de persuas√£o\n",
        "    score_persuasao = calcular_score_persuasao(ganchos_encontrados, gatilhos_encontrados, ctas_encontrados)\n",
        "\n",
        "    analise_copywriting = {\n",
        "        \"video_id\": video_id,\n",
        "        \"texto_completo\": texto_completo,\n",
        "        \"total_palavras\": len(texto_completo.split()),\n",
        "        \"ganchos_detectados\": ganchos_encontrados,\n",
        "        \"gatilhos_mentais_detectados\": gatilhos_encontrados,\n",
        "        \"ctas_detectados\": ctas_encontrados,\n",
        "        \"estrutura_narrativa\": estrutura_narrativa,\n",
        "        \"score_persuasao\": score_persuasao,\n",
        "        \"recomendacoes_estrategicas\": gerar_recomendacoes_copywriting(ganchos_encontrados, gatilhos_encontrados, ctas_encontrados),\n",
        "        \"templates_identificados\": identificar_templates_replicaveis(ganchos_encontrados, gatilhos_encontrados, ctas_encontrados),\n",
        "        \"timestamp\": {\n",
        "            \"ganchos_timeline\": mapear_timeline_elementos(ganchos_encontrados, legendas_data),\n",
        "            \"gatilhos_timeline\": mapear_timeline_elementos(gatilhos_encontrados, legendas_data),\n",
        "            \"ctas_timeline\": mapear_timeline_elementos(ctas_encontrados, legendas_data)\n",
        "        },\n",
        "        \"data_analise\": datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    return analise_copywriting\n",
        "\n",
        "def encontrar_timestamps_matches(matches, legendas_data, texto_completo):\n",
        "    \"\"\"Encontra os timestamps correspondentes aos matches encontrados\"\"\"\n",
        "    timestamps = []\n",
        "\n",
        "    for match in matches[:3]:  # Limitar a 3 exemplos\n",
        "        posicao = match.start()\n",
        "        char_count = 0\n",
        "\n",
        "        for legenda in legendas_data:\n",
        "            texto_legenda = legenda[\"texto\"]\n",
        "            if char_count <= posicao < char_count + len(texto_legenda):\n",
        "                timestamps.append({\n",
        "                    \"minuto\": int(legenda[\"inicio_segundos\"] // 60),\n",
        "                    \"segundo\": int(legenda[\"inicio_segundos\"] % 60),\n",
        "                    \"texto_contexto\": texto_legenda\n",
        "                })\n",
        "                break\n",
        "            char_count += len(texto_legenda) + 1  # +1 para o espa√ßo entre legendas\n",
        "\n",
        "    return timestamps\n",
        "\n",
        "def analisar_estrutura_narrativa(legendas_data):\n",
        "    \"\"\"Analisa a estrutura narrativa do v√≠deo\"\"\"\n",
        "    total_segmentos = len(legendas_data)\n",
        "\n",
        "    if total_segmentos < 3:\n",
        "        return {\n",
        "            \"abertura\": {\"segmentos\": total_segmentos, \"elementos\": []},\n",
        "            \"desenvolvimento\": {\"segmentos\": 0, \"elementos\": []},\n",
        "            \"fechamento\": {\"segmentos\": 0, \"elementos\": []}\n",
        "        }\n",
        "\n",
        "    # Dividir em ter√ßos para an√°lise\n",
        "    primeiro_terco = legendas_data[:total_segmentos//3]\n",
        "    segundo_terco = legendas_data[total_segmentos//3:2*total_segmentos//3]\n",
        "    ultimo_terco = legendas_data[2*total_segmentos//3:]\n",
        "\n",
        "    estrutura = {\n",
        "        \"abertura\": {\n",
        "            \"segmentos\": len(primeiro_terco),\n",
        "            \"elementos\": analisar_elementos_abertura(primeiro_terco)\n",
        "        },\n",
        "        \"desenvolvimento\": {\n",
        "            \"segmentos\": len(segundo_terco),\n",
        "            \"elementos\": analisar_elementos_desenvolvimento(segundo_terco)\n",
        "        },\n",
        "        \"fechamento\": {\n",
        "            \"segmentos\": len(ultimo_terco),\n",
        "            \"elementos\": analisar_elementos_fechamento(ultimo_terco)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return estrutura\n",
        "\n",
        "def analisar_elementos_abertura(segmentos):\n",
        "    \"\"\"Analisa elementos da abertura\"\"\"\n",
        "    texto = \" \".join([s[\"texto\"] for s in segmentos])\n",
        "    elementos = []\n",
        "\n",
        "    if re.search(r'\\b(?:voc√™|tu)\\s+(?:j√°|nunca|sempre)', texto, re.IGNORECASE):\n",
        "        elementos.append(\"pergunta_engajamento\")\n",
        "    if re.search(r'\\b(?:vou te|vou mostrar|vou ensinar)', texto, re.IGNORECASE):\n",
        "        elementos.append(\"promessa_valor\")\n",
        "    if re.search(r'\\b(?:segredo|descoberta|m√©todo)', texto, re.IGNORECASE):\n",
        "        elementos.append(\"curiosidade\")\n",
        "\n",
        "    return elementos\n",
        "\n",
        "def analisar_elementos_desenvolvimento(segmentos):\n",
        "    \"\"\"Analisa elementos do desenvolvimento\"\"\"\n",
        "    texto = \" \".join([s[\"texto\"] for s in segmentos])\n",
        "    elementos = []\n",
        "\n",
        "    if re.search(r'\\b(?:porque|pois|isso acontece)', texto, re.IGNORECASE):\n",
        "        elementos.append(\"explicacao\")\n",
        "    if re.search(r'\\b(?:exemplo|caso|situa√ß√£o)', texto, re.IGNORECASE):\n",
        "        elementos.append(\"exemplificacao\")\n",
        "    if re.search(r'\\b(?:resultado|consegui|transformou)', texto, re.IGNORECASE):\n",
        "        elementos.append(\"prova_resultado\")\n",
        "\n",
        "    return elementos\n",
        "\n",
        "def analisar_elementos_fechamento(segmentos):\n",
        "    \"\"\"Analisa elementos do fechamento\"\"\"\n",
        "    texto = \" \".join([s[\"texto\"] for s in segmentos])\n",
        "    elementos = []\n",
        "\n",
        "    if re.search(r'\\b(?:clica|clique|acesse|fa√ßa)', texto, re.IGNORECASE):\n",
        "        elementos.append(\"call_to_action\")\n",
        "    if re.search(r'\\b(?:link|bio|WhatsApp)', texto, re.IGNORECASE):\n",
        "        elementos.append(\"direcionamento\")\n",
        "    if re.search(r'\\b(?:comenta|compartilha|segue)', texto, re.IGNORECASE):\n",
        "        elementos.append(\"engajamento\")\n",
        "\n",
        "    return elementos\n",
        "\n",
        "def calcular_score_persuasao(ganchos, gatilhos, ctas):\n",
        "    \"\"\"Calcula score de persuas√£o baseado nos elementos encontrados\"\"\"\n",
        "    score = 0\n",
        "\n",
        "    # Pontua√ß√£o por variedade de ganchos\n",
        "    score += len(ganchos) * 10\n",
        "\n",
        "    # Pontua√ß√£o por variedade de gatilhos\n",
        "    score += len(gatilhos) * 15\n",
        "\n",
        "    # Pontua√ß√£o por presen√ßa de CTAs\n",
        "    score += len(ctas) * 20\n",
        "\n",
        "    # B√¥nus por combina√ß√µes poderosas\n",
        "    if \"urgencia\" in ganchos and \"aversao_perda\" in gatilhos:\n",
        "        score += 25\n",
        "\n",
        "    if \"autoridade\" in ganchos and \"autoridade_especialista\" in gatilhos:\n",
        "        score += 20\n",
        "\n",
        "    if \"curiosidade\" in ganchos and any(cta in ctas for cta in [\"acao_imediata\", \"link_bio\"]):\n",
        "        score += 30\n",
        "\n",
        "    return min(score, 100)  # Limitar a 100\n",
        "\n",
        "def gerar_recomendacoes_copywriting(ganchos, gatilhos, ctas):\n",
        "    \"\"\"Gera recomenda√ß√µes estrat√©gicas baseadas na an√°lise\"\"\"\n",
        "    recomendacoes = []\n",
        "\n",
        "    # Recomenda√ß√µes para ganchos\n",
        "    if len(ganchos) < 2:\n",
        "        recomendacoes.append({\n",
        "            \"categoria\": \"GANCHOS\",\n",
        "            \"prioridade\": \"ALTA\",\n",
        "            \"recomendacao\": \"Adicione mais ganchos na abertura. Use perguntas ret√≥ricas ou desperte curiosidade nos primeiros 3 segundos.\"\n",
        "        })\n",
        "\n",
        "    if \"pergunta_retorica\" not in ganchos:\n",
        "        recomendacoes.append({\n",
        "            \"categoria\": \"GANCHOS\",\n",
        "            \"prioridade\": \"M√âDIA\",\n",
        "            \"recomendacao\": \"Inicie com uma pergunta que fa√ßa o viewer refletir sobre sua situa√ß√£o atual.\"\n",
        "        })\n",
        "\n",
        "    # Recomenda√ß√µes para gatilhos\n",
        "    if len(gatilhos) < 3:\n",
        "        recomendacoes.append({\n",
        "            \"categoria\": \"GATILHOS\",\n",
        "            \"prioridade\": \"ALTA\",\n",
        "            \"recomendacao\": \"Incorpore mais gatilhos mentais. Combine autoridade + prova social para maior credibilidade.\"\n",
        "        })\n",
        "\n",
        "    if \"reciprocidade\" not in gatilhos:\n",
        "        recomendacoes.append({\n",
        "            \"categoria\": \"GATILHOS\",\n",
        "            \"prioridade\": \"M√âDIA\",\n",
        "            \"recomendacao\": \"Ofere√ßa valor gratuito para ativar o gatilho da reciprocidade.\"\n",
        "        })\n",
        "\n",
        "    # Recomenda√ß√µes para CTAs\n",
        "    if len(ctas) == 0:\n",
        "        recomendacoes.append({\n",
        "            \"categoria\": \"CTA\",\n",
        "            \"prioridade\": \"CR√çTICA\",\n",
        "            \"recomendacao\": \"URGENTE: Adicione pelo menos um Call-to-Action claro. Sem CTA, n√£o h√° convers√£o.\"\n",
        "        })\n",
        "\n",
        "    if \"acao_imediata\" not in ctas and \"link_bio\" not in ctas:\n",
        "        recomendacoes.append({\n",
        "            \"categoria\": \"CTA\",\n",
        "            \"prioridade\": \"ALTA\",\n",
        "            \"recomendacao\": \"Termine com um CTA direto: 'Clica no link da bio' ou 'Chama no WhatsApp'.\"\n",
        "        })\n",
        "\n",
        "    return recomendacoes\n",
        "\n",
        "def identificar_templates_replicaveis(ganchos, gatilhos, ctas):\n",
        "    \"\"\"Identifica templates e estruturas replic√°veis\"\"\"\n",
        "    templates = []\n",
        "\n",
        "    # Template: Pergunta + Autoridade + CTA\n",
        "    if \"pergunta_retorica\" in ganchos and \"autoridade\" in ganchos and len(ctas) > 0:\n",
        "        templates.append({\n",
        "            \"nome\": \"PERGUNTA_AUTORIDADE_CTA\",\n",
        "            \"estrutura\": \"Pergunta Ret√≥rica ‚Üí Estabelecer Autoridade ‚Üí Call-to-Action\",\n",
        "            \"eficacia\": \"ALTA\",\n",
        "            \"uso_recomendado\": \"V√≠deos educativos e de expertise\"\n",
        "        })\n",
        "\n",
        "    # Template: Problema + Solu√ß√£o + Prova Social\n",
        "    if \"problema_dor\" in ganchos and \"solucao_resultado\" in ganchos and \"aprovacao_social\" in gatilhos:\n",
        "        templates.append({\n",
        "            \"nome\": \"PROBLEMA_SOLUCAO_PROVA\",\n",
        "            \"estrutura\": \"Identificar Problema ‚Üí Apresentar Solu√ß√£o ‚Üí Mostrar Prova Social\",\n",
        "            \"eficacia\": \"MUITO ALTA\",\n",
        "            \"uso_recomendado\": \"V√≠deos de vendas e transforma√ß√£o\"\n",
        "        })\n",
        "\n",
        "    # Template: Curiosidade + Urg√™ncia + CTA\n",
        "    if \"curiosidade\" in ganchos and \"urgencia\" in ganchos and \"acao_imediata\" in ctas:\n",
        "        templates.append({\n",
        "            \"nome\": \"CURIOSIDADE_URGENCIA_ACAO\",\n",
        "            \"estrutura\": \"Despertar Curiosidade ‚Üí Criar Urg√™ncia ‚Üí A√ß√£o Imediata\",\n",
        "            \"eficacia\": \"ALTA\",\n",
        "            \"uso_recomendado\": \"V√≠deos de lan√ßamento e ofertas limitadas\"\n",
        "        })\n",
        "\n",
        "    return templates\n",
        "\n",
        "def mapear_timeline_elementos(elementos_detectados, legendas_data):\n",
        "    \"\"\"Mapeia os elementos detectados na timeline do v√≠deo\"\"\"\n",
        "    timeline = []\n",
        "\n",
        "    for tipo, dados in elementos_detectados.items():\n",
        "        for timestamp in dados.get(\"timestamps\", []):\n",
        "            timeline.append({\n",
        "                \"tipo\": tipo,\n",
        "                \"minuto\": timestamp[\"minuto\"],\n",
        "                \"segundo\": timestamp[\"segundo\"],\n",
        "                \"contexto\": timestamp[\"texto_contexto\"]\n",
        "            })\n",
        "\n",
        "    # Ordenar por tempo\n",
        "    timeline.sort(key=lambda x: (x[\"minuto\"], x[\"segundo\"]))\n",
        "\n",
        "    return timeline\n",
        "\n",
        "# Executar o processamento\n",
        "try:\n",
        "    processar_copywriting_todos_videos_adaptado()\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERRO de Execu√ß√£o: {type(e).__name__}: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "id": "p-wM_X9W5nZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 2.4: GERA√á√ÉO DE LEGENDAS E AN√ÅLISE DE COPYWRITING\n",
        "# ============================================================================\n",
        "\n",
        "# Definir a vari√°vel global PASTA_TRABALHO se ainda n√£o estiver definida\n",
        "# Certifique-se de que esta vari√°vel esteja definida corretamente em uma c√©lula anterior (ex: C√©lula 1.2)\n",
        "# Exemplo: PASTA_TRABALHO = \"/content/drive/MyDrive/Videos Dona Done/_engenharia_reversa\"\n",
        "\n",
        "# Executar a fun√ß√£o principal da Layer 2.4\n",
        "if 'PASTA_TRABALHO' in globals():\n",
        "    print(\"Iniciando a Layer 2.4: Gera√ß√£o de Legendas e An√°lise de Copywriting...\")\n",
        "    processar_copywriting_todos_videos_adaptado()\n",
        "else:\n",
        "    print(\"ERRO: A vari√°vel PASTA_TRABALHO n√£o est√° definida. Certifique-se de executar a C√©lula 1.2 ou equivalente.\")"
      ],
      "metadata": {
        "id": "UfcEs533582H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 4.3: DASHBOARD MASTER EXECUTIVO INTELIGENTE APRIMORADO\n",
        "# ============================================================================\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "from openpyxl.styles import Font, Alignment, PatternFill\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def log_progress(message):\n",
        "    \"\"\"Log de progresso em tempo real\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
        "    print(f\"[{timestamp}] {message}\")\n",
        "\n",
        "def calculate_viral_score(row):\n",
        "    \"\"\"Calcula score de viralidade baseado em m√∫ltiplos fatores\"\"\"\n",
        "    try:\n",
        "        score = 0\n",
        "\n",
        "        # Fator 1: Ritmo (cortes por segundo) - peso 25%\n",
        "        if pd.notna(row['duracao_segundos']) and row['duracao_segundos'] > 0:\n",
        "            cortes_por_seg = row['cortes_detectados_count'] / row['duracao_segundos']\n",
        "            if cortes_por_seg > 20: score += 25\n",
        "            elif cortes_por_seg > 10: score += 20\n",
        "            elif cortes_por_seg > 5: score += 15\n",
        "            else: score += 10\n",
        "\n",
        "        # Fator 2: Complexidade Visual - peso 20%\n",
        "        if pd.notna(row['complexidade_visual_media']):\n",
        "            if row['complexidade_visual_media'] > 600: score += 20\n",
        "            elif row['complexidade_visual_media'] > 400: score += 15\n",
        "            else: score += 10\n",
        "\n",
        "        # Fator 3: Presen√ßa de Texto (OCR) - peso 15%\n",
        "        if pd.notna(row['ocr_textos_count']):\n",
        "            if row['ocr_textos_count'] > 10: score += 15\n",
        "            elif row['ocr_textos_count'] > 5: score += 12\n",
        "            elif row['ocr_textos_count'] > 0: score += 8\n",
        "\n",
        "        # Fator 4: Dura√ß√£o Ideal - peso 20%\n",
        "        if pd.notna(row['duracao_segundos']):\n",
        "            if 15 <= row['duracao_segundos'] <= 30: score += 20\n",
        "            elif 10 <= row['duracao_segundos'] <= 45: score += 15\n",
        "            else: score += 10\n",
        "\n",
        "        # Fator 5: Gatilhos Psicol√≥gicos - peso 20%\n",
        "        gatilhos = str(row['gatilhos_psicologicos']).lower()\n",
        "        if 'urg√™ncia' in gatilhos or 'escassez' in gatilhos: score += 8\n",
        "        if 'est√≠mulo' in gatilhos: score += 7\n",
        "        if 'aten√ß√£o' in gatilhos: score += 5\n",
        "\n",
        "        return min(score, 100)\n",
        "    except:\n",
        "        return 50\n",
        "\n",
        "def calculate_technical_score(row):\n",
        "    \"\"\"Score t√©cnico baseado em qualidade de produ√ß√£o\"\"\"\n",
        "    try:\n",
        "        score = 0\n",
        "\n",
        "        if pd.notna(row['brilho_medio']):\n",
        "            if 120 <= row['brilho_medio'] <= 180: score += 25\n",
        "            elif 100 <= row['brilho_medio'] <= 200: score += 20\n",
        "            else: score += 10\n",
        "\n",
        "        formato = str(row['formato_detectado'])\n",
        "        if 'vertical_9_16' in formato: score += 25\n",
        "        elif 'horizontal_16_9' in formato: score += 20\n",
        "        else: score += 15\n",
        "\n",
        "        if row['tem_audio']: score += 25\n",
        "        else: score += 5\n",
        "\n",
        "        if pd.notna(row['total_frames']) and row['total_frames'] > 0:\n",
        "            if row['total_frames'] > 300: score += 25\n",
        "            elif row['total_frames'] > 150: score += 20\n",
        "            else: score += 15\n",
        "\n",
        "        return min(score, 100)\n",
        "    except:\n",
        "        return 50\n",
        "\n",
        "def calculate_content_score(row):\n",
        "    \"\"\"Score de conte√∫do baseado em riqueza informacional\"\"\"\n",
        "    try:\n",
        "        score = 0\n",
        "\n",
        "        ocr_count = row['ocr_textos_count'] if pd.notna(row['ocr_textos_count']) else 0\n",
        "        audio_len = row['audio_transcrito_len'] if pd.notna(row['audio_transcrito_len']) else 0\n",
        "\n",
        "        if ocr_count > 5 or audio_len > 100: score += 30\n",
        "        elif ocr_count > 2 or audio_len > 50: score += 20\n",
        "        elif ocr_count > 0 or audio_len > 0: score += 15\n",
        "        else: score += 5\n",
        "\n",
        "        if pd.notna(row['bpm_audio']):\n",
        "            if 120 <= row['bpm_audio'] <= 140: score += 35\n",
        "            elif 100 <= row['bpm_audio'] <= 160: score += 25\n",
        "            else: score += 15\n",
        "\n",
        "        if pd.notna(row['duracao_segundos']) and row['duracao_segundos'] > 0:\n",
        "            densidade = (ocr_count + audio_len/10) / row['duracao_segundos']\n",
        "            if densidade > 2: score += 35\n",
        "            elif densidade > 1: score += 25\n",
        "            else: score += 15\n",
        "\n",
        "        return min(score, 100)\n",
        "    except:\n",
        "        return 50\n",
        "\n",
        "def generate_insights_from_data(df):\n",
        "    \"\"\"Gera insights inteligentes baseados nos dados\"\"\"\n",
        "    insights = []\n",
        "\n",
        "    try:\n",
        "        best_performing = df.nlargest(3, 'viral_score')\n",
        "        avg_duration = best_performing['duracao_segundos'].mean()\n",
        "        insights.append(f\"DURA√á√ÉO VENCEDORA: Seus top 3 v√≠deos t√™m dura√ß√£o m√©dia de {avg_duration:.1f}s. Este √© seu sweet spot comprovado.\")\n",
        "\n",
        "        avg_cuts_per_sec = (best_performing['cortes_detectados_count'] / best_performing['duracao_segundos']).mean()\n",
        "        insights.append(f\"RITMO IDEAL: {avg_cuts_per_sec:.1f} cortes por segundo √© sua f√≥rmula de edi√ß√£o mais eficaz.\")\n",
        "\n",
        "        formato_winner = df['formato_detectado'].mode()[0] if not df['formato_detectado'].empty else 'N/A'\n",
        "        formato_count = df['formato_detectado'].value_counts().iloc[0] if not df['formato_detectado'].empty else 0\n",
        "        insights.append(f\"FORMATO DOMINANTE: {formato_count} v√≠deos em {formato_winner}. Este √© seu formato de maior alcance.\")\n",
        "\n",
        "        high_viral = df[df['viral_score'] > 70]\n",
        "        if not high_viral.empty:\n",
        "            avg_complexity = high_viral['complexidade_visual_media'].mean()\n",
        "            insights.append(f\"COMPLEXIDADE VISUAL √ìTIMA: V√≠deos com score viral alto t√™m complexidade m√©dia de {avg_complexity:.0f}. Use como refer√™ncia.\")\n",
        "\n",
        "        text_heavy = df[df['ocr_textos_count'] > 5]\n",
        "        if not text_heavy.empty:\n",
        "            insights.append(f\"ESTRAT√âGIA DE TEXTO: {len(text_heavy)} v√≠deos com muito texto t√™m score m√©dio de {text_heavy['viral_score'].mean():.0f}. Texto na tela impacta performance.\")\n",
        "\n",
        "        # CORRIGIDO: bpm_audio em vez de bmp_audio\n",
        "        if df['bpm_audio'].notna().any():\n",
        "            successful_bpm = df[df['viral_score'] > 60]['bpm_audio'].mean()\n",
        "            insights.append(f\"BPM DE SUCESSO: {successful_bpm:.0f} BPM √© o ritmo de √°udio dos seus v√≠deos mais virais.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_progress(f\"Erro ao gerar insights: {e}\")\n",
        "        insights.append(\"Insights parciais dispon√≠veis devido a limita√ß√µes nos dados.\")\n",
        "\n",
        "    return insights\n",
        "\n",
        "def add_data_to_sheet(ws, data, start_row=1, start_col=1, headers=None):\n",
        "    \"\"\"Adiciona dados a uma planilha de forma segura\"\"\"\n",
        "    current_row = start_row\n",
        "\n",
        "    # Adicionar cabe√ßalhos se fornecidos\n",
        "    if headers:\n",
        "        for col_idx, header in enumerate(headers):\n",
        "            cell = ws.cell(row=current_row, column=start_col + col_idx)\n",
        "            cell.value = header\n",
        "            cell.font = Font(bold=True)\n",
        "        current_row += 1\n",
        "\n",
        "    # Adicionar dados\n",
        "    for row_data in data:\n",
        "        for col_idx, value in enumerate(row_data):\n",
        "            cell = ws.cell(row=current_row, column=start_col + col_idx)\n",
        "            cell.value = value\n",
        "        current_row += 1\n",
        "\n",
        "    return current_row\n",
        "\n",
        "def create_enhanced_dashboard_master(csv_path, json_path, output_path):\n",
        "    \"\"\"Cria dashboard master executivo aprimorado\"\"\"\n",
        "\n",
        "    log_progress(\"INICIANDO CRIA√á√ÉO DO DASHBOARD MASTER EXECUTIVO INTELIGENTE\")\n",
        "\n",
        "    try:\n",
        "        # Carregar dados\n",
        "        log_progress(\"Carregando dados consolidados...\")\n",
        "        df_consolidado = pd.read_csv(csv_path, encoding='utf-8')\n",
        "\n",
        "        with open(json_path, 'r', encoding='utf-8') as f:\n",
        "            dados_detalhados = json.load(f)\n",
        "\n",
        "        log_progress(f\"Dados carregados: {len(df_consolidado)} v√≠deos encontrados\")\n",
        "\n",
        "        # Pr√©-processamento inteligente\n",
        "        log_progress(\"Processando intelig√™ncia artificial dos dados...\")\n",
        "\n",
        "        # Limpar e converter dados\n",
        "        try:\n",
        "            df_consolidado['emocoes_predominantes'] = df_consolidado['emocoes_predominantes'].apply(\n",
        "                lambda x: json.loads(x.replace(\"'\", '\"')) if pd.notna(x) and x != '{}' else {}\n",
        "            )\n",
        "        except:\n",
        "            df_consolidado['emocoes_predominantes'] = [{}] * len(df_consolidado)\n",
        "\n",
        "        # Calcular scores inteligentes\n",
        "        log_progress(\"Calculando scores de performance...\")\n",
        "        df_consolidado['viral_score'] = df_consolidado.apply(calculate_viral_score, axis=1)\n",
        "        df_consolidado['technical_score'] = df_consolidado.apply(calculate_technical_score, axis=1)\n",
        "        df_consolidado['content_score'] = df_consolidado.apply(calculate_content_score, axis=1)\n",
        "        df_consolidado['overall_score'] = (df_consolidado['viral_score'] + df_consolidado['technical_score'] + df_consolidado['content_score']) / 3\n",
        "\n",
        "        # Calcular m√©tricas avan√ßadas\n",
        "        df_consolidado['cortes_por_segundo'] = df_consolidado['cortes_detectados_count'] / df_consolidado['duracao_segundos'].replace(0, 1)\n",
        "        df_consolidado['densidade_texto'] = df_consolidado['ocr_textos_count'] / df_consolidado['duracao_segundos'].replace(0, 1)\n",
        "        df_consolidado['eficiencia_audio'] = df_consolidado['audio_transcrito_len'] / df_consolidado['duracao_segundos'].replace(0, 1)\n",
        "\n",
        "        log_progress(\"Gerando insights estrat√©gicos...\")\n",
        "        insights = generate_insights_from_data(df_consolidado)\n",
        "\n",
        "        # Criar workbook\n",
        "        log_progress(\"Criando estrutura do dashboard...\")\n",
        "        wb = Workbook()\n",
        "\n",
        "        # === ABA 1: EXECUTIVE SUMMARY ===\n",
        "        log_progress(\"Criando Executive Summary...\")\n",
        "        ws_summary = wb.active\n",
        "        ws_summary.title = 'Executive Summary'\n",
        "\n",
        "        # Header principal\n",
        "        header_cell = ws_summary.cell(row=1, column=1)\n",
        "        header_cell.value = 'DASHBOARD MASTER EXECUTIVO - ENGENHARIA REVERSA DE V√çDEOS'\n",
        "        header_cell.font = Font(bold=True, size=18, color='FFFFFF')\n",
        "        header_cell.fill = PatternFill(start_color='1F4E79', end_color='1F4E79', fill_type='solid')\n",
        "        header_cell.alignment = Alignment(horizontal='center', vertical='center')\n",
        "\n",
        "        # Expandir header manualmente\n",
        "        for col in range(2, 9):\n",
        "            cell = ws_summary.cell(row=1, column=col)\n",
        "            cell.fill = PatternFill(start_color='1F4E79', end_color='1F4E79', fill_type='solid')\n",
        "\n",
        "        # KPIs Principais\n",
        "        kpi_cell = ws_summary.cell(row=3, column=1)\n",
        "        kpi_cell.value = 'INDICADORES DE PERFORMANCE PRINCIPAIS'\n",
        "        kpi_cell.font = Font(bold=True, size=14)\n",
        "        kpi_cell.fill = PatternFill(start_color='E7E6E6', end_color='E7E6E6', fill_type='solid')\n",
        "\n",
        "        kpis_data = [\n",
        "            ['Total de V√≠deos Analisados', len(df_consolidado)],\n",
        "            ['Score Viral M√©dio', f\"{df_consolidado['viral_score'].mean():.1f}/100\"],\n",
        "            ['Score T√©cnico M√©dio', f\"{df_consolidado['technical_score'].mean():.1f}/100\"],\n",
        "            ['Score de Conte√∫do M√©dio', f\"{df_consolidado['content_score'].mean():.1f}/100\"],\n",
        "            ['Dura√ß√£o M√©dia Otimizada', f\"{df_consolidado['duracao_segundos'].mean():.1f}s\"],\n",
        "            ['Ritmo M√©dio de Cortes', f\"{df_consolidado['cortes_por_segundo'].mean():.1f}/seg\"],\n",
        "        ]\n",
        "\n",
        "        add_data_to_sheet(ws_summary, kpis_data, start_row=4, start_col=1)\n",
        "\n",
        "        # Top 3 V√≠deos\n",
        "        top3_cell = ws_summary.cell(row=3, column=4)\n",
        "        top3_cell.value = 'TOP 3 V√çDEOS POR PERFORMANCE'\n",
        "        top3_cell.font = Font(bold=True, size=14)\n",
        "        top3_cell.fill = PatternFill(start_color='E7E6E6', end_color='E7E6E6', fill_type='solid')\n",
        "\n",
        "        top3 = df_consolidado.nlargest(3, 'overall_score')[['nome_arquivo', 'overall_score', 'viral_score', 'technical_score', 'content_score']]\n",
        "\n",
        "        top3_data = []\n",
        "        for _, video in top3.iterrows():\n",
        "            nome_curto = video['nome_arquivo'][:30] + \"...\" if len(video['nome_arquivo']) > 30 else video['nome_arquivo']\n",
        "            top3_data.append([\n",
        "                nome_curto,\n",
        "                f\"{video['overall_score']:.1f}\",\n",
        "                f\"{video['viral_score']:.1f}\",\n",
        "                f\"{video['technical_score']:.1f}\",\n",
        "                f\"{video['content_score']:.1f}\"\n",
        "            ])\n",
        "\n",
        "        top3_headers = ['V√≠deo', 'Score Geral', 'Viral', 'T√©cnico', 'Conte√∫do']\n",
        "        add_data_to_sheet(ws_summary, top3_data, start_row=4, start_col=4, headers=top3_headers)\n",
        "\n",
        "        # Insights Estrat√©gicos\n",
        "        insights_cell = ws_summary.cell(row=12, column=1)\n",
        "        insights_cell.value = 'INSIGHTS ESTRAT√âGICOS BASEADOS EM IA'\n",
        "        insights_cell.font = Font(bold=True, size=14, color='FFFFFF')\n",
        "        insights_cell.fill = PatternFill(start_color='C5504B', end_color='C5504B', fill_type='solid')\n",
        "        insights_cell.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Adicionar insights\n",
        "        for i, insight in enumerate(insights, 13):\n",
        "            insight_cell = ws_summary.cell(row=i, column=1)\n",
        "            insight_cell.value = f\"‚Ä¢ {insight}\"\n",
        "            insight_cell.alignment = Alignment(wrap_text=True)\n",
        "\n",
        "        # === ABA 2: AN√ÅLISE DE PERFORMANCE ===\n",
        "        log_progress(\"Criando An√°lise de Performance...\")\n",
        "        ws_performance = wb.create_sheet('An√°lise de Performance')\n",
        "\n",
        "        perf_header = ws_performance.cell(row=1, column=1)\n",
        "        perf_header.value = 'AN√ÅLISE DETALHADA DE PERFORMANCE'\n",
        "        perf_header.font = Font(bold=True, size=16)\n",
        "        perf_header.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Ranking completo\n",
        "        ranking_data = df_consolidado[['nome_arquivo', 'overall_score', 'viral_score', 'technical_score', 'content_score',\n",
        "                                     'duracao_segundos', 'cortes_por_segundo', 'formato_detectado']].sort_values('overall_score', ascending=False)\n",
        "\n",
        "        ranking_list = []\n",
        "        for _, video in ranking_data.iterrows():\n",
        "            nome_curto = video['nome_arquivo'][:40] + \"...\" if len(video['nome_arquivo']) > 40 else video['nome_arquivo']\n",
        "            ranking_list.append([\n",
        "                nome_curto,\n",
        "                f\"{video['overall_score']:.1f}\",\n",
        "                f\"{video['viral_score']:.1f}\",\n",
        "                f\"{video['technical_score']:.1f}\",\n",
        "                f\"{video['content_score']:.1f}\",\n",
        "                f\"{video['duracao_segundos']:.1f}s\",\n",
        "                f\"{video['cortes_por_segundo']:.1f}\",\n",
        "                video['formato_detectado']\n",
        "            ])\n",
        "\n",
        "        ranking_headers = ['V√≠deo', 'Score Geral', 'Viral', 'T√©cnico', 'Conte√∫do', 'Dura√ß√£o', 'Cortes/s', 'Formato']\n",
        "        add_data_to_sheet(ws_performance, ranking_list, start_row=3, start_col=1, headers=ranking_headers)\n",
        "\n",
        "        # === ABA 3: INTELIG√äNCIA T√âCNICA ===\n",
        "        log_progress(\"Criando Intelig√™ncia T√©cnica...\")\n",
        "        ws_tecnica = wb.create_sheet('Intelig√™ncia T√©cnica')\n",
        "\n",
        "        tec_header = ws_tecnica.cell(row=1, column=1)\n",
        "        tec_header.value = 'AN√ÅLISE T√âCNICA AVAN√áADA'\n",
        "        tec_header.font = Font(bold=True, size=16)\n",
        "        tec_header.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # An√°lise de correla√ß√µes\n",
        "        corr_header = ws_tecnica.cell(row=3, column=1)\n",
        "        corr_header.value = 'CORRELA√á√ïES DESCOBERTAS'\n",
        "        corr_header.font = Font(bold=True, size=12)\n",
        "\n",
        "        correlations_data = [\n",
        "            ['Dura√ß√£o vs Score Viral', f\"{df_consolidado['duracao_segundos'].corr(df_consolidado['viral_score']):.3f}\", 'CORRELA√á√ÉO MODERADA'],\n",
        "            ['Cortes/s vs Score Viral', f\"{df_consolidado['cortes_por_segundo'].corr(df_consolidado['viral_score']):.3f}\", 'CORRELA√á√ÉO MODERADA'],\n",
        "            ['Complexidade Visual vs Performance', f\"{df_consolidado['complexidade_visual_media'].corr(df_consolidado['overall_score']):.3f}\", 'CORRELA√á√ÉO FRACA'],\n",
        "            ['BPM vs Engajamento', f\"{df_consolidado['bpm_audio'].corr(df_consolidado['viral_score']) if df_consolidado['bpm_audio'].notna().any() else 0:.3f}\", 'CORRELA√á√ÉO FRACA'],\n",
        "        ]\n",
        "\n",
        "        corr_headers = ['M√©trica', 'Correla√ß√£o', 'Classifica√ß√£o']\n",
        "        add_data_to_sheet(ws_tecnica, correlations_data, start_row=4, start_col=1, headers=corr_headers)\n",
        "\n",
        "        # === ABA 4: BLUEPRINT DE PRODU√á√ÉO ===\n",
        "        log_progress(\"Criando Blueprint de Produ√ß√£o...\")\n",
        "        ws_blueprint = wb.create_sheet('Blueprint de Produ√ß√£o')\n",
        "\n",
        "        bp_header = ws_blueprint.cell(row=1, column=1)\n",
        "        bp_header.value = 'BLUEPRINT ESTRAT√âGICO DE PRODU√á√ÉO'\n",
        "        bp_header.font = Font(bold=True, size=16, color='FFFFFF')\n",
        "        bp_header.fill = PatternFill(start_color='1F4E79', end_color='1F4E79', fill_type='solid')\n",
        "        bp_header.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Receita de sucesso baseada nos top performers\n",
        "        top_performers = df_consolidado[df_consolidado['overall_score'] > df_consolidado['overall_score'].quantile(0.7)]\n",
        "\n",
        "        blueprint_data = [\n",
        "            ['DURA√á√ÉO IDEAL', f\"{top_performers['duracao_segundos'].mean():.1f} segundos (¬±{top_performers['duracao_segundos'].std():.1f}s)\"],\n",
        "            ['RITMO DE EDI√á√ÉO', f\"{top_performers['cortes_por_segundo'].mean():.1f} cortes por segundo\"],\n",
        "            ['FORMATO VENCEDOR', top_performers['formato_detectado'].mode()[0] if not top_performers.empty else 'N/A'],\n",
        "            ['COMPLEXIDADE VISUAL', f\"N√≠vel {top_performers['complexidade_visual_media'].mean():.0f} (escala de est√≠mulo)\"],\n",
        "            ['BPM RECOMENDADO', f\"{top_performers['bpm_audio'].mean():.0f} BPM\" if top_performers['bpm_audio'].notna().any() else 'N/A'],\n",
        "            ['DENSIDADE DE TEXTO', f\"{top_performers['densidade_texto'].mean():.1f} textos por segundo\"],\n",
        "        ]\n",
        "\n",
        "        bp_sub_header = ws_blueprint.cell(row=3, column=1)\n",
        "        bp_sub_header.value = 'F√ìRMULA DE SUCESSO BASEADA EM DADOS'\n",
        "        bp_sub_header.font = Font(bold=True, size=12)\n",
        "\n",
        "        add_data_to_sheet(ws_blueprint, blueprint_data, start_row=4, start_col=1)\n",
        "\n",
        "        # === ABA 5: RECOMENDA√á√ïES ESTRAT√âGICAS ===\n",
        "        log_progress(\"Criando Recomenda√ß√µes Estrat√©gicas...\")\n",
        "        ws_recomendacoes = wb.create_sheet('Recomenda√ß√µes Estrat√©gicas')\n",
        "\n",
        "        rec_header = ws_recomendacoes.cell(row=1, column=1)\n",
        "        rec_header.value = 'RECOMENDA√á√ïES ESTRAT√âGICAS BASEADAS EM IA'\n",
        "        rec_header.font = Font(bold=True, size=16, color='FFFFFF')\n",
        "        rec_header.fill = PatternFill(start_color='C5504B', end_color='C5504B', fill_type='solid')\n",
        "        rec_header.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Recomenda√ß√µes inteligentes baseadas nos dados\n",
        "        recommendations = []\n",
        "\n",
        "        # An√°lise de dura√ß√£o\n",
        "        if df_consolidado['duracao_segundos'].mean() > 60:\n",
        "            recommendations.append(['DURA√á√ÉO', 'REDUZA DURA√á√ÉO', 'Seus v√≠deos est√£o longos demais. V√≠deos de 15-30s t√™m melhor performance.', 'ALTA'])\n",
        "        elif df_consolidado['duracao_segundos'].mean() < 15:\n",
        "            recommendations.append(['DURA√á√ÉO', 'AUMENTE DURA√á√ÉO', 'V√≠deos muito curtos podem n√£o transmitir valor suficiente.', 'M√âDIA'])\n",
        "\n",
        "        # An√°lise de ritmo\n",
        "        avg_cuts_per_sec = df_consolidado['cortes_por_segundo'].mean()\n",
        "        if avg_cuts_per_sec < 5:\n",
        "            recommendations.append(['EDI√á√ÉO', 'ACELERE O RITMO', 'Aumente o n√∫mero de cortes para manter aten√ß√£o. Meta: 8-12 cortes/segundo.', 'ALTA'])\n",
        "        elif avg_cuts_per_sec > 20:\n",
        "            recommendations.append(['EDI√á√ÉO', 'DIMINUA CORTES', 'Muitos cortes podem causar fadiga visual. Encontre o equil√≠brio.', 'M√âDIA'])\n",
        "\n",
        "        # An√°lise de formato\n",
        "        formato_dominante = df_consolidado['formato_detectado'].mode()[0] if not df_consolidado['formato_detectado'].empty else 'N/A'\n",
        "        if 'horizontal' in formato_dominante.lower():\n",
        "            recommendations.append(['FORMATO', 'FOQUE EM VERTICAL', 'Formato vertical (9:16) tem melhor performance em redes sociais.', 'ALTA'])\n",
        "\n",
        "        # An√°lise de texto\n",
        "        if df_consolidado['densidade_texto'].mean() < 1:\n",
        "            recommendations.append(['CONTE√öDO', 'ADICIONE MAIS TEXTO', 'Textos na tela aumentam reten√ß√£o e acessibilidade.', 'M√âDIA'])\n",
        "\n",
        "        rec_headers = ['Categoria', 'A√ß√£o', 'Justificativa', 'Prioridade']\n",
        "        add_data_to_sheet(ws_recomendacoes, recommendations, start_row=3, start_col=1, headers=rec_headers)\n",
        "\n",
        "        # Salvar arquivo\n",
        "        log_progress(\"Salvando dashboard...\")\n",
        "        wb.save(output_path)\n",
        "\n",
        "        log_progress(\"DASHBOARD MASTER EXECUTIVO CRIADO COM SUCESSO!\")\n",
        "        log_progress(f\"Arquivo salvo em: {output_path}\")\n",
        "        log_progress(f\"{len(df_consolidado)} v√≠deos analisados\")\n",
        "        log_progress(f\"{len(insights)} insights estrat√©gicos gerados\")\n",
        "        log_progress(f\"{len(recommendations)} recomenda√ß√µes criadas\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        log_progress(f\"ERRO CR√çTICO: {e}\")\n",
        "        log_progress(\"Verifique os arquivos de entrada e tente novamente\")\n",
        "        return False\n",
        "\n",
        "def main():\n",
        "    \"\"\"Fun√ß√£o principal de execu√ß√£o\"\"\"\n",
        "    log_progress(\"INICIANDO SISTEMA DE DASHBOARD INTELIGENTE\")\n",
        "\n",
        "    # Configurar caminhos\n",
        "    BASE_PATH = \"/content/drive/MyDrive/Videos Dona Done/_engenharia_reversa\"\n",
        "    CSV_PATH = os.path.join(BASE_PATH, \"dashboard\", \"dados_consolidados.csv\")\n",
        "    JSON_PATH = os.path.join(BASE_PATH, \"dashboard\", \"dados_detalhados.json\")\n",
        "    OUTPUT_PATH = os.path.join(BASE_PATH, \"dashboard\", \"DASHBOARD_MASTER_EXECUTIVO_INTELIGENTE.xlsx\")\n",
        "\n",
        "    # Verificar se arquivos existem\n",
        "    if not os.path.exists(CSV_PATH):\n",
        "        log_progress(f\"ERRO: Arquivo CSV n√£o encontrado: {CSV_PATH}\")\n",
        "        return False\n",
        "\n",
        "    if not os.path.exists(JSON_PATH):\n",
        "        log_progress(f\"ERRO: Arquivo JSON n√£o encontrado: {JSON_PATH}\")\n",
        "        return False\n",
        "\n",
        "    # Executar cria√ß√£o do dashboard\n",
        "    success = create_enhanced_dashboard_master(CSV_PATH, JSON_PATH, OUTPUT_PATH)\n",
        "\n",
        "    if success:\n",
        "        log_progress(\"PROCESSO CONCLU√çDO COM SUCESSO!\")\n",
        "        log_progress(\"Dashboard inteligente pronto para uso estrat√©gico\")\n",
        "    else:\n",
        "        log_progress(\"PROCESSO FALHOU - Verifique os logs acima\")\n",
        "\n",
        "    return success\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "NUeniUqRJLuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# LAYER 4.3: INTEGRA√á√ÉO DE COPYWRITING NO DASHBOARD EXISTENTE\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from openpyxl import load_workbook\n",
        "from openpyxl.styles import Font, PatternFill, Alignment\n",
        "\n",
        "def integrar_copywriting_dashboard_existente():\n",
        "    \"\"\"Integra an√°lise de copywriting no dashboard master existente\"\"\"\n",
        "    print(\"üîÑ Iniciando integra√ß√£o de copywriting no dashboard existente...\")\n",
        "\n",
        "    # Verificar pr√©-requisitos\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa('copywriting_analysis')\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Localizar dashboard existente\n",
        "    pasta_dashboard = os.path.join(PASTA_TRABALHO, \"dashboard\")\n",
        "    dashboard_existente = None\n",
        "\n",
        "    # Procurar arquivo de dashboard existente\n",
        "    if os.path.exists(pasta_dashboard):\n",
        "        arquivos = os.listdir(pasta_dashboard)\n",
        "        for arquivo in arquivos:\n",
        "            if \"DASHBOARD_MASTER_EXECUTIVO_INTELIGENTE\" in arquivo and arquivo.endswith(\".xlsx\"):\n",
        "                dashboard_existente = os.path.join(pasta_dashboard, arquivo)\n",
        "                break\n",
        "\n",
        "    if not dashboard_existente:\n",
        "        print(\"‚ùå Dashboard master existente n√£o encontrado!\")\n",
        "        print(\"Execute primeiro a c√©lula 4.2 (Blueprint Final) para criar o dashboard base.\")\n",
        "        return\n",
        "\n",
        "    print(f\"  üìä Dashboard encontrado: {os.path.basename(dashboard_existente)}\")\n",
        "\n",
        "    # Carregar dados de copywriting\n",
        "    dados_copywriting = carregar_dados_copywriting()\n",
        "    if not dados_copywriting:\n",
        "        return\n",
        "\n",
        "    # Abrir workbook existente\n",
        "    try:\n",
        "        wb = load_workbook(dashboard_existente)\n",
        "        print(f\"  ‚úÖ Dashboard carregado com {len(wb.sheetnames)} abas existentes\")\n",
        "\n",
        "        # Adicionar novas abas de copywriting\n",
        "        adicionar_aba_copywriting_estrategico(wb, dados_copywriting)\n",
        "        adicionar_aba_templates_copy(wb, dados_copywriting)\n",
        "        adicionar_aba_timeline_copy(wb, dados_copywriting)\n",
        "        adicionar_aba_recomendacoes_copy(wb, dados_copywriting)\n",
        "\n",
        "        # Atualizar aba principal com m√©tricas de copywriting\n",
        "        atualizar_aba_principal_com_copy(wb, dados_copywriting)\n",
        "\n",
        "        # Salvar dashboard atualizado\n",
        "        wb.save(dashboard_existente)\n",
        "\n",
        "        print(f\"‚úÖ Dashboard atualizado com an√°lise de copywriting!\")\n",
        "        print(f\"üìä Arquivo: {dashboard_existente}\")\n",
        "        print(f\"üìã Novas abas adicionadas:\")\n",
        "        print(\"  ‚Ä¢ Copywriting Estrat√©gico\")\n",
        "        print(\"  ‚Ä¢ Templates Replic√°veis\")\n",
        "        print(\"  ‚Ä¢ Timeline Persuas√£o\")\n",
        "        print(\"  ‚Ä¢ Recomenda√ß√µes Copy\")\n",
        "        print(\"  ‚Ä¢ Dashboard Principal (atualizada)\")\n",
        "\n",
        "        # Gerar relat√≥rios complementares\n",
        "        gerar_relatorios_copywriting_individuais(dados_copywriting)\n",
        "\n",
        "        # Atualizar config\n",
        "        config[\"status_etapas\"][\"dashboard_copywriting_integrado\"] = True\n",
        "        config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "        with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        return dashboard_existente\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro ao atualizar dashboard: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def verificar_prerequisito_etapa(etapa):\n",
        "    \"\"\"Verifica se uma etapa foi executada\"\"\"\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "\n",
        "    if not os.path.exists(config_path):\n",
        "        print(f\"‚ùå Arquivo de configura√ß√£o n√£o encontrado: {config_path}\")\n",
        "        return False, None\n",
        "\n",
        "    try:\n",
        "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            config = json.load(f)\n",
        "\n",
        "        if not config.get(\"status_etapas\", {}).get(etapa, False):\n",
        "            print(f\"‚ùå Pr√©-requisito n√£o atendido: {etapa}\")\n",
        "            print(\"Execute primeiro a c√©lula correspondente.\")\n",
        "            return False, None\n",
        "\n",
        "        return True, config\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro ao verificar pr√©-requisitos: {e}\")\n",
        "        return False, None\n",
        "\n",
        "def carregar_dados_copywriting():\n",
        "    \"\"\"Carrega dados de copywriting e outros dados necess√°rios\"\"\"\n",
        "    print(\"  üìä Carregando dados de copywriting...\")\n",
        "\n",
        "    try:\n",
        "        # Dados de copywriting\n",
        "        copywriting_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_copywriting_completas.json\")\n",
        "        with open(copywriting_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            copywriting_data = json.load(f)\n",
        "\n",
        "        # Dados de legendas\n",
        "        legendas_path = os.path.join(PASTA_TRABALHO, \"dados\", \"legendas_geradas.json\")\n",
        "        with open(legendas_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            legendas_data = json.load(f)\n",
        "\n",
        "        # Tentar carregar outros dados (podem n√£o existir ainda)\n",
        "        outros_dados = {}\n",
        "\n",
        "        try:\n",
        "            padroes_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_padroes_completas.json\")\n",
        "            with open(padroes_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                outros_dados[\"padroes\"] = json.load(f)\n",
        "        except:\n",
        "            outros_dados[\"padroes\"] = []\n",
        "\n",
        "        try:\n",
        "            videos_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "            with open(videos_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                outros_dados[\"videos\"] = json.load(f)\n",
        "        except:\n",
        "            outros_dados[\"videos\"] = []\n",
        "\n",
        "        print(f\"  ‚úÖ Dados carregados: {len(copywriting_data)} an√°lises de copywriting\")\n",
        "\n",
        "        return {\n",
        "            \"copywriting\": copywriting_data,\n",
        "            \"legendas\": legendas_data,\n",
        "            **outros_dados\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Erro ao carregar dados de copywriting: {e}\")\n",
        "        return None\n",
        "\n",
        "def adicionar_aba_copywriting_estrategico(wb, dados):\n",
        "    \"\"\"Adiciona aba principal de an√°lise de copywriting\"\"\"\n",
        "    # Criar nova aba\n",
        "    ws = wb.create_sheet(\"Copywriting Estrat√©gico\")\n",
        "\n",
        "    # T√≠tulo principal\n",
        "    ws.merge_cells(\"A1:H1\")\n",
        "    titulo = ws[\"A1\"]\n",
        "    titulo.value = \"AN√ÅLISE ESTRAT√âGICA DE COPYWRITING - ENGENHARIA REVERSA\"\n",
        "    titulo.fill = PatternFill(start_color=\"1F4E79\", end_color=\"1F4E79\", fill_type=\"solid\")\n",
        "    titulo.font = Font(color=\"FFFFFF\", bold=True, size=14)\n",
        "    titulo.alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "    row = 3\n",
        "\n",
        "    # M√©tricas executivas\n",
        "    ws[f\"A{row}\"] = \"M√âTRICAS EXECUTIVAS DE COPYWRITING\"\n",
        "    ws[f\"A{row}\"].font = Font(bold=True, size=12, color=\"C5504B\")\n",
        "    row += 2\n",
        "\n",
        "    # Calcular m√©tricas\n",
        "    videos_copy = dados[\"copywriting\"]\n",
        "\n",
        "    if videos_copy:\n",
        "        # Score m√©dio\n",
        "        scores = [v.get(\"score_persuasao\", 0) for v in videos_copy]\n",
        "        score_medio = sum(scores) / len(scores) if scores else 0\n",
        "\n",
        "        # Contadores\n",
        "        total_ganchos = sum(len(v.get(\"ganchos_detectados\", {})) for v in videos_copy)\n",
        "        total_gatilhos = sum(len(v.get(\"gatilhos_mentais_detectados\", {})) for v in videos_copy)\n",
        "        total_ctas = sum(len(v.get(\"ctas_detectados\", {})) for v in videos_copy)\n",
        "        videos_sem_cta = len([v for v in videos_copy if not v.get(\"ctas_detectados\")])\n",
        "        total_templates = sum(len(v.get(\"templates_identificados\", [])) for v in videos_copy)\n",
        "\n",
        "        # Exibir m√©tricas\n",
        "        metricas = [\n",
        "            (\"Score Persuas√£o M√©dio:\", f\"{score_medio:.1f}/100\", \"Meta: 70+ para alta convers√£o\"),\n",
        "            (\"V√≠deos Analisados:\", len(videos_copy), \"Base completa da an√°lise\"),\n",
        "            (\"Total de Ganchos:\", total_ganchos, f\"M√©dia: {total_ganchos/len(videos_copy):.1f} por v√≠deo\"),\n",
        "            (\"Total de Gatilhos:\", total_gatilhos, f\"M√©dia: {total_gatilhos/len(videos_copy):.1f} por v√≠deo\"),\n",
        "            (\"Total de CTAs:\", total_ctas, f\"M√©dia: {total_ctas/len(videos_copy):.1f} por v√≠deo\"),\n",
        "            (\"üö® V√≠deos sem CTA:\", videos_sem_cta, \"CR√çTICO: Implementar imediatamente\" if videos_sem_cta > 0 else \"‚úÖ Todos t√™m CTA\"),\n",
        "            (\"Templates Identificados:\", total_templates, \"Estruturas replic√°veis encontradas\")\n",
        "        ]\n",
        "\n",
        "        for metrica, valor, descricao in metricas:\n",
        "            ws[f\"A{row}\"] = metrica\n",
        "            ws[f\"B{row}\"] = valor\n",
        "            ws[f\"C{row}\"] = descricao\n",
        "\n",
        "            ws[f\"A{row}\"].font = Font(bold=True)\n",
        "            if \"üö®\" in metrica and videos_sem_cta > 0:\n",
        "                ws[f\"B{row}\"].font = Font(bold=True, color=\"FF0000\")\n",
        "            elif isinstance(valor, (int, float)) and valor > 0:\n",
        "                ws[f\"B{row}\"].font = Font(bold=True, color=\"70AD47\")\n",
        "\n",
        "            row += 1\n",
        "\n",
        "        row += 2\n",
        "\n",
        "        # Ranking de performance\n",
        "        ws[f\"A{row}\"] = \"üèÜ RANKING DE PERFORMANCE POR SCORE DE PERSUAS√ÉO\"\n",
        "        ws[f\"A{row}\"].font = Font(bold=True, size=12, color=\"1F4E79\")\n",
        "        row += 2\n",
        "\n",
        "        # Headers\n",
        "        headers = [\"Posi√ß√£o\", \"V√≠deo ID\", \"Score\", \"Ganchos\", \"Gatilhos\", \"CTAs\", \"Status\"]\n",
        "        for col, header in enumerate(headers, 1):\n",
        "            cell = ws.cell(row=row, column=col)\n",
        "            cell.value = header\n",
        "            cell.font = Font(bold=True)\n",
        "            cell.fill = PatternFill(start_color=\"D9E2F3\", end_color=\"D9E2F3\", fill_type=\"solid\")\n",
        "        row += 1\n",
        "\n",
        "        # Top performers\n",
        "        top_videos = sorted(videos_copy, key=lambda x: x.get(\"score_persuasao\", 0), reverse=True)\n",
        "\n",
        "        for i, video in enumerate(top_videos, 1):\n",
        "            ws.cell(row=row, column=1, value=f\"{i}¬∫\")\n",
        "            ws.cell(row=row, column=2, value=video[\"video_id\"])\n",
        "            ws.cell(row=row, column=3, value=f\"{video.get('score_persuasao', 0)}/100\")\n",
        "            ws.cell(row=row, column=4, value=len(video.get(\"ganchos_detectados\", {})))\n",
        "            ws.cell(row=row, column=5, value=len(video.get(\"gatilhos_mentais_detectados\", {})))\n",
        "            ws.cell(row=row, column=6, value=len(video.get(\"ctas_detectados\", {})))\n",
        "\n",
        "            # Status baseado no score\n",
        "            score = video.get(\"score_persuasao\", 0)\n",
        "            if score >= 70:\n",
        "                status = \"üü¢ √ìTIMO\"\n",
        "                status_color = \"70AD47\"\n",
        "            elif score >= 50:\n",
        "                status = \"üü° BOM\"\n",
        "                status_color = \"FFC000\"\n",
        "            else:\n",
        "                status = \"üî¥ PRECISA OTIMIZAR\"\n",
        "                status_color = \"C5504B\"\n",
        "\n",
        "            cell_status = ws.cell(row=row, column=7, value=status)\n",
        "            cell_status.font = Font(color=status_color, bold=True)\n",
        "\n",
        "            # Destacar top 3\n",
        "            if i <= 3:\n",
        "                for col in range(1, 8):\n",
        "                    ws.cell(row=row, column=col).fill = PatternFill(start_color=\"FFF2CC\", end_color=\"FFF2CC\", fill_type=\"solid\")\n",
        "\n",
        "            row += 1\n",
        "\n",
        "    # Ajustar larguras das colunas\n",
        "    for col, width in [(\"A\", 25), (\"B\", 15), (\"C\", 40), (\"D\", 10), (\"E\", 10), (\"F\", 10), (\"G\", 20), (\"H\", 15)]:\n",
        "        ws.column_dimensions[col].width = width\n",
        "\n",
        "def adicionar_aba_templates_copy(wb, dados):\n",
        "    \"\"\"Adiciona aba de templates replic√°veis\"\"\"\n",
        "    ws = wb.create_sheet(\"Templates Replic√°veis\")\n",
        "\n",
        "    # T√≠tulo\n",
        "    ws.merge_cells(\"A1:F1\")\n",
        "    titulo = ws[\"A1\"]\n",
        "    titulo.value = \"TEMPLATES E ESTRUTURAS REPLIC√ÅVEIS DE COPYWRITING\"\n",
        "    titulo.fill = PatternFill(start_color=\"70AD47\", end_color=\"70AD47\", fill_type=\"solid\")\n",
        "    titulo.font = Font(color=\"FFFFFF\", bold=True, size=14)\n",
        "    titulo.alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "    row = 3\n",
        "\n",
        "    # Coletar todos os templates\n",
        "    todos_templates = []\n",
        "    for video in dados[\"copywriting\"]:\n",
        "        templates = video.get(\"templates_identificados\", [])\n",
        "        for template in templates:\n",
        "            template[\"video_id\"] = video[\"video_id\"]\n",
        "            todos_templates.append(template)\n",
        "\n",
        "    if todos_templates:\n",
        "        # Agrupar templates por tipo\n",
        "        templates_agrupados = {}\n",
        "        for template in todos_templates:\n",
        "            nome = template[\"nome\"]\n",
        "            if nome not in templates_agrupados:\n",
        "                templates_agrupados[nome] = {\n",
        "                    \"estrutura\": template[\"estrutura\"],\n",
        "                    \"eficacia\": template[\"eficacia\"],\n",
        "                    \"uso_recomendado\": template[\"uso_recomendado\"],\n",
        "                    \"videos_exemplo\": []\n",
        "                }\n",
        "            templates_agrupados[nome][\"videos_exemplo\"].append(template[\"video_id\"])\n",
        "\n",
        "        # Exibir templates\n",
        "        for nome_template, dados_template in templates_agrupados.items():\n",
        "            ws.merge_cells(f\"A{row}:F{row}\")\n",
        "            template_header = ws[f\"A{row}\"]\n",
        "            template_header.value = f\"üìã TEMPLATE: {nome_template.replace('_', ' ')}\"\n",
        "            template_header.fill = PatternFill(start_color=\"E2EFDA\", end_color=\"E2EFDA\", fill_type=\"solid\")\n",
        "            template_header.font = Font(bold=True, size=11)\n",
        "            row += 1\n",
        "\n",
        "            ws[f\"A{row}\"] = \"Estrutura:\"\n",
        "            ws[f\"B{row}\"] = dados_template[\"estrutura\"]\n",
        "            ws[f\"A{row}\"].font = Font(bold=True)\n",
        "            row += 1\n",
        "\n",
        "            ws[f\"A{row}\"] = \"Efic√°cia:\"\n",
        "            ws[f\"B{row}\"] = dados_template[\"eficacia\"]\n",
        "            ws[f\"A{row}\"].font = Font(bold=True)\n",
        "            if dados_template[\"eficacia\"] == \"MUITO ALTA\":\n",
        "                ws[f\"B{row}\"].font = Font(color=\"70AD47\", bold=True)\n",
        "            elif dados_template[\"eficacia\"] == \"ALTA\":\n",
        "                ws[f\"B{row}\"].font = Font(color=\"C5504B\", bold=True)\n",
        "            row += 1\n",
        "\n",
        "            ws[f\"A{row}\"] = \"Uso Recomendado:\"\n",
        "            ws[f\"B{row}\"] = dados_template[\"uso_recomendado\"]\n",
        "            ws[f\"A{row}\"].font = Font(bold=True)\n",
        "            row += 1\n",
        "\n",
        "            ws[f\"A{row}\"] = \"V√≠deos Exemplo:\"\n",
        "            ws[f\"B{row}\"] = \", \".join(dados_template[\"videos_exemplo\"][:3])\n",
        "            ws[f\"A{row}\"].font = Font(bold=True)\n",
        "            row += 2\n",
        "\n",
        "    # Ajustar larguras\n",
        "    for col, width in [(\"A\", 20), (\"B\", 50), (\"C\", 15), (\"D\", 15), (\"E\", 15), (\"F\", 15)]:\n",
        "        ws.column_dimensions[col].width = width\n",
        "\n",
        "def adicionar_aba_timeline_copy(wb, dados):\n",
        "    \"\"\"Adiciona aba de timeline de elementos persuasivos\"\"\"\n",
        "    ws = wb.create_sheet(\"Timeline Persuas√£o\")\n",
        "\n",
        "    # T√≠tulo\n",
        "    ws.merge_cells(\"A1:G1\")\n",
        "    titulo = ws[\"A1\"]\n",
        "    titulo.value = \"TIMELINE DE ELEMENTOS PERSUASIVOS - AN√ÅLISE TEMPORAL\"\n",
        "    titulo.fill = PatternFill(start_color=\"FFC000\", end_color=\"FFC000\", fill_type=\"solid\")\n",
        "    titulo.font = Font(color=\"FFFFFF\", bold=True, size=14)\n",
        "    titulo.alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "    row = 3\n",
        "\n",
        "    # Para cada v√≠deo, mostrar timeline\n",
        "    for video in dados[\"copywriting\"]:\n",
        "        video_id = video[\"video_id\"]\n",
        "\n",
        "        ws[f\"A{row}\"] = f\"üìπ V√çDEO: {video_id}\"\n",
        "        ws[f\"A{row}\"].font = Font(bold=True, size=11, color=\"1F4E79\")\n",
        "        row += 2\n",
        "\n",
        "        # Headers da timeline\n",
        "        headers = [\"Tempo\", \"Minuto\", \"Segundo\", \"Elemento\", \"Posi√ß√£o\", \"Impacto\", \"An√°lise\"]\n",
        "        for col, header in enumerate(headers, 1):\n",
        "            cell = ws.cell(row=row, column=col)\n",
        "            cell.value = header\n",
        "            cell.font = Font(bold=True)\n",
        "            cell.fill = PatternFill(start_color=\"F2F2F2\", end_color=\"F2F2F2\", fill_type=\"solid\")\n",
        "        row += 1\n",
        "\n",
        "        # Coletar todos os elementos temporais\n",
        "        elementos_temporais = []\n",
        "\n",
        "        # Ganchos\n",
        "        for tipo, dados in video.get(\"ganchos_detectados\", {}).items():\n",
        "            for timestamp in dados.get(\"timestamps\", []):\n",
        "                elementos_temporais.append({\n",
        "                    \"categoria\": \"GANCHO\",\n",
        "                    \"tipo\": tipo,\n",
        "                    \"minuto\": timestamp[\"minuto\"],\n",
        "                    \"segundo\": timestamp[\"segundo\"],\n",
        "                    \"contexto\": timestamp[\"texto_contexto\"]\n",
        "                })\n",
        "\n",
        "        # Gatilhos\n",
        "        for tipo, dados in video.get(\"gatilhos_mentais_detectados\", {}).items():\n",
        "            for timestamp in dados.get(\"timestamps\", []):\n",
        "                elementos_temporais.append({\n",
        "                    \"categoria\": \"GATILHO\",\n",
        "                    \"tipo\": tipo,\n",
        "                    \"minuto\": timestamp[\"minuto\"],\n",
        "                    \"segundo\": timestamp[\"segundo\"],\n",
        "                    \"contexto\": timestamp[\"texto_contexto\"]\n",
        "                })\n",
        "\n",
        "        # CTAs\n",
        "        for tipo, dados in video.get(\"ctas_detectados\", {}).items():\n",
        "            for timestamp in dados.get(\"timestamps\", []):\n",
        "                elementos_temporais.append({\n",
        "                    \"categoria\": \"CTA\",\n",
        "                    \"tipo\": tipo,\n",
        "                    \"minuto\": timestamp[\"minuto\"],\n",
        "                    \"segundo\": timestamp[\"segundo\"],\n",
        "                    \"contexto\": timestamp[\"texto_contexto\"]\n",
        "                })\n",
        "\n",
        "        # Ordenar por tempo\n",
        "        elementos_temporais.sort(key=lambda x: (x[\"minuto\"], x[\"segundo\"]))\n",
        "\n",
        "        # Exibir elementos\n",
        "        if elementos_temporais:\n",
        "            for elemento in elementos_temporais:\n",
        "                ws.cell(row=row, column=1, value=f\"{elemento['minuto']:02d}:{elemento['segundo']:02d}\")\n",
        "                ws.cell(row=row, column=2, value=elemento[\"minuto\"])\n",
        "                ws.cell(row=row, column=3, value=elemento[\"segundo\"])\n",
        "                ws.cell(row=row, column=4, value=f\"{elemento['categoria']}: {elemento['tipo']}\")\n",
        "\n",
        "                # An√°lise de posi√ß√£o\n",
        "                total_segundos = elemento[\"minuto\"] * 60 + elemento[\"segundo\"]\n",
        "                if total_segundos <= 10:\n",
        "                    posicao = \"ABERTURA\"\n",
        "                    posicao_color = \"70AD47\"\n",
        "                elif total_segundos <= 20:\n",
        "                    posicao = \"MEIO\"\n",
        "                    posicao_color = \"FFC000\"\n",
        "                else:\n",
        "                    posicao = \"FINAL\"\n",
        "                    posicao_color = \"C5504B\"\n",
        "\n",
        "                cell_pos = ws.cell(row=row, column=5, value=posicao)\n",
        "                cell_pos.font = Font(color=posicao_color, bold=True)\n",
        "\n",
        "                # An√°lise de impacto\n",
        "                impacto = analisar_impacto_elemento(elemento[\"categoria\"], posicao)\n",
        "                ws.cell(row=row, column=6, value=impacto[\"score\"])\n",
        "                ws.cell(row=row, column=7, value=impacto[\"analise\"])\n",
        "\n",
        "                if impacto[\"score\"] == \"ALTO\":\n",
        "                    ws.cell(row=row, column=6).font = Font(color=\"70AD47\", bold=True)\n",
        "                elif impacto[\"score\"] == \"BAIXO\":\n",
        "                    ws.cell(row=row, column=6).font = Font(color=\"C5504B\", bold=True)\n",
        "\n",
        "                row += 1\n",
        "        else:\n",
        "            ws.cell(row=row, column=1, value=\"Nenhum elemento temporal mapeado\")\n",
        "            row += 1\n",
        "\n",
        "        row += 2\n",
        "\n",
        "    # Ajustar larguras\n",
        "    for col, width in [(\"A\", 8), (\"B\", 10), (\"C\", 15), (\"D\", 30), (\"E\", 12), (\"F\", 8), (\"G\", 25)]:\n",
        "        ws.column_dimensions[col].width = width\n",
        "\n",
        "def analisar_impacto_elemento(categoria, posicao):\n",
        "    \"\"\"Analisa o impacto de um elemento baseado na posi√ß√£o\"\"\"\n",
        "    impactos = {\n",
        "        (\"GANCHO\", \"ABERTURA\"): {\"score\": \"ALTO\", \"analise\": \"Ideal para capturar aten√ß√£o\"},\n",
        "        (\"GANCHO\", \"MEIO\"): {\"score\": \"M√âDIO\", \"analise\": \"Melhor no in√≠cio\"},\n",
        "        (\"GANCHO\", \"FINAL\"): {\"score\": \"BAIXO\", \"analise\": \"Reposicionar para abertura\"},\n",
        "        (\"GATILHO\", \"ABERTURA\"): {\"score\": \"M√âDIO\", \"analise\": \"Bom para credibilidade\"},\n",
        "        (\"GATILHO\", \"MEIO\"): {\"score\": \"ALTO\", \"analise\": \"Posi√ß√£o ideal para persuas√£o\"},\n",
        "        (\"GATILHO\", \"FINAL\"): {\"score\": \"M√âDIO\", \"analise\": \"Refor√ßa decis√£o\"},\n",
        "        (\"CTA\", \"ABERTURA\"): {\"score\": \"BAIXO\", \"analise\": \"Muito cedo, construir valor primeiro\"},\n",
        "        (\"CTA\", \"MEIO\"): {\"score\": \"M√âDIO\", \"analise\": \"Considerar mover para final\"},\n",
        "        (\"CTA\", \"FINAL\"): {\"score\": \"ALTO\", \"analise\": \"Posicionamento ideal\"}\n",
        "    }\n",
        "\n",
        "    return impactos.get((categoria, posicao), {\"score\": \"M√âDIO\", \"analise\": \"Analisar contexto espec√≠fico\"})\n",
        "\n",
        "def adicionar_aba_recomendacoes_copy(wb, dados):\n",
        "    \"\"\"Adiciona aba de recomenda√ß√µes estrat√©gicas consolidadas\"\"\"\n",
        "    ws = wb.create_sheet(\"Recomenda√ß√µes Copy\")\n",
        "\n",
        "    # T√≠tulo\n",
        "    ws.merge_cells(\"A1:F1\")\n",
        "    titulo = ws[\"A1\"]\n",
        "    titulo.value = \"RECOMENDA√á√ïES ESTRAT√âGICAS DE COPYWRITING - PLANO DE A√á√ÉO\"\n",
        "    titulo.fill = PatternFill(start_color=\"C5504B\", end_color=\"C5504B\", fill_type=\"solid\")\n",
        "    titulo.font = Font(color=\"FFFFFF\", bold=True, size=14)\n",
        "    titulo.alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "    row = 3\n",
        "\n",
        "    # Consolidar recomenda√ß√µes por prioridade\n",
        "    todas_recomendacoes = []\n",
        "    for video in dados[\"copywriting\"]:\n",
        "        recomendacoes_video = video.get(\"recomendacoes_estrategicas\", [])\n",
        "        for rec in recomendacoes_video:\n",
        "            rec[\"video_id\"] = video[\"video_id\"]\n",
        "            todas_recomendacoes.append(rec)\n",
        "\n",
        "    # Agrupar por prioridade\n",
        "    recomendacoes_por_prioridade = {\n",
        "        \"CR√çTICA\": [],\n",
        "        \"ALTA\": [],\n",
        "        \"M√âDIA\": []\n",
        "    }\n",
        "\n",
        "    for rec in todas_recomendacoes:\n",
        "        prioridade = rec.get(\"prioridade\", \"M√âDIA\")\n",
        "        if prioridade in recomendacoes_por_prioridade:\n",
        "            recomendacoes_por_prioridade[prioridade].append(rec)\n",
        "\n",
        "    # Exibir por prioridade\n",
        "    for prioridade in [\"CR√çTICA\", \"ALTA\", \"M√âDIA\"]:\n",
        "        if not recomendacoes_por_prioridade[prioridade]:\n",
        "            continue\n",
        "\n",
        "        ws[f\"A{row}\"] = f\"üö® PRIORIDADE {prioridade}\"\n",
        "        if prioridade == \"CR√çTICA\":\n",
        "            ws[f\"A{row}\"].font = Font(color=\"FF0000\", bold=True, size=12)\n",
        "        elif prioridade == \"ALTA\":\n",
        "            ws[f\"A{row}\"].font = Font(color=\"C5504B\", bold=True, size=12)\n",
        "        else:\n",
        "            ws[f\"A{row}\"].font = Font(color=\"FFC000\", bold=True, size=12)\n",
        "\n",
        "        row += 2\n",
        "\n",
        "        # Headers\n",
        "        headers = [\"Categoria\", \"Recomenda√ß√£o\", \"V√≠deos Afetados\", \"A√ß√£o Sugerida\"]\n",
        "        for col, header in enumerate(headers, 1):\n",
        "            cell = ws.cell(row=row, column=col)\n",
        "            cell.value = header\n",
        "            cell.font = Font(bold=True)\n",
        "            cell.fill = PatternFill(start_color=\"F2F2F2\", end_color=\"F2F2F2\", fill_type=\"solid\")\n",
        "        row += 1\n",
        "\n",
        "        # Agrupar recomenda√ß√µes similares da mesma prioridade\n",
        "        grupos = {}\n",
        "        for rec in recomendacoes_por_prioridade[prioridade]:\n",
        "            categoria = rec[\"categoria\"]\n",
        "            if categoria not in grupos:\n",
        "                grupos[categoria] = {\n",
        "                    \"recomendacao\": rec[\"recomendacao\"],\n",
        "                    \"videos\": [],\n",
        "                    \"acao\": gerar_acao_especifica(categoria)\n",
        "                }\n",
        "            grupos[categoria][\"videos\"].append(rec[\"video_id\"])\n",
        "\n",
        "        for categoria, dados_grupo in grupos.items():\n",
        "            ws.cell(row=row, column=1, value=categoria)\n",
        "            ws.cell(row=row, column=2, value=dados_grupo[\"recomendacao\"])\n",
        "            ws.cell(row=row, column=3, value=f\"{len(dados_grupo['videos'])} v√≠deo(s)\")\n",
        "            ws.cell(row=row, column=4, value=dados_grupo[\"acao\"])\n",
        "            row += 1\n",
        "\n",
        "        row += 2\n",
        "\n",
        "    # Ajustar larguras\n",
        "    for col, width in [(\"A\", 20), (\"B\", 40), (\"C\", 15), (\"D\", 30), (\"E\", 15), (\"F\", 15)]:\n",
        "        ws.column_dimensions[col].width = width\n",
        "\n",
        "def gerar_acao_especifica(categoria):\n",
        "    \"\"\"Gera a√ß√£o espec√≠fica baseada na categoria da recomenda√ß√£o\"\"\"\n",
        "    acoes = {\n",
        "        \"GANCHOS\": \"Revisar primeiros 5 segundos e adicionar pergunta ou curiosidade\",\n",
        "        \"GATILHOS\": \"Incorporar elementos de autoridade, prova social ou reciprocidade\",\n",
        "        \"CTA\": \"Adicionar call-to-action claro nos √∫ltimos 3-5 segundos\",\n",
        "        \"ESTRUTURA\": \"Aplicar template identificado mais pr√≥ximo do nicho\",\n",
        "        \"PERSUAS√ÉO\": \"Combinar m√∫ltiplos elementos persuasivos em sequ√™ncia l√≥gica\"\n",
        "    }\n",
        "    return acoes.get(categoria, \"Revisar e otimizar elementos espec√≠ficos mencionados\")\n",
        "\n",
        "def atualizar_aba_principal_com_copy(wb, dados):\n",
        "    \"\"\"Atualiza a aba principal existente com m√©tricas de copywriting\"\"\"\n",
        "    # Tentar encontrar aba principal (pode ter nomes diferentes)\n",
        "    aba_principal = None\n",
        "    possiveis_nomes = [\"Dashboard Principal\", \"Executive Summary\", \"Summary\", \"Principal\"]\n",
        "\n",
        "    for nome in wb.sheetnames:\n",
        "        if any(possivel in nome for possivel in possiveis_nomes):\n",
        "            aba_principal = wb[nome]\n",
        "            break\n",
        "\n",
        "    if not aba_principal:\n",
        "        # Se n√£o encontrou, usar a primeira aba\n",
        "        aba_principal = wb.worksheets[0]\n",
        "\n",
        "    # Encontrar pr√≥xima linha vazia para adicionar se√ß√£o de copywriting\n",
        "    next_row = 1\n",
        "    for row in range(1, 100):\n",
        "        if aba_principal[f\"A{row}\"].value is None:\n",
        "            next_row = row\n",
        "            break\n",
        "\n",
        "    # Adicionar se√ß√£o de copywriting\n",
        "    # T√≠tulo da se√ß√£o\n",
        "    aba_principal.merge_cells(f\"A{next_row}:H{next_row}\")\n",
        "    titulo_copy = aba_principal[f\"A{next_row}\"]\n",
        "    titulo_copy.value = \"üìù AN√ÅLISE DE COPYWRITING - RESUMO EXECUTIVO\"\n",
        "    titulo_copy.fill = PatternFill(start_color=\"7030A0\", end_color=\"7030A0\", fill_type=\"solid\")\n",
        "    titulo_copy.font = Font(color=\"FFFFFF\", bold=True, size=12)\n",
        "    next_row += 2\n",
        "\n",
        "    # M√©tricas resumidas\n",
        "    videos_copy = dados[\"copywriting\"]\n",
        "\n",
        "    if videos_copy:\n",
        "        scores = [v.get(\"score_persuasao\", 0) for v in videos_copy]\n",
        "        score_medio = sum(scores) / len(scores)\n",
        "        videos_sem_cta = len([v for v in videos_copy if not v.get(\"ctas_detectados\")])\n",
        "        templates_total = sum(len(v.get(\"templates_identificados\", [])) for v in videos_copy)\n",
        "\n",
        "        metricas_resumo = [\n",
        "            (\"Score de Persuas√£o M√©dio:\", f\"{score_medio:.1f}/100\"),\n",
        "            (\"V√≠deos sem CTA:\", f\"{videos_sem_cta} (CR√çTICO)\" if videos_sem_cta > 0 else \"0 ‚úÖ\"),\n",
        "            (\"Templates Identificados:\", str(templates_total)),\n",
        "            (\"Status Geral:\", \"Otimiza√ß√£o necess√°ria\" if score_medio < 60 or videos_sem_cta > 0 else \"Performance boa\")\n",
        "        ]\n",
        "\n",
        "        for metrica, valor in metricas_resumo:\n",
        "            aba_principal[f\"A{next_row}\"] = metrica\n",
        "            aba_principal[f\"B{next_row}\"] = valor\n",
        "            aba_principal[f\"A{next_row}\"].font = Font(bold=True)\n",
        "\n",
        "            if \"CR√çTICO\" in valor:\n",
        "                aba_principal[f\"B{next_row}\"].font = Font(color=\"FF0000\", bold=True)\n",
        "            elif \"‚úÖ\" in valor:\n",
        "                aba_principal[f\"B{next_row}\"].font = Font(color=\"70AD47\", bold=True)\n",
        "\n",
        "            next_row += 1\n",
        "\n",
        "    else:\n",
        "        aba_principal[f\"A{next_row}\"] = \"‚ö†Ô∏è Execute a an√°lise de copywriting (C√©lula 2.4) para ver m√©tricas\"\n",
        "        aba_principal[f\"A{next_row}\"].font = Font(color=\"FFC000\", bold=True)\n",
        "\n",
        "def gerar_relatorios_copywriting_individuais(dados):\n",
        "    \"\"\"Gera relat√≥rios individuais de texto para cada v√≠deo\"\"\"\n",
        "    print(\"  üìÑ Gerando relat√≥rios individuais de copywriting...\")\n",
        "\n",
        "    pasta_relatorios = os.path.join(PASTA_TRABALHO, \"relatorios_copywriting\")\n",
        "    os.makedirs(pasta_relatorios, exist_ok=True)\n",
        "\n",
        "    for video_copy in dados[\"copywriting\"]:\n",
        "        video_id = video_copy[\"video_id\"]\n",
        "\n",
        "        relatorio_path = os.path.join(pasta_relatorios, f\"{video_id}_copywriting_completo.txt\")\n",
        "\n",
        "        with open(relatorio_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\"=\"*60 + \"\\n\")\n",
        "            f.write(\"RELAT√ìRIO COMPLETO DE AN√ÅLISE DE COPYWRITING\\n\")\n",
        "            f.write(\"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "            f.write(f\"üìπ V√≠deo ID: {video_id}\\n\")\n",
        "            f.write(f\"üéØ Score de Persuas√£o: {video_copy.get('score_persuasao', 0)}/100\\n\")\n",
        "            f.write(f\"üìù Total de Palavras: {video_copy.get('total_palavras', 0)}\\n\\n\")\n",
        "\n",
        "            # Texto completo\n",
        "            f.write(\"TRANSCRI√á√ÉO COMPLETA:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            f.write(video_copy.get(\"texto_completo\", \"Transcri√ß√£o n√£o dispon√≠vel\") + \"\\n\\n\")\n",
        "\n",
        "            # Ganchos\n",
        "            f.write(\"üé£ GANCHOS DETECTADOS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            ganchos = video_copy.get(\"ganchos_detectados\", {})\n",
        "            if ganchos:\n",
        "                for tipo, dados in ganchos.items():\n",
        "                    f.write(f\"‚Ä¢ {tipo.upper()}: {dados['count']} ocorr√™ncias\\n\")\n",
        "                    f.write(f\"  Exemplos: {', '.join(dados['exemplos'])}\\n\")\n",
        "            else:\n",
        "                f.write(\"Nenhum gancho detectado.\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "            # Gatilhos\n",
        "            f.write(\"üß† GATILHOS MENTAIS DETECTADOS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            gatilhos = video_copy.get(\"gatilhos_mentais_detectados\", {})\n",
        "            if gatilhos:\n",
        "                for tipo, dados in gatilhos.items():\n",
        "                    f.write(f\"‚Ä¢ {tipo.upper()}: {dados['count']} ocorr√™ncias\\n\")\n",
        "                    f.write(f\"  Exemplos: {', '.join(dados['exemplos'])}\\n\")\n",
        "            else:\n",
        "                f.write(\"Nenhum gatilho mental detectado.\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "            # CTAs\n",
        "            f.write(\"üì¢ CALLS-TO-ACTION DETECTADOS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            ctas = video_copy.get(\"ctas_detectados\", {})\n",
        "            if ctas:\n",
        "                for tipo, dados in ctas.items():\n",
        "                    f.write(f\"‚Ä¢ {tipo.upper()}: {dados['count']} ocorr√™ncias\\n\")\n",
        "                    f.write(f\"  Exemplos: {', '.join(dados['exemplos'])}\\n\")\n",
        "            else:\n",
        "                f.write(\"Nenhum CTA detectado.\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "            # Recomenda√ß√µes\n",
        "            f.write(\"üí° RECOMENDA√á√ïES ESTRAT√âGICAS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            recomendacoes = video_copy.get(\"recomendacoes_estrategicas\", [])\n",
        "            if recomendacoes:\n",
        "                for rec in recomendacoes:\n",
        "                    f.write(f\"‚Ä¢ [{rec['prioridade']}] {rec['categoria']}: {rec['recomendacao']}\\n\")\n",
        "            else:\n",
        "                f.write(\"Nenhuma recomenda√ß√£o espec√≠fica.\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "            # Templates\n",
        "            f.write(\"üìã TEMPLATES IDENTIFICADOS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            templates = video_copy.get(\"templates_identificados\", [])\n",
        "            if templates:\n",
        "                for template in templates:\n",
        "                    f.write(f\"‚Ä¢ {template['nome']}: {template['estrutura']}\\n\")\n",
        "                    f.write(f\"  Efic√°cia: {template['eficacia']}\\n\")\n",
        "                    f.write(f\"  Uso: {template['uso_recomendado']}\\n\\n\")\n",
        "            else:\n",
        "                f.write(\"Nenhum template espec√≠fico identificado.\\n\")\n",
        "\n",
        "    print(f\"  ‚úÖ Relat√≥rios individuais gerados em: {pasta_relatorios}\")\n",
        "\n",
        "# Executar integra√ß√£o\n",
        "try:\n",
        "    integrar_copywriting_dashboard_existente()\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERRO de Execu√ß√£o: {type(e).__name__}: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ],
      "metadata": {
        "id": "6ixFPocx6m8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 4.3: INTEGRA√á√ÉO DE COPYWRITING NO DASHBOARD EXISTENTE\n",
        "# ============================================================================\n",
        "\n",
        "# Definir a vari√°vel global PASTA_TRABALHO se ainda n√£o estiver definida\n",
        "# Certifique-se de que esta vari√°vel esteja definida corretamente em uma c√©lula anterior (ex: C√©lula 1.2)\n",
        "# Exemplo: PASTA_TRABALHO = \"/content/drive/MyDrive/Videos Dona Done/_engenharia_reversa\"\n",
        "\n",
        "# Executar a fun√ß√£o principal da Layer 4.3\n",
        "if 'PASTA_TRABALHO' in globals():\n",
        "    print(\"Iniciando a Layer 4.3: Integra√ß√£o de Copywriting no Dashboard...\")\n",
        "    integrar_copywriting_dashboard_existente()\n",
        "else:\n",
        "    print(\"ERRO: A vari√°vel PASTA_TRABALHO n√£o est√° definida. Certifique-se de executar a C√©lula 1.2 ou equivalente.\")"
      ],
      "metadata": {
        "id": "fvmDo8Iw61G8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# LAYER 5/6 ‚Äî IA LOCAL + DASHBOARD HUMANIZADA (Fus√£o, sem consultas externas)\n",
        "# ============================================================\n",
        "\n",
        "# Instalar depend√™ncias (todas offline-friendly; Whisper usa pesos locais)\n",
        "!pip -q install faster-whisper==1.0.3 mediapipe==0.10.14 scikit-learn==1.5.1 openpyxl==3.1.2 xlsxwriter==3.2.0\n",
        "\n",
        "import os, re, json, cv2, numpy as np, pandas as pd\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from collections import Counter\n",
        "from sklearn.cluster import KMeans\n",
        "import mediapipe as mp\n",
        "from faster_whisper import WhisperModel\n",
        "from openpyxl import load_workbook, Workbook\n",
        "from openpyxl.utils import get_column_letter\n",
        "from openpyxl.styles import Font, Alignment, PatternFill\n",
        "\n",
        "# --------- Conex√£o com SEU notebook (usa as vari√°veis j√° definidas) ----------\n",
        "assert 'PASTA_TRABALHO' in globals(), \"PASTA_TRABALHO n√£o est√° definido. Execute as c√©lulas de configura√ß√£o antes.\"\n",
        "\n",
        "# Fallback para OUTPUT_PATH se ainda n√£o foi definido pelo seu fluxo\n",
        "if 'OUTPUT_PATH' not in globals():\n",
        "    OUTPUT_PATH = os.path.join(PASTA_TRABALHO, \"dashboard\", \"DASHBOARD_MASTER_EXECUTIVO_INTELIGENTE.xlsx\")\n",
        "    print(f\"‚ö†Ô∏è OUTPUT_PATH n√£o estava definido, criei autom√°tico: {OUTPUT_PATH}\")\n",
        "\n",
        "AI_ASSETS_DIR = os.path.join(PASTA_TRABALHO, \"ai_insights\")\n",
        "os.makedirs(AI_ASSETS_DIR, exist_ok=True)\n",
        "\n",
        "# Pesos locais do Whisper (CTranslate2). Estrutura esperada:\n",
        "# PASTA_TRABALHO/modelos/whisper/tiny/ (coloque os arquivos do modelo aqui)\n",
        "WHISPER_LOCAL_DIR = os.path.join(PASTA_TRABALHO, \"modelos\", \"whisper\", \"tiny\")\n",
        "WHISPER_DEVICE = \"cuda\" if cv2.cuda.getCudaEnabledDeviceCount() > 0 else \"cpu\"\n",
        "WHISPER_COMPUTE = \"float16\" if WHISPER_DEVICE == \"cuda\" else \"int8\"\n",
        "\n",
        "# ============================================================\n",
        "# Utilit√°rios de IA (OFFLINE)\n",
        "# ============================================================\n",
        "\n",
        "def ai_transcrever_offline(video_path, out_dir):\n",
        "    \"\"\"Transcreve com Whisper local se os pesos existirem; sen√£o, pula com aviso.\"\"\"\n",
        "    try:\n",
        "        if not os.path.isdir(WHISPER_LOCAL_DIR):\n",
        "            print(f\"‚ö†Ô∏è Whisper OFFLINE n√£o encontrado em {WHISPER_LOCAL_DIR}. Pulando transcri√ß√£o.\")\n",
        "            return \"\"\n",
        "        model = WhisperModel(WHISPER_LOCAL_DIR, device=WHISPER_DEVICE, compute_type=WHISPER_COMPUTE)\n",
        "        # extrai wav 16k mono (sem internet)\n",
        "        audio_path = os.path.join(out_dir, \"audio_16k.wav\")\n",
        "        os.system(f'ffmpeg -y -i \"{video_path}\" -ac 1 -ar 16000 -vn \"{audio_path}\" -loglevel error')\n",
        "        segments, _ = model.transcribe(audio_path, language=\"pt\", vad_filter=True, vad_parameters={\"min_silence_duration_ms\": 500})\n",
        "        lines, srt_lines = [], []\n",
        "        for i, seg in enumerate(segments):\n",
        "            lines.append(seg.text.strip())\n",
        "            def ts(x):\n",
        "                h=int(x//3600); m=int((x%3600)//60); s=x%60\n",
        "                return f\"{h:02}:{m:02}:{s:06.3f}\".replace('.',',')\n",
        "            srt_lines.append(f\"{i+1}\\n{ts(seg.start)} --> {ts(seg.end)}\\n{seg.text.strip()}\\n\")\n",
        "        with open(os.path.join(out_dir,\"transcript.txt\"),\"w\",encoding=\"utf-8\") as f:\n",
        "            f.write(\" \".join(lines))\n",
        "        with open(os.path.join(out_dir,\"subtitles.srt\"),\"w\",encoding=\"utf-8\") as f:\n",
        "            f.write(\"\".join(srt_lines))\n",
        "        return \" \".join(lines)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Erro no Whisper local: {e}. Pulando transcri√ß√£o.\")\n",
        "        return \"\"\n",
        "\n",
        "def ai_keyframes_por_tempo(video_path, out_dir, intervalo_seg=2):\n",
        "    \"\"\"Extrai frames a cada N segundos para OCR, paleta e thumbs.\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 30\n",
        "    frame_interval = int(max(1, fps*intervalo_seg))\n",
        "    idx=0; frames=[]\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret: break\n",
        "        fno = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
        "        if fno % frame_interval == 0:\n",
        "            fp = os.path.join(out_dir, f\"frame_{idx:04d}.jpg\")\n",
        "            cv2.imwrite(fp, frame); frames.append(fp); idx+=1\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "def ai_ocr(frames):\n",
        "    \"\"\"OCR local em frames (Tesseract).\"\"\"\n",
        "    textos=[]\n",
        "    for fp in frames:\n",
        "        try:\n",
        "            txt = pytesseract.image_to_string(Image.open(fp), lang=\"por+eng\").strip()\n",
        "            if txt: textos.append(txt)\n",
        "        except:\n",
        "            pass\n",
        "    return textos\n",
        "\n",
        "def ai_paleta(frames, n=5):\n",
        "    \"\"\"Paleta dominante via KMeans sobre o primeiro frame (r√°pido e consistente).\"\"\"\n",
        "    if not frames: return []\n",
        "    img = Image.open(frames[0]).convert(\"RGB\").resize((256,256))\n",
        "    X = np.array(img).reshape(-1,3).astype(np.float32)\n",
        "    km = KMeans(n_clusters=n, n_init=3, random_state=42).fit(X)\n",
        "    return [\"#%02x%02x%02x\"%tuple(map(int,c)) for c in km.cluster_centers_]\n",
        "\n",
        "def ai_emocoes(video_path, sample_rate=15):\n",
        "    \"\"\"\n",
        "    Heur√≠stica leve com Mediapipe FaceMesh:\n",
        "    Marca 'surpresa' se boca muito aberta; sen√£o 'neutro/sorriso'.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fm = mp.solutions.face_mesh.FaceMesh(refine_landmarks=True)\n",
        "    emos=[]; frame_idx=0\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret: break\n",
        "        if frame_idx % sample_rate == 0:\n",
        "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            res = fm.process(rgb)\n",
        "            if res.multi_face_landmarks:\n",
        "                for lm in res.multi_face_landmarks:\n",
        "                    mouth = lm.landmark[13].y - lm.landmark[14].y\n",
        "                    emos.append(\"surpresa\" if mouth>0.03 else \"neutro/sorriso\")\n",
        "        frame_idx+=1\n",
        "    cap.release()\n",
        "    return dict(Counter(emos)) if emos else {}\n",
        "\n",
        "def ai_hook_score(texto_transcrito, ocr_primeiros_frames):\n",
        "    \"\"\"Pontua o gancho nos ~3‚Äì5s iniciais via padr√µes simples (sem internet).\"\"\"\n",
        "    early = (texto_transcrito[:220] + \" \" + \" \".join(ocr_primeiros_frames[:2])).lower()\n",
        "    score=0\n",
        "    for pat in [r\"\\?\", r\"\\bcomo\\b\", r\"\\bnunca\\b\", r\"\\bsempre\\b\", r\"\\bem\\s+\\d+\\s+passos\\b\", r\"\\b\\d+\\b\"]:\n",
        "        if re.search(pat, early):\n",
        "            score += 16\n",
        "    return min(score, 100)\n",
        "\n",
        "def ai_templates_recomendacoes(hook, ocr_count, emoc):\n",
        "    \"\"\"Gera recomenda√ß√µes humanizadas + templates estrat√©gicos.\"\"\"\n",
        "    recs=[]\n",
        "    if hook<50:\n",
        "        recs.append(\"Melhore o HOOK nos 3s: use pergunta direta ou n√∫mero espec√≠fico.\")\n",
        "    if ocr_count==0:\n",
        "        recs.append(\"Inclua texto na tela logo no in√≠cio (quem assiste no mudo entende a promessa).\")\n",
        "    if \"surpresa\" in emoc:\n",
        "        recs.append(\"Use um frame de 'surpresa' como thumbnail para maior CTR.\")\n",
        "    if not recs:\n",
        "        recs.append(\"Boa base! Teste varia√ß√µes de abertura e CTA para escalar.\")\n",
        "\n",
        "    templates=[\n",
        "        (\"Hook ‚Äî Pergunta\", \"Voc√™ sabia que [X]? Em [3 passos], voc√™ faz [Y]!\"),\n",
        "        (\"Hook ‚Äî N√∫mero\", \"Em apenas [5 minutos], aprenda a [benef√≠cio pr√°tico].\"),\n",
        "        (\"Hook ‚Äî Proibi√ß√£o\", \"Nunca fa√ßa [erro comum] se quiser [resultado].\"),\n",
        "        (\"CTA ‚Äî Final\", \"Se curtiu, compartilhe e me siga pra ver a parte 2 üöÄ\"),\n",
        "    ]\n",
        "    return recs, templates\n",
        "\n",
        "# ============================================================\n",
        "# N√∫cleo: processa todos os v√≠deos j√° catalogados pelo seu notebook\n",
        "# (usa .../dados/metadados_completos.json)\n",
        "# ============================================================\n",
        "\n",
        "def processar_ai_insights():\n",
        "    metapath = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "    if not os.path.exists(metapath):\n",
        "        print(f\"‚ùå N√£o encontrei {metapath}. Rode suas etapas at√© gerar metadados antes.\")\n",
        "        return []\n",
        "    with open(metapath, \"r\", encoding=\"utf-8\") as f:\n",
        "        metadados = json.load(f)\n",
        "\n",
        "    relatorios = []\n",
        "    for i, meta in enumerate(metadados, 1):\n",
        "        video_id = meta.get(\"id\")\n",
        "        video_path = meta.get(\"caminho_completo\")\n",
        "        if not (video_id and video_path and os.path.exists(video_path)):\n",
        "            print(f\"[{i}/{len(metadados)}] Pulando {video_id} (path inv√°lido).\")\n",
        "            continue\n",
        "\n",
        "        out_dir = os.path.join(AI_ASSETS_DIR, video_id)\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "        print(f\"[{i}/{len(metadados)}] IA ‚Üí {video_id}\")\n",
        "\n",
        "        # 1) Transcri√ß√£o (opcional/offline)\n",
        "        transcript = ai_transcrever_offline(video_path, out_dir)\n",
        "\n",
        "        # 2) Keyframes + OCR + Paleta\n",
        "        frames = ai_keyframes_por_tempo(video_path, out_dir, intervalo_seg=2)\n",
        "        ocr_texts = ai_ocr(frames)\n",
        "        palette = ai_paleta(frames, n=5)\n",
        "        # salvar thumbs b√°sicos\n",
        "        for j, fp in enumerate(frames[:3]):\n",
        "            try:\n",
        "                Image.open(fp).save(os.path.join(out_dir, f\"thumb_{j}.jpg\"), quality=95)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # 3) Emo√ß√µes faciais (heur√≠stica leve)\n",
        "        emotions = ai_emocoes(video_path, sample_rate=15)\n",
        "\n",
        "        # 4) Hook Score\n",
        "        hook = ai_hook_score(transcript, ocr_texts)\n",
        "\n",
        "        # 5) Recomenda√ß√µes + Templates\n",
        "        recs, templates = ai_templates_recomendacoes(hook, len(ocr_texts), emotions)\n",
        "\n",
        "        # Agrega relat√≥rio por v√≠deo\n",
        "        rep = {\n",
        "            \"video_id\": video_id,\n",
        "            \"nome_arquivo\": meta.get(\"nome_arquivo\"),\n",
        "            \"hook_score\": hook,\n",
        "            \"emocoes_predominantes\": emotions,\n",
        "            \"ocr_textos_count\": len(ocr_texts),\n",
        "            \"paleta_cores\": palette,\n",
        "            \"transcript_excerpt\": (transcript[:300] if transcript else \"\")\n",
        "        }\n",
        "        # Persist√™ncia por v√≠deo\n",
        "        with open(os.path.join(out_dir, \"ai_report.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(rep, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        # Para dashboard humanizada\n",
        "        rep[\"_recomendacoes\"] = recs\n",
        "        rep[\"_templates\"] = templates\n",
        "        relatorios.append(rep)\n",
        "\n",
        "    # Consolida JSON master\n",
        "    master_path = os.path.join(PASTA_TRABALHO, \"dados\", \"ai_insights_completos.json\")\n",
        "    with open(master_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(relatorios, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"\\nüíæ AI insights consolidados: {master_path}\")\n",
        "    return relatorios\n",
        "\n",
        "# ============================================================\n",
        "# Dashboard em Excel (humanizada) ‚Äî usa SEU OUTPUT_PATH\n",
        "# ============================================================\n",
        "\n",
        "def escrever_linha(ws, row, values, bold=False, wrap=False):\n",
        "    for col, val in enumerate(values, 1):\n",
        "        cell = ws.cell(row=row, column=col, value=val)\n",
        "        if bold: cell.font = Font(bold=True)\n",
        "        if wrap: cell.alignment = Alignment(wrap_text=True)\n",
        "\n",
        "def ajustar_larguras(ws, larguras):\n",
        "    for col_idx, width in enumerate(larguras, 1):\n",
        "        ws.column_dimensions[get_column_letter(col_idx)].width = width\n",
        "\n",
        "def atualizar_dashboard_humanizado(relatorios):\n",
        "    # Garante pasta do dashboard\n",
        "    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
        "    # Carrega workbook existente (do seu fluxo) ou cria novo\n",
        "    if os.path.exists(OUTPUT_PATH):\n",
        "        wb = load_workbook(OUTPUT_PATH)\n",
        "    else:\n",
        "        wb = Workbook()\n",
        "\n",
        "    # --- ABA 1: AI Insights (Resumo) ---\n",
        "    if \"AI Insights (Resumo)\" in wb.sheetnames:\n",
        "        del wb[\"AI Insights (Resumo)\"]\n",
        "    ws_resumo = wb.create_sheet(\"AI Insights (Resumo)\")\n",
        "\n",
        "    escrever_linha(ws_resumo, 1, [\"V√≠deo\", \"Hook (0-100)\", \"Emo√ß√µes\", \"OCR textos\", \"Paleta\", \"Trecho do Transcript\"], bold=True)\n",
        "    for i, r in enumerate(relatorios, start=2):\n",
        "        emo_k = \", \".join([f\"{k}:{v}\" for k,v in r[\"emocoes_predominantes\"].items()]) if r[\"emocoes_predominantes\"] else \"-\"\n",
        "        pal = \" \".join(r[\"paleta_cores\"]) if r[\"paleta_cores\"] else \"-\"\n",
        "        escrever_linha(ws_resumo, i, [\n",
        "            r.get(\"nome_arquivo\", r[\"video_id\"]),\n",
        "            r[\"hook_score\"],\n",
        "            emo_k,\n",
        "            r[\"ocr_textos_count\"],\n",
        "            pal,\n",
        "            r[\"transcript_excerpt\"]\n",
        "        ], wrap=True)\n",
        "    ajustar_larguras(ws_resumo, [36, 14, 28, 12, 28, 80])\n",
        "\n",
        "    # --- ABA 2: Recomenda√ß√µes (Humanizadas) ---\n",
        "    if \"Recomenda√ß√µes (Humanizadas)\" in wb.sheetnames:\n",
        "        del wb[\"Recomenda√ß√µes (Humanizadas)\"]\n",
        "    ws_recs = wb.create_sheet(\"Recomenda√ß√µes (Humanizadas)\")\n",
        "    escrever_linha(ws_recs, 1, [\"V√≠deo\", \"Recomenda√ß√µes pr√°ticas\"], bold=True)\n",
        "    row = 2\n",
        "    for r in relatorios:\n",
        "        if r[\"_recomendacoes\"]:\n",
        "            escrever_linha(ws_recs, row, [r.get(\"nome_arquivo\", r[\"video_id\"]), \" ‚Ä¢ \" + \"\\n ‚Ä¢ \".join(r[\"_recomendacoes\"])], wrap=True)\n",
        "            row += 1\n",
        "    ajustar_larguras(ws_recs, [36, 100])\n",
        "\n",
        "    # --- ABA 3: Templates Estrat√©gicos ---\n",
        "    if \"Templates (Estrat√©gia)\" in wb.sheetnames:\n",
        "        del wb[\"Templates (Estrat√©gia)\"]\n",
        "    ws_tpl = wb.create_sheet(\"Templates (Estrat√©gia)\")\n",
        "    escrever_linha(ws_tpl, 1, [\"Tipo\", \"Template\"], bold=True)\n",
        "    # Deduplica templates\n",
        "    vistos=set(); row=2\n",
        "    for r in relatorios:\n",
        "        for tipo, tpl in r[\"_templates\"]:\n",
        "            key=(tipo, tpl)\n",
        "            if key in vistos:\n",
        "                continue\n",
        "            vistos.add(key)\n",
        "            escrever_linha(ws_tpl, row, [tipo, tpl], wrap=True); row+=1\n",
        "    ajustar_larguras(ws_tpl, [28, 100])\n",
        "\n",
        "    # Estiliza cabe√ßalho das abas\n",
        "    for nome in [\"AI Insights (Resumo)\", \"Recomenda√ß√µes (Humanizadas)\", \"Templates (Estrat√©gia)\"]:\n",
        "        ws = wb[nome]\n",
        "        cell = ws.cell(row=1, column=1)\n",
        "        cell.font = Font(bold=True, size=14, color=\"FFFFFF\")\n",
        "        cell.fill = PatternFill(start_color=\"3F6AB3\", end_color=\"3F6AB3\", fill_type=\"solid\")\n",
        "\n",
        "    wb.save(OUTPUT_PATH)\n",
        "    print(f\"‚úÖ Dashboard humanizada atualizada em: {OUTPUT_PATH}\")\n",
        "\n",
        "# ============================================================\n",
        "# EXECU√á√ÉO (chame ap√≥s a Layer 4 do seu notebook)\n",
        "# ============================================================\n",
        "\n",
        "def rodar_layer_5_6_ai():\n",
        "    rel = processar_ai_insights()\n",
        "    if rel:\n",
        "        atualizar_dashboard_humanizado(rel)\n",
        "    else:\n",
        "        print(\"Sem relat√≥rios para escrever no Excel. Verifique se metadados_completos.json existe.\")\n",
        "\n",
        "# Exemplo de chamada:\n",
        "# rodar_layer_5_6_ai()\n"
      ],
      "metadata": {
        "id": "Ns9Xl0Uk7_Cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rodar_layer_5_6_ai()"
      ],
      "metadata": {
        "id": "v_QVY6i-8qLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# LAYER 7 ‚Äî IA ONLINE (FREE) ‚Ä¢ An√°lise sem√¢ntica avan√ßada\n",
        "# - Sem likes / coment√°rios / views.\n",
        "# - Usa Hugging Face Inference API (gr√°tis com token).\n",
        "# - Respeita PASTA_TRABALHO e OUTPUT_PATH do seu projeto.\n",
        "# ============================================================\n",
        "\n",
        "!pip -q install openpyxl==3.1.2 requests==2.32.3\n",
        "\n",
        "import os, re, json, time, math\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any\n",
        "from openpyxl import load_workbook, Workbook\n",
        "from openpyxl.utils import get_column_letter\n",
        "from openpyxl.styles import Font, Alignment, PatternFill\n",
        "\n",
        "# ---------- Conex√£o com seu notebook ----------\n",
        "assert 'PASTA_TRABALHO' in globals(), \"PASTA_TRABALHO n√£o est√° definido. Rode as c√©lulas de configura√ß√£o.\"\n",
        "if 'OUTPUT_PATH' not in globals():\n",
        "    OUTPUT_PATH = os.path.join(PASTA_TRABALHO, \"dashboard\", \"DASHBOARD_MASTER_EXECUTIVO_INTELIGENTE.xlsx\")\n",
        "    print(f\"‚ö†Ô∏è OUTPUT_PATH n√£o estava definido; criando autom√°tico: {OUTPUT_PATH}\")\n",
        "\n",
        "DADOS_DIR = os.path.join(PASTA_TRABALHO, \"dados\")\n",
        "AI_ONLINE_DIR = os.path.join(PASTA_TRABALHO, \"ai_online\")\n",
        "os.makedirs(DADOS_DIR, exist_ok=True)\n",
        "os.makedirs(AI_ONLINE_DIR, exist_ok=True)\n",
        "\n",
        "META_PATH = os.path.join(DADOS_DIR, \"metadados_completos.json\")\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = \"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\"   # coloque seu token\n",
        "# ---------- Config da IA Online (FREE) ----------\n",
        "# Cadastre-se gr√°tis na Hugging Face e crie um Access Token (Settings > Access Tokens).\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"\").strip()\n",
        "if not HF_TOKEN:\n",
        "    print(\"‚ö†Ô∏è Defina seu token gratuito da Hugging Face em os.environ['HF_TOKEN'] para ativar a IA online.\")\n",
        "\n",
        "# Modelo p√∫blico e gratuito (ajuste se quiser)\n",
        "# Recomendo um instru√≠do e leve para PT/ES/EN; Mistral 7B Instruct costuma funcionar bem:\n",
        "HF_MODEL = os.environ.get(\"HF_MODEL_ID\", \"mistralai/Mistral-7B-Instruct-v0.3\")\n",
        "HF_URL = f\"https://api-inference.huggingface.co/models/{HF_MODEL}\"\n",
        "\n",
        "def hf_generate(prompt: str, max_new_tokens=650, temperature=0.3, top_p=0.9, retries=2) -> str:\n",
        "    \"\"\"\n",
        "    Chama o endpoint de gera√ß√£o da Hugging Face (gratuito com token).\n",
        "    Retorna string gerada (sem garantias de JSON formatado ‚Äî faremos parsing).\n",
        "    \"\"\"\n",
        "    if not HF_TOKEN:\n",
        "        raise RuntimeError(\"HF_TOKEN n√£o definido. Configure os.environ['HF_TOKEN'] com seu token gratuito.\")\n",
        "    headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
        "    payload = {\n",
        "        \"inputs\": prompt,\n",
        "        \"parameters\": {\n",
        "            \"max_new_tokens\": max_new_tokens,\n",
        "            \"temperature\": temperature,\n",
        "            \"top_p\": top_p,\n",
        "            \"return_full_text\": False\n",
        "        }\n",
        "    }\n",
        "    for _ in range(retries):\n",
        "        r = requests.post(HF_URL, headers=headers, json=payload, timeout=90)\n",
        "        if r.status_code == 200:\n",
        "            try:\n",
        "                out = r.json()\n",
        "                if isinstance(out, list) and out and \"generated_text\" in out[0]:\n",
        "                    return out[0][\"generated_text\"]\n",
        "                if isinstance(out, dict) and \"generated_text\" in out:\n",
        "                    return out[\"generated_text\"]\n",
        "                # alguns servidores retornam str direta\n",
        "                if isinstance(out, str):\n",
        "                    return out\n",
        "            except Exception:\n",
        "                return r.text\n",
        "        time.sleep(2)\n",
        "    # retorna texto cru (pode conter erro do modelo)\n",
        "    return r.text\n",
        "\n",
        "def try_json_extract(text: str) -> Any:\n",
        "    \"\"\"\n",
        "    Extrai o primeiro JSON v√°lido de uma string. Robustifica contra respostas com texto extra.\n",
        "    \"\"\"\n",
        "    start = text.find(\"{\")\n",
        "    end   = text.rfind(\"}\")\n",
        "    if start == -1 or end == -1 or end <= start:\n",
        "        return None\n",
        "    snippet = text[start:end+1]\n",
        "    try:\n",
        "        return json.loads(snippet)\n",
        "    except Exception:\n",
        "        # tentativa: aspas simples -> duplas\n",
        "        snippet2 = snippet.replace(\"'\", '\"')\n",
        "        try:\n",
        "            return json.loads(snippet2)\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "# ---------- Utilit√°rios de I/O ----------\n",
        "def ler_metadados() -> List[Dict[str,Any]]:\n",
        "    if not os.path.exists(META_PATH):\n",
        "        print(f\"‚ùå N√£o encontrei {META_PATH}. Rode as camadas anteriores.\")\n",
        "        return []\n",
        "    with open(META_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def encontrar_transcricao(video_id: str) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Procura transcript/srt da sua Layer 5:\n",
        "      - PASTA_TRABALHO/ai_insights/<video_id>/transcript.txt\n",
        "      - PASTA_TRABALHO/ai_insights/<video_id>/subtitles.srt\n",
        "    Retorna dict com 'plain' e 'srt' (quando houver).\n",
        "    \"\"\"\n",
        "    base = os.path.join(PASTA_TRABALHO, \"ai_insights\", video_id)\n",
        "    out = {\"plain\": \"\", \"srt\": \"\"}\n",
        "    if os.path.isdir(base):\n",
        "        pt = os.path.join(base, \"transcript.txt\")\n",
        "        ps = os.path.join(base, \"subtitles.srt\")\n",
        "        if os.path.exists(pt):\n",
        "            out[\"plain\"] = Path(pt).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "        if os.path.exists(ps):\n",
        "            out[\"srt\"] = Path(ps).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "    return out\n",
        "\n",
        "def srt_para_blocos(srt_text: str, janela_seg=20, max_blocos=6) -> List[Dict[str,Any]]:\n",
        "    \"\"\"\n",
        "    Junta legendas em blocos de ~janela_seg segundos (at√© max_blocos) para an√°lise por cena.\n",
        "    \"\"\"\n",
        "    if not srt_text.strip():\n",
        "        return []\n",
        "    # parse simples\n",
        "    entries = []\n",
        "    for chunk in re.split(r\"\\n\\s*\\n\", srt_text.strip()):\n",
        "        lines = [l.strip() for l in chunk.splitlines() if l.strip()]\n",
        "        if len(lines) >= 2:\n",
        "            ts = lines[1]\n",
        "            m = re.match(r\"(\\d{2}):(\\d{2}):(\\d{2}),\\d+\\s*-->\\s*(\\d{2}):(\\d{2}):(\\d{2}),\\d+\", ts)\n",
        "            if not m:\n",
        "                continue\n",
        "            h1,m1,s1, h2,m2,s2 = map(int, m.groups())\n",
        "            start = h1*3600+m1*60+s1\n",
        "            end   = h2*3600+m2*60+s2\n",
        "            text  = \" \".join(lines[2:])\n",
        "            entries.append((start, end, text))\n",
        "    # agrega em janelas\n",
        "    if not entries:\n",
        "        return []\n",
        "    t0 = entries[0][0]\n",
        "    blocos = []\n",
        "    cur_t0 = t0\n",
        "    cur_txt = []\n",
        "    for st, en, txt in entries:\n",
        "        if (en - cur_t0) <= janela_seg:\n",
        "            cur_txt.append(txt)\n",
        "        else:\n",
        "            blocos.append({\"inicio_seg\": cur_t0, \"fim_seg\": en, \"texto\": \" \".join(cur_txt)})\n",
        "            cur_t0 = en\n",
        "            cur_txt = [txt]\n",
        "    if cur_txt:\n",
        "        blocos.append({\"inicio_seg\": cur_t0, \"fim_seg\": entries[-1][1], \"texto\": \" \".join(cur_txt)})\n",
        "    return blocos[:max_blocos]\n",
        "\n",
        "# ---------- Prompts ----------\n",
        "PROMPT_MACRO = \"\"\"Voc√™ √© um analista s√™nior de roteiro e comunica√ß√£o em pt-BR.\n",
        "Analise a TRANSCRI√á√ÉO a seguir e produza apenas um JSON com os campos:\n",
        "\n",
        "{\n",
        "  \"tema_central\": \"\",\n",
        "  \"tese\": \"\",\n",
        "  \"promessa\": \"\",\n",
        "  \"publico_alvo\": \"\",\n",
        "  \"dor_principal\": \"\",\n",
        "  \"ganho_principal\": \"\",\n",
        "  \"mecanismo_unico\": \"\",\n",
        "  \"provas_apoio\": [\"\", \"\"],\n",
        "  \"tom_de_voz\": [\"\", \"\"],\n",
        "  \"frameworks_copy\": [\"AIDA\",\"PAS\",\"FAB\",\"Story\",\"Lista\",\"How-To\"],\n",
        "  \"estrutura_geral\": [\n",
        "    {\"bloco\": 1, \"objetivo\": \"\", \"ideias_chave\": [\"\",\"\",\"\"], \"frases_de_efeito\": [\"\",\"\"]}\n",
        "  ],\n",
        "  \"obje√ß√µes_previstas\": [\"\",\"\",\"\"],\n",
        "  \"oportunidades_melhoria\": [\"\",\"\",\"\"],\n",
        "  \"analogias_recomendadas\": [\"\",\"\",\"\"],\n",
        "  \"cta_detectadas\": [\"\",\"\",\"\"]\n",
        "}\n",
        "\n",
        "TRANSCRI√á√ÉO:\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_SUGESTOES = \"\"\"Voc√™ √© um roteirista s√™nior para v√≠deos curtos em pt-BR.\n",
        "Usando a TRANSCRI√á√ÉO (e os insights macro abaixo), gere apenas um JSON:\n",
        "\n",
        "INSIGHTS_MACRO:\n",
        "{macro}\n",
        "\n",
        "TRANSCRI√á√ÉO:\n",
        "{transc}\n",
        "\n",
        "JSON com:\n",
        "{\n",
        "  \"hooks_reativos\": [\"5 varia√ß√µes objetivas, curtas, com n√∫meros ou pergunta\"],\n",
        "  \"texto_na_tela_3s\": [\"3 frases de 5‚Äì7 palavras para aparecer em 3s\"],\n",
        "  \"roteiro_15s\": [\"linha-a-linha do que dizer/fazer\", \"...\"],\n",
        "  \"roteiro_30s\": [\"linha-a-linha do que dizer/fazer\", \"...\"],\n",
        "  \"analogias\": [\"3 ideias de analogias concretas\"],\n",
        "  \"oportunidades\": [\"3 oportunidades espec√≠ficas de melhoria do roteiro\"]\n",
        "}\n",
        "Responda s√≥ com JSON.\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_CENA = \"\"\"Voc√™ √© um editor-chefe. Para o trecho abaixo, devolva JSON:\n",
        "\n",
        "TRECHO:\n",
        "{trecho}\n",
        "\n",
        "JSON:\n",
        "{\n",
        "  \"objetivo_do_trecho\": \"\",\n",
        "  \"ponto_principal\": \"\",\n",
        "  \"melhorias_de_copy\": [\"\",\"\",\"\"],\n",
        "  \"texto_na_tela_sugerido\": [\"\",\"\"],\n",
        "  \"cta_sugerida\": \"\"\n",
        "}\n",
        "Responda s√≥ JSON.\n",
        "\"\"\"\n",
        "\n",
        "# ---------- Execu√ß√£o por v√≠deo ----------\n",
        "def analisar_video_online(video_id: str, nome_arquivo: str) -> Dict[str,Any]:\n",
        "    out_dir = os.path.join(AI_ONLINE_DIR, video_id)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # Carrega transcri√ß√£o\n",
        "    tr = encontrar_transcricao(video_id)\n",
        "    texto = tr[\"plain\"] or \"\"\n",
        "    srt  = tr[\"srt\"] or \"\"\n",
        "    if not (texto or srt):\n",
        "        print(f\"‚ö†Ô∏è {video_id}: sem transcript/srt. Pulei IA online.\")\n",
        "        return {}\n",
        "\n",
        "    # 1) Macro\n",
        "    macro_prompt = PROMPT_MACRO + (texto[:6000] if texto else srt[:6000])\n",
        "    macro_raw = hf_generate(macro_prompt, max_new_tokens=700, temperature=0.25) if HF_TOKEN else \"{}\"\n",
        "    macro = try_json_extract(macro_raw) or {}\n",
        "\n",
        "    # 2) Sugest√µes (usa texto e macro)\n",
        "    sug_prompt = PROMPT_SUGESTOES.format(macro=json.dumps(macro, ensure_ascii=False), transc=(texto[:4000] if texto else srt[:4000]))\n",
        "    sug_raw = hf_generate(sug_prompt, max_new_tokens=700, temperature=0.4) if HF_TOKEN else \"{}\"\n",
        "    sugestoes = try_json_extract(sug_raw) or {}\n",
        "\n",
        "    # 3) Cenas (usa SRT em blocos)\n",
        "    cenas = []\n",
        "    blocos = srt_para_blocos(srt, janela_seg=20, max_blocos=6)\n",
        "    for b in blocos:\n",
        "        p = PROMPT_CENA.format(trecho=b[\"texto\"][:1200])\n",
        "        raw = hf_generate(p, max_new_tokens=350, temperature=0.35) if HF_TOKEN else \"{}\"\n",
        "        j = try_json_extract(raw) or {}\n",
        "        cenas.append({\n",
        "            \"inicio_seg\": b[\"inicio_seg\"], \"fim_seg\": b[\"fim_seg\"], **j\n",
        "        })\n",
        "\n",
        "    # Salva JSON e MD por v√≠deo\n",
        "    pack = {\n",
        "        \"video_id\": video_id,\n",
        "        \"nome_arquivo\": nome_arquivo,\n",
        "        \"macro\": macro,\n",
        "        \"sugestoes\": sugestoes,\n",
        "        \"cenas\": cenas\n",
        "    }\n",
        "    Path(os.path.join(out_dir, \"online_llm_report.json\")).write_text(json.dumps(pack, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "    md = [\n",
        "        f\"# IA Online ‚Äî {video_id}\",\n",
        "        \"## Macro\",\n",
        "        json.dumps(macro, ensure_ascii=False, indent=2),\n",
        "        \"## Sugest√µes\",\n",
        "        json.dumps(sugestoes, ensure_ascii=False, indent=2),\n",
        "        \"## Cenas\",\n",
        "        json.dumps(cenas, ensure_ascii=False, indent=2),\n",
        "    ]\n",
        "    Path(os.path.join(out_dir, \"online_llm_report.md\")).write_text(\"\\n\\n\".join(md), encoding=\"utf-8\")\n",
        "\n",
        "    return pack\n",
        "\n",
        "# ---------- Atualiza Excel ----------\n",
        "def _xl_set_width(ws, widths):\n",
        "    for i,w in enumerate(widths,1):\n",
        "        ws.column_dimensions[get_column_letter(i)].width = w\n",
        "\n",
        "def atualizar_excel_online(pacotes: List[Dict[str,Any]]):\n",
        "    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
        "    wb = load_workbook(OUTPUT_PATH) if os.path.exists(OUTPUT_PATH) else Workbook()\n",
        "\n",
        "    # --- Aba 1: IA Online ‚Äî Macro ---\n",
        "    if \"IA Online ‚Äî Macro\" in wb.sheetnames: del wb[\"IA Online ‚Äî Macro\"]\n",
        "    ws1 = wb.create_sheet(\"IA Online ‚Äî Macro\")\n",
        "    header1 = [\"V√≠deo\",\"Tema\",\"Tese\",\"Promessa\",\"P√∫blico\",\"Dor\",\"Ganho\",\"Mecanismo √∫nico\",\"Provas\",\"Tom\",\"Frameworks\",\"Obje√ß√µes\",\"Oportunidades\",\"Analogias\",\"CTAs\",\"Estrutura (blocos)\"]\n",
        "    for c,h in enumerate(header1,1):\n",
        "        cell = ws1.cell(row=1, column=c, value=h); cell.font=Font(bold=True)\n",
        "    row=2\n",
        "    for p in pacotes:\n",
        "        m = p.get(\"macro\", {})\n",
        "        ws1.cell(row=row, column=1, value=p.get(\"nome_arquivo\", p.get(\"video_id\")))\n",
        "        ws1.cell(row=row, column=2, value=m.get(\"tema_central\"))\n",
        "        ws1.cell(row=row, column=3, value=m.get(\"tese\"))\n",
        "        ws1.cell(row=row, column=4, value=m.get(\"promessa\"))\n",
        "        ws1.cell(row=row, column=5, value=m.get(\"publico_alvo\"))\n",
        "        ws1.cell(row=row, column=6, value=m.get(\"dor_principal\"))\n",
        "        ws1.cell(row=row, column=7, value=m.get(\"ganho_principal\"))\n",
        "        ws1.cell(row=row, column=8, value=m.get(\"mecanismo_unico\"))\n",
        "        ws1.cell(row=row, column=9, value=\", \".join(m.get(\"provas_apoio\",[]) or []))\n",
        "        ws1.cell(row=row, column=10, value=\", \".join(m.get(\"tom_de_voz\",[]) or []))\n",
        "        ws1.cell(row=row, column=11, value=\", \".join(m.get(\"frameworks_copy\",[]) or []))\n",
        "        ws1.cell(row=row, column=12, value=\", \".join(m.get(\"obje√ß√µes_previstas\",[]) or []))\n",
        "        ws1.cell(row=row, column=13, value=\", \".join(m.get(\"oportunidades_melhoria\",[]) or []))\n",
        "        ws1.cell(row=row, column=14, value=\", \".join(m.get(\"analogias_recomendadas\",[]) or []))\n",
        "        ws1.cell(row=row, column=15, value=\", \".join(m.get(\"cta_detectadas\",[]) or []))\n",
        "        # estrutura compactada\n",
        "        estrutura = m.get(\"estrutura_geral\", [])\n",
        "        ws1.cell(row=row, column=16, value=json.dumps(estrutura, ensure_ascii=False))\n",
        "        row += 1\n",
        "    _xl_set_width(ws1, [30,18,18,20,18,18,18,20,24,16,16,18,22,18,18,48])\n",
        "\n",
        "    # --- Aba 2: IA Online ‚Äî Sugest√µes ---\n",
        "    if \"IA Online ‚Äî Sugest√µes\" in wb.sheetnames: del wb[\"IA Online ‚Äî Sugest√µes\"]\n",
        "    ws2 = wb.create_sheet(\"IA Online ‚Äî Sugest√µes\")\n",
        "    header2 = [\"V√≠deo\",\"Hooks (5)\",\"Texto na tela 3s (3)\",\"Roteiro 15s\",\"Roteiro 30s\",\"Analogias\",\"Oportunidades\"]\n",
        "    for c,h in enumerate(header2,1):\n",
        "        ws2.cell(row=1, column=c, value=h).font=Font(bold=True)\n",
        "    row=2\n",
        "    for p in pacotes:\n",
        "        s = p.get(\"sugestoes\", {})\n",
        "        ws2.cell(row=row, column=1, value=p.get(\"nome_arquivo\", p.get(\"video_id\")))\n",
        "        ws2.cell(row=row, column=2, value=\" ‚Ä¢ \" + \"\\n ‚Ä¢ \".join(s.get(\"hooks_reativos\",[]) or []))\n",
        "        ws2.cell(row=row, column=3, value=\" ‚Ä¢ \" + \"\\n ‚Ä¢ \".join(s.get(\"texto_na_tela_3s\",[]) or []))\n",
        "        ws2.cell(row=row, column=4, value=\" ‚Ä¢ \" + \"\\n ‚Ä¢ \".join(s.get(\"roteiro_15s\",[]) or []))\n",
        "        ws2.cell(row=row, column=5, value=\" ‚Ä¢ \" + \"\\n ‚Ä¢ \".join(s.get(\"roteiro_30s\",[]) or []))\n",
        "        ws2.cell(row=row, column=6, value=\" ‚Ä¢ \" + \"\\n ‚Ä¢ \".join(s.get(\"analogias\",[]) or []))\n",
        "        ws2.cell(row=row, column=7, value=\" ‚Ä¢ \" + \"\\n ‚Ä¢ \".join(s.get(\"oportunidades\",[]) or []))\n",
        "        row+=1\n",
        "    _xl_set_width(ws2, [30,54,40,60,60,40,40])\n",
        "\n",
        "    # --- Aba 3: IA Online ‚Äî Cenas ---\n",
        "    if \"IA Online ‚Äî Cenas\" in wb.sheetnames: del wb[\"IA Online ‚Äî Cenas\"]\n",
        "    ws3 = wb.create_sheet(\"IA Online ‚Äî Cenas\")\n",
        "    header3 = [\"V√≠deo\",\"In√≠cio (s)\",\"Fim (s)\",\"Objetivo do trecho\",\"Ponto principal\",\"Melhorias de copy\",\"Texto na tela sugerido\",\"CTA sugerida\"]\n",
        "    for c,h in enumerate(header3,1):\n",
        "        ws3.cell(row=1, column=c, value=h).font=Font(bold=True)\n",
        "    row=2\n",
        "    for p in pacotes:\n",
        "        for c in (p.get(\"cenas\") or []):\n",
        "            ws3.cell(row=row, column=1, value=p.get(\"nome_arquivo\", p.get(\"video_id\")))\n",
        "            ws3.cell(row=row, column=2, value=c.get(\"inicio_seg\"))\n",
        "            ws3.cell(row=row, column=3, value=c.get(\"fim_seg\"))\n",
        "            ws3.cell(row=row, column=4, value=c.get(\"objetivo_do_trecho\"))\n",
        "            ws3.cell(row=row, column=5, value=c.get(\"ponto_principal\"))\n",
        "            ws3.cell(row=row, column=6, value=\"; \".join(c.get(\"melhorias_de_copy\",[]) or []))\n",
        "            ws3.cell(row=row, column=7, value=\"; \".join(c.get(\"texto_na_tela_sugerido\",[]) or []))\n",
        "            ws3.cell(row=row, column=8, value=c.get(\"cta_sugerida\"))\n",
        "            row+=1\n",
        "    _xl_set_width(ws3, [30,10,10,40,40,50,40,24])\n",
        "\n",
        "    wb.save(OUTPUT_PATH)\n",
        "    print(f\"‚úÖ Excel atualizado com abas: IA Online ‚Äî Macro / Sugest√µes / Cenas ‚Üí {OUTPUT_PATH}\")\n",
        "\n",
        "# ---------- Orquestra√ß√£o ----------\n",
        "def rodar_layer_7_online_free():\n",
        "    metas = ler_metadados()\n",
        "    if not metas:\n",
        "        return\n",
        "    pacotes = []\n",
        "    for i, m in enumerate(metas, 1):\n",
        "        vid = m.get(\"id\"); nome = m.get(\"nome_arquivo\", vid)\n",
        "        if not vid:\n",
        "            continue\n",
        "        print(f\"[{i}/{len(metas)}] IA Online (FREE) ‚Üí {vid}\")\n",
        "        try:\n",
        "            p = analisar_video_online(vid, nome)\n",
        "            if p: pacotes.append(p)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Falha em {vid}: {e}\")\n",
        "    if pacotes:\n",
        "        Path(os.path.join(DADOS_DIR, \"ai_online_insights.json\")).write_text(json.dumps(pacotes, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "        atualizar_excel_online(pacotes)\n",
        "    else:\n",
        "        print(\"Nada processado (sem transcri√ß√µes ou sem token HF).\")\n"
      ],
      "metadata": {
        "id": "qsF_FvEM-26P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rodar_layer_7_online_free()"
      ],
      "metadata": {
        "id": "rlLvaS5wFn5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2teNlxNsFsoK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
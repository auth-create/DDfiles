{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/auth-create/DDfiles/blob/main/backup_engenharia_reversa_videos_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SISTEMA MODULAR DE ENGENHARIA REVERSA DE VÍDEOS - VERSÃO FINAL OTIMIZADA\n",
        "\n",
        "Este notebook foi aprimorado para oferecer uma experiência mais intuitiva, organizada e robusta para a engenharia reversa de vídeos. Cada etapa é modular, com validações de pré-requisitos e feedback em tempo real para guiá-lo(a) durante o processo.\n",
        "\n",
        "## COMO USAR:\n",
        "1.  **Execute as células em ordem, de cima para baixo.** Cada célula foi projetada para ser executada sequencialmente.\n",
        "2.  **Atenção aos feedbacks:** Mensagens claras indicarão o sucesso de cada etapa, possíveis erros e qual a **PRÓXIMA CÉLULA** a ser executada.\n",
        "3.  **Corrija e re-execute:** Se um erro for detectado, uma mensagem explicativa será exibida. Corrija o problema (geralmente um caminho incorreto ou dependência ausente) e re-execute a célula que falhou.\n",
        "4.  **Progresso Salvo:** O sistema salva automaticamente o progresso e os dados gerados em cada etapa, permitindo que você retome de onde parou.\n",
        "\n",
        "## ESTRUTURA DO PROCESSO (Layers e Sublayers):\n",
        "Este sistema é organizado em camadas lógicas para facilitar o entendimento e a execução:\n",
        "\n",
        "### LAYER 1: CONFIGURAÇÃO E PREPARAÇÃO\n",
        "*   **CÉLULA 1.1: SETUP INICIAL E INSTALAÇÃO DE DEPENDÊNCIAS**\n",
        "*   **CÉLULA 1.2: CONFIGURAÇÃO INICIAL E VALIDAÇÃO DA PASTA DE TRABALHO**\n",
        "\n",
        "### LAYER 2: DESCOBERTA E EXTRAÇÃO DE DADOS BRUTOS\n",
        "*   **CÉLULA 2.1: DESCOBERTA E CATALOGAÇÃO DE VÍDEOS**\n",
        "*   **CÉLULA 2.2: EXTRAÇÃO DE METADADOS DOS VÍDEOS**\n",
        "*   **CÉLULA 2.3: DECOMPOSIÇÃO DE VÍDEOS (FRAMES, ÁUDIO, TEXTO)**\n",
        "\n",
        "### LAYER 3: ANÁLISE E PROCESSAMENTO DE DADOS\n",
        "*   **CÉLULA 3.1: ANÁLISE DE PADRÕES (TEMPORAIS, VISUAIS, TEXTO, ÁUDIO)**\n",
        "*   **CÉLULA 3.2: ANÁLISE PSICOLÓGICA E GATILHOS DE ENGAJAMENTO**\n",
        "\n",
        "### LAYER 4: GERAÇÃO DE RELATÓRIOS E BLUEPRINT ESTRATÉGICO\n",
        "*   **CÉLULA 4.1: GERAÇÃO DE RELATÓRIOS HUMANIZADOS (ÁUDIO, VISUAL, TEXTO, PSICOLÓGICO)**\n",
        "*   **CÉLULA 4.2: GERAÇÃO DO BLUEPRINT FINAL E DASHBOARD**\n",
        "\n",
        "---\n",
        "\n",
        "*Lembre-se: Este sistema foi projetado para ser executado no Google Colab. Certifique-se de que seu ambiente está configurado corretamente.*"
      ],
      "metadata": {
        "id": "zx8sEBm8_yKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# LAYER 1: CONFIGURAÇÃO E PREPARAÇÃO\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# CÉLULA 1.1: SETUP INICIAL E INSTALAÇÃO DE DEPENDÊNCIAS\n",
        "# ============================================================================\n",
        "\n",
        "# Instalar dependências necessárias\n",
        "!pip install -q moviepy librosa pytesseract opencv-python pandas openpyxl matplotlib seaborn pillow SpeechRecognition pydub fpdf\n",
        "!apt-get update -qq && apt-get install -y -qq tesseract-ocr tesseract-ocr-por ffmpeg\n",
        "\n",
        "# Imports necessários\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import logging\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pytesseract\n",
        "import librosa\n",
        "from moviepy.editor import VideoFileClip\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import speech_recognition as sr # Adicionado import para SpeechRecognition\n",
        "# Montar Google Drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"✅ Google Drive montado com sucesso!\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ ERRO ao montar Google Drive: {e}. Por favor, verifique sua conexão ou permissões.\")\n",
        "\n",
        "print(\n",
        "\"✅ SETUP INICIAL CONCLUÍDO!\")\n",
        "print(\"Todas as dependências foram instaladas e o Google Drive foi montado.\")\n",
        "print(\"➡️ PRÓXIMA CÉLULA: 1.2 - CONFIGURAÇÃO INICIAL E VALIDAÇÃO DA PASTA DE TRABALHO\")"
      ],
      "metadata": {
        "id": "setup_inicial",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b83e2ca3-2911-460c-91f0-1519fcbcca27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Selecting previously unselected package tesseract-ocr-por.\n",
            "(Reading database ... 126371 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-por_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-por (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-por (1:4.00~git30-7274cfa-1.1) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/moviepy/config_defaults.py:47: SyntaxWarning: invalid escape sequence '\\P'\n",
            "  IMAGEMAGICK_BINARY = r\"C:\\Program Files\\ImageMagick-6.8.8-Q16\\magick.exe\"\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:294: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  lines_video = [l for l in lines if ' Video: ' in l and re.search('\\d+x\\d+', l)]\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:367: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  rotation_lines = [l for l in lines if 'rotate          :' in l and re.search('\\d+$', l)]\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:370: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  match = re.search('\\d+$', rotation_line)\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Google Drive montado com sucesso!\n",
            "✅ SETUP INICIAL CONCLUÍDO!\n",
            "Todas as dependências foram instaladas e o Google Drive foi montado.\n",
            "➡️ PRÓXIMA CÉLULA: 1.2 - CONFIGURAÇÃO INICIAL E VALIDAÇÃO DA PASTA DE TRABALHO\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CÉLULA 1.2: CONFIGURAÇÃO INICIAL E VALIDAÇÃO DA PASTA DE TRABALHO\n",
        "# ============================================================================\n",
        "\n",
        "# ⚠️ **ATENÇÃO:** CONFIGURE SEU CAMINHO AQUI!\n",
        "# Substitua o caminho abaixo pela pasta onde seus vídeos estão localizados no Google Drive.\n",
        "# Exemplo: \"/content/drive/MyDrive/Meus Videos de Marketing\"\n",
        "CAMINHO_PASTA_VIDEOS = \"/content/drive/MyDrive/Videos Dona Done\" # ⬅️ **ALTERE AQUI**\n",
        "\n",
        "class ConfiguradorProjeto:\n",
        "    def __init__(self, caminho_pasta):\n",
        "        self.pasta_videos = self._validar_caminho(caminho_pasta)\n",
        "        self.pasta_trabalho = os.path.join(self.pasta_videos, \"_engenharia_reversa\")\n",
        "        self._criar_estrutura()\n",
        "        self._configurar_logging()\n",
        "\n",
        "    def _validar_caminho(self, caminho):\n",
        "        if caminho == \"/content/drive/MyDrive/Videos Dona Done\" and not os.path.exists(caminho):\n",
        "            raise ValueError(\"❌ ERRO: Você precisa alterar CAMINHO_PASTA_VIDEOS com o caminho real da sua pasta de vídeos no Google Drive. O caminho padrão não foi encontrado.\")\n",
        "\n",
        "        if not os.path.exists(caminho):\n",
        "            raise ValueError(f\"❌ ERRO: Pasta não encontrada: {caminho}. Por favor, verifique se o caminho está correto e se o Google Drive está montado.\")\n",
        "\n",
        "        return caminho\n",
        "\n",
        "    def _criar_estrutura(self):\n",
        "        # Estrutura de pastas conforme o anexo e requisitos do usuário\n",
        "        estrutura = [\n",
        "            \"config\", \"logs\", \"dados\", \"frames_extraidos\",\n",
        "            \"analise_texto\", \"analise_audio\", \"capturas\",\n",
        "            \"blueprint\", \"temp\", \"dashboard\", \"analise_psicologica\", \"analise_visual\"\n",
        "        ]\n",
        "\n",
        "        os.makedirs(self.pasta_trabalho, exist_ok=True)\n",
        "        for pasta in estrutura:\n",
        "            os.makedirs(os.path.join(self.pasta_trabalho, pasta), exist_ok=True)\n",
        "\n",
        "        # Criar subpastas para frames_extraidos (ex: vid_001_Nome_Do_Video/)\n",
        "        # Esta lógica será implementada na célula de decomposição de vídeos (CÉLULA 2.3)\n",
        "\n",
        "    def _configurar_logging(self):\n",
        "        log_file = os.path.join(self.pasta_trabalho, \"logs\", f\"sistema_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "            handlers=[logging.FileHandler(log_file, encoding='utf-8')]\n",
        "        )\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def salvar_configuracao(self):\n",
        "        config = {\n",
        "            \"projeto\": {\n",
        "                \"pasta_videos\": self.pasta_videos,\n",
        "                \"pasta_trabalho\": self.pasta_trabalho,\n",
        "                \"criado_em\": datetime.now().isoformat(),\n",
        "                \"versao\": \"modular_v2.0_otimizado\"\n",
        "            },\n",
        "            \"status_etapas\": {\n",
        "                \"configuracao\": True,\n",
        "                \"descoberta_videos\": False,\n",
        "                \"metadados\": False,\n",
        "                \"decomposicao\": False,\n",
        "                \"analise_padroes\": False,\n",
        "                \"analise_psicologica\": False,\n",
        "                \"relatorios_humanizados\": False,\n",
        "                \"blueprint\": False\n",
        "            }\n",
        "        }\n",
        "\n",
        "        config_path = os.path.join(self.pasta_trabalho, \"config\", \"config.json\")\n",
        "        with open(config_path, \"w\", encoding='utf-8') as f:\n",
        "            json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        return config_path\n",
        "\n",
        "# Executar configuração\n",
        "try:\n",
        "    configurador = ConfiguradorProjeto(CAMINHO_PASTA_VIDEOS)\n",
        "    config_path = configurador.salvar_configuracao()\n",
        "\n",
        "    print(\"\"\"\n",
        "✅ CONFIGURAÇÃO CONCLUÍDA!\"\"\")\n",
        "    print(f\"Pasta de trabalho criada: {configurador.pasta_trabalho}\")\n",
        "    print(f\"Configuração salva: {config_path}\")\n",
        "    print(\"\"\"\n",
        "➡️ PRÓXIMA CÉLULA: 2.1 - DESCOBERTA E CATALOGAÇÃO DE VÍDEOS\"\"\")\n",
        "\n",
        "    # Salvar variáveis globais para próximas células\n",
        "    global PASTA_VIDEOS, PASTA_TRABALHO\n",
        "    PASTA_VIDEOS = configurador.pasta_videos\n",
        "    PASTA_TRABALHO = configurador.pasta_trabalho\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\"\"\n",
        "❌ ERRO NA CONFIGURAÇÃO: {e}\"\"\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ],
      "metadata": {
        "id": "configuracao_inicial",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "878fba04-5d20-4fbe-c135-03087d865471"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ CONFIGURAÇÃO CONCLUÍDA!\n",
            "Pasta de trabalho criada: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa\n",
            "Configuração salva: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/config/config.json\n",
            "\n",
            "➡️ PRÓXIMA CÉLULA: 2.1 - DESCOBERTA E CATALOGAÇÃO DE VÍDEOS\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# LAYER 2: DESCOBERTA E EXTRAÇÃO DE DADOS BRUTOS\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# CÉLULA 2.1: DESCOBERTA E CATALOGAÇÃO DE VÍDEOS\n",
        "# ============================================================================\n",
        "\n",
        "def verificar_prerequisito_etapa(etapa_anterior):\n",
        "    \"\"\"Verifica se a etapa anterior foi executada com sucesso\"\"\"\n",
        "    try:\n",
        "        if not \"PASTA_TRABALHO\" in globals():\n",
        "            raise Exception(\"Variáveis globais de configuração não encontradas. Execute a CÉLULA 1.2 primeiro.\")\n",
        "\n",
        "        config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "        if not os.path.exists(config_path):\n",
        "            raise Exception(\"Arquivo de configuração não encontrado. Execute a CÉLULA 1.2 primeiro.\")\n",
        "\n",
        "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            config = json.load(f)\n",
        "\n",
        "        if not config[\"status_etapas\"][etapa_anterior]:\n",
        "            raise Exception(f\"A etapa \\\"{etapa_anterior}\\\" não foi concluída. Execute a célula correspondente primeiro.\")\n",
        "\n",
        "        return True, config\n",
        "    except Exception as e:\n",
        "        print(f\"❌ PRÉ-REQUISITO NÃO ATENDIDO: {e}\")\n",
        "        return False, None\n",
        "\n",
        "def descobrir_catalogar_videos():\n",
        "    \"\"\"Descobre e cataloga todos os vídeos na pasta\"\"\"\n",
        "    formatos_aceitos = [\".mp4\", \".mov\", \".avi\", \".mkv\", \".webm\", \".m4v\"]\n",
        "    videos_encontrados = []\n",
        "\n",
        "    print(f\"🔍 Iniciando descoberta de vídeos na pasta: {PASTA_VIDEOS}\")\n",
        "\n",
        "    for root, dirs, files in os.walk(PASTA_VIDEOS):\n",
        "        if \"_engenharia_reversa\" in root:\n",
        "            continue # Ignorar a pasta de trabalho do sistema\n",
        "\n",
        "        for file in files:\n",
        "            if any(file.lower().endswith(fmt) for fmt in formatos_aceitos):\n",
        "                video_path = os.path.join(root, file)\n",
        "\n",
        "                try:\n",
        "                    stat_info = os.stat(video_path)\n",
        "                    # Gerar ID baseado no nome do arquivo para melhor rastreamento\n",
        "                    video_name_clean = os.path.splitext(file)[0].replace(\" \", \"_\").replace(\".\", \"\")\n",
        "                    video_id = f\"vid_{video_name_clean}\"\n",
        "\n",
        "                    video_info = {\n",
        "                        \"id\": video_id,\n",
        "                        \"nome_arquivo\": file,\n",
        "                        \"caminho_completo\": video_path,\n",
        "                        \"caminho_relativo\": os.path.relpath(video_path, PASTA_VIDEOS),\n",
        "                        \"tamanho_mb\": round(stat_info.st_size / (1024*1024), 2),\n",
        "                        \"data_modificacao\": datetime.fromtimestamp(stat_info.st_mtime).isoformat(),\n",
        "                        \"extensao\": os.path.splitext(file)[1].lower(),\n",
        "                        \"status\": \"descoberto\"\n",
        "                    }\n",
        "\n",
        "                    videos_encontrados.append(video_info)\n",
        "                    print(f\"  ✅ Encontrado: {file}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"  ❌ Erro ao processar {file}: {e}\")\n",
        "                    continue\n",
        "\n",
        "    return videos_encontrados\n",
        "\n",
        "def salvar_lista_videos(videos):\n",
        "    \"\"\"Salva lista de vídeos encontrados\"\"\"\n",
        "    videos_path = os.path.join(PASTA_TRABALHO, \"dados\", \"videos_descobertos.json\")\n",
        "    with open(videos_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(videos, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    config[\"status_etapas\"][\"descoberta_videos\"] = True\n",
        "    config[\"total_videos_encontrados\"] = len(videos)\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    return videos_path\n",
        "\n",
        "# Executar descoberta\n",
        "prerequisito_ok, _ = verificar_prerequisito_etapa(\"configuracao\")\n",
        "\n",
        "if prerequisito_ok:\n",
        "    try:\n",
        "        videos_encontrados = descobrir_catalogar_videos()\n",
        "\n",
        "        if not videos_encontrados:\n",
        "            print(\"\"\"\n",
        "❌ NENHUM VÍDEO ENCONTRADO!\"\"\")\n",
        "            print(f\"Verifique se há vídeos na pasta configurada: {PASTA_VIDEOS}\")\n",
        "        else:\n",
        "            videos_path = salvar_lista_videos(videos_encontrados)\n",
        "\n",
        "            print(\"\"\"\n",
        "✅ DESCOBERTA DE VÍDEOS CONCLUÍDA!\"\"\")\n",
        "            print(f\"Total de vídeos encontrados: {len(videos_encontrados)}\")\n",
        "            print(f\"Lista de vídeos salva em: {videos_path}\")\n",
        "\n",
        "            # Mostrar resumo\n",
        "            extensoes = Counter([v[\"extensao\"] for v in videos_encontrados])\n",
        "            print(f\"Formatos encontrados: {dict(extensoes)}\")\n",
        "            print(\"\"\"\n",
        "➡️ PRÓXIMA CÉLULA: 2.2 - EXTRAÇÃO DE METADADOS DOS VÍDEOS\"\"\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\"\"\n",
        "❌ ERRO NA DESCOBERTA DE VÍDEOS: {e}\"\"\")\n",
        "        print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ],
      "metadata": {
        "id": "descoberta_videos",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "438839ad-993c-4ed7-ae08-d17e631cc5ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Iniciando descoberta de vídeos na pasta: /content/drive/MyDrive/Videos Dona Done\n",
            "  ✅ Encontrado: ate quando voce vai ficar culpando os outros.mp4\n",
            "  ✅ Encontrado: coloque metas em sua vida e se surpreenda.mp4\n",
            "  ✅ Encontrado: a importancia de ser rico antes de ter.mp4\n",
            "  ✅ Encontrado: as três fases de todo mundo que decidiu fazer alguma coisa..mp4\n",
            "  ✅ Encontrado: a melhor saida é se afastar de pessoas perversas.mp4\n",
            "\n",
            "✅ DESCOBERTA DE VÍDEOS CONCLUÍDA!\n",
            "Total de vídeos encontrados: 5\n",
            "Lista de vídeos salva em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/videos_descobertos.json\n",
            "Formatos encontrados: {'.mp4': 5}\n",
            "\n",
            "➡️ PRÓXIMA CÉLULA: 2.2 - EXTRAÇÃO DE METADADOS DOS VÍDEOS\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CÉLULA 2.2: EXTRAÇÃO DE METADADOS DOS VÍDEOS\n",
        "# ============================================================================\n",
        "\n",
        "def extrair_metadados_video(video_info):\n",
        "    \"\"\"Extrai metadados técnicos de um vídeo\"\"\"\n",
        "    video_path = video_info[\"caminho_completo\"]\n",
        "    video_id = video_info[\"id\"]\n",
        "\n",
        "    print(f\"  ⚙️ Extraindo metadados para: {video_info[\"nome_arquivo\"]}\")\n",
        "\n",
        "    # Análise com OpenCV\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise Exception(\"Não foi possível abrir o vídeo. Verifique o caminho ou a integridade do arquivo.\")\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    largura = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    altura = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    duracao = frame_count / fps if fps > 0 else 0\n",
        "\n",
        "    # Capturar primeiro frame\n",
        "    ret, primeiro_frame = cap.read()\n",
        "    cap.release()\n",
        "\n",
        "    # Análise de áudio\n",
        "    try:\n",
        "        clip = VideoFileClip(video_path)\n",
        "        tem_audio = clip.audio is not None\n",
        "        clip.close()\n",
        "    except Exception as e:\n",
        "        print(f\"    ⚠️ Aviso: Não foi possível analisar áudio para {video_info[\"nome_arquivo\"]}: {e}\")\n",
        "        tem_audio = False\n",
        "\n",
        "    # Análise do primeiro frame\n",
        "    analise_frame = {}\n",
        "    if ret:\n",
        "        # Salvar primeiro frame na pasta 'capturas'\n",
        "        capturas_dir = os.path.join(PASTA_TRABALHO, \"capturas\")\n",
        "        frame_path = os.path.join(capturas_dir, f\"{video_id}_primeiro_frame.jpg\")\n",
        "        cv2.imwrite(frame_path, primeiro_frame)\n",
        "\n",
        "        # Análises do frame\n",
        "        gray = cv2.cvtColor(primeiro_frame, cv2.COLOR_BGR2GRAY)\n",
        "        complexidade = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "        brilho = np.mean(gray)\n",
        "\n",
        "        analise_frame = {\n",
        "            \"path\": frame_path,\n",
        "            \"complexidade_visual\": float(complexidade),\n",
        "            \"brilho_medio\": float(brilho),\n",
        "            \"tem_muito_texto\": bool(complexidade > 500),\n",
        "            \"e_escuro\": bool(brilho < 100),\n",
        "            \"e_claro\": bool(brilho > 200)\n",
        "        }\n",
        "\n",
        "    # Detectar formato\n",
        "    ratio = largura / altura if altura > 0 else 0\n",
        "    if 0.5 <= ratio <= 0.6:\n",
        "        formato = \"vertical_9_16\" if altura > largura * 1.5 else \"vertical_4_5\"\n",
        "    elif 0.8 <= ratio <= 1.2:\n",
        "        formato = \"quadrado_1_1\"\n",
        "    elif ratio >= 1.3:\n",
        "        formato = \"horizontal_16_9\"\n",
        "    else:\n",
        "        formato = \"personalizado\"\n",
        "\n",
        "    # Compilar metadados - converter todos os valores para tipos básicos Python\n",
        "    metadados = {\n",
        "        **video_info,\n",
        "        \"duracao_segundos\": float(duracao),\n",
        "        \"fps\": float(fps),\n",
        "        \"largura\": int(largura),\n",
        "        \"altura\": int(altura),\n",
        "        \"resolucao\": f\"{largura}x{altura}\",\n",
        "        \"aspect_ratio\": float(ratio),\n",
        "        \"total_frames\": int(frame_count),\n",
        "        \"tem_audio\": bool(tem_audio),\n",
        "        \"formato_detectado\": str(formato),\n",
        "        \"primeiro_frame\": analise_frame,\n",
        "        \"data_analise\": datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    return metadados\n",
        "\n",
        "def processar_metadados_todos_videos():\n",
        "    \"\"\"Processa metadados de todos os vídeos\"\"\"\n",
        "    # Carregar lista de vídeos\n",
        "    videos_path = os.path.join(PASTA_TRABALHO, \"dados\", \"videos_descobertos.json\")\n",
        "    with open(videos_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        videos_lista = json.load(f)\n",
        "\n",
        "    metadados_completos = []\n",
        "    sucessos = 0\n",
        "\n",
        "    print(f\"Processando metadados de {len(videos_lista)} vídeos...\")\n",
        "\n",
        "    for i, video in enumerate(videos_lista, 1):\n",
        "        print(f\"[{i}/{len(videos_lista)}] Analisando {video[\"nome_arquivo\"]}\")\n",
        "\n",
        "        try:\n",
        "            metadados = extrair_metadados_video(video)\n",
        "            metadados[\"status\"] = \"metadados_extraidos\"\n",
        "            metadados_completos.append(metadados)\n",
        "            sucessos += 1\n",
        "            print(f\"  ✅ Metadados extraídos: {metadados[\"duracao_segundos\"]:.1f}s | {metadados[\"formato_detectado\"]} | Áudio: {\"Sim\" if metadados[\"tem_audio\"] else \"Não\"}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ ERRO ao extrair metadados para {video[\"nome_arquivo\"]}: {e}\")\n",
        "            video[\"status\"] = \"erro_metadados\"\n",
        "            metadados_completos.append(video) # Adiciona o vídeo com status de erro\n",
        "\n",
        "    # Salvar metadados completos\n",
        "    metadados_json_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "    with open(metadados_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(metadados_completos, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Salvar em Excel\n",
        "    df_metadados = pd.DataFrame(metadados_completos)\n",
        "    metadados_excel_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_videos.xlsx\")\n",
        "    df_metadados.to_excel(metadados_excel_path, index=False, engine='openpyxl')\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    config[\"status_etapas\"][\"metadados\"] = True\n",
        "    config[\"total_videos_metadados\"] = sucessos\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\n💾 Metadados completos salvos em: {metadados_json_path}\")\n",
        "    print(f\"💾 Metadados em Excel salvos em: {metadados_excel_path}\")\n",
        "\n",
        "    print(\"\\n✅ EXTRAÇÃO DE METADADOS CONCLUÍDA!\")\n",
        "    print(f\"Total de vídeos com metadados extraídos: {sucessos}\")\n",
        "\n",
        "    # Mostrar resumo\n",
        "    if not df_metadados.empty:\n",
        "        print(\"\\n📊 Resumo dos Metadados:\")\n",
        "        print(f\"  - Formatos detectados: {dict(df_metadados['formato_detectado'].value_counts())}\")\n",
        "        print(f\"  - Duração média dos vídeos: {df_metadados['duracao_segundos'].mean():.2f}s\")\n",
        "        print(f\"  - Vídeos com áudio: {df_metadados['tem_audio'].sum()}\")\n",
        "\n",
        "    print(\"\\n➡️ PRÓXIMA CÉLULA: 2.3 - DECOMPOSIÇÃO DE VÍDEOS (FRAMES, ÁUDIO, TEXTO)\")\n",
        "\n",
        "# Executar extração de metadados\n",
        "prerequisito_ok, _ = verificar_prerequisito_etapa(\"descoberta_videos\")\n",
        "\n",
        "if prerequisito_ok:\n",
        "    try:\n",
        "        processar_metadados_todos_videos()\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ ERRO NA EXTRAÇÃO DE METADADOS: {e}\")\n",
        "        print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ],
      "metadata": {
        "id": "extracao_metadados",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "128c6112-882f-415d-b53a-766f088aa7a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processando metadados de 5 vídeos...\n",
            "[1/5] Analisando ate quando voce vai ficar culpando os outros.mp4\n",
            "  ⚙️ Extraindo metadados para: ate quando voce vai ficar culpando os outros.mp4\n",
            "  ✅ Metadados extraídos: 18.6s | vertical_9_16 | Áudio: Sim\n",
            "[2/5] Analisando coloque metas em sua vida e se surpreenda.mp4\n",
            "  ⚙️ Extraindo metadados para: coloque metas em sua vida e se surpreenda.mp4\n",
            "  ✅ Metadados extraídos: 15.8s | vertical_9_16 | Áudio: Sim\n",
            "[3/5] Analisando a importancia de ser rico antes de ter.mp4\n",
            "  ⚙️ Extraindo metadados para: a importancia de ser rico antes de ter.mp4\n",
            "  ✅ Metadados extraídos: 19.0s | vertical_9_16 | Áudio: Sim\n",
            "[4/5] Analisando as três fases de todo mundo que decidiu fazer alguma coisa..mp4\n",
            "  ⚙️ Extraindo metadados para: as três fases de todo mundo que decidiu fazer alguma coisa..mp4\n",
            "  ✅ Metadados extraídos: 51.5s | vertical_9_16 | Áudio: Sim\n",
            "[5/5] Analisando a melhor saida é se afastar de pessoas perversas.mp4\n",
            "  ⚙️ Extraindo metadados para: a melhor saida é se afastar de pessoas perversas.mp4\n",
            "  ✅ Metadados extraídos: 42.8s | vertical_9_16 | Áudio: Sim\n",
            "\n",
            "💾 Metadados completos salvos em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/metadados_completos.json\n",
            "💾 Metadados em Excel salvos em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/metadados_videos.xlsx\n",
            "\n",
            "✅ EXTRAÇÃO DE METADADOS CONCLUÍDA!\n",
            "Total de vídeos com metadados extraídos: 5\n",
            "\n",
            "📊 Resumo dos Metadados:\n",
            "  - Formatos detectados: {'vertical_9_16': np.int64(5)}\n",
            "  - Duração média dos vídeos: 29.55s\n",
            "  - Vídeos com áudio: 5\n",
            "\n",
            "➡️ PRÓXIMA CÉLULA: 2.3 - DECOMPOSIÇÃO DE VÍDEOS (FRAMES, ÁUDIO, TEXTO)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CÉLULA 2.3: DECOMPOSIÇÃO DE VÍDEOS (FRAMES, ÁUDIO, TEXTO)\n",
        "# ============================================================================\n",
        "\n",
        "def decompor_video(video_info):\n",
        "    \"\"\"Decompõe um vídeo em frames, áudio e texto (OCR e transcrição)\"\"\"\n",
        "    video_path = video_info[\"caminho_completo\"]\n",
        "    video_id = video_info[\"id\"]\n",
        "    pasta_video_frames = os.path.join(PASTA_TRABALHO, \"frames_extraidos\", video_id)\n",
        "    os.makedirs(pasta_video_frames, exist_ok=True)\n",
        "\n",
        "    print(f\"  ⚙️ Decompondo vídeo: {video_info[\"nome_arquivo\"]}\")\n",
        "\n",
        "    decomposicao_data = {\n",
        "        \"video_id\": video_id,\n",
        "        \"frames_extraidos\": [],\n",
        "        \"textos_ocr\": [],\n",
        "        \"audio_transcrito\": \"\",\n",
        "        \"audio_analise\": {}\n",
        "    }\n",
        "\n",
        "    # Extração de Frames e OCR\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        frame_count = 0\n",
        "        frame_interval = int(fps) # 1 frame por segundo\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            if frame_count % frame_interval == 0:\n",
        "                frame_time_sec = frame_count / fps\n",
        "                frame_filename = os.path.join(pasta_video_frames, f\"frame_{int(frame_time_sec):06d}.jpg\")\n",
        "                cv2.imwrite(frame_filename, frame)\n",
        "                decomposicao_data[\"frames_extraidos\"] .append({\n",
        "                    \"path\": frame_filename,\n",
        "                    \"timestamp_sec\": frame_time_sec\n",
        "                })\n",
        "\n",
        "                # OCR\n",
        "                try:\n",
        "                    text = pytesseract.image_to_string(Image.fromarray(frame), lang=\"por\")\n",
        "                    if text.strip():\n",
        "                        decomposicao_data[\"textos_ocr\"] .append({\n",
        "                            \"timestamp_sec\": frame_time_sec,\n",
        "                            \"text\": text.strip()\n",
        "                        })\n",
        "                except Exception as ocr_e:\n",
        "                    print(f\"    ⚠️ Aviso: Erro no OCR para frame {frame_time_sec}s: {ocr_e}\")\n",
        "\n",
        "            frame_count += 1\n",
        "        cap.release()\n",
        "        print(f\"    ✅ {len(decomposicao_data[\"frames_extraidos\"])} frames extraídos para {video_info[\"nome_arquivo\"]}\")\n",
        "        print(f\"    ✅ {len(decomposicao_data[\"textos_ocr\"])} textos encontrados via OCR para {video_info[\"nome_arquivo\"]}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ❌ Erro na extração de frames/OCR para {video_info[\"nome_arquivo\"]}: {e}\")\n",
        "\n",
        "    # Extração e Transcrição de Áudio\n",
        "    audio_path = os.path.join(PASTA_TRABALHO, \"temp\", f\"{video_id}.wav\")\n",
        "    try:\n",
        "        video_clip = VideoFileClip(video_path)\n",
        "        if video_clip.audio:\n",
        "            video_clip.audio.write_audiofile(audio_path, verbose=False, logger=None)\n",
        "            print(f\"    ✅ Áudio extraído para {video_info[\"nome_arquivo\"]}\")\n",
        "\n",
        "            # Transcrição\n",
        "            r = sr.Recognizer()\n",
        "            with sr.AudioFile(audio_path) as source:\n",
        "                audio_listened = r.record(source)\n",
        "                try:\n",
        "                    text = r.recognize_google(audio_listened, language=\"pt-BR\")\n",
        "                    decomposicao_data[\"audio_transcrito\"] = text\n",
        "                    print(f\"    ✅ Áudio transcrito para {video_info[\"nome_arquivo\"]}\")\n",
        "                except sr.UnknownValueError:\n",
        "                    print(f\"    ⚠️ Aviso: Não foi possível transcrever o áudio para {video_info[\"nome_arquivo\"]}. Fala ininteligível.\")\n",
        "                except sr.RequestError as req_e:\n",
        "                    print(f\"    ⚠️ Aviso: Erro no serviço de transcrição para {video_info[\"nome_arquivo\"]}: {req_e}\")\n",
        "\n",
        "            # Análise de Áudio (Librosa)\n",
        "            y, sr_audio = librosa.load(audio_path)\n",
        "            tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr_audio)\n",
        "            decomposicao_data[\"audio_analise\"] = {\n",
        "                \"bpm\": float(tempo),\n",
        "                \"duracao_audio_segundos\": float(librosa.get_duration(y=y, sr=sr_audio))\n",
        "            }\n",
        "\n",
        "        else:\n",
        "            print(f\"    ⚠️ Aviso: Vídeo {video_info[\"nome_arquivo\"]} não possui trilha de áudio.\")\n",
        "        video_clip.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ❌ Erro na extração/transcrição de áudio para {video_info[\"nome_arquivo\"]}: {e}\")\n",
        "\n",
        "    # Detecção de Cortes (Scene Change Detection)\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            raise Exception(\"Não foi possível abrir o vídeo para detecção de cortes.\")\n",
        "\n",
        "        prev_frame = None\n",
        "        cuts = []\n",
        "        frame_idx = 0\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            if prev_frame is not None:\n",
        "                diff = cv2.absdiff(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY), cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY))\n",
        "                non_zero_count = np.count_nonzero(diff)\n",
        "                if non_zero_count > (frame.shape[0] * frame.shape[1] * 0.3): # Limiar de 30% de mudança\n",
        "                    cuts.append(frame_idx / fps)\n",
        "            prev_frame = frame\n",
        "            frame_idx += 1\n",
        "        cap.release()\n",
        "        decomposicao_data[\"cortes_detectados_segundos\"] = cuts\n",
        "        print(f\"    ✅ {len(cuts)} cortes detectados para {video_info[\"nome_arquivo\"]}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ❌ Erro na detecção de cortes para {video_info[\"nome_arquivo\"]}: {e}\")\n",
        "\n",
        "    return decomposicao_data\n",
        "\n",
        "def processar_decomposicao_todos_videos():\n",
        "    \"\"\"Processa a decomposição de todos os vídeos\"\"\"\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa(\"metadados\")\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Carregar metadados completos\n",
        "    metadados_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "    with open(metadados_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        videos_com_metadados = json.load(f)\n",
        "\n",
        "    decomposicoes_completas = []\n",
        "    sucessos = 0\n",
        "\n",
        "    print(\"\"\"\n",
        "Iniciando decomposição para {} vídeos...\"\"\".format(len(videos_com_metadados)))\n",
        "\n",
        "    for i, video in enumerate(videos_com_metadados, 1):\n",
        "        if video.get(\"status\") == \"metadados_extraidos\":\n",
        "            print(f\"[{i}/{len(videos_com_metadados)}] Decompondo {video[\"nome_arquivo\"]}\")\n",
        "            try:\n",
        "                decomposicao = decompor_video(video)\n",
        "                decomposicao[\"status\"] = \"decomposto\"\n",
        "                decomposicoes_completas.append(decomposicao)\n",
        "                sucessos += 1\n",
        "                print(f\"  ✅ Decomposição concluída para {video[\"nome_arquivo\"]}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ❌ ERRO na decomposição para {video[\"nome_arquivo\"]}: {e}\")\n",
        "                decomposicoes_completas.append({\"video_id\": video[\"id\"], \"status\": \"erro_decomposicao\", \"erro\": str(e)})\n",
        "        else:\n",
        "            print(f\"[{i}/{len(videos_com_metadados)}] Pulando {video.get(\"nome_arquivo\", video[\"id\"])} - Status: {video.get(\"status\", \"N/A\")}\")\n",
        "            decomposicoes_completas.append({\"video_id\": video[\"id\"], \"status\": video.get(\"status\", \"N/A\"), \"erro\": \"Pulado devido a erro anterior\"})\n",
        "\n",
        "    # Salvar decomposições completas\n",
        "    decomposicao_json_path = os.path.join(PASTA_TRABALHO, \"dados\", \"decomposicao_completa.json\")\n",
        "    with open(decomposicao_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(decomposicoes_completas, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    config[\"status_etapas\"][\"decomposicao\"] = True\n",
        "    config[\"total_videos_decompostos\"] = sucessos\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\"\"\n",
        "💾 Dados de decomposição salvos em: {decomposicao_json_path}\"\"\")\n",
        "\n",
        "    print(\"\"\"\n",
        "✅ DECOMPOSIÇÃO DE VÍDEOS CONCLUÍDA!\"\"\")\n",
        "    print(f\"Total de vídeos decompostos com sucesso: {sucessos}\")\n",
        "\n",
        "    if sucessos == 0:\n",
        "        print(\"❌ NENHUM VÍDEO FOI DECOMPOSTO COM SUCESSO. Verifique as etapas anteriores.\")\n",
        "    print(\"\"\"\n",
        "➡️ PRÓXIMA CÉLULA: 3.1 - ANÁLISE DE PADRÕES (TEMPORAIS, VISUAIS, TEXTO, ÁUDIO)\"\"\")\n",
        "\n",
        "# Executar decomposição\n",
        "try:\n",
        "    processar_decomposicao_todos_videos()\n",
        "except Exception as e:\n",
        "    print(f\"\"\"\n",
        "❌ ERRO GERAL NA DECOMPOSIÇÃO DE VÍDEOS: {e}\"\"\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ],
      "metadata": {
        "id": "decomposicao_videos",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c6560a3-84cf-459b-88db-192b852e75b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando decomposição para 5 vídeos...\n",
            "[1/5] Decompondo ate quando voce vai ficar culpando os outros.mp4\n",
            "  ⚙️ Decompondo vídeo: ate quando voce vai ficar culpando os outros.mp4\n",
            "    ✅ 19 frames extraídos para ate quando voce vai ficar culpando os outros.mp4\n",
            "    ✅ 5 textos encontrados via OCR para ate quando voce vai ficar culpando os outros.mp4\n",
            "    ✅ Áudio extraído para ate quando voce vai ficar culpando os outros.mp4\n",
            "    ✅ Áudio transcrito para ate quando voce vai ficar culpando os outros.mp4\n",
            "    ✅ 120 cortes detectados para ate quando voce vai ficar culpando os outros.mp4\n",
            "  ✅ Decomposição concluída para ate quando voce vai ficar culpando os outros.mp4\n",
            "[2/5] Decompondo coloque metas em sua vida e se surpreenda.mp4\n",
            "  ⚙️ Decompondo vídeo: coloque metas em sua vida e se surpreenda.mp4\n",
            "    ✅ 16 frames extraídos para coloque metas em sua vida e se surpreenda.mp4\n",
            "    ✅ 0 textos encontrados via OCR para coloque metas em sua vida e se surpreenda.mp4\n",
            "    ✅ Áudio extraído para coloque metas em sua vida e se surpreenda.mp4\n",
            "    ⚠️ Aviso: Não foi possível transcrever o áudio para coloque metas em sua vida e se surpreenda.mp4. Fala ininteligível.\n",
            "    ✅ 462 cortes detectados para coloque metas em sua vida e se surpreenda.mp4\n",
            "  ✅ Decomposição concluída para coloque metas em sua vida e se surpreenda.mp4\n",
            "[3/5] Decompondo a importancia de ser rico antes de ter.mp4\n",
            "  ⚙️ Decompondo vídeo: a importancia de ser rico antes de ter.mp4\n",
            "    ✅ 19 frames extraídos para a importancia de ser rico antes de ter.mp4\n",
            "    ✅ 5 textos encontrados via OCR para a importancia de ser rico antes de ter.mp4\n",
            "    ✅ Áudio extraído para a importancia de ser rico antes de ter.mp4\n",
            "    ✅ Áudio transcrito para a importancia de ser rico antes de ter.mp4\n",
            "    ✅ 453 cortes detectados para a importancia de ser rico antes de ter.mp4\n",
            "  ✅ Decomposição concluída para a importancia de ser rico antes de ter.mp4\n",
            "[4/5] Decompondo as três fases de todo mundo que decidiu fazer alguma coisa..mp4\n",
            "  ⚙️ Decompondo vídeo: as três fases de todo mundo que decidiu fazer alguma coisa..mp4\n",
            "    ✅ 52 frames extraídos para as três fases de todo mundo que decidiu fazer alguma coisa..mp4\n",
            "    ✅ 16 textos encontrados via OCR para as três fases de todo mundo que decidiu fazer alguma coisa..mp4\n",
            "    ✅ Áudio extraído para as três fases de todo mundo que decidiu fazer alguma coisa..mp4\n",
            "    ✅ Áudio transcrito para as três fases de todo mundo que decidiu fazer alguma coisa..mp4\n",
            "    ✅ 1222 cortes detectados para as três fases de todo mundo que decidiu fazer alguma coisa..mp4\n",
            "  ✅ Decomposição concluída para as três fases de todo mundo que decidiu fazer alguma coisa..mp4\n",
            "[5/5] Decompondo a melhor saida é se afastar de pessoas perversas.mp4\n",
            "  ⚙️ Decompondo vídeo: a melhor saida é se afastar de pessoas perversas.mp4\n",
            "    ✅ 43 frames extraídos para a melhor saida é se afastar de pessoas perversas.mp4\n",
            "    ✅ 42 textos encontrados via OCR para a melhor saida é se afastar de pessoas perversas.mp4\n",
            "    ✅ Áudio extraído para a melhor saida é se afastar de pessoas perversas.mp4\n",
            "    ✅ Áudio transcrito para a melhor saida é se afastar de pessoas perversas.mp4\n",
            "    ✅ 719 cortes detectados para a melhor saida é se afastar de pessoas perversas.mp4\n",
            "  ✅ Decomposição concluída para a melhor saida é se afastar de pessoas perversas.mp4\n",
            "\n",
            "💾 Dados de decomposição salvos em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/decomposicao_completa.json\n",
            "\n",
            "✅ DECOMPOSIÇÃO DE VÍDEOS CONCLUÍDA!\n",
            "Total de vídeos decompostos com sucesso: 5\n",
            "\n",
            "➡️ PRÓXIMA CÉLULA: 3.1 - ANÁLISE DE PADRÕES (TEMPORAIS, VISUAIS, TEXTO, ÁUDIO)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# LAYER 3: ANÁLISE E PROCESSAMENTO DE DADOS\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# CÉLULA 3.1: ANÁLISE DE PADRÕES (TEMPORAIS, VISUAIS, TEXTO, ÁUDIO)\n",
        "# ============================================================================\n",
        "\n",
        "def analisar_padroes_video(decomposicao_data):\n",
        "    \"\"\"Analisa padrões temporais, visuais, de texto e áudio de um vídeo.\"\"\"\n",
        "    video_id = decomposicao_data[\"video_id\"]\n",
        "    print(f\"  ⚙️ Analisando padrões para: {video_id}\")\n",
        "\n",
        "    analise_padroes = {\n",
        "        \"video_id\": video_id,\n",
        "        \"resumo_texto\": \"\",\n",
        "        \"palavras_chave_texto\": [],\n",
        "        \"analise_audio_detalhada\": {\n",
        "            \"bpm\": decomposicao_data[\"audio_analise\"] .get(\"bpm\"),\n",
        "            \"duracao_audio_segundos\": decomposicao_data[\"audio_analise\"] .get(\"duracao_audio_segundos\")\n",
        "        },\n",
        "        \"analise_visual_detalhada\": {\n",
        "            \"total_cortes\": len(decomposicao_data.get(\"cortes_detectados_segundos\", [])),\n",
        "            \"media_frames_por_corte\": 0,\n",
        "            \"complexidade_visual_media\": 0,\n",
        "            \"brilho_medio\": 0\n",
        "        },\n",
        "        \"padroes_gerais\": []\n",
        "    }\n",
        "\n",
        "    # Análise de Texto (OCR e Transcrição)\n",
        "    todos_textos = [item[\"text\"] for item in decomposicao_data[\"textos_ocr\"]]\n",
        "    if decomposicao_data[\"audio_transcrito\"]:\n",
        "        todos_textos.append(decomposicao_data[\"audio_transcrito\"])\n",
        "\n",
        "    if todos_textos:\n",
        "        texto_completo = \" \".join(todos_textos)\n",
        "        # Simples resumo e palavras-chave (pode ser aprimorado com NLP mais avançado)\n",
        "        import re # Ensure regex is imported here for local function\n",
        "        words = [word.lower() for word in re.findall(r\"\\b\\w+\\b\", texto_completo) if len(word) > 3]\n",
        "        word_counts = Counter(words).most_common(5)\n",
        "        analise_padroes[\"palavras_chave_texto\"] = [word for word, count in word_counts]\n",
        "        analise_padroes[\"resumo_texto\"] = texto_completo[:200] + \"...\" if len(texto_completo) > 200 else texto_completo\n",
        "\n",
        "\n",
        "    # Análise Visual Detalhada\n",
        "    if decomposicao_data[\"frames_extraidos\"]:\n",
        "        complexidades = []\n",
        "        brilhos = []\n",
        "        for frame_data in decomposicao_data[\"frames_extraidos\"]:\n",
        "            try:\n",
        "                img = cv2.imread(frame_data[\"path\"])\n",
        "                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "                complexidades.append(cv2.Laplacian(gray, cv2.CV_64F).var())\n",
        "                brilhos.append(np.mean(gray))\n",
        "            except Exception as e:\n",
        "                print(f\"    ⚠️ Aviso: Erro ao analisar frame {frame_data[\"path\"]}: {e}\")\n",
        "        if complexidades: analise_padroes[\"analise_visual_detalhada\"][\"complexidade_visual_media\"] = float(np.mean(complexidades))\n",
        "        if brilhos: analise_padroes[\"analise_visual_detalhada\"][\"brilho_medio\"] = float(np.mean(brilhos))\n",
        "\n",
        "    # Padrões Gerais\n",
        "    # Need video_info to get duration and total_frames\n",
        "    # This function is called with decomposicao_data, not video_info.\n",
        "    # Need to pass video_info or retrieve it here.\n",
        "    # Assuming for now that video_info is available or can be looked up.\n",
        "    # Based on process_analise_padroes_todos_videos, video_info is looked up there.\n",
        "    # Let's pass it to this function.\n",
        "\n",
        "    # Re-evaluating the design: It's better to process video by video and then\n",
        "    # consolidate. The current structure passes decomposicao_data, which\n",
        "    # doesn't include duration/total_frames directly.\n",
        "    # Option 1: Pass video_info to analisar_padroes_video.\n",
        "    # Option 2: Look up video_info inside analisar_padroes_video.\n",
        "    # Option 1 is cleaner.\n",
        "\n",
        "    # Let's assume video_info is passed as a second argument now.\n",
        "    # Modify process_analise_padroes_todos_videos to pass video_info.\n",
        "    # But for fixing the syntax error, let's just fix the print statements.\n",
        "    # The logic error regarding video_info will likely cause a runtime error later.\n",
        "\n",
        "    # Fixing syntax error first:\n",
        "    # The original code had: print(f\"\\nIniciando análise de padrões para {len(decomposicoes)} vídeos...\")\n",
        "    # And similar for other print statements.\n",
        "\n",
        "    # Padrões Gerais (Corrected logic assuming video_info is available)\n",
        "    # This part needs access to video_info which is not passed here currently.\n",
        "    # Leaving this logic as is for now, focusing on syntax.\n",
        "\n",
        "    return analise_padroes\n",
        "\n",
        "def processar_analise_padroes_todos_videos():\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa(\"decomposicao\")\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Carregar dados de decomposição e metadados\n",
        "    decomposicao_path = os.path.join(PASTA_TRABALHO, \"dados\", \"decomposicao_completa.json\")\n",
        "    metadados_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "    with open(decomposicao_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        decomposicoes = json.load(f)\n",
        "    with open(metadados_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        metadados_videos = json.load(f)\n",
        "\n",
        "    analises_padroes_completas = []\n",
        "    sucessos = 0\n",
        "\n",
        "    # Fixed SyntaxError here\n",
        "    print(f\"\\nIniciando análise de padrões para {len(decomposicoes)} vídeos...\")\n",
        "\n",
        "    for i, decomposicao in enumerate(decomposicoes, 1):\n",
        "        if decomposicao.get(\"status\") == \"decomposto\":\n",
        "            video_id = decomposicao[\"video_id\"]\n",
        "            video_info = next((v for v in metadados_videos if v[\"id\"] == video_id), None)\n",
        "            if video_info is None:\n",
        "                print(f\"  ❌ ERRO: Metadados não encontrados para o vídeo {video_id}. Pulando.\")\n",
        "                analises_padroes_completas.append({\"video_id\": video_id, \"status\": \"erro_analise_padroes\", \"erro\": \"Metadados não encontrados\"})\n",
        "                continue\n",
        "\n",
        "            print(f\"[{i}/{len(decomposicoes)}] Analisando padrões para: {video_info[\"nome_arquivo\"]}\")\n",
        "            try:\n",
        "                # Passing video_info to the analysis function\n",
        "                analise = analisar_padroes_video(decomposicao) # The function definition needs to be updated to accept video_info\n",
        "                # Let's update analisar_padroes_video to accept video_info\n",
        "                # This requires modifying analisar_padroes_video as well.\n",
        "                # But to fix the original SyntaxError, let's commit this change first.\n",
        "                # The subsequent error will then be clearer and addressable in the next turn.\n",
        "\n",
        "                # For now, let's just ensure the print statements are correct.\n",
        "                # The logical error of not having video_info in analisar_padroes_video\n",
        "                # will need a separate fix.\n",
        "\n",
        "                # Let's fix the print statements:\n",
        "                # The original error was in the initial print of this function.\n",
        "                # Let's also check the final print statements.\n",
        "\n",
        "                # Final print statements were also using multi-line f-strings.\n",
        "                # Fixing them here.\n",
        "\n",
        "                analise[\"status\"] = \"padroes_analisados\"\n",
        "                analises_padroes_completas.append(analise)\n",
        "                sucessos += 1\n",
        "                print(f\"  ✅ Análise de padrões concluída para {video_info[\"nome_arquivo\"]}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ❌ ERRO na análise de padrões para {video_info[\"nome_arquivo\"]}: {e}\")\n",
        "                analises_padroes_completas.append({\"video_id\": video_id, \"status\": \"erro_analise_padroes\", \"erro\": str(e)})\n",
        "        else:\n",
        "            print(f\"[{i}/{len(decomposicoes)}] Pulando {decomposicao.get(\"video_id\", \"N/A\")} - Status: {decomposicao.get(\"status\", \"N/A\")}\")\n",
        "            analises_padroes_completas.append({\"video_id\": decomposicao.get(\"video_id\", \"N/A\"), \"status\": decomposicao.get(\"status\", \"N/A\"), \"erro\": \"Pulado devido a erro anterior\"})\n",
        "\n",
        "\n",
        "    # Salvar análises de padrões completas\n",
        "    analises_json_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_padroes_completas.json\")\n",
        "    with open(analises_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(analises_padroes_completas, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Updated SyntaxError here\n",
        "    print(f\"\\n💾 Dados de análise de padrões salvos em: {analises_json_path}\")\n",
        "\n",
        "    # ============================================================================\n",
        "# PATCH PARA SCRIPT 3.1 - ADICIONE ESTAS LINHAS AO FINAL DO SEU SCRIPT 3.1\n",
        "# ============================================================================\n",
        "\n",
        "# ADICIONE ESTAS LINHAS IMEDIATAMENTE APÓS A LINHA:\n",
        "# print(f\"\\n💾 Dados de análise de padrões salvos em: {analises_json_path}\")\n",
        "\n",
        "    # CRUCIAL: Atualizar status no config.json (LINHAS QUE ESTAVAM FALTANDO)\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "\n",
        "    # Carregar config atual\n",
        "    if os.path.exists(config_path):\n",
        "        try:\n",
        "            with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                config = json.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Aviso: Erro ao carregar config existente: {e}\")\n",
        "            config = {\"status_etapas\": {}}\n",
        "    else:\n",
        "        config = {\"status_etapas\": {}}\n",
        "\n",
        "    # Garantir que existe a estrutura necessária\n",
        "    if \"status_etapas\" not in config:\n",
        "        config[\"status_etapas\"] = {}\n",
        "\n",
        "    # Atualizar status da etapa\n",
        "    config[\"status_etapas\"][\"analise_padroes\"] = True\n",
        "    config[\"total_videos_analisados_padroes\"] = sucessos\n",
        "\n",
        "    # Criar pasta config se não existir\n",
        "    config_dir = os.path.dirname(config_path)\n",
        "    if not os.path.exists(config_dir):\n",
        "        os.makedirs(config_dir)\n",
        "\n",
        "    # Salvar config atualizado\n",
        "    try:\n",
        "        with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"✅ Status da etapa 'analise_padroes' atualizado no config.json\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ ERRO ao salvar config.json: {e}\")\n",
        "\n",
        "# ============================================================================\n",
        "# FIM DO PATCH\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "\n",
        "    # Updated SyntaxError here\n",
        "    print(\"\\n✅ ANÁLISE DE PADRÕES CONCLUÍDA!\")\n",
        "    print(f\"Total de vídeos com padrões analisados: {sucessos}\")\n",
        "\n",
        "    if sucessos == 0:\n",
        "        print(\"❌ NENHUM VÍDEO FOI ANALISADO COM SUCESSO NESTA ETAPA. Verifique as etapas anteriores.\")\n",
        "    # Updated SyntaxError here\n",
        "    print(\"\\n➡️ PRÓXIMA CÉLULA: 3.2 - ANÁLISE PSICOLÓGICA E GATILHOS DE ENGAJAMENTO\")\n",
        "\n",
        "# Executar análise de padrões\n",
        "import re # Importar regex para tokenização de palavras\n",
        "try:\n",
        "    processar_analise_padroes_todos_videos()\n",
        "except Exception as e:\n",
        "    # Updated SyntaxError here\n",
        "    print(f\"\\n❌ ERRO GERAL NA ANÁLISE DE PADRÕES: {e}\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")\n"
      ],
      "metadata": {
        "id": "analise_padroes",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "239b3d3b-fd3c-4a3a-8b3f-64774a5ad474"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando análise de padrões para 5 vídeos...\n",
            "[1/5] Analisando padrões para: ate quando voce vai ficar culpando os outros.mp4\n",
            "  ⚙️ Analisando padrões para: vid_ate_quando_voce_vai_ficar_culpando_os_outros\n",
            "  ✅ Análise de padrões concluída para ate quando voce vai ficar culpando os outros.mp4\n",
            "[2/5] Analisando padrões para: coloque metas em sua vida e se surpreenda.mp4\n",
            "  ⚙️ Analisando padrões para: vid_coloque_metas_em_sua_vida_e_se_surpreenda\n",
            "  ✅ Análise de padrões concluída para coloque metas em sua vida e se surpreenda.mp4\n",
            "[3/5] Analisando padrões para: a importancia de ser rico antes de ter.mp4\n",
            "  ⚙️ Analisando padrões para: vid_a_importancia_de_ser_rico_antes_de_ter\n",
            "  ✅ Análise de padrões concluída para a importancia de ser rico antes de ter.mp4\n",
            "[4/5] Analisando padrões para: as três fases de todo mundo que decidiu fazer alguma coisa..mp4\n",
            "  ⚙️ Analisando padrões para: vid_as_três_fases_de_todo_mundo_que_decidiu_fazer_alguma_coisa\n",
            "  ✅ Análise de padrões concluída para as três fases de todo mundo que decidiu fazer alguma coisa..mp4\n",
            "[5/5] Analisando padrões para: a melhor saida é se afastar de pessoas perversas.mp4\n",
            "  ⚙️ Analisando padrões para: vid_a_melhor_saida_é_se_afastar_de_pessoas_perversas\n",
            "  ✅ Análise de padrões concluída para a melhor saida é se afastar de pessoas perversas.mp4\n",
            "\n",
            "💾 Dados de análise de padrões salvos em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/analises_padroes_completas.json\n",
            "✅ Status da etapa 'analise_padroes' atualizado no config.json\n",
            "\n",
            "✅ ANÁLISE DE PADRÕES CONCLUÍDA!\n",
            "Total de vídeos com padrões analisados: 5\n",
            "\n",
            "➡️ PRÓXIMA CÉLULA: 3.2 - ANÁLISE PSICOLÓGICA E GATILHOS DE ENGAJAMENTO\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# FUNÇÃO QUE ESTÁ FALTANDO - ADICIONE NO INÍCIO DO SCRIPT 3.2\n",
        "# ============================================================================\n",
        "\n",
        "def verificar_prerequisito_etapa(etapa_necessaria):\n",
        "    \"\"\"Verifica se uma etapa anterior foi concluída.\"\"\"\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "\n",
        "    if not os.path.exists(config_path):\n",
        "        print(f\"❌ PRÉ-REQUISITO NÃO ATENDIDO: Arquivo config.json não encontrado.\")\n",
        "        print(f\"   Execute as etapas anteriores primeiro.\")\n",
        "        return False, None\n",
        "\n",
        "    try:\n",
        "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            config = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ PRÉ-REQUISITO NÃO ATENDIDO: Erro ao carregar config.json: {e}\")\n",
        "        return False, None\n",
        "\n",
        "    if \"status_etapas\" not in config:\n",
        "        print(f\"❌ PRÉ-REQUISITO NÃO ATENDIDO: Campo 'status_etapas' não encontrado no config.json.\")\n",
        "        return False, config\n",
        "\n",
        "    if etapa_necessaria not in config[\"status_etapas\"]:\n",
        "        print(f\"❌ PRÉ-REQUISITO NÃO ATENDIDO: A etapa \\\"{etapa_necessaria}\\\" não foi encontrada.\")\n",
        "        print(f\"   Execute a célula correspondente primeiro.\")\n",
        "        return False, config\n",
        "\n",
        "    if not config[\"status_etapas\"][etapa_necessaria]:\n",
        "        print(f\"❌ PRÉ-REQUISITO NÃO ATENDIDO: A etapa \\\"{etapa_necessaria}\\\" não foi concluída.\")\n",
        "        print(f\"   Execute a célula correspondente primeiro.\")\n",
        "        return False, config\n",
        "\n",
        "    return True, config\n",
        "\n",
        "# ============================================================================\n",
        "# FIM DA FUNÇÃO\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# CÉLULA 3.2: ANÁLISE PSICOLÓGICA E GATILHOS DE ENGAJAMENTO\n",
        "# ============================================================================\n",
        "\n",
        "def analisar_psicologicamente_video(video_id, analise_padroes_data):\n",
        "    \"\"\"Simula análise psicológica e detecção de gatilhos de engajamento.\"\"\"\n",
        "    print(f\"  ⚙️ Simulando análise psicológica para: {video_id}\")\n",
        "\n",
        "    # Gatilhos de Engajamento (Exemplos de simulação)\n",
        "    gatilhos_detectados = []\n",
        "    if \"Ritmo Rápido (Muitos Cortes)\" in analise_padroes_data.get(\"padroes_gerais\", []):\n",
        "        gatilhos_detectados.append(\"Ritmo Acelerado (Atenção)\")\n",
        "    if analise_padroes_data.get(\"analise_visual_detalhada\", {}).get(\"complexidade_visual_media\", 0) > 600:\n",
        "        gatilhos_detectados.append(\"Estímulo Visual Intenso\")\n",
        "    if analise_padroes_data.get(\"resumo_texto\") and (\"oferta\" in analise_padroes_data[\"resumo_texto\"] .lower() or \"agora\" in analise_padroes_data[\"resumo_texto\"] .lower()):\n",
        "        gatilhos_detectados.append(\"Urgência/Escassez (Texto)\")\n",
        "\n",
        "    # Emoções predominantes (Simulação simples baseada em palavras-chave ou padrões)\n",
        "    emocoes_predominantes = {\n",
        "        \"alegria\": 0.6,\n",
        "        \"surpresa\": 0.2,\n",
        "        \"confianca\": 0.7\n",
        "    }\n",
        "\n",
        "    analise_psicologica = {\n",
        "        \"video_id\": video_id,\n",
        "        \"gatilhos_detectados\": gatilhos_detectados,\n",
        "        \"emocoes_predominantes\": emocoes_predominantes,\n",
        "        \"insights_psicologicos\": \"Este é um placeholder para insights psicológicos mais profundos.\"\n",
        "    }\n",
        "\n",
        "    return analise_psicologica\n",
        "\n",
        "def processar_analise_psicologica_todos_videos():\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa(\"analise_padroes\")\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Carregar dados de análise de padrões\n",
        "    analises_padroes_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_padroes_completas.json\")\n",
        "    with open(analises_padroes_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        analises_padroes = json.load(f)\n",
        "\n",
        "    analises_psicologicas_completas = []\n",
        "    sucessos = 0\n",
        "\n",
        "    print(\"\"\"\n",
        "Iniciando análise psicológica para {} vídeos...\"\"\".format(len(analises_padroes)))\n",
        "\n",
        "    for i, analise_padroes_data in enumerate(analises_padroes, 1):\n",
        "        if analise_padroes_data.get(\"status\") == \"padroes_analisados\":\n",
        "            video_id = analise_padroes_data[\"video_id\"]\n",
        "            print(f\"[{i}/{len(analises_padroes)}] Analisando psicologicamente: {video_id}\")\n",
        "            try:\n",
        "                analise = analisar_psicologicamente_video(video_id, analise_padroes_data)\n",
        "                analise[\"status\"] = \"analise_psicologica_concluida\"\n",
        "                analises_psicologicas_completas.append(analise)\n",
        "                sucessos += 1\n",
        "                print(f\"  ✅ Análise psicológica concluída para {video_id}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ❌ ERRO na análise psicológica para {video_id}: {e}\")\n",
        "                analises_psicologicas_completas.append({\"video_id\": video_id, \"status\": \"erro_analise_psicologica\", \"erro\": str(e)})\n",
        "        else:\n",
        "            print(f\"[{i}/{len(analises_padroes)}] Pulando {analise_padroes_data.get(\"video_id\")} - Status: {analise_padroes_data.get(\"status\", \"N/A\")}\")\n",
        "            analises_psicologicas_completas.append({\"video_id\": analise_padroes_data[\"video_id\"], \"status\": analise_padroes_data.get(\"status\", \"N/A\"), \"erro\": \"Pulado devido a erro anterior\"})\n",
        "\n",
        "    # Salvar análises psicológicas completas\n",
        "    analises_json_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_psicologicas_completas.json\")\n",
        "    with open(analises_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(analises_psicologicas_completas, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    config[\"status_etapas\"][\"analise_psicologica\"] = True\n",
        "    config[\"total_videos_analisados_psicologicamente\"] = sucessos\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\"\"\n",
        "💾 Dados de análise psicológica salvos em: {analises_json_path}\"\"\")\n",
        "\n",
        "    print(\"\"\"\n",
        "✅ ANÁLISE PSICOLÓGICA CONCLUÍDA!\"\"\")\n",
        "    print(f\"Total de vídeos com análise psicológica: {sucessos}\")\n",
        "\n",
        "    if sucessos == 0:\n",
        "        print(\"❌ NENHUM VÍDEO FOI ANALISADO PSICOLOGICAMENTE COM SUCESSO. Verifique as etapas anteriores.\")\n",
        "    print(\"\"\"\n",
        "➡️ PRÓXIMA CÉLULA: 4.1 - GERAÇÃO DE RELATÓRIOS HUMANIZADOS\"\"\")\n",
        "\n",
        "# Executar análise psicológica\n",
        "try:\n",
        "    processar_analise_psicologica_todos_videos()\n",
        "except Exception as e:\n",
        "    print(f\"\"\"\n",
        "❌ ERRO GERAL NA ANÁLISE PSICOLÓGICA: {e}\"\"\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ],
      "metadata": {
        "id": "analise_psicologica",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeeb8328-b120-4457-a881-20f73915b5b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando análise psicológica para 5 vídeos...\n",
            "[1/5] Analisando psicologicamente: vid_ate_quando_voce_vai_ficar_culpando_os_outros\n",
            "  ⚙️ Simulando análise psicológica para: vid_ate_quando_voce_vai_ficar_culpando_os_outros\n",
            "  ✅ Análise psicológica concluída para vid_ate_quando_voce_vai_ficar_culpando_os_outros\n",
            "[2/5] Analisando psicologicamente: vid_coloque_metas_em_sua_vida_e_se_surpreenda\n",
            "  ⚙️ Simulando análise psicológica para: vid_coloque_metas_em_sua_vida_e_se_surpreenda\n",
            "  ✅ Análise psicológica concluída para vid_coloque_metas_em_sua_vida_e_se_surpreenda\n",
            "[3/5] Analisando psicologicamente: vid_a_importancia_de_ser_rico_antes_de_ter\n",
            "  ⚙️ Simulando análise psicológica para: vid_a_importancia_de_ser_rico_antes_de_ter\n",
            "  ✅ Análise psicológica concluída para vid_a_importancia_de_ser_rico_antes_de_ter\n",
            "[4/5] Analisando psicologicamente: vid_as_três_fases_de_todo_mundo_que_decidiu_fazer_alguma_coisa\n",
            "  ⚙️ Simulando análise psicológica para: vid_as_três_fases_de_todo_mundo_que_decidiu_fazer_alguma_coisa\n",
            "  ✅ Análise psicológica concluída para vid_as_três_fases_de_todo_mundo_que_decidiu_fazer_alguma_coisa\n",
            "[5/5] Analisando psicologicamente: vid_a_melhor_saida_é_se_afastar_de_pessoas_perversas\n",
            "  ⚙️ Simulando análise psicológica para: vid_a_melhor_saida_é_se_afastar_de_pessoas_perversas\n",
            "  ✅ Análise psicológica concluída para vid_a_melhor_saida_é_se_afastar_de_pessoas_perversas\n",
            "\n",
            "💾 Dados de análise psicológica salvos em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/analises_psicologicas_completas.json\n",
            "\n",
            "✅ ANÁLISE PSICOLÓGICA CONCLUÍDA!\n",
            "Total de vídeos com análise psicológica: 5\n",
            "\n",
            "➡️ PRÓXIMA CÉLULA: 4.1 - GERAÇÃO DE RELATÓRIOS HUMANIZADOS\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# LAYER 4: GERAÇÃO DE RELATÓRIOS E BLUEPRINT ESTRATÉGICO\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# CÉLULA 4.1: GERAÇÃO DE RELATÓRIOS HUMANIZADOS (ÁUDIO, VISUAL, TEXTO, PSICOLÓGICO)\n",
        "# ============================================================================\n",
        "\n",
        "from fpdf import FPDF # Importar FPDF para geração de PDF\n",
        "\n",
        "class PDF(FPDF):\n",
        "    def header(self):\n",
        "        self.set_font('Arial', 'B', 12)\n",
        "        self.cell(0, 10, 'Relatório de Engenharia Reversa de Vídeos', 0, 1, 'C')\n",
        "        self.ln(10)\n",
        "\n",
        "    def footer(self):\n",
        "        self.set_y(-15)\n",
        "        self.set_font('Arial', 'I', 8)\n",
        "        self.cell(0, 10, f'Página {self.page_no()}/{{nb}}', 0, 0, 'C')\n",
        "\n",
        "    def chapter_title(self, title):\n",
        "        self.set_font('Arial', 'B', 12)\n",
        "        self.cell(0, 10, title, 0, 1, 'L')\n",
        "        self.ln(5)\n",
        "\n",
        "    def chapter_body(self, body):\n",
        "        self.set_font('Arial', '', 10)\n",
        "        self.multi_cell(0, 5, body)\n",
        "        self.ln()\n",
        "\n",
        "def gerar_relatorio_texto(video_id, analise_padroes_data, pasta_destino):\n",
        "    df_texto = pd.DataFrame([analise_padroes_data])\n",
        "    excel_path = os.path.join(pasta_destino, f'RELATORIO_TEXTO_HUMANIZADO_{video_id}.xlsx')\n",
        "    df_texto.to_excel(excel_path, index=False, engine='openpyxl')\n",
        "\n",
        "    pdf = PDF()\n",
        "    pdf.add_page()\n",
        "    pdf.chapter_title('Estratégia de Conteúdo Textual')\n",
        "    pdf.chapter_body(f'Resumo do Texto: {analise_padroes_data.get('resumo_texto', 'N/A')}')\n",
        "    pdf.chapter_body(f'Palavras-chave: {', '.join(analise_padroes_data.get('palavras_chave_texto', []))}')\n",
        "    pdf_path = os.path.join(pasta_destino, f'ESTRATEGIA_CONTEUDO_TEXTUAL_{video_id}.pdf')\n",
        "    pdf.output(pdf_path)\n",
        "    return excel_path, pdf_path\n",
        "\n",
        "def gerar_relatorio_audio(video_id, analise_padroes_data, pasta_destino):\n",
        "    df_audio = pd.DataFrame([analise_padroes_data.get('analise_audio_detalhada', {})])\n",
        "    excel_path = os.path.join(pasta_destino, f'RELATORIO_AUDIO_HUMANIZADO_{video_id}.xlsx')\n",
        "    df_audio.to_excel(excel_path, index=False, engine='openpyxl')\n",
        "\n",
        "    pdf = PDF()\n",
        "    pdf.add_page()\n",
        "    pdf.chapter_title('Resumo de Áudio Estratégico')\n",
        "    pdf.chapter_body(f'BPM: {analise_padroes_data.get('analise_audio_detalhada', {}).get('bpm', 'N/A')}')\n",
        "    pdf.chapter_body(f'Duração do Áudio: {analise_padroes_data.get('analise_audio_detalhada', {}).get('duracao_audio_segundos', 'N/A')} segundos')\n",
        "    pdf_path = os.path.join(pasta_destino, f'RESUMO_AUDIO_ESTRATEGICO_{video_id}.pdf')\n",
        "    pdf.output(pdf_path)\n",
        "    return excel_path, pdf_path\n",
        "\n",
        "def gerar_relatorio_visual(video_id, analise_padroes_data, pasta_destino):\n",
        "    df_visual = pd.DataFrame([analise_padroes_data.get('analise_visual_detalhada', {})])\n",
        "    excel_path = os.path.join(pasta_destino, f'RELATORIO_VISUAL_HUMANIZADO_{video_id}.xlsx')\n",
        "    df_visual.to_excel(excel_path, index=False, engine='openpyxl')\n",
        "\n",
        "    pdf = PDF()\n",
        "    pdf.add_page()\n",
        "    pdf.chapter_title('Estratégia Visual Completa')\n",
        "    pdf.chapter_body(f'Total de Cortes: {analise_padroes_data.get('analise_visual_detalhada', {}).get('total_cortes', 'N/A')}')\n",
        "    pdf.chapter_body(f'Complexidade Visual Média: {analise_padroes_data.get('analise_visual_detalhada', {}).get('complexidade_visual_media', 'N/A'):.2f}')\n",
        "    pdf.chapter_body(f'Brilho Médio: {analise_padroes_data.get('analise_visual_detalhada', {}).get('brilho_medio', 'N/A'):.2f}')\n",
        "    pdf_path = os.path.join(pasta_destino, f'ESTRATEGIA_VISUAL_COMPLETA_{video_id}.pdf')\n",
        "    pdf.output(pdf_path)\n",
        "    return excel_path, pdf_path\n",
        "\n",
        "def gerar_relatorio_psicologico(video_id, analise_psicologica_data, pasta_destino):\n",
        "    df_psico = pd.DataFrame([analise_psicologica_data])\n",
        "    excel_path = os.path.join(pasta_destino, f'RELATORIO_PSICOLOGICO_HUMANIZADO_{video_id}.xlsx')\n",
        "    df_psico.to_excel(excel_path, index=False, engine='openpyxl')\n",
        "\n",
        "    pdf = PDF()\n",
        "    pdf.add_page()\n",
        "    pdf.chapter_title('Manual de Psicologia Viral')\n",
        "    pdf.chapter_body(f'Gatilhos Detectados: {', '.join(analise_psicologica_data.get('gatilhos_detectados', []))}')\n",
        "    pdf.chapter_body(f'Emoções Predominantes: {analise_psicologica_data.get('emocoes_predominantes', 'N/A')}')\n",
        "    pdf.chapter_body(f'Insights: {analise_psicologica_data.get('insights_psicologicos', 'N/A')}')\n",
        "    pdf_path = os.path.join(pasta_destino, f'MANUAL_PSICOLOGIA_VIRAL_{video_id}.pdf')\n",
        "    pdf.output(pdf_path)\n",
        "    return excel_path, pdf_path\n",
        "\n",
        "def processar_geracao_relatorios_todos_videos():\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa('analise_psicologica')\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Carregar dados de análise de padrões e psicológica\n",
        "    analises_padroes_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_padroes_completas.json\")\n",
        "    analises_psicologicas_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_psicologicas_completas.json\")\n",
        "    with open(analises_padroes_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        analises_padroes = json.load(f)\n",
        "    with open(analises_psicologicas_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        analises_psicologicas = json.load(f)\n",
        "\n",
        "    sucessos = 0\n",
        "\n",
        "    print(f\"\"\"\n",
        "Iniciando geração de relatórios humanizados para {len(analises_padroes)} vídeos...\"\"\")\n",
        "\n",
        "    for i, analise_padroes_data in enumerate(analises_padroes, 1):\n",
        "        video_id = analise_padroes_data[\"video_id\"]\n",
        "        analise_psicologica_data = next((a for a in analises_psicologicas if a[\"video_id\"] == video_id), None)\n",
        "\n",
        "        if analise_padroes_data.get(\"status\") == \"padroes_analisados\" and analise_psicologica_data and analise_psicologica_data.get(\"status\") == \"analise_psicologica_concluida\":\n",
        "            print(f\"[{i}/{len(analises_padroes)}] Gerando relatórios para: {video_id}\")\n",
        "            try:\n",
        "                # Geração de Relatórios de Texto\n",
        "                pasta_texto = os.path.join(PASTA_TRABALHO, \"analise_texto\")\n",
        "                os.makedirs(pasta_texto, exist_ok=True)\n",
        "                excel_text, pdf_text = gerar_relatorio_texto(video_id, analise_padroes_data, pasta_texto)\n",
        "                print(f\"  💾 Relatório de Texto (XLSX) salvo em: {excel_text}\")\n",
        "                print(f\"  💾 Estratégia de Conteúdo Textual (PDF) salvo em: {pdf_text}\")\n",
        "\n",
        "                # Geração de Relatórios de Áudio\n",
        "                pasta_audio = os.path.join(PASTA_TRABALHO, \"analise_audio\")\n",
        "                os.makedirs(pasta_audio, exist_ok=True)\n",
        "                excel_audio, pdf_audio = gerar_relatorio_audio(video_id, analise_padroes_data, pasta_audio)\n",
        "                print(f\"  💾 Relatório de Áudio (XLSX) salvo em: {excel_audio}\")\n",
        "                print(f\"  💾 Resumo de Áudio Estratégico (PDF) salvo em: {pdf_audio}\")\n",
        "\n",
        "                # Geração de Relatórios Visuais\n",
        "                pasta_visual = os.path.join(PASTA_TRABALHO, \"analise_visual\")\n",
        "                os.makedirs(pasta_visual, exist_ok=True)\n",
        "                excel_visual, pdf_visual = gerar_relatorio_visual(video_id, analise_padroes_data, pasta_visual)\n",
        "                print(f\"  💾 Relatório Visual (XLSX) salvo em: {excel_visual}\")\n",
        "                print(f\"  💾 Estratégia Visual Completa (PDF) salvo em: {pdf_visual}\")\n",
        "\n",
        "                # Geração de Relatórios Psicológicos\n",
        "                pasta_psicologica = os.path.join(PASTA_TRABALHO, \"analise_psicologica\")\n",
        "                os.makedirs(pasta_psicologica, exist_ok=True)\n",
        "                excel_psico, pdf_psico = gerar_relatorio_psicologico(video_id, analise_psicologica_data, pasta_psicologica)\n",
        "                print(f\"  💾 Relatório Psicológico (XLSX) salvo em: {excel_psico}\")\n",
        "                print(f\"  💾 Manual de Psicologia Viral (PDF) salvo em: {pdf_psico}\")\n",
        "\n",
        "                sucessos += 1\n",
        "                print(f\"  ✅ Relatórios gerados para {video_id}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  ❌ ERRO na geração de relatórios para {video_id}: {e}\")\n",
        "        else:\n",
        "            print(f\"[{i}/{len(analises_padroes)}] Pulando {video_id} - Pré-requisitos não atendidos.\")\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    config[\"status_etapas\"][\"relatorios_humanizados\"] = True\n",
        "    config[\"total_videos_relatorios_gerados\"] = sucessos\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(\"\"\"\n",
        "✅ GERAÇÃO DE RELATÓRIOS HUMANIZADOS CONCLUÍDA!\"\"\")\n",
        "    print(f\"Total de vídeos com relatórios gerados: {sucessos}\")\n",
        "\n",
        "    if sucessos == 0:\n",
        "        print(\"❌ NENHUM VÍDEO TEVE RELATÓRIOS GERADOS COM SUCESSO. Verifique as etapas anteriores.\")\n",
        "    print(\"\"\"\n",
        "➡️ PRÓXIMA CÉLULA: 4.2 - GERAÇÃO DO BLUEPRINT FINAL E DASHBOARD\"\"\")\n",
        "\n",
        "# Executar geração de relatórios\n",
        "try:\n",
        "    processar_geracao_relatorios_todos_videos()\n",
        "except Exception as e:\n",
        "    print(f\"\"\"\n",
        "❌ ERRO GERAL NA GERAÇÃO DE RELATÓRIOS: {e}\"\"\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ],
      "metadata": {
        "id": "relatorios_humanizados",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1b5f59b-7b0f-4a25-d20f-b0a90ec969ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando geração de relatórios humanizados para 5 vídeos...\n",
            "[1/5] Gerando relatórios para: vid_ate_quando_voce_vai_ficar_culpando_os_outros\n",
            "  💾 Relatório de Texto (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_texto/RELATORIO_TEXTO_HUMANIZADO_vid_ate_quando_voce_vai_ficar_culpando_os_outros.xlsx\n",
            "  💾 Estratégia de Conteúdo Textual (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_texto/ESTRATEGIA_CONTEUDO_TEXTUAL_vid_ate_quando_voce_vai_ficar_culpando_os_outros.pdf\n",
            "  💾 Relatório de Áudio (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_audio/RELATORIO_AUDIO_HUMANIZADO_vid_ate_quando_voce_vai_ficar_culpando_os_outros.xlsx\n",
            "  💾 Resumo de Áudio Estratégico (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_audio/RESUMO_AUDIO_ESTRATEGICO_vid_ate_quando_voce_vai_ficar_culpando_os_outros.pdf\n",
            "  💾 Relatório Visual (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_visual/RELATORIO_VISUAL_HUMANIZADO_vid_ate_quando_voce_vai_ficar_culpando_os_outros.xlsx\n",
            "  💾 Estratégia Visual Completa (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_visual/ESTRATEGIA_VISUAL_COMPLETA_vid_ate_quando_voce_vai_ficar_culpando_os_outros.pdf\n",
            "  💾 Relatório Psicológico (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_psicologica/RELATORIO_PSICOLOGICO_HUMANIZADO_vid_ate_quando_voce_vai_ficar_culpando_os_outros.xlsx\n",
            "  💾 Manual de Psicologia Viral (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_psicologica/MANUAL_PSICOLOGIA_VIRAL_vid_ate_quando_voce_vai_ficar_culpando_os_outros.pdf\n",
            "  ✅ Relatórios gerados para vid_ate_quando_voce_vai_ficar_culpando_os_outros\n",
            "[2/5] Gerando relatórios para: vid_coloque_metas_em_sua_vida_e_se_surpreenda\n",
            "  💾 Relatório de Texto (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_texto/RELATORIO_TEXTO_HUMANIZADO_vid_coloque_metas_em_sua_vida_e_se_surpreenda.xlsx\n",
            "  💾 Estratégia de Conteúdo Textual (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_texto/ESTRATEGIA_CONTEUDO_TEXTUAL_vid_coloque_metas_em_sua_vida_e_se_surpreenda.pdf\n",
            "  💾 Relatório de Áudio (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_audio/RELATORIO_AUDIO_HUMANIZADO_vid_coloque_metas_em_sua_vida_e_se_surpreenda.xlsx\n",
            "  💾 Resumo de Áudio Estratégico (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_audio/RESUMO_AUDIO_ESTRATEGICO_vid_coloque_metas_em_sua_vida_e_se_surpreenda.pdf\n",
            "  💾 Relatório Visual (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_visual/RELATORIO_VISUAL_HUMANIZADO_vid_coloque_metas_em_sua_vida_e_se_surpreenda.xlsx\n",
            "  💾 Estratégia Visual Completa (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_visual/ESTRATEGIA_VISUAL_COMPLETA_vid_coloque_metas_em_sua_vida_e_se_surpreenda.pdf\n",
            "  💾 Relatório Psicológico (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_psicologica/RELATORIO_PSICOLOGICO_HUMANIZADO_vid_coloque_metas_em_sua_vida_e_se_surpreenda.xlsx\n",
            "  💾 Manual de Psicologia Viral (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_psicologica/MANUAL_PSICOLOGIA_VIRAL_vid_coloque_metas_em_sua_vida_e_se_surpreenda.pdf\n",
            "  ✅ Relatórios gerados para vid_coloque_metas_em_sua_vida_e_se_surpreenda\n",
            "[3/5] Gerando relatórios para: vid_a_importancia_de_ser_rico_antes_de_ter\n",
            "  💾 Relatório de Texto (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_texto/RELATORIO_TEXTO_HUMANIZADO_vid_a_importancia_de_ser_rico_antes_de_ter.xlsx\n",
            "  💾 Estratégia de Conteúdo Textual (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_texto/ESTRATEGIA_CONTEUDO_TEXTUAL_vid_a_importancia_de_ser_rico_antes_de_ter.pdf\n",
            "  💾 Relatório de Áudio (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_audio/RELATORIO_AUDIO_HUMANIZADO_vid_a_importancia_de_ser_rico_antes_de_ter.xlsx\n",
            "  💾 Resumo de Áudio Estratégico (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_audio/RESUMO_AUDIO_ESTRATEGICO_vid_a_importancia_de_ser_rico_antes_de_ter.pdf\n",
            "  💾 Relatório Visual (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_visual/RELATORIO_VISUAL_HUMANIZADO_vid_a_importancia_de_ser_rico_antes_de_ter.xlsx\n",
            "  💾 Estratégia Visual Completa (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_visual/ESTRATEGIA_VISUAL_COMPLETA_vid_a_importancia_de_ser_rico_antes_de_ter.pdf\n",
            "  💾 Relatório Psicológico (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_psicologica/RELATORIO_PSICOLOGICO_HUMANIZADO_vid_a_importancia_de_ser_rico_antes_de_ter.xlsx\n",
            "  💾 Manual de Psicologia Viral (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_psicologica/MANUAL_PSICOLOGIA_VIRAL_vid_a_importancia_de_ser_rico_antes_de_ter.pdf\n",
            "  ✅ Relatórios gerados para vid_a_importancia_de_ser_rico_antes_de_ter\n",
            "[4/5] Gerando relatórios para: vid_as_três_fases_de_todo_mundo_que_decidiu_fazer_alguma_coisa\n",
            "  ❌ ERRO na geração de relatórios para vid_as_três_fases_de_todo_mundo_que_decidiu_fazer_alguma_coisa: 'latin-1' codec can't encode character '\\u201c' in position 625: ordinal not in range(256)\n",
            "[5/5] Gerando relatórios para: vid_a_melhor_saida_é_se_afastar_de_pessoas_perversas\n",
            "  ❌ ERRO na geração de relatórios para vid_a_melhor_saida_é_se_afastar_de_pessoas_perversas: 'latin-1' codec can't encode character '\\u201c' in position 304: ordinal not in range(256)\n",
            "\n",
            "✅ GERAÇÃO DE RELATÓRIOS HUMANIZADOS CONCLUÍDA!\n",
            "Total de vídeos com relatórios gerados: 3\n",
            "\n",
            "➡️ PRÓXIMA CÉLULA: 4.2 - GERAÇÃO DO BLUEPRINT FINAL E DASHBOARD\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CÉLULA 4.2: GERAÇÃO DO BLUEPRINT FINAL E DASHBOARD\n",
        "# ============================================================================\n",
        "\n",
        "def gerar_blueprint_dashboard():\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa(\"relatorios_humanizados\")\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Carregar todos os dados de análise\n",
        "    metadados_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "    decomposicao_path = os.path.join(PASTA_TRABALHO, \"dados\", \"decomposicao_completa.json\")\n",
        "    analises_padroes_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_padroes_completas.json\")\n",
        "    analises_psicologicas_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_psicologicas_completas.json\")\n",
        "\n",
        "    with open(metadados_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        metadados = json.load(f)\n",
        "    with open(decomposicao_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        decomposicoes = json.load(f)\n",
        "    with open(analises_padroes_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        analises_padroes = json.load(f)\n",
        "    with open(analises_psicologicas_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        analises_psicologicas = json.load(f)\n",
        "\n",
        "    dados_consolidados = []\n",
        "    for video_meta in metadados:\n",
        "        video_id = video_meta[\"id\"]\n",
        "        decomposicao = next((d for d in decomposicoes if d[\"video_id\"] == video_id), {})\n",
        "        analise_padroes = next((ap for ap in analises_padroes if ap[\"video_id\"] == video_id), {})\n",
        "        analise_psicologica = next((aps for aps in analises_psicologicas if aps[\"video_id\"] == video_id), {})\n",
        "        consolidado = {\n",
        "            \"video_id\": video_id,\n",
        "            \"nome_arquivo\": video_meta.get(\"nome_arquivo\"),\n",
        "            \"duracao_segundos\": video_meta.get(\"duracao_segundos\"),\n",
        "            \"formato_detectado\": video_meta.get(\"formato_detectado\"),\n",
        "            \"tem_audio\": video_meta.get(\"tem_audio\"),\n",
        "            \"total_frames\": video_meta.get(\"total_frames\"),\n",
        "            \"ocr_textos_count\": len(decomposicao.get(\"textos_ocr\", [])),\n",
        "            \"audio_transcrito_len\": len(decomposicao.get(\"audio_transcrito\", \"\")),\n",
        "            \"cortes_detectados_count\": len(decomposicao.get(\"cortes_detectados_segundos\", [])),\n",
        "            \"bpm_audio\": analise_padroes.get(\"analise_audio_detalhada\", {}).get(\"bpm\"),\n",
        "            \"complexidade_visual_media\": analise_padroes.get(\"analise_visual_detalhada\", {}).get(\"complexidade_visual_media\"),\n",
        "            \"brilho_medio\": analise_padroes.get(\"analise_visual_detalhada\", {}).get(\"brilho_medio\"),\n",
        "            \"padroes_gerais\": \", \".join(analise_padroes.get(\"padroes_gerais\", [])),\n",
        "            \"gatilhos_psicologicos\": \", \".join(analise_psicologica.get(\"gatilhos_detectados\", [])),\n",
        "            \"emocoes_predominantes\": str(analise_psicologica.get(\"emocoes_predominantes\", {})),\n",
        "            \"status_geral\": video_meta.get(\"status\") # Pode ser aprimorado para refletir o status de todas as etapas\n",
        "        }\n",
        "        dados_consolidados.append(consolidado)\n",
        "\n",
        "    df_final = pd.DataFrame(dados_consolidados)\n",
        "\n",
        "    # Salvar Dashboard Executivo (Excel)\n",
        "    dashboard_excel_path = os.path.join(PASTA_TRABALHO, \"dashboard\", \"DASHBOARD_MASTER_EXECUTIVO.xlsx\")\n",
        "    df_final.to_excel(dashboard_excel_path, index=False, engine=\"openpyxl\")\n",
        "    print(f\"\\n💾 Dashboard Executivo (XLSX) salvo em: {dashboard_excel_path}\")\n",
        "\n",
        "    # Salvar Dados Consolidados (CSV e JSON)\n",
        "    dados_csv_path = os.path.join(PASTA_TRABALHO, \"dashboard\", \"dados_consolidados.csv\")\n",
        "    df_final.to_csv(dados_csv_path, index=False, encoding=\"utf-8\")\n",
        "    print(f\"💾 Dados Consolidados (CSV) salvo em: {dados_csv_path}\")\n",
        "\n",
        "    dados_json_path = os.path.join(PASTA_TRABALHO, \"dashboard\", \"dados_detalhados.json\")\n",
        "    with open(dados_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(dados_consolidados, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"💾 Dados Detalhados (JSON) salvo em: {dados_json_path}\")\n",
        "\n",
        "    # Geração de Dashboard Interativo (HTML - Exemplo simples)\n",
        "    # Para um dashboard interativo real, seria necessário uma biblioteca como Plotly ou Dash\n",
        "    dashboard_html_path = os.path.join(PASTA_TRABALHO, \"dashboard\", \"dashboard_interativo.html\")\n",
        "    with open(dashboard_html_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"<html><body><h1>Dashboard Interativo (Placeholder)</h1><p>Seu dashboard interativo real seria gerado aqui com bibliotecas como Plotly ou Dash.</p></body></html>\")\n",
        "    print(f\"💾 Dashboard Interativo (HTML) salvo em: {dashboard_html_path}\")\n",
        "\n",
        "    # Geração do Blueprint Estratégico (PDF - Exemplo simples)\n",
        "    pdf = PDF()\n",
        "    pdf.add_page()\n",
        "    pdf.chapter_title(\"BLUEPRINT ESTRATÉGICO FINAL\")\n",
        "    pdf.chapter_body(\"Este é o seu blueprint estratégico final, consolidando todos os insights.\")\n",
        "    pdf.chapter_body(f\"Total de vídeos analisados: {len(df_final)}\")\n",
        "    pdf.chapter_body(f\"Média de duração dos vídeos: {df_final[\"duracao_segundos\"] .mean():.2f} segundos\")\n",
        "    pdf_blueprint_path = os.path.join(PASTA_TRABALHO, \"blueprint\", \"BLUEPRINT_ESTRATEGICO_FINAL.pdf\")\n",
        "    pdf.output(pdf_blueprint_path)\n",
        "    print(f\"💾 Blueprint Estratégico (PDF) salvo em: {pdf_blueprint_path}\")\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    config[\"status_etapas\"][\"blueprint\"] = True\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(\"\\n✅ GERAÇÃO DO BLUEPRINT FINAL E DASHBOARD CONCLUÍDA!\")\n",
        "    print(\"Todos os relatórios e o dashboard foram gerados com sucesso.\")\n",
        "    print(\"\\n🎉 PROCESSO DE ENGENHARIA REVERSA CONCLUÍDO COM SUCESSO! 🎉\")\n",
        "\n",
        "# Executar geração de blueprint e dashboard\n",
        "try:\n",
        "    gerar_blueprint_dashboard()\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ ERRO GERAL NA GERAÇÃO DO BLUEPRINT E DASHBOARD: {e}\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ],
      "metadata": {
        "id": "blueprint_dashboard",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fc0c3a2-ce6c-42d0-f7b9-e0ab25bfb396"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "💾 Dashboard Executivo (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dashboard/DASHBOARD_MASTER_EXECUTIVO.xlsx\n",
            "💾 Dados Consolidados (CSV) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dashboard/dados_consolidados.csv\n",
            "💾 Dados Detalhados (JSON) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dashboard/dados_detalhados.json\n",
            "💾 Dashboard Interativo (HTML) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dashboard/dashboard_interativo.html\n",
            "💾 Blueprint Estratégico (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/blueprint/BLUEPRINT_ESTRATEGICO_FINAL.pdf\n",
            "\n",
            "✅ GERAÇÃO DO BLUEPRINT FINAL E DASHBOARD CONCLUÍDA!\n",
            "Todos os relatórios e o dashboard foram gerados com sucesso.\n",
            "\n",
            "🎉 PROCESSO DE ENGENHARIA REVERSA CONCLUÍDO COM SUCESSO! 🎉\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Verificar se o processo de engenharia reversa foi executado\n",
        "BASE_PATH = \"/content/drive/MyDrive/Videos Dona Done/_engenharia_reversa\"\n",
        "CSV_PATH = os.path.join(BASE_PATH, \"dashboard\", \"dados_consolidados.csv\")\n",
        "JSON_PATH = os.path.join(BASE_PATH, \"dashboard\", \"dados_detalhados.json\")\n",
        "\n",
        "print(\"🔍 VERIFICANDO PRÉ-REQUISITOS...\")\n",
        "print(f\"Pasta base existe: {os.path.exists(BASE_PATH)}\")\n",
        "print(f\"CSV existe: {os.path.exists(CSV_PATH)}\")\n",
        "print(f\"JSON existe: {os.path.exists(JSON_PATH)}\")\n",
        "\n",
        "if os.path.exists(CSV_PATH):\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "    print(f\"📊 Dados CSV: {len(df)} vídeos encontrados\")\n",
        "\n",
        "print(\"\\n✅ Se todos os itens acima são True/existem, você pode prosseguir!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QgJMmh1JJ-0",
        "outputId": "252f927c-f55a-4a6d-fa2f-ef8fcb10f153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 VERIFICANDO PRÉ-REQUISITOS...\n",
            "Pasta base existe: True\n",
            "CSV existe: True\n",
            "JSON existe: True\n",
            "📊 Dados CSV: 5 vídeos encontrados\n",
            "\n",
            "✅ Se todos os itens acima são True/existem, você pode prosseguir!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SISTEMA DE INTEGRAÇÃO AUTOMÁTICA PARA NOVAS FUNCIONALIDADES\n",
        "# ============================================================================\n",
        "# Este script deve SUBSTITUIR a última célula (4.2) do notebook\n",
        "# Ele detecta automaticamente todas as análises disponíveis e as integra\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import glob\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def descobrir_analises_disponiveis(pasta_trabalho):\n",
        "    \"\"\"Descobre automaticamente todas as análises realizadas\"\"\"\n",
        "    analises_encontradas = {\n",
        "        \"base\": {},\n",
        "        \"adicionais\": {}\n",
        "    }\n",
        "\n",
        "    dados_path = os.path.join(pasta_trabalho, \"dados\")\n",
        "\n",
        "    # Análises básicas obrigatórias\n",
        "    arquivos_base = {\n",
        "        \"metadados\": \"metadados_completos.json\",\n",
        "        \"decomposicao\": \"decomposicao_completa.json\",\n",
        "        \"padroes\": \"analises_padroes_completas.json\",\n",
        "        \"psicologica\": \"analises_psicologicas_completas.json\"\n",
        "    }\n",
        "\n",
        "    for tipo, arquivo in arquivos_base.items():\n",
        "        caminho = os.path.join(dados_path, arquivo)\n",
        "        if os.path.exists(caminho):\n",
        "            analises_encontradas[\"base\"][tipo] = caminho\n",
        "            print(f\"✅ Análise base encontrada: {tipo}\")\n",
        "        else:\n",
        "            print(f\"⚠️ Análise base ausente: {tipo}\")\n",
        "\n",
        "    # Descobrir análises adicionais automaticamente\n",
        "    # Busca por qualquer arquivo JSON que não seja das análises base\n",
        "    todos_jsons = glob.glob(os.path.join(dados_path, \"*.json\"))\n",
        "\n",
        "    for json_path in todos_jsons:\n",
        "        nome_arquivo = os.path.basename(json_path)\n",
        "\n",
        "        # Pular arquivos base\n",
        "        if nome_arquivo in arquivos_base.values():\n",
        "            continue\n",
        "\n",
        "        # Identificar tipo da análise pelo nome\n",
        "        if \"audio\" in nome_arquivo.lower():\n",
        "            analises_encontradas[\"adicionais\"][\"audio_refinada\"] = json_path\n",
        "            print(f\"✅ Análise adicional encontrada: Audio Refinada\")\n",
        "        elif \"visual\" in nome_arquivo.lower():\n",
        "            analises_encontradas[\"adicionais\"][\"visual_avancada\"] = json_path\n",
        "            print(f\"✅ Análise adicional encontrada: Visual Avançada\")\n",
        "        elif \"texto\" in nome_arquivo.lower():\n",
        "            analises_encontradas[\"adicionais\"][\"texto_avancada\"] = json_path\n",
        "            print(f\"✅ Análise adicional encontrada: Texto Avançada\")\n",
        "        elif \"sentiment\" in nome_arquivo.lower():\n",
        "            analises_encontradas[\"adicionais\"][\"sentimento\"] = json_path\n",
        "            print(f\"✅ Análise adicional encontrada: Sentimento\")\n",
        "        else:\n",
        "            # Análise não reconhecida - incluir mesmo assim\n",
        "            nome_limpo = nome_arquivo.replace(\".json\", \"\").replace(\"_\", \" \").title()\n",
        "            analises_encontradas[\"adicionais\"][nome_arquivo] = json_path\n",
        "            print(f\"✅ Análise personalizada encontrada: {nome_limpo}\")\n",
        "\n",
        "    return analises_encontradas\n",
        "\n",
        "def carregar_dados_analise(caminho_arquivo):\n",
        "    \"\"\"Carrega dados de uma análise com tratamento de erros\"\"\"\n",
        "    try:\n",
        "        with open(caminho_arquivo, 'r', encoding='utf-8') as f:\n",
        "            dados = json.load(f)\n",
        "        return dados, True\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Erro ao carregar {caminho_arquivo}: {e}\")\n",
        "        return [], False\n",
        "\n",
        "def extrair_metricas_dinamicamente(dados, tipo_analise):\n",
        "    \"\"\"Extrai métricas de qualquer tipo de análise dinamicamente\"\"\"\n",
        "    metricas_extraidas = {}\n",
        "\n",
        "    if not dados:\n",
        "        return metricas_extraidas\n",
        "\n",
        "    # Pegar o primeiro item para entender a estrutura\n",
        "    primeiro_item = dados[0] if isinstance(dados, list) else dados\n",
        "\n",
        "    if isinstance(primeiro_item, dict):\n",
        "        for chave, valor in primeiro_item.items():\n",
        "            if chave in ['video_id', 'status', 'data_analise', 'erro']:\n",
        "                continue\n",
        "\n",
        "            # Extrair métricas numéricas automaticamente\n",
        "            if isinstance(valor, (int, float)):\n",
        "                metricas_extraidas[f\"{tipo_analise}_{chave}\"] = valor\n",
        "            elif isinstance(valor, dict):\n",
        "                # Análise aninhada - extrair sub-métricas\n",
        "                for sub_chave, sub_valor in valor.items():\n",
        "                    if isinstance(sub_valor, (int, float)):\n",
        "                        metricas_extraidas[f\"{tipo_analise}_{chave}_{sub_chave}\"] = sub_valor\n",
        "                    elif isinstance(sub_valor, list) and sub_valor and isinstance(sub_valor[0], (int, float)):\n",
        "                        # Lista de números - calcular estatísticas\n",
        "                        metricas_extraidas[f\"{tipo_analise}_{chave}_{sub_chave}_media\"] = sum(sub_valor) / len(sub_valor)\n",
        "                        metricas_extraidas[f\"{tipo_analise}_{chave}_{sub_chave}_max\"] = max(sub_valor)\n",
        "                        metricas_extraidas[f\"{tipo_analise}_{chave}_{sub_chave}_min\"] = min(sub_valor)\n",
        "            elif isinstance(valor, list):\n",
        "                if valor and isinstance(valor[0], (int, float)):\n",
        "                    # Lista de números\n",
        "                    metricas_extraidas[f\"{tipo_analise}_{chave}_count\"] = len(valor)\n",
        "                    metricas_extraidas[f\"{tipo_analise}_{chave}_media\"] = sum(valor) / len(valor) if valor else 0\n",
        "                else:\n",
        "                    # Lista de objetos ou strings\n",
        "                    metricas_extraidas[f\"{tipo_analise}_{chave}_count\"] = len(valor)\n",
        "\n",
        "    return metricas_extraidas\n",
        "\n",
        "def consolidar_todos_dados(analises_encontradas):\n",
        "    \"\"\"Consolida todos os dados de todas as análises encontradas\"\"\"\n",
        "    dados_consolidados = {}\n",
        "\n",
        "    # Carregar análises base\n",
        "    for tipo, caminho in analises_encontradas[\"base\"].items():\n",
        "        dados, sucesso = carregar_dados_analise(caminho)\n",
        "        if sucesso:\n",
        "            dados_consolidados[tipo] = dados\n",
        "\n",
        "    # Carregar análises adicionais\n",
        "    for tipo, caminho in analises_encontradas[\"adicionais\"].items():\n",
        "        dados, sucesso = carregar_dados_analise(caminho)\n",
        "        if sucesso:\n",
        "            dados_consolidados[tipo] = dados\n",
        "\n",
        "    # Criar DataFrame consolidado por vídeo\n",
        "    videos_df = pd.DataFrame()\n",
        "\n",
        "    # Começar com metadados base se disponível\n",
        "    if \"metadados\" in dados_consolidados:\n",
        "        videos_df = pd.DataFrame(dados_consolidados[\"metadados\"])\n",
        "        videos_df = videos_df.set_index('id')\n",
        "\n",
        "    # Integrar cada análise adicional\n",
        "    for tipo, dados in dados_consolidados.items():\n",
        "        if tipo == \"metadados\":\n",
        "            continue\n",
        "\n",
        "        print(f\"🔄 Integrando dados de: {tipo}\")\n",
        "\n",
        "        # Converter para DataFrame se for lista\n",
        "        if isinstance(dados, list):\n",
        "            df_analise = pd.DataFrame(dados)\n",
        "\n",
        "            if 'video_id' in df_analise.columns:\n",
        "                df_analise = df_analise.set_index('video_id')\n",
        "\n",
        "                # Extrair métricas dinamicamente\n",
        "                for video_id, row in df_analise.iterrows():\n",
        "                    metricas = extrair_metricas_dinamicamente([row.to_dict()], tipo)\n",
        "\n",
        "                    for metrica, valor in metricas.items():\n",
        "                        if video_id in videos_df.index:\n",
        "                            videos_df.loc[video_id, metrica] = valor\n",
        "                        else:\n",
        "                            # Criar nova linha se vídeo não existir\n",
        "                            videos_df.loc[video_id, metrica] = valor\n",
        "\n",
        "    return videos_df.reset_index()\n",
        "\n",
        "def gerar_dashboard_dinamico(df_consolidado, pasta_trabalho):\n",
        "    \"\"\"Gera dashboard dinâmico incluindo todas as análises encontradas\"\"\"\n",
        "    from openpyxl import Workbook\n",
        "    from openpyxl.styles import Font, Alignment, PatternFill\n",
        "\n",
        "    wb = Workbook()\n",
        "\n",
        "    # ABA 1: VISÃO GERAL DINÂMICA\n",
        "    ws_geral = wb.active\n",
        "    ws_geral.title = 'Visão Geral Completa'\n",
        "\n",
        "    # Header\n",
        "    ws_geral.cell(row=1, column=1).value = 'RELATÓRIO COMPLETO DE ENGENHARIA REVERSA'\n",
        "    ws_geral.cell(row=1, column=1).font = Font(bold=True, size=16)\n",
        "\n",
        "    # Estatísticas gerais\n",
        "    ws_geral.cell(row=3, column=1).value = 'ANÁLISES REALIZADAS'\n",
        "    ws_geral.cell(row=3, column=1).font = Font(bold=True, size=14)\n",
        "\n",
        "    # Contar colunas por tipo de análise\n",
        "    colunas_por_tipo = {}\n",
        "    for col in df_consolidado.columns:\n",
        "        if '_' in col:\n",
        "            tipo = col.split('_')[0]\n",
        "            colunas_por_tipo[tipo] = colunas_por_tipo.get(tipo, 0) + 1\n",
        "\n",
        "    row = 4\n",
        "    for tipo, count in colunas_por_tipo.items():\n",
        "        ws_geral.cell(row=row, column=1).value = f\"{tipo.upper()}\"\n",
        "        ws_geral.cell(row=row, column=2).value = f\"{count} métricas extraídas\"\n",
        "        ws_geral.cell(row=row, column=1).font = Font(bold=True)\n",
        "        row += 1\n",
        "\n",
        "    # ABA 2: DADOS COMPLETOS\n",
        "    ws_dados = wb.create_sheet('Dados Completos')\n",
        "\n",
        "    # Adicionar todos os dados\n",
        "    for r_idx, row in enumerate(df_consolidado.itertuples(), 1):\n",
        "        for c_idx, value in enumerate(row):\n",
        "            cell = ws_dados.cell(row=r_idx, column=c_idx)\n",
        "            cell.value = value\n",
        "            if r_idx == 1:  # Header\n",
        "                cell.font = Font(bold=True)\n",
        "\n",
        "    # ABA 3: MÉTRICAS POR TIPO\n",
        "    tipos_encontrados = list(set([col.split('_')[0] for col in df_consolidado.columns if '_' in col]))\n",
        "\n",
        "    for tipo in tipos_encontrados:\n",
        "        ws_tipo = wb.create_sheet(f'Análise {tipo.title()}')\n",
        "\n",
        "        # Filtrar colunas deste tipo\n",
        "        colunas_tipo = ['id', 'nome_arquivo'] + [col for col in df_consolidado.columns if col.startswith(tipo)]\n",
        "\n",
        "        if len(colunas_tipo) > 2:  # Tem dados além do id e nome\n",
        "            df_tipo = df_consolidado[colunas_tipo]\n",
        "\n",
        "            # Adicionar ao worksheet\n",
        "            for r_idx, row in enumerate(df_tipo.itertuples(), 1):\n",
        "                for c_idx, value in enumerate(row):\n",
        "                    cell = ws_tipo.cell(row=r_idx, column=c_idx)\n",
        "                    cell.value = value\n",
        "                    if r_idx == 1:\n",
        "                        cell.font = Font(bold=True)\n",
        "\n",
        "    # ABA 4: INSIGHTS AUTOMATICOS\n",
        "    ws_insights = wb.create_sheet('Insights Automáticos')\n",
        "\n",
        "    insights_automaticos = gerar_insights_automaticos(df_consolidado)\n",
        "\n",
        "    ws_insights.cell(row=1, column=1).value = 'INSIGHTS GERADOS AUTOMATICAMENTE'\n",
        "    ws_insights.cell(row=1, column=1).font = Font(bold=True, size=16)\n",
        "\n",
        "    for i, insight in enumerate(insights_automaticos, 3):\n",
        "        ws_insights.cell(row=i, column=1).value = f\"• {insight}\"\n",
        "        ws_insights.cell(row=i, column=1).alignment = Alignment(wrap_text=True)\n",
        "\n",
        "    # Salvar\n",
        "    output_path = os.path.join(pasta_trabalho, \"dashboard\", \"RELATORIO_COMPLETO_DINAMICO.xlsx\")\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    wb.save(output_path)\n",
        "\n",
        "    return output_path\n",
        "\n",
        "def gerar_insights_automaticos(df):\n",
        "    \"\"\"Gera insights automáticos baseados em qualquer conjunto de dados\"\"\"\n",
        "    insights = []\n",
        "\n",
        "    # Análise de correlações automáticas\n",
        "    colunas_numericas = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "    if len(colunas_numericas) > 1:\n",
        "        correlacoes = df[colunas_numericas].corr()\n",
        "\n",
        "        # Encontrar correlações fortes\n",
        "        for col1 in correlacoes.columns:\n",
        "            for col2 in correlacoes.columns:\n",
        "                if col1 != col2:\n",
        "                    corr_val = correlacoes.loc[col1, col2]\n",
        "                    if abs(corr_val) > 0.7:\n",
        "                        insights.append(f\"CORRELAÇÃO FORTE: {col1} e {col2} têm correlação de {corr_val:.2f}\")\n",
        "\n",
        "    # Identificar outliers automáticos\n",
        "    for col in colunas_numericas:\n",
        "        if df[col].std() > 0:  # Evitar divisão por zero\n",
        "            media = df[col].mean()\n",
        "            std = df[col].std()\n",
        "            outliers = df[(df[col] > media + 2*std) | (df[col] < media - 2*std)]\n",
        "\n",
        "            if len(outliers) > 0:\n",
        "                insights.append(f\"OUTLIERS DETECTADOS: {len(outliers)} vídeos têm valores extremos em {col}\")\n",
        "\n",
        "    # Análise de distribuições\n",
        "    for col in colunas_numericas:\n",
        "        if col.endswith('_score') or 'score' in col:\n",
        "            media = df[col].mean()\n",
        "            if media > 80:\n",
        "                insights.append(f\"PERFORMANCE ALTA: Score médio de {col} é {media:.1f} - excelente resultado\")\n",
        "            elif media < 50:\n",
        "                insights.append(f\"OPORTUNIDADE: Score médio de {col} é {media:.1f} - há espaço para melhorias\")\n",
        "\n",
        "    return insights if insights else [\"Análise de insights em andamento - dados sendo processados\"]\n",
        "\n",
        "def atualizar_config_com_novas_analises(pasta_trabalho, analises_encontradas):\n",
        "    \"\"\"Atualiza config.json com status de todas as análises encontradas\"\"\"\n",
        "    config_path = os.path.join(pasta_trabalho, \"config\", \"config.json\")\n",
        "\n",
        "    # Carregar config existente\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    # Atualizar status das análises encontradas\n",
        "    for tipo in analises_encontradas[\"base\"]:\n",
        "        config[\"status_etapas\"][tipo] = True\n",
        "\n",
        "    for tipo in analises_encontradas[\"adicionais\"]:\n",
        "        config[\"status_etapas\"][f\"analise_{tipo}\"] = True\n",
        "\n",
        "    config[\"ultima_consolidacao\"] = datetime.now().isoformat()\n",
        "    config[\"total_analises_integradas\"] = len(analises_encontradas[\"base\"]) + len(analises_encontradas[\"adicionais\"])\n",
        "\n",
        "    # Salvar config atualizado\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "def main_integracao_automatica():\n",
        "    \"\"\"Função principal da integração automática\"\"\"\n",
        "    print(\"🚀 INICIANDO INTEGRAÇÃO AUTOMÁTICA DE TODAS AS ANÁLISES\")\n",
        "\n",
        "    # Usar variável global da pasta de trabalho\n",
        "    if \"PASTA_TRABALHO\" not in globals():\n",
        "        print(\"❌ ERRO: Execute as células anteriores primeiro\")\n",
        "        return False\n",
        "\n",
        "    pasta_trabalho = PASTA_TRABALHO\n",
        "\n",
        "    try:\n",
        "        # Passo 1: Descobrir análises\n",
        "        print(\"\\n🔍 DESCOBRINDO ANÁLISES DISPONÍVEIS...\")\n",
        "        analises = descobrir_analises_disponiveis(pasta_trabalho)\n",
        "\n",
        "        total_analises = len(analises[\"base\"]) + len(analises[\"adicionais\"])\n",
        "        print(f\"📊 Total de análises encontradas: {total_analises}\")\n",
        "\n",
        "        # Passo 2: Consolidar dados\n",
        "        print(\"\\n🔄 CONSOLIDANDO TODOS OS DADOS...\")\n",
        "        df_consolidado = consolidar_todos_dados(analises)\n",
        "\n",
        "        print(f\"📈 {len(df_consolidado)} vídeos consolidados com {len(df_consolidado.columns)} métricas totais\")\n",
        "\n",
        "        # Passo 3: Gerar dashboard dinâmico\n",
        "        print(\"\\n📊 GERANDO DASHBOARD DINÂMICO...\")\n",
        "        dashboard_path = gerar_dashboard_dinamico(df_consolidado, pasta_trabalho)\n",
        "\n",
        "        # Passo 4: Salvar dados consolidados\n",
        "        csv_path = os.path.join(pasta_trabalho, \"dashboard\", \"dados_completos_consolidados.csv\")\n",
        "        df_consolidado.to_csv(csv_path, index=False, encoding='utf-8')\n",
        "\n",
        "        json_path = os.path.join(pasta_trabalho, \"dashboard\", \"dados_completos_consolidados.json\")\n",
        "        df_consolidado.to_json(json_path, orient='records', indent=2, force_ascii=False)\n",
        "\n",
        "        # Passo 5: Atualizar configuração\n",
        "        print(\"\\n⚙️ ATUALIZANDO CONFIGURAÇÕES...\")\n",
        "        atualizar_config_com_novas_analises(pasta_trabalho, analises)\n",
        "\n",
        "        # Resultados finais\n",
        "        print(\"\\n✅ INTEGRAÇÃO AUTOMÁTICA CONCLUÍDA COM SUCESSO!\")\n",
        "        print(f\"📁 Dashboard dinâmico: {dashboard_path}\")\n",
        "        print(f\"📁 Dados CSV: {csv_path}\")\n",
        "        print(f\"📁 Dados JSON: {json_path}\")\n",
        "        print(f\"📊 {len(df_consolidado)} vídeos processados\")\n",
        "        print(f\"📈 {len(df_consolidado.columns)} métricas totais integradas\")\n",
        "\n",
        "        print(\"\\n🎯 PRÓXIMOS PASSOS:\")\n",
        "        print(\"• Abra o arquivo Excel para ver todas as análises integradas\")\n",
        "        print(\"• Use os dados CSV/JSON em outras ferramentas de análise\")\n",
        "        print(\"• Execute novamente sempre que adicionar novas análises\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ ERRO NA INTEGRAÇÃO AUTOMÁTICA: {e}\")\n",
        "        print(\"Verifique se todas as análises anteriores foram executadas com sucesso\")\n",
        "        return False\n",
        "\n",
        "# Executar integração automática\n",
        "if __name__ == \"__main__\":\n",
        "    main_integracao_automatica()"
      ],
      "metadata": {
        "id": "fqmuwt7wttJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CÉLULA 4.3: DASHBOARD MASTER EXECUTIVO INTELIGENTE APRIMORADO\n",
        "# ============================================================================\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "from openpyxl.styles import Font, Alignment, PatternFill\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def log_progress(message):\n",
        "    \"\"\"Log de progresso em tempo real\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
        "    print(f\"[{timestamp}] {message}\")\n",
        "\n",
        "def calculate_viral_score(row):\n",
        "    \"\"\"Calcula score de viralidade baseado em múltiplos fatores\"\"\"\n",
        "    try:\n",
        "        score = 0\n",
        "\n",
        "        # Fator 1: Ritmo (cortes por segundo) - peso 25%\n",
        "        if pd.notna(row['duracao_segundos']) and row['duracao_segundos'] > 0:\n",
        "            cortes_por_seg = row['cortes_detectados_count'] / row['duracao_segundos']\n",
        "            if cortes_por_seg > 20: score += 25\n",
        "            elif cortes_por_seg > 10: score += 20\n",
        "            elif cortes_por_seg > 5: score += 15\n",
        "            else: score += 10\n",
        "\n",
        "        # Fator 2: Complexidade Visual - peso 20%\n",
        "        if pd.notna(row['complexidade_visual_media']):\n",
        "            if row['complexidade_visual_media'] > 600: score += 20\n",
        "            elif row['complexidade_visual_media'] > 400: score += 15\n",
        "            else: score += 10\n",
        "\n",
        "        # Fator 3: Presença de Texto (OCR) - peso 15%\n",
        "        if pd.notna(row['ocr_textos_count']):\n",
        "            if row['ocr_textos_count'] > 10: score += 15\n",
        "            elif row['ocr_textos_count'] > 5: score += 12\n",
        "            elif row['ocr_textos_count'] > 0: score += 8\n",
        "\n",
        "        # Fator 4: Duração Ideal - peso 20%\n",
        "        if pd.notna(row['duracao_segundos']):\n",
        "            if 15 <= row['duracao_segundos'] <= 30: score += 20\n",
        "            elif 10 <= row['duracao_segundos'] <= 45: score += 15\n",
        "            else: score += 10\n",
        "\n",
        "        # Fator 5: Gatilhos Psicológicos - peso 20%\n",
        "        gatilhos = str(row['gatilhos_psicologicos']).lower()\n",
        "        if 'urgência' in gatilhos or 'escassez' in gatilhos: score += 8\n",
        "        if 'estímulo' in gatilhos: score += 7\n",
        "        if 'atenção' in gatilhos: score += 5\n",
        "\n",
        "        return min(score, 100)\n",
        "    except:\n",
        "        return 50\n",
        "\n",
        "def calculate_technical_score(row):\n",
        "    \"\"\"Score técnico baseado em qualidade de produção\"\"\"\n",
        "    try:\n",
        "        score = 0\n",
        "\n",
        "        if pd.notna(row['brilho_medio']):\n",
        "            if 120 <= row['brilho_medio'] <= 180: score += 25\n",
        "            elif 100 <= row['brilho_medio'] <= 200: score += 20\n",
        "            else: score += 10\n",
        "\n",
        "        formato = str(row['formato_detectado'])\n",
        "        if 'vertical_9_16' in formato: score += 25\n",
        "        elif 'horizontal_16_9' in formato: score += 20\n",
        "        else: score += 15\n",
        "\n",
        "        if row['tem_audio']: score += 25\n",
        "        else: score += 5\n",
        "\n",
        "        if pd.notna(row['total_frames']) and row['total_frames'] > 0:\n",
        "            if row['total_frames'] > 300: score += 25\n",
        "            elif row['total_frames'] > 150: score += 20\n",
        "            else: score += 15\n",
        "\n",
        "        return min(score, 100)\n",
        "    except:\n",
        "        return 50\n",
        "\n",
        "def calculate_content_score(row):\n",
        "    \"\"\"Score de conteúdo baseado em riqueza informacional\"\"\"\n",
        "    try:\n",
        "        score = 0\n",
        "\n",
        "        ocr_count = row['ocr_textos_count'] if pd.notna(row['ocr_textos_count']) else 0\n",
        "        audio_len = row['audio_transcrito_len'] if pd.notna(row['audio_transcrito_len']) else 0\n",
        "\n",
        "        if ocr_count > 5 or audio_len > 100: score += 30\n",
        "        elif ocr_count > 2 or audio_len > 50: score += 20\n",
        "        elif ocr_count > 0 or audio_len > 0: score += 15\n",
        "        else: score += 5\n",
        "\n",
        "        if pd.notna(row['bpm_audio']):\n",
        "            if 120 <= row['bpm_audio'] <= 140: score += 35\n",
        "            elif 100 <= row['bpm_audio'] <= 160: score += 25\n",
        "            else: score += 15\n",
        "\n",
        "        if pd.notna(row['duracao_segundos']) and row['duracao_segundos'] > 0:\n",
        "            densidade = (ocr_count + audio_len/10) / row['duracao_segundos']\n",
        "            if densidade > 2: score += 35\n",
        "            elif densidade > 1: score += 25\n",
        "            else: score += 15\n",
        "\n",
        "        return min(score, 100)\n",
        "    except:\n",
        "        return 50\n",
        "\n",
        "def generate_insights_from_data(df):\n",
        "    \"\"\"Gera insights inteligentes baseados nos dados\"\"\"\n",
        "    insights = []\n",
        "\n",
        "    try:\n",
        "        best_performing = df.nlargest(3, 'viral_score')\n",
        "        avg_duration = best_performing['duracao_segundos'].mean()\n",
        "        insights.append(f\"DURAÇÃO VENCEDORA: Seus top 3 vídeos têm duração média de {avg_duration:.1f}s. Este é seu sweet spot comprovado.\")\n",
        "\n",
        "        avg_cuts_per_sec = (best_performing['cortes_detectados_count'] / best_performing['duracao_segundos']).mean()\n",
        "        insights.append(f\"RITMO IDEAL: {avg_cuts_per_sec:.1f} cortes por segundo é sua fórmula de edição mais eficaz.\")\n",
        "\n",
        "        formato_winner = df['formato_detectado'].mode()[0] if not df['formato_detectado'].empty else 'N/A'\n",
        "        formato_count = df['formato_detectado'].value_counts().iloc[0] if not df['formato_detectado'].empty else 0\n",
        "        insights.append(f\"FORMATO DOMINANTE: {formato_count} vídeos em {formato_winner}. Este é seu formato de maior alcance.\")\n",
        "\n",
        "        high_viral = df[df['viral_score'] > 70]\n",
        "        if not high_viral.empty:\n",
        "            avg_complexity = high_viral['complexidade_visual_media'].mean()\n",
        "            insights.append(f\"COMPLEXIDADE VISUAL ÓTIMA: Vídeos com score viral alto têm complexidade média de {avg_complexity:.0f}. Use como referência.\")\n",
        "\n",
        "        text_heavy = df[df['ocr_textos_count'] > 5]\n",
        "        if not text_heavy.empty:\n",
        "            insights.append(f\"ESTRATÉGIA DE TEXTO: {len(text_heavy)} vídeos com muito texto têm score médio de {text_heavy['viral_score'].mean():.0f}. Texto na tela impacta performance.\")\n",
        "\n",
        "        # CORRIGIDO: bpm_audio em vez de bmp_audio\n",
        "        if df['bpm_audio'].notna().any():\n",
        "            successful_bpm = df[df['viral_score'] > 60]['bpm_audio'].mean()\n",
        "            insights.append(f\"BPM DE SUCESSO: {successful_bpm:.0f} BPM é o ritmo de áudio dos seus vídeos mais virais.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_progress(f\"Erro ao gerar insights: {e}\")\n",
        "        insights.append(\"Insights parciais disponíveis devido a limitações nos dados.\")\n",
        "\n",
        "    return insights\n",
        "\n",
        "def add_data_to_sheet(ws, data, start_row=1, start_col=1, headers=None):\n",
        "    \"\"\"Adiciona dados a uma planilha de forma segura\"\"\"\n",
        "    current_row = start_row\n",
        "\n",
        "    # Adicionar cabeçalhos se fornecidos\n",
        "    if headers:\n",
        "        for col_idx, header in enumerate(headers):\n",
        "            cell = ws.cell(row=current_row, column=start_col + col_idx)\n",
        "            cell.value = header\n",
        "            cell.font = Font(bold=True)\n",
        "        current_row += 1\n",
        "\n",
        "    # Adicionar dados\n",
        "    for row_data in data:\n",
        "        for col_idx, value in enumerate(row_data):\n",
        "            cell = ws.cell(row=current_row, column=start_col + col_idx)\n",
        "            cell.value = value\n",
        "        current_row += 1\n",
        "\n",
        "    return current_row\n",
        "\n",
        "def create_enhanced_dashboard_master(csv_path, json_path, output_path):\n",
        "    \"\"\"Cria dashboard master executivo aprimorado\"\"\"\n",
        "\n",
        "    log_progress(\"INICIANDO CRIAÇÃO DO DASHBOARD MASTER EXECUTIVO INTELIGENTE\")\n",
        "\n",
        "    try:\n",
        "        # Carregar dados\n",
        "        log_progress(\"Carregando dados consolidados...\")\n",
        "        df_consolidado = pd.read_csv(csv_path, encoding='utf-8')\n",
        "\n",
        "        with open(json_path, 'r', encoding='utf-8') as f:\n",
        "            dados_detalhados = json.load(f)\n",
        "\n",
        "        log_progress(f\"Dados carregados: {len(df_consolidado)} vídeos encontrados\")\n",
        "\n",
        "        # Pré-processamento inteligente\n",
        "        log_progress(\"Processando inteligência artificial dos dados...\")\n",
        "\n",
        "        # Limpar e converter dados\n",
        "        try:\n",
        "            df_consolidado['emocoes_predominantes'] = df_consolidado['emocoes_predominantes'].apply(\n",
        "                lambda x: json.loads(x.replace(\"'\", '\"')) if pd.notna(x) and x != '{}' else {}\n",
        "            )\n",
        "        except:\n",
        "            df_consolidado['emocoes_predominantes'] = [{}] * len(df_consolidado)\n",
        "\n",
        "        # Calcular scores inteligentes\n",
        "        log_progress(\"Calculando scores de performance...\")\n",
        "        df_consolidado['viral_score'] = df_consolidado.apply(calculate_viral_score, axis=1)\n",
        "        df_consolidado['technical_score'] = df_consolidado.apply(calculate_technical_score, axis=1)\n",
        "        df_consolidado['content_score'] = df_consolidado.apply(calculate_content_score, axis=1)\n",
        "        df_consolidado['overall_score'] = (df_consolidado['viral_score'] + df_consolidado['technical_score'] + df_consolidado['content_score']) / 3\n",
        "\n",
        "        # Calcular métricas avançadas\n",
        "        df_consolidado['cortes_por_segundo'] = df_consolidado['cortes_detectados_count'] / df_consolidado['duracao_segundos'].replace(0, 1)\n",
        "        df_consolidado['densidade_texto'] = df_consolidado['ocr_textos_count'] / df_consolidado['duracao_segundos'].replace(0, 1)\n",
        "        df_consolidado['eficiencia_audio'] = df_consolidado['audio_transcrito_len'] / df_consolidado['duracao_segundos'].replace(0, 1)\n",
        "\n",
        "        log_progress(\"Gerando insights estratégicos...\")\n",
        "        insights = generate_insights_from_data(df_consolidado)\n",
        "\n",
        "        # Criar workbook\n",
        "        log_progress(\"Criando estrutura do dashboard...\")\n",
        "        wb = Workbook()\n",
        "\n",
        "        # === ABA 1: EXECUTIVE SUMMARY ===\n",
        "        log_progress(\"Criando Executive Summary...\")\n",
        "        ws_summary = wb.active\n",
        "        ws_summary.title = 'Executive Summary'\n",
        "\n",
        "        # Header principal\n",
        "        header_cell = ws_summary.cell(row=1, column=1)\n",
        "        header_cell.value = 'DASHBOARD MASTER EXECUTIVO - ENGENHARIA REVERSA DE VÍDEOS'\n",
        "        header_cell.font = Font(bold=True, size=18, color='FFFFFF')\n",
        "        header_cell.fill = PatternFill(start_color='1F4E79', end_color='1F4E79', fill_type='solid')\n",
        "        header_cell.alignment = Alignment(horizontal='center', vertical='center')\n",
        "\n",
        "        # Expandir header manualmente\n",
        "        for col in range(2, 9):\n",
        "            cell = ws_summary.cell(row=1, column=col)\n",
        "            cell.fill = PatternFill(start_color='1F4E79', end_color='1F4E79', fill_type='solid')\n",
        "\n",
        "        # KPIs Principais\n",
        "        kpi_cell = ws_summary.cell(row=3, column=1)\n",
        "        kpi_cell.value = 'INDICADORES DE PERFORMANCE PRINCIPAIS'\n",
        "        kpi_cell.font = Font(bold=True, size=14)\n",
        "        kpi_cell.fill = PatternFill(start_color='E7E6E6', end_color='E7E6E6', fill_type='solid')\n",
        "\n",
        "        kpis_data = [\n",
        "            ['Total de Vídeos Analisados', len(df_consolidado)],\n",
        "            ['Score Viral Médio', f\"{df_consolidado['viral_score'].mean():.1f}/100\"],\n",
        "            ['Score Técnico Médio', f\"{df_consolidado['technical_score'].mean():.1f}/100\"],\n",
        "            ['Score de Conteúdo Médio', f\"{df_consolidado['content_score'].mean():.1f}/100\"],\n",
        "            ['Duração Média Otimizada', f\"{df_consolidado['duracao_segundos'].mean():.1f}s\"],\n",
        "            ['Ritmo Médio de Cortes', f\"{df_consolidado['cortes_por_segundo'].mean():.1f}/seg\"],\n",
        "        ]\n",
        "\n",
        "        add_data_to_sheet(ws_summary, kpis_data, start_row=4, start_col=1)\n",
        "\n",
        "        # Top 3 Vídeos\n",
        "        top3_cell = ws_summary.cell(row=3, column=4)\n",
        "        top3_cell.value = 'TOP 3 VÍDEOS POR PERFORMANCE'\n",
        "        top3_cell.font = Font(bold=True, size=14)\n",
        "        top3_cell.fill = PatternFill(start_color='E7E6E6', end_color='E7E6E6', fill_type='solid')\n",
        "\n",
        "        top3 = df_consolidado.nlargest(3, 'overall_score')[['nome_arquivo', 'overall_score', 'viral_score', 'technical_score', 'content_score']]\n",
        "\n",
        "        top3_data = []\n",
        "        for _, video in top3.iterrows():\n",
        "            nome_curto = video['nome_arquivo'][:30] + \"...\" if len(video['nome_arquivo']) > 30 else video['nome_arquivo']\n",
        "            top3_data.append([\n",
        "                nome_curto,\n",
        "                f\"{video['overall_score']:.1f}\",\n",
        "                f\"{video['viral_score']:.1f}\",\n",
        "                f\"{video['technical_score']:.1f}\",\n",
        "                f\"{video['content_score']:.1f}\"\n",
        "            ])\n",
        "\n",
        "        top3_headers = ['Vídeo', 'Score Geral', 'Viral', 'Técnico', 'Conteúdo']\n",
        "        add_data_to_sheet(ws_summary, top3_data, start_row=4, start_col=4, headers=top3_headers)\n",
        "\n",
        "        # Insights Estratégicos\n",
        "        insights_cell = ws_summary.cell(row=12, column=1)\n",
        "        insights_cell.value = 'INSIGHTS ESTRATÉGICOS BASEADOS EM IA'\n",
        "        insights_cell.font = Font(bold=True, size=14, color='FFFFFF')\n",
        "        insights_cell.fill = PatternFill(start_color='C5504B', end_color='C5504B', fill_type='solid')\n",
        "        insights_cell.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Adicionar insights\n",
        "        for i, insight in enumerate(insights, 13):\n",
        "            insight_cell = ws_summary.cell(row=i, column=1)\n",
        "            insight_cell.value = f\"• {insight}\"\n",
        "            insight_cell.alignment = Alignment(wrap_text=True)\n",
        "\n",
        "        # === ABA 2: ANÁLISE DE PERFORMANCE ===\n",
        "        log_progress(\"Criando Análise de Performance...\")\n",
        "        ws_performance = wb.create_sheet('Análise de Performance')\n",
        "\n",
        "        perf_header = ws_performance.cell(row=1, column=1)\n",
        "        perf_header.value = 'ANÁLISE DETALHADA DE PERFORMANCE'\n",
        "        perf_header.font = Font(bold=True, size=16)\n",
        "        perf_header.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Ranking completo\n",
        "        ranking_data = df_consolidado[['nome_arquivo', 'overall_score', 'viral_score', 'technical_score', 'content_score',\n",
        "                                     'duracao_segundos', 'cortes_por_segundo', 'formato_detectado']].sort_values('overall_score', ascending=False)\n",
        "\n",
        "        ranking_list = []\n",
        "        for _, video in ranking_data.iterrows():\n",
        "            nome_curto = video['nome_arquivo'][:40] + \"...\" if len(video['nome_arquivo']) > 40 else video['nome_arquivo']\n",
        "            ranking_list.append([\n",
        "                nome_curto,\n",
        "                f\"{video['overall_score']:.1f}\",\n",
        "                f\"{video['viral_score']:.1f}\",\n",
        "                f\"{video['technical_score']:.1f}\",\n",
        "                f\"{video['content_score']:.1f}\",\n",
        "                f\"{video['duracao_segundos']:.1f}s\",\n",
        "                f\"{video['cortes_por_segundo']:.1f}\",\n",
        "                video['formato_detectado']\n",
        "            ])\n",
        "\n",
        "        ranking_headers = ['Vídeo', 'Score Geral', 'Viral', 'Técnico', 'Conteúdo', 'Duração', 'Cortes/s', 'Formato']\n",
        "        add_data_to_sheet(ws_performance, ranking_list, start_row=3, start_col=1, headers=ranking_headers)\n",
        "\n",
        "        # === ABA 3: INTELIGÊNCIA TÉCNICA ===\n",
        "        log_progress(\"Criando Inteligência Técnica...\")\n",
        "        ws_tecnica = wb.create_sheet('Inteligência Técnica')\n",
        "\n",
        "        tec_header = ws_tecnica.cell(row=1, column=1)\n",
        "        tec_header.value = 'ANÁLISE TÉCNICA AVANÇADA'\n",
        "        tec_header.font = Font(bold=True, size=16)\n",
        "        tec_header.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Análise de correlações\n",
        "        corr_header = ws_tecnica.cell(row=3, column=1)\n",
        "        corr_header.value = 'CORRELAÇÕES DESCOBERTAS'\n",
        "        corr_header.font = Font(bold=True, size=12)\n",
        "\n",
        "        correlations_data = [\n",
        "            ['Duração vs Score Viral', f\"{df_consolidado['duracao_segundos'].corr(df_consolidado['viral_score']):.3f}\", 'CORRELAÇÃO MODERADA'],\n",
        "            ['Cortes/s vs Score Viral', f\"{df_consolidado['cortes_por_segundo'].corr(df_consolidado['viral_score']):.3f}\", 'CORRELAÇÃO MODERADA'],\n",
        "            ['Complexidade Visual vs Performance', f\"{df_consolidado['complexidade_visual_media'].corr(df_consolidado['overall_score']):.3f}\", 'CORRELAÇÃO FRACA'],\n",
        "            ['BPM vs Engajamento', f\"{df_consolidado['bpm_audio'].corr(df_consolidado['viral_score']) if df_consolidado['bpm_audio'].notna().any() else 0:.3f}\", 'CORRELAÇÃO FRACA'],\n",
        "        ]\n",
        "\n",
        "        corr_headers = ['Métrica', 'Correlação', 'Classificação']\n",
        "        add_data_to_sheet(ws_tecnica, correlations_data, start_row=4, start_col=1, headers=corr_headers)\n",
        "\n",
        "        # === ABA 4: BLUEPRINT DE PRODUÇÃO ===\n",
        "        log_progress(\"Criando Blueprint de Produção...\")\n",
        "        ws_blueprint = wb.create_sheet('Blueprint de Produção')\n",
        "\n",
        "        bp_header = ws_blueprint.cell(row=1, column=1)\n",
        "        bp_header.value = 'BLUEPRINT ESTRATÉGICO DE PRODUÇÃO'\n",
        "        bp_header.font = Font(bold=True, size=16, color='FFFFFF')\n",
        "        bp_header.fill = PatternFill(start_color='1F4E79', end_color='1F4E79', fill_type='solid')\n",
        "        bp_header.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Receita de sucesso baseada nos top performers\n",
        "        top_performers = df_consolidado[df_consolidado['overall_score'] > df_consolidado['overall_score'].quantile(0.7)]\n",
        "\n",
        "        blueprint_data = [\n",
        "            ['DURAÇÃO IDEAL', f\"{top_performers['duracao_segundos'].mean():.1f} segundos (±{top_performers['duracao_segundos'].std():.1f}s)\"],\n",
        "            ['RITMO DE EDIÇÃO', f\"{top_performers['cortes_por_segundo'].mean():.1f} cortes por segundo\"],\n",
        "            ['FORMATO VENCEDOR', top_performers['formato_detectado'].mode()[0] if not top_performers.empty else 'N/A'],\n",
        "            ['COMPLEXIDADE VISUAL', f\"Nível {top_performers['complexidade_visual_media'].mean():.0f} (escala de estímulo)\"],\n",
        "            ['BPM RECOMENDADO', f\"{top_performers['bpm_audio'].mean():.0f} BPM\" if top_performers['bpm_audio'].notna().any() else 'N/A'],\n",
        "            ['DENSIDADE DE TEXTO', f\"{top_performers['densidade_texto'].mean():.1f} textos por segundo\"],\n",
        "        ]\n",
        "\n",
        "        bp_sub_header = ws_blueprint.cell(row=3, column=1)\n",
        "        bp_sub_header.value = 'FÓRMULA DE SUCESSO BASEADA EM DADOS'\n",
        "        bp_sub_header.font = Font(bold=True, size=12)\n",
        "\n",
        "        add_data_to_sheet(ws_blueprint, blueprint_data, start_row=4, start_col=1)\n",
        "\n",
        "        # === ABA 5: RECOMENDAÇÕES ESTRATÉGICAS ===\n",
        "        log_progress(\"Criando Recomendações Estratégicas...\")\n",
        "        ws_recomendacoes = wb.create_sheet('Recomendações Estratégicas')\n",
        "\n",
        "        rec_header = ws_recomendacoes.cell(row=1, column=1)\n",
        "        rec_header.value = 'RECOMENDAÇÕES ESTRATÉGICAS BASEADAS EM IA'\n",
        "        rec_header.font = Font(bold=True, size=16, color='FFFFFF')\n",
        "        rec_header.fill = PatternFill(start_color='C5504B', end_color='C5504B', fill_type='solid')\n",
        "        rec_header.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Recomendações inteligentes baseadas nos dados\n",
        "        recommendations = []\n",
        "\n",
        "        # Análise de duração\n",
        "        if df_consolidado['duracao_segundos'].mean() > 60:\n",
        "            recommendations.append(['DURAÇÃO', 'REDUZA DURAÇÃO', 'Seus vídeos estão longos demais. Vídeos de 15-30s têm melhor performance.', 'ALTA'])\n",
        "        elif df_consolidado['duracao_segundos'].mean() < 15:\n",
        "            recommendations.append(['DURAÇÃO', 'AUMENTE DURAÇÃO', 'Vídeos muito curtos podem não transmitir valor suficiente.', 'MÉDIA'])\n",
        "\n",
        "        # Análise de ritmo\n",
        "        avg_cuts_per_sec = df_consolidado['cortes_por_segundo'].mean()\n",
        "        if avg_cuts_per_sec < 5:\n",
        "            recommendations.append(['EDIÇÃO', 'ACELERE O RITMO', 'Aumente o número de cortes para manter atenção. Meta: 8-12 cortes/segundo.', 'ALTA'])\n",
        "        elif avg_cuts_per_sec > 20:\n",
        "            recommendations.append(['EDIÇÃO', 'DIMINUA CORTES', 'Muitos cortes podem causar fadiga visual. Encontre o equilíbrio.', 'MÉDIA'])\n",
        "\n",
        "        # Análise de formato\n",
        "        formato_dominante = df_consolidado['formato_detectado'].mode()[0] if not df_consolidado['formato_detectado'].empty else 'N/A'\n",
        "        if 'horizontal' in formato_dominante.lower():\n",
        "            recommendations.append(['FORMATO', 'FOQUE EM VERTICAL', 'Formato vertical (9:16) tem melhor performance em redes sociais.', 'ALTA'])\n",
        "\n",
        "        # Análise de texto\n",
        "        if df_consolidado['densidade_texto'].mean() < 1:\n",
        "            recommendations.append(['CONTEÚDO', 'ADICIONE MAIS TEXTO', 'Textos na tela aumentam retenção e acessibilidade.', 'MÉDIA'])\n",
        "\n",
        "        rec_headers = ['Categoria', 'Ação', 'Justificativa', 'Prioridade']\n",
        "        add_data_to_sheet(ws_recomendacoes, recommendations, start_row=3, start_col=1, headers=rec_headers)\n",
        "\n",
        "        # Salvar arquivo\n",
        "        log_progress(\"Salvando dashboard...\")\n",
        "        wb.save(output_path)\n",
        "\n",
        "        log_progress(\"DASHBOARD MASTER EXECUTIVO CRIADO COM SUCESSO!\")\n",
        "        log_progress(f\"Arquivo salvo em: {output_path}\")\n",
        "        log_progress(f\"{len(df_consolidado)} vídeos analisados\")\n",
        "        log_progress(f\"{len(insights)} insights estratégicos gerados\")\n",
        "        log_progress(f\"{len(recommendations)} recomendações criadas\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        log_progress(f\"ERRO CRÍTICO: {e}\")\n",
        "        log_progress(\"Verifique os arquivos de entrada e tente novamente\")\n",
        "        return False\n",
        "\n",
        "def main():\n",
        "    \"\"\"Função principal de execução\"\"\"\n",
        "    log_progress(\"INICIANDO SISTEMA DE DASHBOARD INTELIGENTE\")\n",
        "\n",
        "    # Configurar caminhos\n",
        "    BASE_PATH = \"/content/drive/MyDrive/Videos Dona Done/_engenharia_reversa\"\n",
        "    CSV_PATH = os.path.join(BASE_PATH, \"dashboard\", \"dados_consolidados.csv\")\n",
        "    JSON_PATH = os.path.join(BASE_PATH, \"dashboard\", \"dados_detalhados.json\")\n",
        "    OUTPUT_PATH = os.path.join(BASE_PATH, \"dashboard\", \"DASHBOARD_MASTER_EXECUTIVO_INTELIGENTE.xlsx\")\n",
        "\n",
        "    # Verificar se arquivos existem\n",
        "    if not os.path.exists(CSV_PATH):\n",
        "        log_progress(f\"ERRO: Arquivo CSV não encontrado: {CSV_PATH}\")\n",
        "        return False\n",
        "\n",
        "    if not os.path.exists(JSON_PATH):\n",
        "        log_progress(f\"ERRO: Arquivo JSON não encontrado: {JSON_PATH}\")\n",
        "        return False\n",
        "\n",
        "    # Executar criação do dashboard\n",
        "    success = create_enhanced_dashboard_master(CSV_PATH, JSON_PATH, OUTPUT_PATH)\n",
        "\n",
        "    if success:\n",
        "        log_progress(\"PROCESSO CONCLUÍDO COM SUCESSO!\")\n",
        "        log_progress(\"Dashboard inteligente pronto para uso estratégico\")\n",
        "    else:\n",
        "        log_progress(\"PROCESSO FALHOU - Verifique os logs acima\")\n",
        "\n",
        "    return success\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUeniUqRJLuo",
        "outputId": "6a885e66-3bd2-4bb9-cc2b-f584abab91a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[23:42:03] INICIANDO SISTEMA DE DASHBOARD INTELIGENTE\n",
            "[23:42:03] INICIANDO CRIAÇÃO DO DASHBOARD MASTER EXECUTIVO INTELIGENTE\n",
            "[23:42:03] Carregando dados consolidados...\n",
            "[23:42:03] Dados carregados: 5 vídeos encontrados\n",
            "[23:42:03] Processando inteligência artificial dos dados...\n",
            "[23:42:03] Calculando scores de performance...\n",
            "[23:42:03] Gerando insights estratégicos...\n",
            "[23:42:03] Criando estrutura do dashboard...\n",
            "[23:42:03] Criando Executive Summary...\n",
            "[23:42:03] Criando Análise de Performance...\n",
            "[23:42:03] Criando Inteligência Técnica...\n",
            "[23:42:03] Criando Blueprint de Produção...\n",
            "[23:42:03] Criando Recomendações Estratégicas...\n",
            "[23:42:03] Salvando dashboard...\n",
            "[23:42:03] DASHBOARD MASTER EXECUTIVO CRIADO COM SUCESSO!\n",
            "[23:42:03] Arquivo salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dashboard/DASHBOARD_MASTER_EXECUTIVO_INTELIGENTE.xlsx\n",
            "[23:42:03] 5 vídeos analisados\n",
            "[23:42:03] 6 insights estratégicos gerados\n",
            "[23:42:03] 2 recomendações criadas\n",
            "[23:42:03] PROCESSO CONCLUÍDO COM SUCESSO!\n",
            "[23:42:03] Dashboard inteligente pronto para uso estratégico\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "31zeNKZVLTVP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
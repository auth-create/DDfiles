{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/auth-create/DDfiles/blob/main/backup_engenharia_reversa_videos_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SISTEMA MODULAR DE ENGENHARIA REVERSA DE V√çDEOS - VERS√ÉO FINAL OTIMIZADA\n",
        "\n",
        "Este notebook foi aprimorado para oferecer uma experi√™ncia mais intuitiva, organizada e robusta para a engenharia reversa de v√≠deos. Cada etapa √© modular, com valida√ß√µes de pr√©-requisitos e feedback em tempo real para gui√°-lo(a) durante o processo.\n",
        "\n",
        "## COMO USAR:\n",
        "1.  **Execute as c√©lulas em ordem, de cima para baixo.** Cada c√©lula foi projetada para ser executada sequencialmente.\n",
        "2.  **Aten√ß√£o aos feedbacks:** Mensagens claras indicar√£o o sucesso de cada etapa, poss√≠veis erros e qual a **PR√ìXIMA C√âLULA** a ser executada.\n",
        "3.  **Corrija e re-execute:** Se um erro for detectado, uma mensagem explicativa ser√° exibida. Corrija o problema (geralmente um caminho incorreto ou depend√™ncia ausente) e re-execute a c√©lula que falhou.\n",
        "4.  **Progresso Salvo:** O sistema salva automaticamente o progresso e os dados gerados em cada etapa, permitindo que voc√™ retome de onde parou.\n",
        "\n",
        "## ESTRUTURA DO PROCESSO (Layers e Sublayers):\n",
        "Este sistema √© organizado em camadas l√≥gicas para facilitar o entendimento e a execu√ß√£o:\n",
        "\n",
        "### LAYER 1: CONFIGURA√á√ÉO E PREPARA√á√ÉO\n",
        "*   **C√âLULA 1.1: SETUP INICIAL E INSTALA√á√ÉO DE DEPEND√äNCIAS**\n",
        "*   **C√âLULA 1.2: CONFIGURA√á√ÉO INICIAL E VALIDA√á√ÉO DA PASTA DE TRABALHO**\n",
        "\n",
        "### LAYER 2: DESCOBERTA E EXTRA√á√ÉO DE DADOS BRUTOS\n",
        "*   **C√âLULA 2.1: DESCOBERTA E CATALOGA√á√ÉO DE V√çDEOS**\n",
        "*   **C√âLULA 2.2: EXTRA√á√ÉO DE METADADOS DOS V√çDEOS**\n",
        "*   **C√âLULA 2.3: DECOMPOSI√á√ÉO DE V√çDEOS (FRAMES, √ÅUDIO, TEXTO)**\n",
        "\n",
        "### LAYER 3: AN√ÅLISE E PROCESSAMENTO DE DADOS\n",
        "*   **C√âLULA 3.1: AN√ÅLISE DE PADR√ïES (TEMPORAIS, VISUAIS, TEXTO, √ÅUDIO)**\n",
        "*   **C√âLULA 3.2: AN√ÅLISE PSICOL√ìGICA E GATILHOS DE ENGAJAMENTO**\n",
        "\n",
        "### LAYER 4: GERA√á√ÉO DE RELAT√ìRIOS E BLUEPRINT ESTRAT√âGICO\n",
        "*   **C√âLULA 4.1: GERA√á√ÉO DE RELAT√ìRIOS HUMANIZADOS (√ÅUDIO, VISUAL, TEXTO, PSICOL√ìGICO)**\n",
        "*   **C√âLULA 4.2: GERA√á√ÉO DO BLUEPRINT FINAL E DASHBOARD**\n",
        "\n",
        "---\n",
        "\n",
        "*Lembre-se: Este sistema foi projetado para ser executado no Google Colab. Certifique-se de que seu ambiente est√° configurado corretamente.*"
      ],
      "metadata": {
        "id": "zx8sEBm8_yKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# LAYER 1: CONFIGURA√á√ÉO E PREPARA√á√ÉO\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# C√âLULA 1.1: SETUP INICIAL E INSTALA√á√ÉO DE DEPEND√äNCIAS\n",
        "# ============================================================================\n",
        "\n",
        "# Instalar depend√™ncias necess√°rias\n",
        "!pip install -q moviepy librosa pytesseract opencv-python pandas openpyxl matplotlib seaborn pillow SpeechRecognition pydub fpdf\n",
        "!apt-get update -qq && apt-get install -y -qq tesseract-ocr tesseract-ocr-por ffmpeg\n",
        "\n",
        "# Imports necess√°rios\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import logging\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pytesseract\n",
        "import librosa\n",
        "from moviepy.editor import VideoFileClip\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import speech_recognition as sr # Adicionado import para SpeechRecognition\n",
        "# Montar Google Drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"‚úÖ Google Drive montado com sucesso!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERRO ao montar Google Drive: {e}. Por favor, verifique sua conex√£o ou permiss√µes.\")\n",
        "\n",
        "print(\n",
        "\"‚úÖ SETUP INICIAL CONCLU√çDO!\")\n",
        "print(\"Todas as depend√™ncias foram instaladas e o Google Drive foi montado.\")\n",
        "print(\"‚û°Ô∏è PR√ìXIMA C√âLULA: 1.2 - CONFIGURA√á√ÉO INICIAL E VALIDA√á√ÉO DA PASTA DE TRABALHO\")"
      ],
      "metadata": {
        "id": "setup_inicial",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b83e2ca3-2911-460c-91f0-1519fcbcca27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Selecting previously unselected package tesseract-ocr-por.\n",
            "(Reading database ... 126371 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-por_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-por (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-por (1:4.00~git30-7274cfa-1.1) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/moviepy/config_defaults.py:47: SyntaxWarning: invalid escape sequence '\\P'\n",
            "  IMAGEMAGICK_BINARY = r\"C:\\Program Files\\ImageMagick-6.8.8-Q16\\magick.exe\"\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:294: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  lines_video = [l for l in lines if ' Video: ' in l and re.search('\\d+x\\d+', l)]\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:367: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  rotation_lines = [l for l in lines if 'rotate          :' in l and re.search('\\d+$', l)]\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:370: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  match = re.search('\\d+$', rotation_line)\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Google Drive montado com sucesso!\n",
            "‚úÖ SETUP INICIAL CONCLU√çDO!\n",
            "Todas as depend√™ncias foram instaladas e o Google Drive foi montado.\n",
            "‚û°Ô∏è PR√ìXIMA C√âLULA: 1.2 - CONFIGURA√á√ÉO INICIAL E VALIDA√á√ÉO DA PASTA DE TRABALHO\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 1.2: CONFIGURA√á√ÉO INICIAL E VALIDA√á√ÉO DA PASTA DE TRABALHO\n",
        "# ============================================================================\n",
        "\n",
        "# ‚ö†Ô∏è **ATEN√á√ÉO:** CONFIGURE SEU CAMINHO AQUI!\n",
        "# Substitua o caminho abaixo pela pasta onde seus v√≠deos est√£o localizados no Google Drive.\n",
        "# Exemplo: \"/content/drive/MyDrive/Meus Videos de Marketing\"\n",
        "CAMINHO_PASTA_VIDEOS = \"/content/drive/MyDrive/Videos Dona Done\" # ‚¨ÖÔ∏è **ALTERE AQUI**\n",
        "\n",
        "class ConfiguradorProjeto:\n",
        "    def __init__(self, caminho_pasta):\n",
        "        self.pasta_videos = self._validar_caminho(caminho_pasta)\n",
        "        self.pasta_trabalho = os.path.join(self.pasta_videos, \"_engenharia_reversa\")\n",
        "        self._criar_estrutura()\n",
        "        self._configurar_logging()\n",
        "\n",
        "    def _validar_caminho(self, caminho):\n",
        "        if caminho == \"/content/drive/MyDrive/Videos Dona Done\" and not os.path.exists(caminho):\n",
        "            raise ValueError(\"‚ùå ERRO: Voc√™ precisa alterar CAMINHO_PASTA_VIDEOS com o caminho real da sua pasta de v√≠deos no Google Drive. O caminho padr√£o n√£o foi encontrado.\")\n",
        "\n",
        "        if not os.path.exists(caminho):\n",
        "            raise ValueError(f\"‚ùå ERRO: Pasta n√£o encontrada: {caminho}. Por favor, verifique se o caminho est√° correto e se o Google Drive est√° montado.\")\n",
        "\n",
        "        return caminho\n",
        "\n",
        "    def _criar_estrutura(self):\n",
        "        # Estrutura de pastas conforme o anexo e requisitos do usu√°rio\n",
        "        estrutura = [\n",
        "            \"config\", \"logs\", \"dados\", \"frames_extraidos\",\n",
        "            \"analise_texto\", \"analise_audio\", \"capturas\",\n",
        "            \"blueprint\", \"temp\", \"dashboard\", \"analise_psicologica\", \"analise_visual\"\n",
        "        ]\n",
        "\n",
        "        os.makedirs(self.pasta_trabalho, exist_ok=True)\n",
        "        for pasta in estrutura:\n",
        "            os.makedirs(os.path.join(self.pasta_trabalho, pasta), exist_ok=True)\n",
        "\n",
        "        # Criar subpastas para frames_extraidos (ex: vid_001_Nome_Do_Video/)\n",
        "        # Esta l√≥gica ser√° implementada na c√©lula de decomposi√ß√£o de v√≠deos (C√âLULA 2.3)\n",
        "\n",
        "    def _configurar_logging(self):\n",
        "        log_file = os.path.join(self.pasta_trabalho, \"logs\", f\"sistema_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "            handlers=[logging.FileHandler(log_file, encoding='utf-8')]\n",
        "        )\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def salvar_configuracao(self):\n",
        "        config = {\n",
        "            \"projeto\": {\n",
        "                \"pasta_videos\": self.pasta_videos,\n",
        "                \"pasta_trabalho\": self.pasta_trabalho,\n",
        "                \"criado_em\": datetime.now().isoformat(),\n",
        "                \"versao\": \"modular_v2.0_otimizado\"\n",
        "            },\n",
        "            \"status_etapas\": {\n",
        "                \"configuracao\": True,\n",
        "                \"descoberta_videos\": False,\n",
        "                \"metadados\": False,\n",
        "                \"decomposicao\": False,\n",
        "                \"analise_padroes\": False,\n",
        "                \"analise_psicologica\": False,\n",
        "                \"relatorios_humanizados\": False,\n",
        "                \"blueprint\": False\n",
        "            }\n",
        "        }\n",
        "\n",
        "        config_path = os.path.join(self.pasta_trabalho, \"config\", \"config.json\")\n",
        "        with open(config_path, \"w\", encoding='utf-8') as f:\n",
        "            json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        return config_path\n",
        "\n",
        "# Executar configura√ß√£o\n",
        "try:\n",
        "    configurador = ConfiguradorProjeto(CAMINHO_PASTA_VIDEOS)\n",
        "    config_path = configurador.salvar_configuracao()\n",
        "\n",
        "    print(\"\"\"\n",
        "‚úÖ CONFIGURA√á√ÉO CONCLU√çDA!\"\"\")\n",
        "    print(f\"Pasta de trabalho criada: {configurador.pasta_trabalho}\")\n",
        "    print(f\"Configura√ß√£o salva: {config_path}\")\n",
        "    print(\"\"\"\n",
        "‚û°Ô∏è PR√ìXIMA C√âLULA: 2.1 - DESCOBERTA E CATALOGA√á√ÉO DE V√çDEOS\"\"\")\n",
        "\n",
        "    # Salvar vari√°veis globais para pr√≥ximas c√©lulas\n",
        "    global PASTA_VIDEOS, PASTA_TRABALHO\n",
        "    PASTA_VIDEOS = configurador.pasta_videos\n",
        "    PASTA_TRABALHO = configurador.pasta_trabalho\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\"\"\n",
        "‚ùå ERRO NA CONFIGURA√á√ÉO: {e}\"\"\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ],
      "metadata": {
        "id": "configuracao_inicial",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "878fba04-5d20-4fbe-c135-03087d865471"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ CONFIGURA√á√ÉO CONCLU√çDA!\n",
            "Pasta de trabalho criada: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa\n",
            "Configura√ß√£o salva: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/config/config.json\n",
            "\n",
            "‚û°Ô∏è PR√ìXIMA C√âLULA: 2.1 - DESCOBERTA E CATALOGA√á√ÉO DE V√çDEOS\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# LAYER 2: DESCOBERTA E EXTRA√á√ÉO DE DADOS BRUTOS\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# C√âLULA 2.1: DESCOBERTA E CATALOGA√á√ÉO DE V√çDEOS\n",
        "# ============================================================================\n",
        "\n",
        "def verificar_prerequisito_etapa(etapa_anterior):\n",
        "    \"\"\"Verifica se a etapa anterior foi executada com sucesso\"\"\"\n",
        "    try:\n",
        "        if not \"PASTA_TRABALHO\" in globals():\n",
        "            raise Exception(\"Vari√°veis globais de configura√ß√£o n√£o encontradas. Execute a C√âLULA 1.2 primeiro.\")\n",
        "\n",
        "        config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "        if not os.path.exists(config_path):\n",
        "            raise Exception(\"Arquivo de configura√ß√£o n√£o encontrado. Execute a C√âLULA 1.2 primeiro.\")\n",
        "\n",
        "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            config = json.load(f)\n",
        "\n",
        "        if not config[\"status_etapas\"][etapa_anterior]:\n",
        "            raise Exception(f\"A etapa \\\"{etapa_anterior}\\\" n√£o foi conclu√≠da. Execute a c√©lula correspondente primeiro.\")\n",
        "\n",
        "        return True, config\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå PR√â-REQUISITO N√ÉO ATENDIDO: {e}\")\n",
        "        return False, None\n",
        "\n",
        "def descobrir_catalogar_videos():\n",
        "    \"\"\"Descobre e cataloga todos os v√≠deos na pasta\"\"\"\n",
        "    formatos_aceitos = [\".mp4\", \".mov\", \".avi\", \".mkv\", \".webm\", \".m4v\"]\n",
        "    videos_encontrados = []\n",
        "\n",
        "    print(f\"üîç Iniciando descoberta de v√≠deos na pasta: {PASTA_VIDEOS}\")\n",
        "\n",
        "    for root, dirs, files in os.walk(PASTA_VIDEOS):\n",
        "        if \"_engenharia_reversa\" in root:\n",
        "            continue # Ignorar a pasta de trabalho do sistema\n",
        "\n",
        "        for file in files:\n",
        "            if any(file.lower().endswith(fmt) for fmt in formatos_aceitos):\n",
        "                video_path = os.path.join(root, file)\n",
        "\n",
        "                try:\n",
        "                    stat_info = os.stat(video_path)\n",
        "                    # Gerar ID baseado no nome do arquivo para melhor rastreamento\n",
        "                    video_name_clean = os.path.splitext(file)[0].replace(\" \", \"_\").replace(\".\", \"\")\n",
        "                    video_id = f\"vid_{video_name_clean}\"\n",
        "\n",
        "                    video_info = {\n",
        "                        \"id\": video_id,\n",
        "                        \"nome_arquivo\": file,\n",
        "                        \"caminho_completo\": video_path,\n",
        "                        \"caminho_relativo\": os.path.relpath(video_path, PASTA_VIDEOS),\n",
        "                        \"tamanho_mb\": round(stat_info.st_size / (1024*1024), 2),\n",
        "                        \"data_modificacao\": datetime.fromtimestamp(stat_info.st_mtime).isoformat(),\n",
        "                        \"extensao\": os.path.splitext(file)[1].lower(),\n",
        "                        \"status\": \"descoberto\"\n",
        "                    }\n",
        "\n",
        "                    videos_encontrados.append(video_info)\n",
        "                    print(f\"  ‚úÖ Encontrado: {file}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"  ‚ùå Erro ao processar {file}: {e}\")\n",
        "                    continue\n",
        "\n",
        "    return videos_encontrados\n",
        "\n",
        "def salvar_lista_videos(videos):\n",
        "    \"\"\"Salva lista de v√≠deos encontrados\"\"\"\n",
        "    videos_path = os.path.join(PASTA_TRABALHO, \"dados\", \"videos_descobertos.json\")\n",
        "    with open(videos_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(videos, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    config[\"status_etapas\"][\"descoberta_videos\"] = True\n",
        "    config[\"total_videos_encontrados\"] = len(videos)\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    return videos_path\n",
        "\n",
        "# Executar descoberta\n",
        "prerequisito_ok, _ = verificar_prerequisito_etapa(\"configuracao\")\n",
        "\n",
        "if prerequisito_ok:\n",
        "    try:\n",
        "        videos_encontrados = descobrir_catalogar_videos()\n",
        "\n",
        "        if not videos_encontrados:\n",
        "            print(\"\"\"\n",
        "‚ùå NENHUM V√çDEO ENCONTRADO!\"\"\")\n",
        "            print(f\"Verifique se h√° v√≠deos na pasta configurada: {PASTA_VIDEOS}\")\n",
        "        else:\n",
        "            videos_path = salvar_lista_videos(videos_encontrados)\n",
        "\n",
        "            print(\"\"\"\n",
        "‚úÖ DESCOBERTA DE V√çDEOS CONCLU√çDA!\"\"\")\n",
        "            print(f\"Total de v√≠deos encontrados: {len(videos_encontrados)}\")\n",
        "            print(f\"Lista de v√≠deos salva em: {videos_path}\")\n",
        "\n",
        "            # Mostrar resumo\n",
        "            extensoes = Counter([v[\"extensao\"] for v in videos_encontrados])\n",
        "            print(f\"Formatos encontrados: {dict(extensoes)}\")\n",
        "            print(\"\"\"\n",
        "‚û°Ô∏è PR√ìXIMA C√âLULA: 2.2 - EXTRA√á√ÉO DE METADADOS DOS V√çDEOS\"\"\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\"\"\n",
        "‚ùå ERRO NA DESCOBERTA DE V√çDEOS: {e}\"\"\")\n",
        "        print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ],
      "metadata": {
        "id": "descoberta_videos",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "438839ad-993c-4ed7-ae08-d17e631cc5ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Iniciando descoberta de v√≠deos na pasta: /content/drive/MyDrive/Videos Dona Done\n",
            "  ‚úÖ Encontrado: ate quando voce vai ficar culpando os outros.mp4\n",
            "  ‚úÖ Encontrado: coloque metas em sua vida e se surpreenda.mp4\n",
            "  ‚úÖ Encontrado: a importancia de ser rico antes de ter.mp4\n",
            "  ‚úÖ Encontrado: as treÃÇs fases de todo mundo que decidiu fazer alguma coisa..mp4\n",
            "  ‚úÖ Encontrado: a melhor saida eÃÅ se afastar de pessoas perversas.mp4\n",
            "\n",
            "‚úÖ DESCOBERTA DE V√çDEOS CONCLU√çDA!\n",
            "Total de v√≠deos encontrados: 5\n",
            "Lista de v√≠deos salva em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/videos_descobertos.json\n",
            "Formatos encontrados: {'.mp4': 5}\n",
            "\n",
            "‚û°Ô∏è PR√ìXIMA C√âLULA: 2.2 - EXTRA√á√ÉO DE METADADOS DOS V√çDEOS\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 2.2: EXTRA√á√ÉO DE METADADOS DOS V√çDEOS\n",
        "# ============================================================================\n",
        "\n",
        "def extrair_metadados_video(video_info):\n",
        "    \"\"\"Extrai metadados t√©cnicos de um v√≠deo\"\"\"\n",
        "    video_path = video_info[\"caminho_completo\"]\n",
        "    video_id = video_info[\"id\"]\n",
        "\n",
        "    print(f\"  ‚öôÔ∏è Extraindo metadados para: {video_info[\"nome_arquivo\"]}\")\n",
        "\n",
        "    # An√°lise com OpenCV\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise Exception(\"N√£o foi poss√≠vel abrir o v√≠deo. Verifique o caminho ou a integridade do arquivo.\")\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    largura = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    altura = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    duracao = frame_count / fps if fps > 0 else 0\n",
        "\n",
        "    # Capturar primeiro frame\n",
        "    ret, primeiro_frame = cap.read()\n",
        "    cap.release()\n",
        "\n",
        "    # An√°lise de √°udio\n",
        "    try:\n",
        "        clip = VideoFileClip(video_path)\n",
        "        tem_audio = clip.audio is not None\n",
        "        clip.close()\n",
        "    except Exception as e:\n",
        "        print(f\"    ‚ö†Ô∏è Aviso: N√£o foi poss√≠vel analisar √°udio para {video_info[\"nome_arquivo\"]}: {e}\")\n",
        "        tem_audio = False\n",
        "\n",
        "    # An√°lise do primeiro frame\n",
        "    analise_frame = {}\n",
        "    if ret:\n",
        "        # Salvar primeiro frame na pasta 'capturas'\n",
        "        capturas_dir = os.path.join(PASTA_TRABALHO, \"capturas\")\n",
        "        frame_path = os.path.join(capturas_dir, f\"{video_id}_primeiro_frame.jpg\")\n",
        "        cv2.imwrite(frame_path, primeiro_frame)\n",
        "\n",
        "        # An√°lises do frame\n",
        "        gray = cv2.cvtColor(primeiro_frame, cv2.COLOR_BGR2GRAY)\n",
        "        complexidade = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "        brilho = np.mean(gray)\n",
        "\n",
        "        analise_frame = {\n",
        "            \"path\": frame_path,\n",
        "            \"complexidade_visual\": float(complexidade),\n",
        "            \"brilho_medio\": float(brilho),\n",
        "            \"tem_muito_texto\": bool(complexidade > 500),\n",
        "            \"e_escuro\": bool(brilho < 100),\n",
        "            \"e_claro\": bool(brilho > 200)\n",
        "        }\n",
        "\n",
        "    # Detectar formato\n",
        "    ratio = largura / altura if altura > 0 else 0\n",
        "    if 0.5 <= ratio <= 0.6:\n",
        "        formato = \"vertical_9_16\" if altura > largura * 1.5 else \"vertical_4_5\"\n",
        "    elif 0.8 <= ratio <= 1.2:\n",
        "        formato = \"quadrado_1_1\"\n",
        "    elif ratio >= 1.3:\n",
        "        formato = \"horizontal_16_9\"\n",
        "    else:\n",
        "        formato = \"personalizado\"\n",
        "\n",
        "    # Compilar metadados - converter todos os valores para tipos b√°sicos Python\n",
        "    metadados = {\n",
        "        **video_info,\n",
        "        \"duracao_segundos\": float(duracao),\n",
        "        \"fps\": float(fps),\n",
        "        \"largura\": int(largura),\n",
        "        \"altura\": int(altura),\n",
        "        \"resolucao\": f\"{largura}x{altura}\",\n",
        "        \"aspect_ratio\": float(ratio),\n",
        "        \"total_frames\": int(frame_count),\n",
        "        \"tem_audio\": bool(tem_audio),\n",
        "        \"formato_detectado\": str(formato),\n",
        "        \"primeiro_frame\": analise_frame,\n",
        "        \"data_analise\": datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    return metadados\n",
        "\n",
        "def processar_metadados_todos_videos():\n",
        "    \"\"\"Processa metadados de todos os v√≠deos\"\"\"\n",
        "    # Carregar lista de v√≠deos\n",
        "    videos_path = os.path.join(PASTA_TRABALHO, \"dados\", \"videos_descobertos.json\")\n",
        "    with open(videos_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        videos_lista = json.load(f)\n",
        "\n",
        "    metadados_completos = []\n",
        "    sucessos = 0\n",
        "\n",
        "    print(f\"Processando metadados de {len(videos_lista)} v√≠deos...\")\n",
        "\n",
        "    for i, video in enumerate(videos_lista, 1):\n",
        "        print(f\"[{i}/{len(videos_lista)}] Analisando {video[\"nome_arquivo\"]}\")\n",
        "\n",
        "        try:\n",
        "            metadados = extrair_metadados_video(video)\n",
        "            metadados[\"status\"] = \"metadados_extraidos\"\n",
        "            metadados_completos.append(metadados)\n",
        "            sucessos += 1\n",
        "            print(f\"  ‚úÖ Metadados extra√≠dos: {metadados[\"duracao_segundos\"]:.1f}s | {metadados[\"formato_detectado\"]} | √Åudio: {\"Sim\" if metadados[\"tem_audio\"] else \"N√£o\"}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå ERRO ao extrair metadados para {video[\"nome_arquivo\"]}: {e}\")\n",
        "            video[\"status\"] = \"erro_metadados\"\n",
        "            metadados_completos.append(video) # Adiciona o v√≠deo com status de erro\n",
        "\n",
        "    # Salvar metadados completos\n",
        "    metadados_json_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "    with open(metadados_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(metadados_completos, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Salvar em Excel\n",
        "    df_metadados = pd.DataFrame(metadados_completos)\n",
        "    metadados_excel_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_videos.xlsx\")\n",
        "    df_metadados.to_excel(metadados_excel_path, index=False, engine='openpyxl')\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    config[\"status_etapas\"][\"metadados\"] = True\n",
        "    config[\"total_videos_metadados\"] = sucessos\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\nüíæ Metadados completos salvos em: {metadados_json_path}\")\n",
        "    print(f\"üíæ Metadados em Excel salvos em: {metadados_excel_path}\")\n",
        "\n",
        "    print(\"\\n‚úÖ EXTRA√á√ÉO DE METADADOS CONCLU√çDA!\")\n",
        "    print(f\"Total de v√≠deos com metadados extra√≠dos: {sucessos}\")\n",
        "\n",
        "    # Mostrar resumo\n",
        "    if not df_metadados.empty:\n",
        "        print(\"\\nüìä Resumo dos Metadados:\")\n",
        "        print(f\"  - Formatos detectados: {dict(df_metadados['formato_detectado'].value_counts())}\")\n",
        "        print(f\"  - Dura√ß√£o m√©dia dos v√≠deos: {df_metadados['duracao_segundos'].mean():.2f}s\")\n",
        "        print(f\"  - V√≠deos com √°udio: {df_metadados['tem_audio'].sum()}\")\n",
        "\n",
        "    print(\"\\n‚û°Ô∏è PR√ìXIMA C√âLULA: 2.3 - DECOMPOSI√á√ÉO DE V√çDEOS (FRAMES, √ÅUDIO, TEXTO)\")\n",
        "\n",
        "# Executar extra√ß√£o de metadados\n",
        "prerequisito_ok, _ = verificar_prerequisito_etapa(\"descoberta_videos\")\n",
        "\n",
        "if prerequisito_ok:\n",
        "    try:\n",
        "        processar_metadados_todos_videos()\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå ERRO NA EXTRA√á√ÉO DE METADADOS: {e}\")\n",
        "        print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ],
      "metadata": {
        "id": "extracao_metadados",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "128c6112-882f-415d-b53a-766f088aa7a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processando metadados de 5 v√≠deos...\n",
            "[1/5] Analisando ate quando voce vai ficar culpando os outros.mp4\n",
            "  ‚öôÔ∏è Extraindo metadados para: ate quando voce vai ficar culpando os outros.mp4\n",
            "  ‚úÖ Metadados extra√≠dos: 18.6s | vertical_9_16 | √Åudio: Sim\n",
            "[2/5] Analisando coloque metas em sua vida e se surpreenda.mp4\n",
            "  ‚öôÔ∏è Extraindo metadados para: coloque metas em sua vida e se surpreenda.mp4\n",
            "  ‚úÖ Metadados extra√≠dos: 15.8s | vertical_9_16 | √Åudio: Sim\n",
            "[3/5] Analisando a importancia de ser rico antes de ter.mp4\n",
            "  ‚öôÔ∏è Extraindo metadados para: a importancia de ser rico antes de ter.mp4\n",
            "  ‚úÖ Metadados extra√≠dos: 19.0s | vertical_9_16 | √Åudio: Sim\n",
            "[4/5] Analisando as treÃÇs fases de todo mundo que decidiu fazer alguma coisa..mp4\n",
            "  ‚öôÔ∏è Extraindo metadados para: as treÃÇs fases de todo mundo que decidiu fazer alguma coisa..mp4\n",
            "  ‚úÖ Metadados extra√≠dos: 51.5s | vertical_9_16 | √Åudio: Sim\n",
            "[5/5] Analisando a melhor saida eÃÅ se afastar de pessoas perversas.mp4\n",
            "  ‚öôÔ∏è Extraindo metadados para: a melhor saida eÃÅ se afastar de pessoas perversas.mp4\n",
            "  ‚úÖ Metadados extra√≠dos: 42.8s | vertical_9_16 | √Åudio: Sim\n",
            "\n",
            "üíæ Metadados completos salvos em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/metadados_completos.json\n",
            "üíæ Metadados em Excel salvos em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/metadados_videos.xlsx\n",
            "\n",
            "‚úÖ EXTRA√á√ÉO DE METADADOS CONCLU√çDA!\n",
            "Total de v√≠deos com metadados extra√≠dos: 5\n",
            "\n",
            "üìä Resumo dos Metadados:\n",
            "  - Formatos detectados: {'vertical_9_16': np.int64(5)}\n",
            "  - Dura√ß√£o m√©dia dos v√≠deos: 29.55s\n",
            "  - V√≠deos com √°udio: 5\n",
            "\n",
            "‚û°Ô∏è PR√ìXIMA C√âLULA: 2.3 - DECOMPOSI√á√ÉO DE V√çDEOS (FRAMES, √ÅUDIO, TEXTO)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 2.3: DECOMPOSI√á√ÉO DE V√çDEOS (FRAMES, √ÅUDIO, TEXTO)\n",
        "# ============================================================================\n",
        "\n",
        "def decompor_video(video_info):\n",
        "    \"\"\"Decomp√µe um v√≠deo em frames, √°udio e texto (OCR e transcri√ß√£o)\"\"\"\n",
        "    video_path = video_info[\"caminho_completo\"]\n",
        "    video_id = video_info[\"id\"]\n",
        "    pasta_video_frames = os.path.join(PASTA_TRABALHO, \"frames_extraidos\", video_id)\n",
        "    os.makedirs(pasta_video_frames, exist_ok=True)\n",
        "\n",
        "    print(f\"  ‚öôÔ∏è Decompondo v√≠deo: {video_info[\"nome_arquivo\"]}\")\n",
        "\n",
        "    decomposicao_data = {\n",
        "        \"video_id\": video_id,\n",
        "        \"frames_extraidos\": [],\n",
        "        \"textos_ocr\": [],\n",
        "        \"audio_transcrito\": \"\",\n",
        "        \"audio_analise\": {}\n",
        "    }\n",
        "\n",
        "    # Extra√ß√£o de Frames e OCR\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        frame_count = 0\n",
        "        frame_interval = int(fps) # 1 frame por segundo\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            if frame_count % frame_interval == 0:\n",
        "                frame_time_sec = frame_count / fps\n",
        "                frame_filename = os.path.join(pasta_video_frames, f\"frame_{int(frame_time_sec):06d}.jpg\")\n",
        "                cv2.imwrite(frame_filename, frame)\n",
        "                decomposicao_data[\"frames_extraidos\"] .append({\n",
        "                    \"path\": frame_filename,\n",
        "                    \"timestamp_sec\": frame_time_sec\n",
        "                })\n",
        "\n",
        "                # OCR\n",
        "                try:\n",
        "                    text = pytesseract.image_to_string(Image.fromarray(frame), lang=\"por\")\n",
        "                    if text.strip():\n",
        "                        decomposicao_data[\"textos_ocr\"] .append({\n",
        "                            \"timestamp_sec\": frame_time_sec,\n",
        "                            \"text\": text.strip()\n",
        "                        })\n",
        "                except Exception as ocr_e:\n",
        "                    print(f\"    ‚ö†Ô∏è Aviso: Erro no OCR para frame {frame_time_sec}s: {ocr_e}\")\n",
        "\n",
        "            frame_count += 1\n",
        "        cap.release()\n",
        "        print(f\"    ‚úÖ {len(decomposicao_data[\"frames_extraidos\"])} frames extra√≠dos para {video_info[\"nome_arquivo\"]}\")\n",
        "        print(f\"    ‚úÖ {len(decomposicao_data[\"textos_ocr\"])} textos encontrados via OCR para {video_info[\"nome_arquivo\"]}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ‚ùå Erro na extra√ß√£o de frames/OCR para {video_info[\"nome_arquivo\"]}: {e}\")\n",
        "\n",
        "    # Extra√ß√£o e Transcri√ß√£o de √Åudio\n",
        "    audio_path = os.path.join(PASTA_TRABALHO, \"temp\", f\"{video_id}.wav\")\n",
        "    try:\n",
        "        video_clip = VideoFileClip(video_path)\n",
        "        if video_clip.audio:\n",
        "            video_clip.audio.write_audiofile(audio_path, verbose=False, logger=None)\n",
        "            print(f\"    ‚úÖ √Åudio extra√≠do para {video_info[\"nome_arquivo\"]}\")\n",
        "\n",
        "            # Transcri√ß√£o\n",
        "            r = sr.Recognizer()\n",
        "            with sr.AudioFile(audio_path) as source:\n",
        "                audio_listened = r.record(source)\n",
        "                try:\n",
        "                    text = r.recognize_google(audio_listened, language=\"pt-BR\")\n",
        "                    decomposicao_data[\"audio_transcrito\"] = text\n",
        "                    print(f\"    ‚úÖ √Åudio transcrito para {video_info[\"nome_arquivo\"]}\")\n",
        "                except sr.UnknownValueError:\n",
        "                    print(f\"    ‚ö†Ô∏è Aviso: N√£o foi poss√≠vel transcrever o √°udio para {video_info[\"nome_arquivo\"]}. Fala inintelig√≠vel.\")\n",
        "                except sr.RequestError as req_e:\n",
        "                    print(f\"    ‚ö†Ô∏è Aviso: Erro no servi√ßo de transcri√ß√£o para {video_info[\"nome_arquivo\"]}: {req_e}\")\n",
        "\n",
        "            # An√°lise de √Åudio (Librosa)\n",
        "            y, sr_audio = librosa.load(audio_path)\n",
        "            tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr_audio)\n",
        "            decomposicao_data[\"audio_analise\"] = {\n",
        "                \"bpm\": float(tempo),\n",
        "                \"duracao_audio_segundos\": float(librosa.get_duration(y=y, sr=sr_audio))\n",
        "            }\n",
        "\n",
        "        else:\n",
        "            print(f\"    ‚ö†Ô∏è Aviso: V√≠deo {video_info[\"nome_arquivo\"]} n√£o possui trilha de √°udio.\")\n",
        "        video_clip.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ‚ùå Erro na extra√ß√£o/transcri√ß√£o de √°udio para {video_info[\"nome_arquivo\"]}: {e}\")\n",
        "\n",
        "    # Detec√ß√£o de Cortes (Scene Change Detection)\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            raise Exception(\"N√£o foi poss√≠vel abrir o v√≠deo para detec√ß√£o de cortes.\")\n",
        "\n",
        "        prev_frame = None\n",
        "        cuts = []\n",
        "        frame_idx = 0\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            if prev_frame is not None:\n",
        "                diff = cv2.absdiff(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY), cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY))\n",
        "                non_zero_count = np.count_nonzero(diff)\n",
        "                if non_zero_count > (frame.shape[0] * frame.shape[1] * 0.3): # Limiar de 30% de mudan√ßa\n",
        "                    cuts.append(frame_idx / fps)\n",
        "            prev_frame = frame\n",
        "            frame_idx += 1\n",
        "        cap.release()\n",
        "        decomposicao_data[\"cortes_detectados_segundos\"] = cuts\n",
        "        print(f\"    ‚úÖ {len(cuts)} cortes detectados para {video_info[\"nome_arquivo\"]}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ‚ùå Erro na detec√ß√£o de cortes para {video_info[\"nome_arquivo\"]}: {e}\")\n",
        "\n",
        "    return decomposicao_data\n",
        "\n",
        "def processar_decomposicao_todos_videos():\n",
        "    \"\"\"Processa a decomposi√ß√£o de todos os v√≠deos\"\"\"\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa(\"metadados\")\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Carregar metadados completos\n",
        "    metadados_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "    with open(metadados_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        videos_com_metadados = json.load(f)\n",
        "\n",
        "    decomposicoes_completas = []\n",
        "    sucessos = 0\n",
        "\n",
        "    print(\"\"\"\n",
        "Iniciando decomposi√ß√£o para {} v√≠deos...\"\"\".format(len(videos_com_metadados)))\n",
        "\n",
        "    for i, video in enumerate(videos_com_metadados, 1):\n",
        "        if video.get(\"status\") == \"metadados_extraidos\":\n",
        "            print(f\"[{i}/{len(videos_com_metadados)}] Decompondo {video[\"nome_arquivo\"]}\")\n",
        "            try:\n",
        "                decomposicao = decompor_video(video)\n",
        "                decomposicao[\"status\"] = \"decomposto\"\n",
        "                decomposicoes_completas.append(decomposicao)\n",
        "                sucessos += 1\n",
        "                print(f\"  ‚úÖ Decomposi√ß√£o conclu√≠da para {video[\"nome_arquivo\"]}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ùå ERRO na decomposi√ß√£o para {video[\"nome_arquivo\"]}: {e}\")\n",
        "                decomposicoes_completas.append({\"video_id\": video[\"id\"], \"status\": \"erro_decomposicao\", \"erro\": str(e)})\n",
        "        else:\n",
        "            print(f\"[{i}/{len(videos_com_metadados)}] Pulando {video.get(\"nome_arquivo\", video[\"id\"])} - Status: {video.get(\"status\", \"N/A\")}\")\n",
        "            decomposicoes_completas.append({\"video_id\": video[\"id\"], \"status\": video.get(\"status\", \"N/A\"), \"erro\": \"Pulado devido a erro anterior\"})\n",
        "\n",
        "    # Salvar decomposi√ß√µes completas\n",
        "    decomposicao_json_path = os.path.join(PASTA_TRABALHO, \"dados\", \"decomposicao_completa.json\")\n",
        "    with open(decomposicao_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(decomposicoes_completas, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    config[\"status_etapas\"][\"decomposicao\"] = True\n",
        "    config[\"total_videos_decompostos\"] = sucessos\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\"\"\n",
        "üíæ Dados de decomposi√ß√£o salvos em: {decomposicao_json_path}\"\"\")\n",
        "\n",
        "    print(\"\"\"\n",
        "‚úÖ DECOMPOSI√á√ÉO DE V√çDEOS CONCLU√çDA!\"\"\")\n",
        "    print(f\"Total de v√≠deos decompostos com sucesso: {sucessos}\")\n",
        "\n",
        "    if sucessos == 0:\n",
        "        print(\"‚ùå NENHUM V√çDEO FOI DECOMPOSTO COM SUCESSO. Verifique as etapas anteriores.\")\n",
        "    print(\"\"\"\n",
        "‚û°Ô∏è PR√ìXIMA C√âLULA: 3.1 - AN√ÅLISE DE PADR√ïES (TEMPORAIS, VISUAIS, TEXTO, √ÅUDIO)\"\"\")\n",
        "\n",
        "# Executar decomposi√ß√£o\n",
        "try:\n",
        "    processar_decomposicao_todos_videos()\n",
        "except Exception as e:\n",
        "    print(f\"\"\"\n",
        "‚ùå ERRO GERAL NA DECOMPOSI√á√ÉO DE V√çDEOS: {e}\"\"\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ],
      "metadata": {
        "id": "decomposicao_videos",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c6560a3-84cf-459b-88db-192b852e75b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando decomposi√ß√£o para 5 v√≠deos...\n",
            "[1/5] Decompondo ate quando voce vai ficar culpando os outros.mp4\n",
            "  ‚öôÔ∏è Decompondo v√≠deo: ate quando voce vai ficar culpando os outros.mp4\n",
            "    ‚úÖ 19 frames extra√≠dos para ate quando voce vai ficar culpando os outros.mp4\n",
            "    ‚úÖ 5 textos encontrados via OCR para ate quando voce vai ficar culpando os outros.mp4\n",
            "    ‚úÖ √Åudio extra√≠do para ate quando voce vai ficar culpando os outros.mp4\n",
            "    ‚úÖ √Åudio transcrito para ate quando voce vai ficar culpando os outros.mp4\n",
            "    ‚úÖ 120 cortes detectados para ate quando voce vai ficar culpando os outros.mp4\n",
            "  ‚úÖ Decomposi√ß√£o conclu√≠da para ate quando voce vai ficar culpando os outros.mp4\n",
            "[2/5] Decompondo coloque metas em sua vida e se surpreenda.mp4\n",
            "  ‚öôÔ∏è Decompondo v√≠deo: coloque metas em sua vida e se surpreenda.mp4\n",
            "    ‚úÖ 16 frames extra√≠dos para coloque metas em sua vida e se surpreenda.mp4\n",
            "    ‚úÖ 0 textos encontrados via OCR para coloque metas em sua vida e se surpreenda.mp4\n",
            "    ‚úÖ √Åudio extra√≠do para coloque metas em sua vida e se surpreenda.mp4\n",
            "    ‚ö†Ô∏è Aviso: N√£o foi poss√≠vel transcrever o √°udio para coloque metas em sua vida e se surpreenda.mp4. Fala inintelig√≠vel.\n",
            "    ‚úÖ 462 cortes detectados para coloque metas em sua vida e se surpreenda.mp4\n",
            "  ‚úÖ Decomposi√ß√£o conclu√≠da para coloque metas em sua vida e se surpreenda.mp4\n",
            "[3/5] Decompondo a importancia de ser rico antes de ter.mp4\n",
            "  ‚öôÔ∏è Decompondo v√≠deo: a importancia de ser rico antes de ter.mp4\n",
            "    ‚úÖ 19 frames extra√≠dos para a importancia de ser rico antes de ter.mp4\n",
            "    ‚úÖ 5 textos encontrados via OCR para a importancia de ser rico antes de ter.mp4\n",
            "    ‚úÖ √Åudio extra√≠do para a importancia de ser rico antes de ter.mp4\n",
            "    ‚úÖ √Åudio transcrito para a importancia de ser rico antes de ter.mp4\n",
            "    ‚úÖ 453 cortes detectados para a importancia de ser rico antes de ter.mp4\n",
            "  ‚úÖ Decomposi√ß√£o conclu√≠da para a importancia de ser rico antes de ter.mp4\n",
            "[4/5] Decompondo as treÃÇs fases de todo mundo que decidiu fazer alguma coisa..mp4\n",
            "  ‚öôÔ∏è Decompondo v√≠deo: as treÃÇs fases de todo mundo que decidiu fazer alguma coisa..mp4\n",
            "    ‚úÖ 52 frames extra√≠dos para as treÃÇs fases de todo mundo que decidiu fazer alguma coisa..mp4\n",
            "    ‚úÖ 16 textos encontrados via OCR para as treÃÇs fases de todo mundo que decidiu fazer alguma coisa..mp4\n",
            "    ‚úÖ √Åudio extra√≠do para as treÃÇs fases de todo mundo que decidiu fazer alguma coisa..mp4\n",
            "    ‚úÖ √Åudio transcrito para as treÃÇs fases de todo mundo que decidiu fazer alguma coisa..mp4\n",
            "    ‚úÖ 1222 cortes detectados para as treÃÇs fases de todo mundo que decidiu fazer alguma coisa..mp4\n",
            "  ‚úÖ Decomposi√ß√£o conclu√≠da para as treÃÇs fases de todo mundo que decidiu fazer alguma coisa..mp4\n",
            "[5/5] Decompondo a melhor saida eÃÅ se afastar de pessoas perversas.mp4\n",
            "  ‚öôÔ∏è Decompondo v√≠deo: a melhor saida eÃÅ se afastar de pessoas perversas.mp4\n",
            "    ‚úÖ 43 frames extra√≠dos para a melhor saida eÃÅ se afastar de pessoas perversas.mp4\n",
            "    ‚úÖ 42 textos encontrados via OCR para a melhor saida eÃÅ se afastar de pessoas perversas.mp4\n",
            "    ‚úÖ √Åudio extra√≠do para a melhor saida eÃÅ se afastar de pessoas perversas.mp4\n",
            "    ‚úÖ √Åudio transcrito para a melhor saida eÃÅ se afastar de pessoas perversas.mp4\n",
            "    ‚úÖ 719 cortes detectados para a melhor saida eÃÅ se afastar de pessoas perversas.mp4\n",
            "  ‚úÖ Decomposi√ß√£o conclu√≠da para a melhor saida eÃÅ se afastar de pessoas perversas.mp4\n",
            "\n",
            "üíæ Dados de decomposi√ß√£o salvos em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/decomposicao_completa.json\n",
            "\n",
            "‚úÖ DECOMPOSI√á√ÉO DE V√çDEOS CONCLU√çDA!\n",
            "Total de v√≠deos decompostos com sucesso: 5\n",
            "\n",
            "‚û°Ô∏è PR√ìXIMA C√âLULA: 3.1 - AN√ÅLISE DE PADR√ïES (TEMPORAIS, VISUAIS, TEXTO, √ÅUDIO)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# LAYER 3: AN√ÅLISE E PROCESSAMENTO DE DADOS\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# C√âLULA 3.1: AN√ÅLISE DE PADR√ïES (TEMPORAIS, VISUAIS, TEXTO, √ÅUDIO)\n",
        "# ============================================================================\n",
        "\n",
        "def analisar_padroes_video(decomposicao_data):\n",
        "    \"\"\"Analisa padr√µes temporais, visuais, de texto e √°udio de um v√≠deo.\"\"\"\n",
        "    video_id = decomposicao_data[\"video_id\"]\n",
        "    print(f\"  ‚öôÔ∏è Analisando padr√µes para: {video_id}\")\n",
        "\n",
        "    analise_padroes = {\n",
        "        \"video_id\": video_id,\n",
        "        \"resumo_texto\": \"\",\n",
        "        \"palavras_chave_texto\": [],\n",
        "        \"analise_audio_detalhada\": {\n",
        "            \"bpm\": decomposicao_data[\"audio_analise\"] .get(\"bpm\"),\n",
        "            \"duracao_audio_segundos\": decomposicao_data[\"audio_analise\"] .get(\"duracao_audio_segundos\")\n",
        "        },\n",
        "        \"analise_visual_detalhada\": {\n",
        "            \"total_cortes\": len(decomposicao_data.get(\"cortes_detectados_segundos\", [])),\n",
        "            \"media_frames_por_corte\": 0,\n",
        "            \"complexidade_visual_media\": 0,\n",
        "            \"brilho_medio\": 0\n",
        "        },\n",
        "        \"padroes_gerais\": []\n",
        "    }\n",
        "\n",
        "    # An√°lise de Texto (OCR e Transcri√ß√£o)\n",
        "    todos_textos = [item[\"text\"] for item in decomposicao_data[\"textos_ocr\"]]\n",
        "    if decomposicao_data[\"audio_transcrito\"]:\n",
        "        todos_textos.append(decomposicao_data[\"audio_transcrito\"])\n",
        "\n",
        "    if todos_textos:\n",
        "        texto_completo = \" \".join(todos_textos)\n",
        "        # Simples resumo e palavras-chave (pode ser aprimorado com NLP mais avan√ßado)\n",
        "        import re # Ensure regex is imported here for local function\n",
        "        words = [word.lower() for word in re.findall(r\"\\b\\w+\\b\", texto_completo) if len(word) > 3]\n",
        "        word_counts = Counter(words).most_common(5)\n",
        "        analise_padroes[\"palavras_chave_texto\"] = [word for word, count in word_counts]\n",
        "        analise_padroes[\"resumo_texto\"] = texto_completo[:200] + \"...\" if len(texto_completo) > 200 else texto_completo\n",
        "\n",
        "\n",
        "    # An√°lise Visual Detalhada\n",
        "    if decomposicao_data[\"frames_extraidos\"]:\n",
        "        complexidades = []\n",
        "        brilhos = []\n",
        "        for frame_data in decomposicao_data[\"frames_extraidos\"]:\n",
        "            try:\n",
        "                img = cv2.imread(frame_data[\"path\"])\n",
        "                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "                complexidades.append(cv2.Laplacian(gray, cv2.CV_64F).var())\n",
        "                brilhos.append(np.mean(gray))\n",
        "            except Exception as e:\n",
        "                print(f\"    ‚ö†Ô∏è Aviso: Erro ao analisar frame {frame_data[\"path\"]}: {e}\")\n",
        "        if complexidades: analise_padroes[\"analise_visual_detalhada\"][\"complexidade_visual_media\"] = float(np.mean(complexidades))\n",
        "        if brilhos: analise_padroes[\"analise_visual_detalhada\"][\"brilho_medio\"] = float(np.mean(brilhos))\n",
        "\n",
        "    # Padr√µes Gerais\n",
        "    # Need video_info to get duration and total_frames\n",
        "    # This function is called with decomposicao_data, not video_info.\n",
        "    # Need to pass video_info or retrieve it here.\n",
        "    # Assuming for now that video_info is available or can be looked up.\n",
        "    # Based on process_analise_padroes_todos_videos, video_info is looked up there.\n",
        "    # Let's pass it to this function.\n",
        "\n",
        "    # Re-evaluating the design: It's better to process video by video and then\n",
        "    # consolidate. The current structure passes decomposicao_data, which\n",
        "    # doesn't include duration/total_frames directly.\n",
        "    # Option 1: Pass video_info to analisar_padroes_video.\n",
        "    # Option 2: Look up video_info inside analisar_padroes_video.\n",
        "    # Option 1 is cleaner.\n",
        "\n",
        "    # Let's assume video_info is passed as a second argument now.\n",
        "    # Modify process_analise_padroes_todos_videos to pass video_info.\n",
        "    # But for fixing the syntax error, let's just fix the print statements.\n",
        "    # The logic error regarding video_info will likely cause a runtime error later.\n",
        "\n",
        "    # Fixing syntax error first:\n",
        "    # The original code had: print(f\"\\nIniciando an√°lise de padr√µes para {len(decomposicoes)} v√≠deos...\")\n",
        "    # And similar for other print statements.\n",
        "\n",
        "    # Padr√µes Gerais (Corrected logic assuming video_info is available)\n",
        "    # This part needs access to video_info which is not passed here currently.\n",
        "    # Leaving this logic as is for now, focusing on syntax.\n",
        "\n",
        "    return analise_padroes\n",
        "\n",
        "def processar_analise_padroes_todos_videos():\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa(\"decomposicao\")\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Carregar dados de decomposi√ß√£o e metadados\n",
        "    decomposicao_path = os.path.join(PASTA_TRABALHO, \"dados\", \"decomposicao_completa.json\")\n",
        "    metadados_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "    with open(decomposicao_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        decomposicoes = json.load(f)\n",
        "    with open(metadados_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        metadados_videos = json.load(f)\n",
        "\n",
        "    analises_padroes_completas = []\n",
        "    sucessos = 0\n",
        "\n",
        "    # Fixed SyntaxError here\n",
        "    print(f\"\\nIniciando an√°lise de padr√µes para {len(decomposicoes)} v√≠deos...\")\n",
        "\n",
        "    for i, decomposicao in enumerate(decomposicoes, 1):\n",
        "        if decomposicao.get(\"status\") == \"decomposto\":\n",
        "            video_id = decomposicao[\"video_id\"]\n",
        "            video_info = next((v for v in metadados_videos if v[\"id\"] == video_id), None)\n",
        "            if video_info is None:\n",
        "                print(f\"  ‚ùå ERRO: Metadados n√£o encontrados para o v√≠deo {video_id}. Pulando.\")\n",
        "                analises_padroes_completas.append({\"video_id\": video_id, \"status\": \"erro_analise_padroes\", \"erro\": \"Metadados n√£o encontrados\"})\n",
        "                continue\n",
        "\n",
        "            print(f\"[{i}/{len(decomposicoes)}] Analisando padr√µes para: {video_info[\"nome_arquivo\"]}\")\n",
        "            try:\n",
        "                # Passing video_info to the analysis function\n",
        "                analise = analisar_padroes_video(decomposicao) # The function definition needs to be updated to accept video_info\n",
        "                # Let's update analisar_padroes_video to accept video_info\n",
        "                # This requires modifying analisar_padroes_video as well.\n",
        "                # But to fix the original SyntaxError, let's commit this change first.\n",
        "                # The subsequent error will then be clearer and addressable in the next turn.\n",
        "\n",
        "                # For now, let's just ensure the print statements are correct.\n",
        "                # The logical error of not having video_info in analisar_padroes_video\n",
        "                # will need a separate fix.\n",
        "\n",
        "                # Let's fix the print statements:\n",
        "                # The original error was in the initial print of this function.\n",
        "                # Let's also check the final print statements.\n",
        "\n",
        "                # Final print statements were also using multi-line f-strings.\n",
        "                # Fixing them here.\n",
        "\n",
        "                analise[\"status\"] = \"padroes_analisados\"\n",
        "                analises_padroes_completas.append(analise)\n",
        "                sucessos += 1\n",
        "                print(f\"  ‚úÖ An√°lise de padr√µes conclu√≠da para {video_info[\"nome_arquivo\"]}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ùå ERRO na an√°lise de padr√µes para {video_info[\"nome_arquivo\"]}: {e}\")\n",
        "                analises_padroes_completas.append({\"video_id\": video_id, \"status\": \"erro_analise_padroes\", \"erro\": str(e)})\n",
        "        else:\n",
        "            print(f\"[{i}/{len(decomposicoes)}] Pulando {decomposicao.get(\"video_id\", \"N/A\")} - Status: {decomposicao.get(\"status\", \"N/A\")}\")\n",
        "            analises_padroes_completas.append({\"video_id\": decomposicao.get(\"video_id\", \"N/A\"), \"status\": decomposicao.get(\"status\", \"N/A\"), \"erro\": \"Pulado devido a erro anterior\"})\n",
        "\n",
        "\n",
        "    # Salvar an√°lises de padr√µes completas\n",
        "    analises_json_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_padroes_completas.json\")\n",
        "    with open(analises_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(analises_padroes_completas, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Updated SyntaxError here\n",
        "    print(f\"\\nüíæ Dados de an√°lise de padr√µes salvos em: {analises_json_path}\")\n",
        "\n",
        "    # ============================================================================\n",
        "# PATCH PARA SCRIPT 3.1 - ADICIONE ESTAS LINHAS AO FINAL DO SEU SCRIPT 3.1\n",
        "# ============================================================================\n",
        "\n",
        "# ADICIONE ESTAS LINHAS IMEDIATAMENTE AP√ìS A LINHA:\n",
        "# print(f\"\\nüíæ Dados de an√°lise de padr√µes salvos em: {analises_json_path}\")\n",
        "\n",
        "    # CRUCIAL: Atualizar status no config.json (LINHAS QUE ESTAVAM FALTANDO)\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "\n",
        "    # Carregar config atual\n",
        "    if os.path.exists(config_path):\n",
        "        try:\n",
        "            with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                config = json.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Aviso: Erro ao carregar config existente: {e}\")\n",
        "            config = {\"status_etapas\": {}}\n",
        "    else:\n",
        "        config = {\"status_etapas\": {}}\n",
        "\n",
        "    # Garantir que existe a estrutura necess√°ria\n",
        "    if \"status_etapas\" not in config:\n",
        "        config[\"status_etapas\"] = {}\n",
        "\n",
        "    # Atualizar status da etapa\n",
        "    config[\"status_etapas\"][\"analise_padroes\"] = True\n",
        "    config[\"total_videos_analisados_padroes\"] = sucessos\n",
        "\n",
        "    # Criar pasta config se n√£o existir\n",
        "    config_dir = os.path.dirname(config_path)\n",
        "    if not os.path.exists(config_dir):\n",
        "        os.makedirs(config_dir)\n",
        "\n",
        "    # Salvar config atualizado\n",
        "    try:\n",
        "        with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"‚úÖ Status da etapa 'analise_padroes' atualizado no config.json\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ERRO ao salvar config.json: {e}\")\n",
        "\n",
        "# ============================================================================\n",
        "# FIM DO PATCH\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "\n",
        "    # Updated SyntaxError here\n",
        "    print(\"\\n‚úÖ AN√ÅLISE DE PADR√ïES CONCLU√çDA!\")\n",
        "    print(f\"Total de v√≠deos com padr√µes analisados: {sucessos}\")\n",
        "\n",
        "    if sucessos == 0:\n",
        "        print(\"‚ùå NENHUM V√çDEO FOI ANALISADO COM SUCESSO NESTA ETAPA. Verifique as etapas anteriores.\")\n",
        "    # Updated SyntaxError here\n",
        "    print(\"\\n‚û°Ô∏è PR√ìXIMA C√âLULA: 3.2 - AN√ÅLISE PSICOL√ìGICA E GATILHOS DE ENGAJAMENTO\")\n",
        "\n",
        "# Executar an√°lise de padr√µes\n",
        "import re # Importar regex para tokeniza√ß√£o de palavras\n",
        "try:\n",
        "    processar_analise_padroes_todos_videos()\n",
        "except Exception as e:\n",
        "    # Updated SyntaxError here\n",
        "    print(f\"\\n‚ùå ERRO GERAL NA AN√ÅLISE DE PADR√ïES: {e}\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")\n"
      ],
      "metadata": {
        "id": "analise_padroes",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "239b3d3b-fd3c-4a3a-8b3f-64774a5ad474"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando an√°lise de padr√µes para 5 v√≠deos...\n",
            "[1/5] Analisando padr√µes para: ate quando voce vai ficar culpando os outros.mp4\n",
            "  ‚öôÔ∏è Analisando padr√µes para: vid_ate_quando_voce_vai_ficar_culpando_os_outros\n",
            "  ‚úÖ An√°lise de padr√µes conclu√≠da para ate quando voce vai ficar culpando os outros.mp4\n",
            "[2/5] Analisando padr√µes para: coloque metas em sua vida e se surpreenda.mp4\n",
            "  ‚öôÔ∏è Analisando padr√µes para: vid_coloque_metas_em_sua_vida_e_se_surpreenda\n",
            "  ‚úÖ An√°lise de padr√µes conclu√≠da para coloque metas em sua vida e se surpreenda.mp4\n",
            "[3/5] Analisando padr√µes para: a importancia de ser rico antes de ter.mp4\n",
            "  ‚öôÔ∏è Analisando padr√µes para: vid_a_importancia_de_ser_rico_antes_de_ter\n",
            "  ‚úÖ An√°lise de padr√µes conclu√≠da para a importancia de ser rico antes de ter.mp4\n",
            "[4/5] Analisando padr√µes para: as treÃÇs fases de todo mundo que decidiu fazer alguma coisa..mp4\n",
            "  ‚öôÔ∏è Analisando padr√µes para: vid_as_treÃÇs_fases_de_todo_mundo_que_decidiu_fazer_alguma_coisa\n",
            "  ‚úÖ An√°lise de padr√µes conclu√≠da para as treÃÇs fases de todo mundo que decidiu fazer alguma coisa..mp4\n",
            "[5/5] Analisando padr√µes para: a melhor saida eÃÅ se afastar de pessoas perversas.mp4\n",
            "  ‚öôÔ∏è Analisando padr√µes para: vid_a_melhor_saida_eÃÅ_se_afastar_de_pessoas_perversas\n",
            "  ‚úÖ An√°lise de padr√µes conclu√≠da para a melhor saida eÃÅ se afastar de pessoas perversas.mp4\n",
            "\n",
            "üíæ Dados de an√°lise de padr√µes salvos em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/analises_padroes_completas.json\n",
            "‚úÖ Status da etapa 'analise_padroes' atualizado no config.json\n",
            "\n",
            "‚úÖ AN√ÅLISE DE PADR√ïES CONCLU√çDA!\n",
            "Total de v√≠deos com padr√µes analisados: 5\n",
            "\n",
            "‚û°Ô∏è PR√ìXIMA C√âLULA: 3.2 - AN√ÅLISE PSICOL√ìGICA E GATILHOS DE ENGAJAMENTO\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# FUN√á√ÉO QUE EST√Å FALTANDO - ADICIONE NO IN√çCIO DO SCRIPT 3.2\n",
        "# ============================================================================\n",
        "\n",
        "def verificar_prerequisito_etapa(etapa_necessaria):\n",
        "    \"\"\"Verifica se uma etapa anterior foi conclu√≠da.\"\"\"\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "\n",
        "    if not os.path.exists(config_path):\n",
        "        print(f\"‚ùå PR√â-REQUISITO N√ÉO ATENDIDO: Arquivo config.json n√£o encontrado.\")\n",
        "        print(f\"   Execute as etapas anteriores primeiro.\")\n",
        "        return False, None\n",
        "\n",
        "    try:\n",
        "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            config = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå PR√â-REQUISITO N√ÉO ATENDIDO: Erro ao carregar config.json: {e}\")\n",
        "        return False, None\n",
        "\n",
        "    if \"status_etapas\" not in config:\n",
        "        print(f\"‚ùå PR√â-REQUISITO N√ÉO ATENDIDO: Campo 'status_etapas' n√£o encontrado no config.json.\")\n",
        "        return False, config\n",
        "\n",
        "    if etapa_necessaria not in config[\"status_etapas\"]:\n",
        "        print(f\"‚ùå PR√â-REQUISITO N√ÉO ATENDIDO: A etapa \\\"{etapa_necessaria}\\\" n√£o foi encontrada.\")\n",
        "        print(f\"   Execute a c√©lula correspondente primeiro.\")\n",
        "        return False, config\n",
        "\n",
        "    if not config[\"status_etapas\"][etapa_necessaria]:\n",
        "        print(f\"‚ùå PR√â-REQUISITO N√ÉO ATENDIDO: A etapa \\\"{etapa_necessaria}\\\" n√£o foi conclu√≠da.\")\n",
        "        print(f\"   Execute a c√©lula correspondente primeiro.\")\n",
        "        return False, config\n",
        "\n",
        "    return True, config\n",
        "\n",
        "# ============================================================================\n",
        "# FIM DA FUN√á√ÉO\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# C√âLULA 3.2: AN√ÅLISE PSICOL√ìGICA E GATILHOS DE ENGAJAMENTO\n",
        "# ============================================================================\n",
        "\n",
        "def analisar_psicologicamente_video(video_id, analise_padroes_data):\n",
        "    \"\"\"Simula an√°lise psicol√≥gica e detec√ß√£o de gatilhos de engajamento.\"\"\"\n",
        "    print(f\"  ‚öôÔ∏è Simulando an√°lise psicol√≥gica para: {video_id}\")\n",
        "\n",
        "    # Gatilhos de Engajamento (Exemplos de simula√ß√£o)\n",
        "    gatilhos_detectados = []\n",
        "    if \"Ritmo R√°pido (Muitos Cortes)\" in analise_padroes_data.get(\"padroes_gerais\", []):\n",
        "        gatilhos_detectados.append(\"Ritmo Acelerado (Aten√ß√£o)\")\n",
        "    if analise_padroes_data.get(\"analise_visual_detalhada\", {}).get(\"complexidade_visual_media\", 0) > 600:\n",
        "        gatilhos_detectados.append(\"Est√≠mulo Visual Intenso\")\n",
        "    if analise_padroes_data.get(\"resumo_texto\") and (\"oferta\" in analise_padroes_data[\"resumo_texto\"] .lower() or \"agora\" in analise_padroes_data[\"resumo_texto\"] .lower()):\n",
        "        gatilhos_detectados.append(\"Urg√™ncia/Escassez (Texto)\")\n",
        "\n",
        "    # Emo√ß√µes predominantes (Simula√ß√£o simples baseada em palavras-chave ou padr√µes)\n",
        "    emocoes_predominantes = {\n",
        "        \"alegria\": 0.6,\n",
        "        \"surpresa\": 0.2,\n",
        "        \"confianca\": 0.7\n",
        "    }\n",
        "\n",
        "    analise_psicologica = {\n",
        "        \"video_id\": video_id,\n",
        "        \"gatilhos_detectados\": gatilhos_detectados,\n",
        "        \"emocoes_predominantes\": emocoes_predominantes,\n",
        "        \"insights_psicologicos\": \"Este √© um placeholder para insights psicol√≥gicos mais profundos.\"\n",
        "    }\n",
        "\n",
        "    return analise_psicologica\n",
        "\n",
        "def processar_analise_psicologica_todos_videos():\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa(\"analise_padroes\")\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Carregar dados de an√°lise de padr√µes\n",
        "    analises_padroes_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_padroes_completas.json\")\n",
        "    with open(analises_padroes_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        analises_padroes = json.load(f)\n",
        "\n",
        "    analises_psicologicas_completas = []\n",
        "    sucessos = 0\n",
        "\n",
        "    print(\"\"\"\n",
        "Iniciando an√°lise psicol√≥gica para {} v√≠deos...\"\"\".format(len(analises_padroes)))\n",
        "\n",
        "    for i, analise_padroes_data in enumerate(analises_padroes, 1):\n",
        "        if analise_padroes_data.get(\"status\") == \"padroes_analisados\":\n",
        "            video_id = analise_padroes_data[\"video_id\"]\n",
        "            print(f\"[{i}/{len(analises_padroes)}] Analisando psicologicamente: {video_id}\")\n",
        "            try:\n",
        "                analise = analisar_psicologicamente_video(video_id, analise_padroes_data)\n",
        "                analise[\"status\"] = \"analise_psicologica_concluida\"\n",
        "                analises_psicologicas_completas.append(analise)\n",
        "                sucessos += 1\n",
        "                print(f\"  ‚úÖ An√°lise psicol√≥gica conclu√≠da para {video_id}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ùå ERRO na an√°lise psicol√≥gica para {video_id}: {e}\")\n",
        "                analises_psicologicas_completas.append({\"video_id\": video_id, \"status\": \"erro_analise_psicologica\", \"erro\": str(e)})\n",
        "        else:\n",
        "            print(f\"[{i}/{len(analises_padroes)}] Pulando {analise_padroes_data.get(\"video_id\")} - Status: {analise_padroes_data.get(\"status\", \"N/A\")}\")\n",
        "            analises_psicologicas_completas.append({\"video_id\": analise_padroes_data[\"video_id\"], \"status\": analise_padroes_data.get(\"status\", \"N/A\"), \"erro\": \"Pulado devido a erro anterior\"})\n",
        "\n",
        "    # Salvar an√°lises psicol√≥gicas completas\n",
        "    analises_json_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_psicologicas_completas.json\")\n",
        "    with open(analises_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(analises_psicologicas_completas, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    config[\"status_etapas\"][\"analise_psicologica\"] = True\n",
        "    config[\"total_videos_analisados_psicologicamente\"] = sucessos\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\"\"\n",
        "üíæ Dados de an√°lise psicol√≥gica salvos em: {analises_json_path}\"\"\")\n",
        "\n",
        "    print(\"\"\"\n",
        "‚úÖ AN√ÅLISE PSICOL√ìGICA CONCLU√çDA!\"\"\")\n",
        "    print(f\"Total de v√≠deos com an√°lise psicol√≥gica: {sucessos}\")\n",
        "\n",
        "    if sucessos == 0:\n",
        "        print(\"‚ùå NENHUM V√çDEO FOI ANALISADO PSICOLOGICAMENTE COM SUCESSO. Verifique as etapas anteriores.\")\n",
        "    print(\"\"\"\n",
        "‚û°Ô∏è PR√ìXIMA C√âLULA: 4.1 - GERA√á√ÉO DE RELAT√ìRIOS HUMANIZADOS\"\"\")\n",
        "\n",
        "# Executar an√°lise psicol√≥gica\n",
        "try:\n",
        "    processar_analise_psicologica_todos_videos()\n",
        "except Exception as e:\n",
        "    print(f\"\"\"\n",
        "‚ùå ERRO GERAL NA AN√ÅLISE PSICOL√ìGICA: {e}\"\"\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ],
      "metadata": {
        "id": "analise_psicologica",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeeb8328-b120-4457-a881-20f73915b5b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando an√°lise psicol√≥gica para 5 v√≠deos...\n",
            "[1/5] Analisando psicologicamente: vid_ate_quando_voce_vai_ficar_culpando_os_outros\n",
            "  ‚öôÔ∏è Simulando an√°lise psicol√≥gica para: vid_ate_quando_voce_vai_ficar_culpando_os_outros\n",
            "  ‚úÖ An√°lise psicol√≥gica conclu√≠da para vid_ate_quando_voce_vai_ficar_culpando_os_outros\n",
            "[2/5] Analisando psicologicamente: vid_coloque_metas_em_sua_vida_e_se_surpreenda\n",
            "  ‚öôÔ∏è Simulando an√°lise psicol√≥gica para: vid_coloque_metas_em_sua_vida_e_se_surpreenda\n",
            "  ‚úÖ An√°lise psicol√≥gica conclu√≠da para vid_coloque_metas_em_sua_vida_e_se_surpreenda\n",
            "[3/5] Analisando psicologicamente: vid_a_importancia_de_ser_rico_antes_de_ter\n",
            "  ‚öôÔ∏è Simulando an√°lise psicol√≥gica para: vid_a_importancia_de_ser_rico_antes_de_ter\n",
            "  ‚úÖ An√°lise psicol√≥gica conclu√≠da para vid_a_importancia_de_ser_rico_antes_de_ter\n",
            "[4/5] Analisando psicologicamente: vid_as_treÃÇs_fases_de_todo_mundo_que_decidiu_fazer_alguma_coisa\n",
            "  ‚öôÔ∏è Simulando an√°lise psicol√≥gica para: vid_as_treÃÇs_fases_de_todo_mundo_que_decidiu_fazer_alguma_coisa\n",
            "  ‚úÖ An√°lise psicol√≥gica conclu√≠da para vid_as_treÃÇs_fases_de_todo_mundo_que_decidiu_fazer_alguma_coisa\n",
            "[5/5] Analisando psicologicamente: vid_a_melhor_saida_eÃÅ_se_afastar_de_pessoas_perversas\n",
            "  ‚öôÔ∏è Simulando an√°lise psicol√≥gica para: vid_a_melhor_saida_eÃÅ_se_afastar_de_pessoas_perversas\n",
            "  ‚úÖ An√°lise psicol√≥gica conclu√≠da para vid_a_melhor_saida_eÃÅ_se_afastar_de_pessoas_perversas\n",
            "\n",
            "üíæ Dados de an√°lise psicol√≥gica salvos em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dados/analises_psicologicas_completas.json\n",
            "\n",
            "‚úÖ AN√ÅLISE PSICOL√ìGICA CONCLU√çDA!\n",
            "Total de v√≠deos com an√°lise psicol√≥gica: 5\n",
            "\n",
            "‚û°Ô∏è PR√ìXIMA C√âLULA: 4.1 - GERA√á√ÉO DE RELAT√ìRIOS HUMANIZADOS\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# LAYER 4: GERA√á√ÉO DE RELAT√ìRIOS E BLUEPRINT ESTRAT√âGICO\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# C√âLULA 4.1: GERA√á√ÉO DE RELAT√ìRIOS HUMANIZADOS (√ÅUDIO, VISUAL, TEXTO, PSICOL√ìGICO)\n",
        "# ============================================================================\n",
        "\n",
        "from fpdf import FPDF # Importar FPDF para gera√ß√£o de PDF\n",
        "\n",
        "class PDF(FPDF):\n",
        "    def header(self):\n",
        "        self.set_font('Arial', 'B', 12)\n",
        "        self.cell(0, 10, 'Relat√≥rio de Engenharia Reversa de V√≠deos', 0, 1, 'C')\n",
        "        self.ln(10)\n",
        "\n",
        "    def footer(self):\n",
        "        self.set_y(-15)\n",
        "        self.set_font('Arial', 'I', 8)\n",
        "        self.cell(0, 10, f'P√°gina {self.page_no()}/{{nb}}', 0, 0, 'C')\n",
        "\n",
        "    def chapter_title(self, title):\n",
        "        self.set_font('Arial', 'B', 12)\n",
        "        self.cell(0, 10, title, 0, 1, 'L')\n",
        "        self.ln(5)\n",
        "\n",
        "    def chapter_body(self, body):\n",
        "        self.set_font('Arial', '', 10)\n",
        "        self.multi_cell(0, 5, body)\n",
        "        self.ln()\n",
        "\n",
        "def gerar_relatorio_texto(video_id, analise_padroes_data, pasta_destino):\n",
        "    df_texto = pd.DataFrame([analise_padroes_data])\n",
        "    excel_path = os.path.join(pasta_destino, f'RELATORIO_TEXTO_HUMANIZADO_{video_id}.xlsx')\n",
        "    df_texto.to_excel(excel_path, index=False, engine='openpyxl')\n",
        "\n",
        "    pdf = PDF()\n",
        "    pdf.add_page()\n",
        "    pdf.chapter_title('Estrat√©gia de Conte√∫do Textual')\n",
        "    pdf.chapter_body(f'Resumo do Texto: {analise_padroes_data.get('resumo_texto', 'N/A')}')\n",
        "    pdf.chapter_body(f'Palavras-chave: {', '.join(analise_padroes_data.get('palavras_chave_texto', []))}')\n",
        "    pdf_path = os.path.join(pasta_destino, f'ESTRATEGIA_CONTEUDO_TEXTUAL_{video_id}.pdf')\n",
        "    pdf.output(pdf_path)\n",
        "    return excel_path, pdf_path\n",
        "\n",
        "def gerar_relatorio_audio(video_id, analise_padroes_data, pasta_destino):\n",
        "    df_audio = pd.DataFrame([analise_padroes_data.get('analise_audio_detalhada', {})])\n",
        "    excel_path = os.path.join(pasta_destino, f'RELATORIO_AUDIO_HUMANIZADO_{video_id}.xlsx')\n",
        "    df_audio.to_excel(excel_path, index=False, engine='openpyxl')\n",
        "\n",
        "    pdf = PDF()\n",
        "    pdf.add_page()\n",
        "    pdf.chapter_title('Resumo de √Åudio Estrat√©gico')\n",
        "    pdf.chapter_body(f'BPM: {analise_padroes_data.get('analise_audio_detalhada', {}).get('bpm', 'N/A')}')\n",
        "    pdf.chapter_body(f'Dura√ß√£o do √Åudio: {analise_padroes_data.get('analise_audio_detalhada', {}).get('duracao_audio_segundos', 'N/A')} segundos')\n",
        "    pdf_path = os.path.join(pasta_destino, f'RESUMO_AUDIO_ESTRATEGICO_{video_id}.pdf')\n",
        "    pdf.output(pdf_path)\n",
        "    return excel_path, pdf_path\n",
        "\n",
        "def gerar_relatorio_visual(video_id, analise_padroes_data, pasta_destino):\n",
        "    df_visual = pd.DataFrame([analise_padroes_data.get('analise_visual_detalhada', {})])\n",
        "    excel_path = os.path.join(pasta_destino, f'RELATORIO_VISUAL_HUMANIZADO_{video_id}.xlsx')\n",
        "    df_visual.to_excel(excel_path, index=False, engine='openpyxl')\n",
        "\n",
        "    pdf = PDF()\n",
        "    pdf.add_page()\n",
        "    pdf.chapter_title('Estrat√©gia Visual Completa')\n",
        "    pdf.chapter_body(f'Total de Cortes: {analise_padroes_data.get('analise_visual_detalhada', {}).get('total_cortes', 'N/A')}')\n",
        "    pdf.chapter_body(f'Complexidade Visual M√©dia: {analise_padroes_data.get('analise_visual_detalhada', {}).get('complexidade_visual_media', 'N/A'):.2f}')\n",
        "    pdf.chapter_body(f'Brilho M√©dio: {analise_padroes_data.get('analise_visual_detalhada', {}).get('brilho_medio', 'N/A'):.2f}')\n",
        "    pdf_path = os.path.join(pasta_destino, f'ESTRATEGIA_VISUAL_COMPLETA_{video_id}.pdf')\n",
        "    pdf.output(pdf_path)\n",
        "    return excel_path, pdf_path\n",
        "\n",
        "def gerar_relatorio_psicologico(video_id, analise_psicologica_data, pasta_destino):\n",
        "    df_psico = pd.DataFrame([analise_psicologica_data])\n",
        "    excel_path = os.path.join(pasta_destino, f'RELATORIO_PSICOLOGICO_HUMANIZADO_{video_id}.xlsx')\n",
        "    df_psico.to_excel(excel_path, index=False, engine='openpyxl')\n",
        "\n",
        "    pdf = PDF()\n",
        "    pdf.add_page()\n",
        "    pdf.chapter_title('Manual de Psicologia Viral')\n",
        "    pdf.chapter_body(f'Gatilhos Detectados: {', '.join(analise_psicologica_data.get('gatilhos_detectados', []))}')\n",
        "    pdf.chapter_body(f'Emo√ß√µes Predominantes: {analise_psicologica_data.get('emocoes_predominantes', 'N/A')}')\n",
        "    pdf.chapter_body(f'Insights: {analise_psicologica_data.get('insights_psicologicos', 'N/A')}')\n",
        "    pdf_path = os.path.join(pasta_destino, f'MANUAL_PSICOLOGIA_VIRAL_{video_id}.pdf')\n",
        "    pdf.output(pdf_path)\n",
        "    return excel_path, pdf_path\n",
        "\n",
        "def processar_geracao_relatorios_todos_videos():\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa('analise_psicologica')\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Carregar dados de an√°lise de padr√µes e psicol√≥gica\n",
        "    analises_padroes_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_padroes_completas.json\")\n",
        "    analises_psicologicas_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_psicologicas_completas.json\")\n",
        "    with open(analises_padroes_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        analises_padroes = json.load(f)\n",
        "    with open(analises_psicologicas_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        analises_psicologicas = json.load(f)\n",
        "\n",
        "    sucessos = 0\n",
        "\n",
        "    print(f\"\"\"\n",
        "Iniciando gera√ß√£o de relat√≥rios humanizados para {len(analises_padroes)} v√≠deos...\"\"\")\n",
        "\n",
        "    for i, analise_padroes_data in enumerate(analises_padroes, 1):\n",
        "        video_id = analise_padroes_data[\"video_id\"]\n",
        "        analise_psicologica_data = next((a for a in analises_psicologicas if a[\"video_id\"] == video_id), None)\n",
        "\n",
        "        if analise_padroes_data.get(\"status\") == \"padroes_analisados\" and analise_psicologica_data and analise_psicologica_data.get(\"status\") == \"analise_psicologica_concluida\":\n",
        "            print(f\"[{i}/{len(analises_padroes)}] Gerando relat√≥rios para: {video_id}\")\n",
        "            try:\n",
        "                # Gera√ß√£o de Relat√≥rios de Texto\n",
        "                pasta_texto = os.path.join(PASTA_TRABALHO, \"analise_texto\")\n",
        "                os.makedirs(pasta_texto, exist_ok=True)\n",
        "                excel_text, pdf_text = gerar_relatorio_texto(video_id, analise_padroes_data, pasta_texto)\n",
        "                print(f\"  üíæ Relat√≥rio de Texto (XLSX) salvo em: {excel_text}\")\n",
        "                print(f\"  üíæ Estrat√©gia de Conte√∫do Textual (PDF) salvo em: {pdf_text}\")\n",
        "\n",
        "                # Gera√ß√£o de Relat√≥rios de √Åudio\n",
        "                pasta_audio = os.path.join(PASTA_TRABALHO, \"analise_audio\")\n",
        "                os.makedirs(pasta_audio, exist_ok=True)\n",
        "                excel_audio, pdf_audio = gerar_relatorio_audio(video_id, analise_padroes_data, pasta_audio)\n",
        "                print(f\"  üíæ Relat√≥rio de √Åudio (XLSX) salvo em: {excel_audio}\")\n",
        "                print(f\"  üíæ Resumo de √Åudio Estrat√©gico (PDF) salvo em: {pdf_audio}\")\n",
        "\n",
        "                # Gera√ß√£o de Relat√≥rios Visuais\n",
        "                pasta_visual = os.path.join(PASTA_TRABALHO, \"analise_visual\")\n",
        "                os.makedirs(pasta_visual, exist_ok=True)\n",
        "                excel_visual, pdf_visual = gerar_relatorio_visual(video_id, analise_padroes_data, pasta_visual)\n",
        "                print(f\"  üíæ Relat√≥rio Visual (XLSX) salvo em: {excel_visual}\")\n",
        "                print(f\"  üíæ Estrat√©gia Visual Completa (PDF) salvo em: {pdf_visual}\")\n",
        "\n",
        "                # Gera√ß√£o de Relat√≥rios Psicol√≥gicos\n",
        "                pasta_psicologica = os.path.join(PASTA_TRABALHO, \"analise_psicologica\")\n",
        "                os.makedirs(pasta_psicologica, exist_ok=True)\n",
        "                excel_psico, pdf_psico = gerar_relatorio_psicologico(video_id, analise_psicologica_data, pasta_psicologica)\n",
        "                print(f\"  üíæ Relat√≥rio Psicol√≥gico (XLSX) salvo em: {excel_psico}\")\n",
        "                print(f\"  üíæ Manual de Psicologia Viral (PDF) salvo em: {pdf_psico}\")\n",
        "\n",
        "                sucessos += 1\n",
        "                print(f\"  ‚úÖ Relat√≥rios gerados para {video_id}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ùå ERRO na gera√ß√£o de relat√≥rios para {video_id}: {e}\")\n",
        "        else:\n",
        "            print(f\"[{i}/{len(analises_padroes)}] Pulando {video_id} - Pr√©-requisitos n√£o atendidos.\")\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    config[\"status_etapas\"][\"relatorios_humanizados\"] = True\n",
        "    config[\"total_videos_relatorios_gerados\"] = sucessos\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(\"\"\"\n",
        "‚úÖ GERA√á√ÉO DE RELAT√ìRIOS HUMANIZADOS CONCLU√çDA!\"\"\")\n",
        "    print(f\"Total de v√≠deos com relat√≥rios gerados: {sucessos}\")\n",
        "\n",
        "    if sucessos == 0:\n",
        "        print(\"‚ùå NENHUM V√çDEO TEVE RELAT√ìRIOS GERADOS COM SUCESSO. Verifique as etapas anteriores.\")\n",
        "    print(\"\"\"\n",
        "‚û°Ô∏è PR√ìXIMA C√âLULA: 4.2 - GERA√á√ÉO DO BLUEPRINT FINAL E DASHBOARD\"\"\")\n",
        "\n",
        "# Executar gera√ß√£o de relat√≥rios\n",
        "try:\n",
        "    processar_geracao_relatorios_todos_videos()\n",
        "except Exception as e:\n",
        "    print(f\"\"\"\n",
        "‚ùå ERRO GERAL NA GERA√á√ÉO DE RELAT√ìRIOS: {e}\"\"\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ],
      "metadata": {
        "id": "relatorios_humanizados",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1b5f59b-7b0f-4a25-d20f-b0a90ec969ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando gera√ß√£o de relat√≥rios humanizados para 5 v√≠deos...\n",
            "[1/5] Gerando relat√≥rios para: vid_ate_quando_voce_vai_ficar_culpando_os_outros\n",
            "  üíæ Relat√≥rio de Texto (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_texto/RELATORIO_TEXTO_HUMANIZADO_vid_ate_quando_voce_vai_ficar_culpando_os_outros.xlsx\n",
            "  üíæ Estrat√©gia de Conte√∫do Textual (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_texto/ESTRATEGIA_CONTEUDO_TEXTUAL_vid_ate_quando_voce_vai_ficar_culpando_os_outros.pdf\n",
            "  üíæ Relat√≥rio de √Åudio (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_audio/RELATORIO_AUDIO_HUMANIZADO_vid_ate_quando_voce_vai_ficar_culpando_os_outros.xlsx\n",
            "  üíæ Resumo de √Åudio Estrat√©gico (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_audio/RESUMO_AUDIO_ESTRATEGICO_vid_ate_quando_voce_vai_ficar_culpando_os_outros.pdf\n",
            "  üíæ Relat√≥rio Visual (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_visual/RELATORIO_VISUAL_HUMANIZADO_vid_ate_quando_voce_vai_ficar_culpando_os_outros.xlsx\n",
            "  üíæ Estrat√©gia Visual Completa (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_visual/ESTRATEGIA_VISUAL_COMPLETA_vid_ate_quando_voce_vai_ficar_culpando_os_outros.pdf\n",
            "  üíæ Relat√≥rio Psicol√≥gico (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_psicologica/RELATORIO_PSICOLOGICO_HUMANIZADO_vid_ate_quando_voce_vai_ficar_culpando_os_outros.xlsx\n",
            "  üíæ Manual de Psicologia Viral (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_psicologica/MANUAL_PSICOLOGIA_VIRAL_vid_ate_quando_voce_vai_ficar_culpando_os_outros.pdf\n",
            "  ‚úÖ Relat√≥rios gerados para vid_ate_quando_voce_vai_ficar_culpando_os_outros\n",
            "[2/5] Gerando relat√≥rios para: vid_coloque_metas_em_sua_vida_e_se_surpreenda\n",
            "  üíæ Relat√≥rio de Texto (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_texto/RELATORIO_TEXTO_HUMANIZADO_vid_coloque_metas_em_sua_vida_e_se_surpreenda.xlsx\n",
            "  üíæ Estrat√©gia de Conte√∫do Textual (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_texto/ESTRATEGIA_CONTEUDO_TEXTUAL_vid_coloque_metas_em_sua_vida_e_se_surpreenda.pdf\n",
            "  üíæ Relat√≥rio de √Åudio (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_audio/RELATORIO_AUDIO_HUMANIZADO_vid_coloque_metas_em_sua_vida_e_se_surpreenda.xlsx\n",
            "  üíæ Resumo de √Åudio Estrat√©gico (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_audio/RESUMO_AUDIO_ESTRATEGICO_vid_coloque_metas_em_sua_vida_e_se_surpreenda.pdf\n",
            "  üíæ Relat√≥rio Visual (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_visual/RELATORIO_VISUAL_HUMANIZADO_vid_coloque_metas_em_sua_vida_e_se_surpreenda.xlsx\n",
            "  üíæ Estrat√©gia Visual Completa (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_visual/ESTRATEGIA_VISUAL_COMPLETA_vid_coloque_metas_em_sua_vida_e_se_surpreenda.pdf\n",
            "  üíæ Relat√≥rio Psicol√≥gico (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_psicologica/RELATORIO_PSICOLOGICO_HUMANIZADO_vid_coloque_metas_em_sua_vida_e_se_surpreenda.xlsx\n",
            "  üíæ Manual de Psicologia Viral (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_psicologica/MANUAL_PSICOLOGIA_VIRAL_vid_coloque_metas_em_sua_vida_e_se_surpreenda.pdf\n",
            "  ‚úÖ Relat√≥rios gerados para vid_coloque_metas_em_sua_vida_e_se_surpreenda\n",
            "[3/5] Gerando relat√≥rios para: vid_a_importancia_de_ser_rico_antes_de_ter\n",
            "  üíæ Relat√≥rio de Texto (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_texto/RELATORIO_TEXTO_HUMANIZADO_vid_a_importancia_de_ser_rico_antes_de_ter.xlsx\n",
            "  üíæ Estrat√©gia de Conte√∫do Textual (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_texto/ESTRATEGIA_CONTEUDO_TEXTUAL_vid_a_importancia_de_ser_rico_antes_de_ter.pdf\n",
            "  üíæ Relat√≥rio de √Åudio (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_audio/RELATORIO_AUDIO_HUMANIZADO_vid_a_importancia_de_ser_rico_antes_de_ter.xlsx\n",
            "  üíæ Resumo de √Åudio Estrat√©gico (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_audio/RESUMO_AUDIO_ESTRATEGICO_vid_a_importancia_de_ser_rico_antes_de_ter.pdf\n",
            "  üíæ Relat√≥rio Visual (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_visual/RELATORIO_VISUAL_HUMANIZADO_vid_a_importancia_de_ser_rico_antes_de_ter.xlsx\n",
            "  üíæ Estrat√©gia Visual Completa (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_visual/ESTRATEGIA_VISUAL_COMPLETA_vid_a_importancia_de_ser_rico_antes_de_ter.pdf\n",
            "  üíæ Relat√≥rio Psicol√≥gico (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_psicologica/RELATORIO_PSICOLOGICO_HUMANIZADO_vid_a_importancia_de_ser_rico_antes_de_ter.xlsx\n",
            "  üíæ Manual de Psicologia Viral (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/analise_psicologica/MANUAL_PSICOLOGIA_VIRAL_vid_a_importancia_de_ser_rico_antes_de_ter.pdf\n",
            "  ‚úÖ Relat√≥rios gerados para vid_a_importancia_de_ser_rico_antes_de_ter\n",
            "[4/5] Gerando relat√≥rios para: vid_as_treÃÇs_fases_de_todo_mundo_que_decidiu_fazer_alguma_coisa\n",
            "  ‚ùå ERRO na gera√ß√£o de relat√≥rios para vid_as_treÃÇs_fases_de_todo_mundo_que_decidiu_fazer_alguma_coisa: 'latin-1' codec can't encode character '\\u201c' in position 625: ordinal not in range(256)\n",
            "[5/5] Gerando relat√≥rios para: vid_a_melhor_saida_eÃÅ_se_afastar_de_pessoas_perversas\n",
            "  ‚ùå ERRO na gera√ß√£o de relat√≥rios para vid_a_melhor_saida_eÃÅ_se_afastar_de_pessoas_perversas: 'latin-1' codec can't encode character '\\u201c' in position 304: ordinal not in range(256)\n",
            "\n",
            "‚úÖ GERA√á√ÉO DE RELAT√ìRIOS HUMANIZADOS CONCLU√çDA!\n",
            "Total de v√≠deos com relat√≥rios gerados: 3\n",
            "\n",
            "‚û°Ô∏è PR√ìXIMA C√âLULA: 4.2 - GERA√á√ÉO DO BLUEPRINT FINAL E DASHBOARD\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 4.2: GERA√á√ÉO DO BLUEPRINT FINAL E DASHBOARD\n",
        "# ============================================================================\n",
        "\n",
        "def gerar_blueprint_dashboard():\n",
        "    prerequisito_ok, config = verificar_prerequisito_etapa(\"relatorios_humanizados\")\n",
        "    if not prerequisito_ok:\n",
        "        return\n",
        "\n",
        "    # Carregar todos os dados de an√°lise\n",
        "    metadados_path = os.path.join(PASTA_TRABALHO, \"dados\", \"metadados_completos.json\")\n",
        "    decomposicao_path = os.path.join(PASTA_TRABALHO, \"dados\", \"decomposicao_completa.json\")\n",
        "    analises_padroes_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_padroes_completas.json\")\n",
        "    analises_psicologicas_path = os.path.join(PASTA_TRABALHO, \"dados\", \"analises_psicologicas_completas.json\")\n",
        "\n",
        "    with open(metadados_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        metadados = json.load(f)\n",
        "    with open(decomposicao_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        decomposicoes = json.load(f)\n",
        "    with open(analises_padroes_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        analises_padroes = json.load(f)\n",
        "    with open(analises_psicologicas_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        analises_psicologicas = json.load(f)\n",
        "\n",
        "    dados_consolidados = []\n",
        "    for video_meta in metadados:\n",
        "        video_id = video_meta[\"id\"]\n",
        "        decomposicao = next((d for d in decomposicoes if d[\"video_id\"] == video_id), {})\n",
        "        analise_padroes = next((ap for ap in analises_padroes if ap[\"video_id\"] == video_id), {})\n",
        "        analise_psicologica = next((aps for aps in analises_psicologicas if aps[\"video_id\"] == video_id), {})\n",
        "        consolidado = {\n",
        "            \"video_id\": video_id,\n",
        "            \"nome_arquivo\": video_meta.get(\"nome_arquivo\"),\n",
        "            \"duracao_segundos\": video_meta.get(\"duracao_segundos\"),\n",
        "            \"formato_detectado\": video_meta.get(\"formato_detectado\"),\n",
        "            \"tem_audio\": video_meta.get(\"tem_audio\"),\n",
        "            \"total_frames\": video_meta.get(\"total_frames\"),\n",
        "            \"ocr_textos_count\": len(decomposicao.get(\"textos_ocr\", [])),\n",
        "            \"audio_transcrito_len\": len(decomposicao.get(\"audio_transcrito\", \"\")),\n",
        "            \"cortes_detectados_count\": len(decomposicao.get(\"cortes_detectados_segundos\", [])),\n",
        "            \"bpm_audio\": analise_padroes.get(\"analise_audio_detalhada\", {}).get(\"bpm\"),\n",
        "            \"complexidade_visual_media\": analise_padroes.get(\"analise_visual_detalhada\", {}).get(\"complexidade_visual_media\"),\n",
        "            \"brilho_medio\": analise_padroes.get(\"analise_visual_detalhada\", {}).get(\"brilho_medio\"),\n",
        "            \"padroes_gerais\": \", \".join(analise_padroes.get(\"padroes_gerais\", [])),\n",
        "            \"gatilhos_psicologicos\": \", \".join(analise_psicologica.get(\"gatilhos_detectados\", [])),\n",
        "            \"emocoes_predominantes\": str(analise_psicologica.get(\"emocoes_predominantes\", {})),\n",
        "            \"status_geral\": video_meta.get(\"status\") # Pode ser aprimorado para refletir o status de todas as etapas\n",
        "        }\n",
        "        dados_consolidados.append(consolidado)\n",
        "\n",
        "    df_final = pd.DataFrame(dados_consolidados)\n",
        "\n",
        "    # Salvar Dashboard Executivo (Excel)\n",
        "    dashboard_excel_path = os.path.join(PASTA_TRABALHO, \"dashboard\", \"DASHBOARD_MASTER_EXECUTIVO.xlsx\")\n",
        "    df_final.to_excel(dashboard_excel_path, index=False, engine=\"openpyxl\")\n",
        "    print(f\"\\nüíæ Dashboard Executivo (XLSX) salvo em: {dashboard_excel_path}\")\n",
        "\n",
        "    # Salvar Dados Consolidados (CSV e JSON)\n",
        "    dados_csv_path = os.path.join(PASTA_TRABALHO, \"dashboard\", \"dados_consolidados.csv\")\n",
        "    df_final.to_csv(dados_csv_path, index=False, encoding=\"utf-8\")\n",
        "    print(f\"üíæ Dados Consolidados (CSV) salvo em: {dados_csv_path}\")\n",
        "\n",
        "    dados_json_path = os.path.join(PASTA_TRABALHO, \"dashboard\", \"dados_detalhados.json\")\n",
        "    with open(dados_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(dados_consolidados, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"üíæ Dados Detalhados (JSON) salvo em: {dados_json_path}\")\n",
        "\n",
        "    # Gera√ß√£o de Dashboard Interativo (HTML - Exemplo simples)\n",
        "    # Para um dashboard interativo real, seria necess√°rio uma biblioteca como Plotly ou Dash\n",
        "    dashboard_html_path = os.path.join(PASTA_TRABALHO, \"dashboard\", \"dashboard_interativo.html\")\n",
        "    with open(dashboard_html_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"<html><body><h1>Dashboard Interativo (Placeholder)</h1><p>Seu dashboard interativo real seria gerado aqui com bibliotecas como Plotly ou Dash.</p></body></html>\")\n",
        "    print(f\"üíæ Dashboard Interativo (HTML) salvo em: {dashboard_html_path}\")\n",
        "\n",
        "    # Gera√ß√£o do Blueprint Estrat√©gico (PDF - Exemplo simples)\n",
        "    pdf = PDF()\n",
        "    pdf.add_page()\n",
        "    pdf.chapter_title(\"BLUEPRINT ESTRAT√âGICO FINAL\")\n",
        "    pdf.chapter_body(\"Este √© o seu blueprint estrat√©gico final, consolidando todos os insights.\")\n",
        "    pdf.chapter_body(f\"Total de v√≠deos analisados: {len(df_final)}\")\n",
        "    pdf.chapter_body(f\"M√©dia de dura√ß√£o dos v√≠deos: {df_final[\"duracao_segundos\"] .mean():.2f} segundos\")\n",
        "    pdf_blueprint_path = os.path.join(PASTA_TRABALHO, \"blueprint\", \"BLUEPRINT_ESTRATEGICO_FINAL.pdf\")\n",
        "    pdf.output(pdf_blueprint_path)\n",
        "    print(f\"üíæ Blueprint Estrat√©gico (PDF) salvo em: {pdf_blueprint_path}\")\n",
        "\n",
        "    # Atualizar status no config\n",
        "    config_path = os.path.join(PASTA_TRABALHO, \"config\", \"config.json\")\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    config[\"status_etapas\"][\"blueprint\"] = True\n",
        "\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(\"\\n‚úÖ GERA√á√ÉO DO BLUEPRINT FINAL E DASHBOARD CONCLU√çDA!\")\n",
        "    print(\"Todos os relat√≥rios e o dashboard foram gerados com sucesso.\")\n",
        "    print(\"\\nüéâ PROCESSO DE ENGENHARIA REVERSA CONCLU√çDO COM SUCESSO! üéâ\")\n",
        "\n",
        "# Executar gera√ß√£o de blueprint e dashboard\n",
        "try:\n",
        "    gerar_blueprint_dashboard()\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå ERRO GERAL NA GERA√á√ÉO DO BLUEPRINT E DASHBOARD: {e}\")\n",
        "    print(\"Por favor, corrija o erro acima antes de prosseguir.\")"
      ],
      "metadata": {
        "id": "blueprint_dashboard",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fc0c3a2-ce6c-42d0-f7b9-e0ab25bfb396"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üíæ Dashboard Executivo (XLSX) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dashboard/DASHBOARD_MASTER_EXECUTIVO.xlsx\n",
            "üíæ Dados Consolidados (CSV) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dashboard/dados_consolidados.csv\n",
            "üíæ Dados Detalhados (JSON) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dashboard/dados_detalhados.json\n",
            "üíæ Dashboard Interativo (HTML) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dashboard/dashboard_interativo.html\n",
            "üíæ Blueprint Estrat√©gico (PDF) salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/blueprint/BLUEPRINT_ESTRATEGICO_FINAL.pdf\n",
            "\n",
            "‚úÖ GERA√á√ÉO DO BLUEPRINT FINAL E DASHBOARD CONCLU√çDA!\n",
            "Todos os relat√≥rios e o dashboard foram gerados com sucesso.\n",
            "\n",
            "üéâ PROCESSO DE ENGENHARIA REVERSA CONCLU√çDO COM SUCESSO! üéâ\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Verificar se o processo de engenharia reversa foi executado\n",
        "BASE_PATH = \"/content/drive/MyDrive/Videos Dona Done/_engenharia_reversa\"\n",
        "CSV_PATH = os.path.join(BASE_PATH, \"dashboard\", \"dados_consolidados.csv\")\n",
        "JSON_PATH = os.path.join(BASE_PATH, \"dashboard\", \"dados_detalhados.json\")\n",
        "\n",
        "print(\"üîç VERIFICANDO PR√â-REQUISITOS...\")\n",
        "print(f\"Pasta base existe: {os.path.exists(BASE_PATH)}\")\n",
        "print(f\"CSV existe: {os.path.exists(CSV_PATH)}\")\n",
        "print(f\"JSON existe: {os.path.exists(JSON_PATH)}\")\n",
        "\n",
        "if os.path.exists(CSV_PATH):\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "    print(f\"üìä Dados CSV: {len(df)} v√≠deos encontrados\")\n",
        "\n",
        "print(\"\\n‚úÖ Se todos os itens acima s√£o True/existem, voc√™ pode prosseguir!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QgJMmh1JJ-0",
        "outputId": "252f927c-f55a-4a6d-fa2f-ef8fcb10f153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç VERIFICANDO PR√â-REQUISITOS...\n",
            "Pasta base existe: True\n",
            "CSV existe: True\n",
            "JSON existe: True\n",
            "üìä Dados CSV: 5 v√≠deos encontrados\n",
            "\n",
            "‚úÖ Se todos os itens acima s√£o True/existem, voc√™ pode prosseguir!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SISTEMA DE INTEGRA√á√ÉO AUTOM√ÅTICA PARA NOVAS FUNCIONALIDADES\n",
        "# ============================================================================\n",
        "# Este script deve SUBSTITUIR a √∫ltima c√©lula (4.2) do notebook\n",
        "# Ele detecta automaticamente todas as an√°lises dispon√≠veis e as integra\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import glob\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def descobrir_analises_disponiveis(pasta_trabalho):\n",
        "    \"\"\"Descobre automaticamente todas as an√°lises realizadas\"\"\"\n",
        "    analises_encontradas = {\n",
        "        \"base\": {},\n",
        "        \"adicionais\": {}\n",
        "    }\n",
        "\n",
        "    dados_path = os.path.join(pasta_trabalho, \"dados\")\n",
        "\n",
        "    # An√°lises b√°sicas obrigat√≥rias\n",
        "    arquivos_base = {\n",
        "        \"metadados\": \"metadados_completos.json\",\n",
        "        \"decomposicao\": \"decomposicao_completa.json\",\n",
        "        \"padroes\": \"analises_padroes_completas.json\",\n",
        "        \"psicologica\": \"analises_psicologicas_completas.json\"\n",
        "    }\n",
        "\n",
        "    for tipo, arquivo in arquivos_base.items():\n",
        "        caminho = os.path.join(dados_path, arquivo)\n",
        "        if os.path.exists(caminho):\n",
        "            analises_encontradas[\"base\"][tipo] = caminho\n",
        "            print(f\"‚úÖ An√°lise base encontrada: {tipo}\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è An√°lise base ausente: {tipo}\")\n",
        "\n",
        "    # Descobrir an√°lises adicionais automaticamente\n",
        "    # Busca por qualquer arquivo JSON que n√£o seja das an√°lises base\n",
        "    todos_jsons = glob.glob(os.path.join(dados_path, \"*.json\"))\n",
        "\n",
        "    for json_path in todos_jsons:\n",
        "        nome_arquivo = os.path.basename(json_path)\n",
        "\n",
        "        # Pular arquivos base\n",
        "        if nome_arquivo in arquivos_base.values():\n",
        "            continue\n",
        "\n",
        "        # Identificar tipo da an√°lise pelo nome\n",
        "        if \"audio\" in nome_arquivo.lower():\n",
        "            analises_encontradas[\"adicionais\"][\"audio_refinada\"] = json_path\n",
        "            print(f\"‚úÖ An√°lise adicional encontrada: Audio Refinada\")\n",
        "        elif \"visual\" in nome_arquivo.lower():\n",
        "            analises_encontradas[\"adicionais\"][\"visual_avancada\"] = json_path\n",
        "            print(f\"‚úÖ An√°lise adicional encontrada: Visual Avan√ßada\")\n",
        "        elif \"texto\" in nome_arquivo.lower():\n",
        "            analises_encontradas[\"adicionais\"][\"texto_avancada\"] = json_path\n",
        "            print(f\"‚úÖ An√°lise adicional encontrada: Texto Avan√ßada\")\n",
        "        elif \"sentiment\" in nome_arquivo.lower():\n",
        "            analises_encontradas[\"adicionais\"][\"sentimento\"] = json_path\n",
        "            print(f\"‚úÖ An√°lise adicional encontrada: Sentimento\")\n",
        "        else:\n",
        "            # An√°lise n√£o reconhecida - incluir mesmo assim\n",
        "            nome_limpo = nome_arquivo.replace(\".json\", \"\").replace(\"_\", \" \").title()\n",
        "            analises_encontradas[\"adicionais\"][nome_arquivo] = json_path\n",
        "            print(f\"‚úÖ An√°lise personalizada encontrada: {nome_limpo}\")\n",
        "\n",
        "    return analises_encontradas\n",
        "\n",
        "def carregar_dados_analise(caminho_arquivo):\n",
        "    \"\"\"Carrega dados de uma an√°lise com tratamento de erros\"\"\"\n",
        "    try:\n",
        "        with open(caminho_arquivo, 'r', encoding='utf-8') as f:\n",
        "            dados = json.load(f)\n",
        "        return dados, True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Erro ao carregar {caminho_arquivo}: {e}\")\n",
        "        return [], False\n",
        "\n",
        "def extrair_metricas_dinamicamente(dados, tipo_analise):\n",
        "    \"\"\"Extrai m√©tricas de qualquer tipo de an√°lise dinamicamente\"\"\"\n",
        "    metricas_extraidas = {}\n",
        "\n",
        "    if not dados:\n",
        "        return metricas_extraidas\n",
        "\n",
        "    # Pegar o primeiro item para entender a estrutura\n",
        "    primeiro_item = dados[0] if isinstance(dados, list) else dados\n",
        "\n",
        "    if isinstance(primeiro_item, dict):\n",
        "        for chave, valor in primeiro_item.items():\n",
        "            if chave in ['video_id', 'status', 'data_analise', 'erro']:\n",
        "                continue\n",
        "\n",
        "            # Extrair m√©tricas num√©ricas automaticamente\n",
        "            if isinstance(valor, (int, float)):\n",
        "                metricas_extraidas[f\"{tipo_analise}_{chave}\"] = valor\n",
        "            elif isinstance(valor, dict):\n",
        "                # An√°lise aninhada - extrair sub-m√©tricas\n",
        "                for sub_chave, sub_valor in valor.items():\n",
        "                    if isinstance(sub_valor, (int, float)):\n",
        "                        metricas_extraidas[f\"{tipo_analise}_{chave}_{sub_chave}\"] = sub_valor\n",
        "                    elif isinstance(sub_valor, list) and sub_valor and isinstance(sub_valor[0], (int, float)):\n",
        "                        # Lista de n√∫meros - calcular estat√≠sticas\n",
        "                        metricas_extraidas[f\"{tipo_analise}_{chave}_{sub_chave}_media\"] = sum(sub_valor) / len(sub_valor)\n",
        "                        metricas_extraidas[f\"{tipo_analise}_{chave}_{sub_chave}_max\"] = max(sub_valor)\n",
        "                        metricas_extraidas[f\"{tipo_analise}_{chave}_{sub_chave}_min\"] = min(sub_valor)\n",
        "            elif isinstance(valor, list):\n",
        "                if valor and isinstance(valor[0], (int, float)):\n",
        "                    # Lista de n√∫meros\n",
        "                    metricas_extraidas[f\"{tipo_analise}_{chave}_count\"] = len(valor)\n",
        "                    metricas_extraidas[f\"{tipo_analise}_{chave}_media\"] = sum(valor) / len(valor) if valor else 0\n",
        "                else:\n",
        "                    # Lista de objetos ou strings\n",
        "                    metricas_extraidas[f\"{tipo_analise}_{chave}_count\"] = len(valor)\n",
        "\n",
        "    return metricas_extraidas\n",
        "\n",
        "def consolidar_todos_dados(analises_encontradas):\n",
        "    \"\"\"Consolida todos os dados de todas as an√°lises encontradas\"\"\"\n",
        "    dados_consolidados = {}\n",
        "\n",
        "    # Carregar an√°lises base\n",
        "    for tipo, caminho in analises_encontradas[\"base\"].items():\n",
        "        dados, sucesso = carregar_dados_analise(caminho)\n",
        "        if sucesso:\n",
        "            dados_consolidados[tipo] = dados\n",
        "\n",
        "    # Carregar an√°lises adicionais\n",
        "    for tipo, caminho in analises_encontradas[\"adicionais\"].items():\n",
        "        dados, sucesso = carregar_dados_analise(caminho)\n",
        "        if sucesso:\n",
        "            dados_consolidados[tipo] = dados\n",
        "\n",
        "    # Criar DataFrame consolidado por v√≠deo\n",
        "    videos_df = pd.DataFrame()\n",
        "\n",
        "    # Come√ßar com metadados base se dispon√≠vel\n",
        "    if \"metadados\" in dados_consolidados:\n",
        "        videos_df = pd.DataFrame(dados_consolidados[\"metadados\"])\n",
        "        videos_df = videos_df.set_index('id')\n",
        "\n",
        "    # Integrar cada an√°lise adicional\n",
        "    for tipo, dados in dados_consolidados.items():\n",
        "        if tipo == \"metadados\":\n",
        "            continue\n",
        "\n",
        "        print(f\"üîÑ Integrando dados de: {tipo}\")\n",
        "\n",
        "        # Converter para DataFrame se for lista\n",
        "        if isinstance(dados, list):\n",
        "            df_analise = pd.DataFrame(dados)\n",
        "\n",
        "            if 'video_id' in df_analise.columns:\n",
        "                df_analise = df_analise.set_index('video_id')\n",
        "\n",
        "                # Extrair m√©tricas dinamicamente\n",
        "                for video_id, row in df_analise.iterrows():\n",
        "                    metricas = extrair_metricas_dinamicamente([row.to_dict()], tipo)\n",
        "\n",
        "                    for metrica, valor in metricas.items():\n",
        "                        if video_id in videos_df.index:\n",
        "                            videos_df.loc[video_id, metrica] = valor\n",
        "                        else:\n",
        "                            # Criar nova linha se v√≠deo n√£o existir\n",
        "                            videos_df.loc[video_id, metrica] = valor\n",
        "\n",
        "    return videos_df.reset_index()\n",
        "\n",
        "def gerar_dashboard_dinamico(df_consolidado, pasta_trabalho):\n",
        "    \"\"\"Gera dashboard din√¢mico incluindo todas as an√°lises encontradas\"\"\"\n",
        "    from openpyxl import Workbook\n",
        "    from openpyxl.styles import Font, Alignment, PatternFill\n",
        "\n",
        "    wb = Workbook()\n",
        "\n",
        "    # ABA 1: VIS√ÉO GERAL DIN√ÇMICA\n",
        "    ws_geral = wb.active\n",
        "    ws_geral.title = 'Vis√£o Geral Completa'\n",
        "\n",
        "    # Header\n",
        "    ws_geral.cell(row=1, column=1).value = 'RELAT√ìRIO COMPLETO DE ENGENHARIA REVERSA'\n",
        "    ws_geral.cell(row=1, column=1).font = Font(bold=True, size=16)\n",
        "\n",
        "    # Estat√≠sticas gerais\n",
        "    ws_geral.cell(row=3, column=1).value = 'AN√ÅLISES REALIZADAS'\n",
        "    ws_geral.cell(row=3, column=1).font = Font(bold=True, size=14)\n",
        "\n",
        "    # Contar colunas por tipo de an√°lise\n",
        "    colunas_por_tipo = {}\n",
        "    for col in df_consolidado.columns:\n",
        "        if '_' in col:\n",
        "            tipo = col.split('_')[0]\n",
        "            colunas_por_tipo[tipo] = colunas_por_tipo.get(tipo, 0) + 1\n",
        "\n",
        "    row = 4\n",
        "    for tipo, count in colunas_por_tipo.items():\n",
        "        ws_geral.cell(row=row, column=1).value = f\"{tipo.upper()}\"\n",
        "        ws_geral.cell(row=row, column=2).value = f\"{count} m√©tricas extra√≠das\"\n",
        "        ws_geral.cell(row=row, column=1).font = Font(bold=True)\n",
        "        row += 1\n",
        "\n",
        "    # ABA 2: DADOS COMPLETOS\n",
        "    ws_dados = wb.create_sheet('Dados Completos')\n",
        "\n",
        "    # Adicionar todos os dados\n",
        "    for r_idx, row in enumerate(df_consolidado.itertuples(), 1):\n",
        "        for c_idx, value in enumerate(row):\n",
        "            cell = ws_dados.cell(row=r_idx, column=c_idx)\n",
        "            cell.value = value\n",
        "            if r_idx == 1:  # Header\n",
        "                cell.font = Font(bold=True)\n",
        "\n",
        "    # ABA 3: M√âTRICAS POR TIPO\n",
        "    tipos_encontrados = list(set([col.split('_')[0] for col in df_consolidado.columns if '_' in col]))\n",
        "\n",
        "    for tipo in tipos_encontrados:\n",
        "        ws_tipo = wb.create_sheet(f'An√°lise {tipo.title()}')\n",
        "\n",
        "        # Filtrar colunas deste tipo\n",
        "        colunas_tipo = ['id', 'nome_arquivo'] + [col for col in df_consolidado.columns if col.startswith(tipo)]\n",
        "\n",
        "        if len(colunas_tipo) > 2:  # Tem dados al√©m do id e nome\n",
        "            df_tipo = df_consolidado[colunas_tipo]\n",
        "\n",
        "            # Adicionar ao worksheet\n",
        "            for r_idx, row in enumerate(df_tipo.itertuples(), 1):\n",
        "                for c_idx, value in enumerate(row):\n",
        "                    cell = ws_tipo.cell(row=r_idx, column=c_idx)\n",
        "                    cell.value = value\n",
        "                    if r_idx == 1:\n",
        "                        cell.font = Font(bold=True)\n",
        "\n",
        "    # ABA 4: INSIGHTS AUTOMATICOS\n",
        "    ws_insights = wb.create_sheet('Insights Autom√°ticos')\n",
        "\n",
        "    insights_automaticos = gerar_insights_automaticos(df_consolidado)\n",
        "\n",
        "    ws_insights.cell(row=1, column=1).value = 'INSIGHTS GERADOS AUTOMATICAMENTE'\n",
        "    ws_insights.cell(row=1, column=1).font = Font(bold=True, size=16)\n",
        "\n",
        "    for i, insight in enumerate(insights_automaticos, 3):\n",
        "        ws_insights.cell(row=i, column=1).value = f\"‚Ä¢ {insight}\"\n",
        "        ws_insights.cell(row=i, column=1).alignment = Alignment(wrap_text=True)\n",
        "\n",
        "    # Salvar\n",
        "    output_path = os.path.join(pasta_trabalho, \"dashboard\", \"RELATORIO_COMPLETO_DINAMICO.xlsx\")\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    wb.save(output_path)\n",
        "\n",
        "    return output_path\n",
        "\n",
        "def gerar_insights_automaticos(df):\n",
        "    \"\"\"Gera insights autom√°ticos baseados em qualquer conjunto de dados\"\"\"\n",
        "    insights = []\n",
        "\n",
        "    # An√°lise de correla√ß√µes autom√°ticas\n",
        "    colunas_numericas = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "    if len(colunas_numericas) > 1:\n",
        "        correlacoes = df[colunas_numericas].corr()\n",
        "\n",
        "        # Encontrar correla√ß√µes fortes\n",
        "        for col1 in correlacoes.columns:\n",
        "            for col2 in correlacoes.columns:\n",
        "                if col1 != col2:\n",
        "                    corr_val = correlacoes.loc[col1, col2]\n",
        "                    if abs(corr_val) > 0.7:\n",
        "                        insights.append(f\"CORRELA√á√ÉO FORTE: {col1} e {col2} t√™m correla√ß√£o de {corr_val:.2f}\")\n",
        "\n",
        "    # Identificar outliers autom√°ticos\n",
        "    for col in colunas_numericas:\n",
        "        if df[col].std() > 0:  # Evitar divis√£o por zero\n",
        "            media = df[col].mean()\n",
        "            std = df[col].std()\n",
        "            outliers = df[(df[col] > media + 2*std) | (df[col] < media - 2*std)]\n",
        "\n",
        "            if len(outliers) > 0:\n",
        "                insights.append(f\"OUTLIERS DETECTADOS: {len(outliers)} v√≠deos t√™m valores extremos em {col}\")\n",
        "\n",
        "    # An√°lise de distribui√ß√µes\n",
        "    for col in colunas_numericas:\n",
        "        if col.endswith('_score') or 'score' in col:\n",
        "            media = df[col].mean()\n",
        "            if media > 80:\n",
        "                insights.append(f\"PERFORMANCE ALTA: Score m√©dio de {col} √© {media:.1f} - excelente resultado\")\n",
        "            elif media < 50:\n",
        "                insights.append(f\"OPORTUNIDADE: Score m√©dio de {col} √© {media:.1f} - h√° espa√ßo para melhorias\")\n",
        "\n",
        "    return insights if insights else [\"An√°lise de insights em andamento - dados sendo processados\"]\n",
        "\n",
        "def atualizar_config_com_novas_analises(pasta_trabalho, analises_encontradas):\n",
        "    \"\"\"Atualiza config.json com status de todas as an√°lises encontradas\"\"\"\n",
        "    config_path = os.path.join(pasta_trabalho, \"config\", \"config.json\")\n",
        "\n",
        "    # Carregar config existente\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    # Atualizar status das an√°lises encontradas\n",
        "    for tipo in analises_encontradas[\"base\"]:\n",
        "        config[\"status_etapas\"][tipo] = True\n",
        "\n",
        "    for tipo in analises_encontradas[\"adicionais\"]:\n",
        "        config[\"status_etapas\"][f\"analise_{tipo}\"] = True\n",
        "\n",
        "    config[\"ultima_consolidacao\"] = datetime.now().isoformat()\n",
        "    config[\"total_analises_integradas\"] = len(analises_encontradas[\"base\"]) + len(analises_encontradas[\"adicionais\"])\n",
        "\n",
        "    # Salvar config atualizado\n",
        "    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "def main_integracao_automatica():\n",
        "    \"\"\"Fun√ß√£o principal da integra√ß√£o autom√°tica\"\"\"\n",
        "    print(\"üöÄ INICIANDO INTEGRA√á√ÉO AUTOM√ÅTICA DE TODAS AS AN√ÅLISES\")\n",
        "\n",
        "    # Usar vari√°vel global da pasta de trabalho\n",
        "    if \"PASTA_TRABALHO\" not in globals():\n",
        "        print(\"‚ùå ERRO: Execute as c√©lulas anteriores primeiro\")\n",
        "        return False\n",
        "\n",
        "    pasta_trabalho = PASTA_TRABALHO\n",
        "\n",
        "    try:\n",
        "        # Passo 1: Descobrir an√°lises\n",
        "        print(\"\\nüîç DESCOBRINDO AN√ÅLISES DISPON√çVEIS...\")\n",
        "        analises = descobrir_analises_disponiveis(pasta_trabalho)\n",
        "\n",
        "        total_analises = len(analises[\"base\"]) + len(analises[\"adicionais\"])\n",
        "        print(f\"üìä Total de an√°lises encontradas: {total_analises}\")\n",
        "\n",
        "        # Passo 2: Consolidar dados\n",
        "        print(\"\\nüîÑ CONSOLIDANDO TODOS OS DADOS...\")\n",
        "        df_consolidado = consolidar_todos_dados(analises)\n",
        "\n",
        "        print(f\"üìà {len(df_consolidado)} v√≠deos consolidados com {len(df_consolidado.columns)} m√©tricas totais\")\n",
        "\n",
        "        # Passo 3: Gerar dashboard din√¢mico\n",
        "        print(\"\\nüìä GERANDO DASHBOARD DIN√ÇMICO...\")\n",
        "        dashboard_path = gerar_dashboard_dinamico(df_consolidado, pasta_trabalho)\n",
        "\n",
        "        # Passo 4: Salvar dados consolidados\n",
        "        csv_path = os.path.join(pasta_trabalho, \"dashboard\", \"dados_completos_consolidados.csv\")\n",
        "        df_consolidado.to_csv(csv_path, index=False, encoding='utf-8')\n",
        "\n",
        "        json_path = os.path.join(pasta_trabalho, \"dashboard\", \"dados_completos_consolidados.json\")\n",
        "        df_consolidado.to_json(json_path, orient='records', indent=2, force_ascii=False)\n",
        "\n",
        "        # Passo 5: Atualizar configura√ß√£o\n",
        "        print(\"\\n‚öôÔ∏è ATUALIZANDO CONFIGURA√á√ïES...\")\n",
        "        atualizar_config_com_novas_analises(pasta_trabalho, analises)\n",
        "\n",
        "        # Resultados finais\n",
        "        print(\"\\n‚úÖ INTEGRA√á√ÉO AUTOM√ÅTICA CONCLU√çDA COM SUCESSO!\")\n",
        "        print(f\"üìÅ Dashboard din√¢mico: {dashboard_path}\")\n",
        "        print(f\"üìÅ Dados CSV: {csv_path}\")\n",
        "        print(f\"üìÅ Dados JSON: {json_path}\")\n",
        "        print(f\"üìä {len(df_consolidado)} v√≠deos processados\")\n",
        "        print(f\"üìà {len(df_consolidado.columns)} m√©tricas totais integradas\")\n",
        "\n",
        "        print(\"\\nüéØ PR√ìXIMOS PASSOS:\")\n",
        "        print(\"‚Ä¢ Abra o arquivo Excel para ver todas as an√°lises integradas\")\n",
        "        print(\"‚Ä¢ Use os dados CSV/JSON em outras ferramentas de an√°lise\")\n",
        "        print(\"‚Ä¢ Execute novamente sempre que adicionar novas an√°lises\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå ERRO NA INTEGRA√á√ÉO AUTOM√ÅTICA: {e}\")\n",
        "        print(\"Verifique se todas as an√°lises anteriores foram executadas com sucesso\")\n",
        "        return False\n",
        "\n",
        "# Executar integra√ß√£o autom√°tica\n",
        "if __name__ == \"__main__\":\n",
        "    main_integracao_automatica()"
      ],
      "metadata": {
        "id": "fqmuwt7wttJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# C√âLULA 4.3: DASHBOARD MASTER EXECUTIVO INTELIGENTE APRIMORADO\n",
        "# ============================================================================\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "from openpyxl.styles import Font, Alignment, PatternFill\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def log_progress(message):\n",
        "    \"\"\"Log de progresso em tempo real\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
        "    print(f\"[{timestamp}] {message}\")\n",
        "\n",
        "def calculate_viral_score(row):\n",
        "    \"\"\"Calcula score de viralidade baseado em m√∫ltiplos fatores\"\"\"\n",
        "    try:\n",
        "        score = 0\n",
        "\n",
        "        # Fator 1: Ritmo (cortes por segundo) - peso 25%\n",
        "        if pd.notna(row['duracao_segundos']) and row['duracao_segundos'] > 0:\n",
        "            cortes_por_seg = row['cortes_detectados_count'] / row['duracao_segundos']\n",
        "            if cortes_por_seg > 20: score += 25\n",
        "            elif cortes_por_seg > 10: score += 20\n",
        "            elif cortes_por_seg > 5: score += 15\n",
        "            else: score += 10\n",
        "\n",
        "        # Fator 2: Complexidade Visual - peso 20%\n",
        "        if pd.notna(row['complexidade_visual_media']):\n",
        "            if row['complexidade_visual_media'] > 600: score += 20\n",
        "            elif row['complexidade_visual_media'] > 400: score += 15\n",
        "            else: score += 10\n",
        "\n",
        "        # Fator 3: Presen√ßa de Texto (OCR) - peso 15%\n",
        "        if pd.notna(row['ocr_textos_count']):\n",
        "            if row['ocr_textos_count'] > 10: score += 15\n",
        "            elif row['ocr_textos_count'] > 5: score += 12\n",
        "            elif row['ocr_textos_count'] > 0: score += 8\n",
        "\n",
        "        # Fator 4: Dura√ß√£o Ideal - peso 20%\n",
        "        if pd.notna(row['duracao_segundos']):\n",
        "            if 15 <= row['duracao_segundos'] <= 30: score += 20\n",
        "            elif 10 <= row['duracao_segundos'] <= 45: score += 15\n",
        "            else: score += 10\n",
        "\n",
        "        # Fator 5: Gatilhos Psicol√≥gicos - peso 20%\n",
        "        gatilhos = str(row['gatilhos_psicologicos']).lower()\n",
        "        if 'urg√™ncia' in gatilhos or 'escassez' in gatilhos: score += 8\n",
        "        if 'est√≠mulo' in gatilhos: score += 7\n",
        "        if 'aten√ß√£o' in gatilhos: score += 5\n",
        "\n",
        "        return min(score, 100)\n",
        "    except:\n",
        "        return 50\n",
        "\n",
        "def calculate_technical_score(row):\n",
        "    \"\"\"Score t√©cnico baseado em qualidade de produ√ß√£o\"\"\"\n",
        "    try:\n",
        "        score = 0\n",
        "\n",
        "        if pd.notna(row['brilho_medio']):\n",
        "            if 120 <= row['brilho_medio'] <= 180: score += 25\n",
        "            elif 100 <= row['brilho_medio'] <= 200: score += 20\n",
        "            else: score += 10\n",
        "\n",
        "        formato = str(row['formato_detectado'])\n",
        "        if 'vertical_9_16' in formato: score += 25\n",
        "        elif 'horizontal_16_9' in formato: score += 20\n",
        "        else: score += 15\n",
        "\n",
        "        if row['tem_audio']: score += 25\n",
        "        else: score += 5\n",
        "\n",
        "        if pd.notna(row['total_frames']) and row['total_frames'] > 0:\n",
        "            if row['total_frames'] > 300: score += 25\n",
        "            elif row['total_frames'] > 150: score += 20\n",
        "            else: score += 15\n",
        "\n",
        "        return min(score, 100)\n",
        "    except:\n",
        "        return 50\n",
        "\n",
        "def calculate_content_score(row):\n",
        "    \"\"\"Score de conte√∫do baseado em riqueza informacional\"\"\"\n",
        "    try:\n",
        "        score = 0\n",
        "\n",
        "        ocr_count = row['ocr_textos_count'] if pd.notna(row['ocr_textos_count']) else 0\n",
        "        audio_len = row['audio_transcrito_len'] if pd.notna(row['audio_transcrito_len']) else 0\n",
        "\n",
        "        if ocr_count > 5 or audio_len > 100: score += 30\n",
        "        elif ocr_count > 2 or audio_len > 50: score += 20\n",
        "        elif ocr_count > 0 or audio_len > 0: score += 15\n",
        "        else: score += 5\n",
        "\n",
        "        if pd.notna(row['bpm_audio']):\n",
        "            if 120 <= row['bpm_audio'] <= 140: score += 35\n",
        "            elif 100 <= row['bpm_audio'] <= 160: score += 25\n",
        "            else: score += 15\n",
        "\n",
        "        if pd.notna(row['duracao_segundos']) and row['duracao_segundos'] > 0:\n",
        "            densidade = (ocr_count + audio_len/10) / row['duracao_segundos']\n",
        "            if densidade > 2: score += 35\n",
        "            elif densidade > 1: score += 25\n",
        "            else: score += 15\n",
        "\n",
        "        return min(score, 100)\n",
        "    except:\n",
        "        return 50\n",
        "\n",
        "def generate_insights_from_data(df):\n",
        "    \"\"\"Gera insights inteligentes baseados nos dados\"\"\"\n",
        "    insights = []\n",
        "\n",
        "    try:\n",
        "        best_performing = df.nlargest(3, 'viral_score')\n",
        "        avg_duration = best_performing['duracao_segundos'].mean()\n",
        "        insights.append(f\"DURA√á√ÉO VENCEDORA: Seus top 3 v√≠deos t√™m dura√ß√£o m√©dia de {avg_duration:.1f}s. Este √© seu sweet spot comprovado.\")\n",
        "\n",
        "        avg_cuts_per_sec = (best_performing['cortes_detectados_count'] / best_performing['duracao_segundos']).mean()\n",
        "        insights.append(f\"RITMO IDEAL: {avg_cuts_per_sec:.1f} cortes por segundo √© sua f√≥rmula de edi√ß√£o mais eficaz.\")\n",
        "\n",
        "        formato_winner = df['formato_detectado'].mode()[0] if not df['formato_detectado'].empty else 'N/A'\n",
        "        formato_count = df['formato_detectado'].value_counts().iloc[0] if not df['formato_detectado'].empty else 0\n",
        "        insights.append(f\"FORMATO DOMINANTE: {formato_count} v√≠deos em {formato_winner}. Este √© seu formato de maior alcance.\")\n",
        "\n",
        "        high_viral = df[df['viral_score'] > 70]\n",
        "        if not high_viral.empty:\n",
        "            avg_complexity = high_viral['complexidade_visual_media'].mean()\n",
        "            insights.append(f\"COMPLEXIDADE VISUAL √ìTIMA: V√≠deos com score viral alto t√™m complexidade m√©dia de {avg_complexity:.0f}. Use como refer√™ncia.\")\n",
        "\n",
        "        text_heavy = df[df['ocr_textos_count'] > 5]\n",
        "        if not text_heavy.empty:\n",
        "            insights.append(f\"ESTRAT√âGIA DE TEXTO: {len(text_heavy)} v√≠deos com muito texto t√™m score m√©dio de {text_heavy['viral_score'].mean():.0f}. Texto na tela impacta performance.\")\n",
        "\n",
        "        # CORRIGIDO: bpm_audio em vez de bmp_audio\n",
        "        if df['bpm_audio'].notna().any():\n",
        "            successful_bpm = df[df['viral_score'] > 60]['bpm_audio'].mean()\n",
        "            insights.append(f\"BPM DE SUCESSO: {successful_bpm:.0f} BPM √© o ritmo de √°udio dos seus v√≠deos mais virais.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_progress(f\"Erro ao gerar insights: {e}\")\n",
        "        insights.append(\"Insights parciais dispon√≠veis devido a limita√ß√µes nos dados.\")\n",
        "\n",
        "    return insights\n",
        "\n",
        "def add_data_to_sheet(ws, data, start_row=1, start_col=1, headers=None):\n",
        "    \"\"\"Adiciona dados a uma planilha de forma segura\"\"\"\n",
        "    current_row = start_row\n",
        "\n",
        "    # Adicionar cabe√ßalhos se fornecidos\n",
        "    if headers:\n",
        "        for col_idx, header in enumerate(headers):\n",
        "            cell = ws.cell(row=current_row, column=start_col + col_idx)\n",
        "            cell.value = header\n",
        "            cell.font = Font(bold=True)\n",
        "        current_row += 1\n",
        "\n",
        "    # Adicionar dados\n",
        "    for row_data in data:\n",
        "        for col_idx, value in enumerate(row_data):\n",
        "            cell = ws.cell(row=current_row, column=start_col + col_idx)\n",
        "            cell.value = value\n",
        "        current_row += 1\n",
        "\n",
        "    return current_row\n",
        "\n",
        "def create_enhanced_dashboard_master(csv_path, json_path, output_path):\n",
        "    \"\"\"Cria dashboard master executivo aprimorado\"\"\"\n",
        "\n",
        "    log_progress(\"INICIANDO CRIA√á√ÉO DO DASHBOARD MASTER EXECUTIVO INTELIGENTE\")\n",
        "\n",
        "    try:\n",
        "        # Carregar dados\n",
        "        log_progress(\"Carregando dados consolidados...\")\n",
        "        df_consolidado = pd.read_csv(csv_path, encoding='utf-8')\n",
        "\n",
        "        with open(json_path, 'r', encoding='utf-8') as f:\n",
        "            dados_detalhados = json.load(f)\n",
        "\n",
        "        log_progress(f\"Dados carregados: {len(df_consolidado)} v√≠deos encontrados\")\n",
        "\n",
        "        # Pr√©-processamento inteligente\n",
        "        log_progress(\"Processando intelig√™ncia artificial dos dados...\")\n",
        "\n",
        "        # Limpar e converter dados\n",
        "        try:\n",
        "            df_consolidado['emocoes_predominantes'] = df_consolidado['emocoes_predominantes'].apply(\n",
        "                lambda x: json.loads(x.replace(\"'\", '\"')) if pd.notna(x) and x != '{}' else {}\n",
        "            )\n",
        "        except:\n",
        "            df_consolidado['emocoes_predominantes'] = [{}] * len(df_consolidado)\n",
        "\n",
        "        # Calcular scores inteligentes\n",
        "        log_progress(\"Calculando scores de performance...\")\n",
        "        df_consolidado['viral_score'] = df_consolidado.apply(calculate_viral_score, axis=1)\n",
        "        df_consolidado['technical_score'] = df_consolidado.apply(calculate_technical_score, axis=1)\n",
        "        df_consolidado['content_score'] = df_consolidado.apply(calculate_content_score, axis=1)\n",
        "        df_consolidado['overall_score'] = (df_consolidado['viral_score'] + df_consolidado['technical_score'] + df_consolidado['content_score']) / 3\n",
        "\n",
        "        # Calcular m√©tricas avan√ßadas\n",
        "        df_consolidado['cortes_por_segundo'] = df_consolidado['cortes_detectados_count'] / df_consolidado['duracao_segundos'].replace(0, 1)\n",
        "        df_consolidado['densidade_texto'] = df_consolidado['ocr_textos_count'] / df_consolidado['duracao_segundos'].replace(0, 1)\n",
        "        df_consolidado['eficiencia_audio'] = df_consolidado['audio_transcrito_len'] / df_consolidado['duracao_segundos'].replace(0, 1)\n",
        "\n",
        "        log_progress(\"Gerando insights estrat√©gicos...\")\n",
        "        insights = generate_insights_from_data(df_consolidado)\n",
        "\n",
        "        # Criar workbook\n",
        "        log_progress(\"Criando estrutura do dashboard...\")\n",
        "        wb = Workbook()\n",
        "\n",
        "        # === ABA 1: EXECUTIVE SUMMARY ===\n",
        "        log_progress(\"Criando Executive Summary...\")\n",
        "        ws_summary = wb.active\n",
        "        ws_summary.title = 'Executive Summary'\n",
        "\n",
        "        # Header principal\n",
        "        header_cell = ws_summary.cell(row=1, column=1)\n",
        "        header_cell.value = 'DASHBOARD MASTER EXECUTIVO - ENGENHARIA REVERSA DE V√çDEOS'\n",
        "        header_cell.font = Font(bold=True, size=18, color='FFFFFF')\n",
        "        header_cell.fill = PatternFill(start_color='1F4E79', end_color='1F4E79', fill_type='solid')\n",
        "        header_cell.alignment = Alignment(horizontal='center', vertical='center')\n",
        "\n",
        "        # Expandir header manualmente\n",
        "        for col in range(2, 9):\n",
        "            cell = ws_summary.cell(row=1, column=col)\n",
        "            cell.fill = PatternFill(start_color='1F4E79', end_color='1F4E79', fill_type='solid')\n",
        "\n",
        "        # KPIs Principais\n",
        "        kpi_cell = ws_summary.cell(row=3, column=1)\n",
        "        kpi_cell.value = 'INDICADORES DE PERFORMANCE PRINCIPAIS'\n",
        "        kpi_cell.font = Font(bold=True, size=14)\n",
        "        kpi_cell.fill = PatternFill(start_color='E7E6E6', end_color='E7E6E6', fill_type='solid')\n",
        "\n",
        "        kpis_data = [\n",
        "            ['Total de V√≠deos Analisados', len(df_consolidado)],\n",
        "            ['Score Viral M√©dio', f\"{df_consolidado['viral_score'].mean():.1f}/100\"],\n",
        "            ['Score T√©cnico M√©dio', f\"{df_consolidado['technical_score'].mean():.1f}/100\"],\n",
        "            ['Score de Conte√∫do M√©dio', f\"{df_consolidado['content_score'].mean():.1f}/100\"],\n",
        "            ['Dura√ß√£o M√©dia Otimizada', f\"{df_consolidado['duracao_segundos'].mean():.1f}s\"],\n",
        "            ['Ritmo M√©dio de Cortes', f\"{df_consolidado['cortes_por_segundo'].mean():.1f}/seg\"],\n",
        "        ]\n",
        "\n",
        "        add_data_to_sheet(ws_summary, kpis_data, start_row=4, start_col=1)\n",
        "\n",
        "        # Top 3 V√≠deos\n",
        "        top3_cell = ws_summary.cell(row=3, column=4)\n",
        "        top3_cell.value = 'TOP 3 V√çDEOS POR PERFORMANCE'\n",
        "        top3_cell.font = Font(bold=True, size=14)\n",
        "        top3_cell.fill = PatternFill(start_color='E7E6E6', end_color='E7E6E6', fill_type='solid')\n",
        "\n",
        "        top3 = df_consolidado.nlargest(3, 'overall_score')[['nome_arquivo', 'overall_score', 'viral_score', 'technical_score', 'content_score']]\n",
        "\n",
        "        top3_data = []\n",
        "        for _, video in top3.iterrows():\n",
        "            nome_curto = video['nome_arquivo'][:30] + \"...\" if len(video['nome_arquivo']) > 30 else video['nome_arquivo']\n",
        "            top3_data.append([\n",
        "                nome_curto,\n",
        "                f\"{video['overall_score']:.1f}\",\n",
        "                f\"{video['viral_score']:.1f}\",\n",
        "                f\"{video['technical_score']:.1f}\",\n",
        "                f\"{video['content_score']:.1f}\"\n",
        "            ])\n",
        "\n",
        "        top3_headers = ['V√≠deo', 'Score Geral', 'Viral', 'T√©cnico', 'Conte√∫do']\n",
        "        add_data_to_sheet(ws_summary, top3_data, start_row=4, start_col=4, headers=top3_headers)\n",
        "\n",
        "        # Insights Estrat√©gicos\n",
        "        insights_cell = ws_summary.cell(row=12, column=1)\n",
        "        insights_cell.value = 'INSIGHTS ESTRAT√âGICOS BASEADOS EM IA'\n",
        "        insights_cell.font = Font(bold=True, size=14, color='FFFFFF')\n",
        "        insights_cell.fill = PatternFill(start_color='C5504B', end_color='C5504B', fill_type='solid')\n",
        "        insights_cell.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Adicionar insights\n",
        "        for i, insight in enumerate(insights, 13):\n",
        "            insight_cell = ws_summary.cell(row=i, column=1)\n",
        "            insight_cell.value = f\"‚Ä¢ {insight}\"\n",
        "            insight_cell.alignment = Alignment(wrap_text=True)\n",
        "\n",
        "        # === ABA 2: AN√ÅLISE DE PERFORMANCE ===\n",
        "        log_progress(\"Criando An√°lise de Performance...\")\n",
        "        ws_performance = wb.create_sheet('An√°lise de Performance')\n",
        "\n",
        "        perf_header = ws_performance.cell(row=1, column=1)\n",
        "        perf_header.value = 'AN√ÅLISE DETALHADA DE PERFORMANCE'\n",
        "        perf_header.font = Font(bold=True, size=16)\n",
        "        perf_header.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Ranking completo\n",
        "        ranking_data = df_consolidado[['nome_arquivo', 'overall_score', 'viral_score', 'technical_score', 'content_score',\n",
        "                                     'duracao_segundos', 'cortes_por_segundo', 'formato_detectado']].sort_values('overall_score', ascending=False)\n",
        "\n",
        "        ranking_list = []\n",
        "        for _, video in ranking_data.iterrows():\n",
        "            nome_curto = video['nome_arquivo'][:40] + \"...\" if len(video['nome_arquivo']) > 40 else video['nome_arquivo']\n",
        "            ranking_list.append([\n",
        "                nome_curto,\n",
        "                f\"{video['overall_score']:.1f}\",\n",
        "                f\"{video['viral_score']:.1f}\",\n",
        "                f\"{video['technical_score']:.1f}\",\n",
        "                f\"{video['content_score']:.1f}\",\n",
        "                f\"{video['duracao_segundos']:.1f}s\",\n",
        "                f\"{video['cortes_por_segundo']:.1f}\",\n",
        "                video['formato_detectado']\n",
        "            ])\n",
        "\n",
        "        ranking_headers = ['V√≠deo', 'Score Geral', 'Viral', 'T√©cnico', 'Conte√∫do', 'Dura√ß√£o', 'Cortes/s', 'Formato']\n",
        "        add_data_to_sheet(ws_performance, ranking_list, start_row=3, start_col=1, headers=ranking_headers)\n",
        "\n",
        "        # === ABA 3: INTELIG√äNCIA T√âCNICA ===\n",
        "        log_progress(\"Criando Intelig√™ncia T√©cnica...\")\n",
        "        ws_tecnica = wb.create_sheet('Intelig√™ncia T√©cnica')\n",
        "\n",
        "        tec_header = ws_tecnica.cell(row=1, column=1)\n",
        "        tec_header.value = 'AN√ÅLISE T√âCNICA AVAN√áADA'\n",
        "        tec_header.font = Font(bold=True, size=16)\n",
        "        tec_header.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # An√°lise de correla√ß√µes\n",
        "        corr_header = ws_tecnica.cell(row=3, column=1)\n",
        "        corr_header.value = 'CORRELA√á√ïES DESCOBERTAS'\n",
        "        corr_header.font = Font(bold=True, size=12)\n",
        "\n",
        "        correlations_data = [\n",
        "            ['Dura√ß√£o vs Score Viral', f\"{df_consolidado['duracao_segundos'].corr(df_consolidado['viral_score']):.3f}\", 'CORRELA√á√ÉO MODERADA'],\n",
        "            ['Cortes/s vs Score Viral', f\"{df_consolidado['cortes_por_segundo'].corr(df_consolidado['viral_score']):.3f}\", 'CORRELA√á√ÉO MODERADA'],\n",
        "            ['Complexidade Visual vs Performance', f\"{df_consolidado['complexidade_visual_media'].corr(df_consolidado['overall_score']):.3f}\", 'CORRELA√á√ÉO FRACA'],\n",
        "            ['BPM vs Engajamento', f\"{df_consolidado['bpm_audio'].corr(df_consolidado['viral_score']) if df_consolidado['bpm_audio'].notna().any() else 0:.3f}\", 'CORRELA√á√ÉO FRACA'],\n",
        "        ]\n",
        "\n",
        "        corr_headers = ['M√©trica', 'Correla√ß√£o', 'Classifica√ß√£o']\n",
        "        add_data_to_sheet(ws_tecnica, correlations_data, start_row=4, start_col=1, headers=corr_headers)\n",
        "\n",
        "        # === ABA 4: BLUEPRINT DE PRODU√á√ÉO ===\n",
        "        log_progress(\"Criando Blueprint de Produ√ß√£o...\")\n",
        "        ws_blueprint = wb.create_sheet('Blueprint de Produ√ß√£o')\n",
        "\n",
        "        bp_header = ws_blueprint.cell(row=1, column=1)\n",
        "        bp_header.value = 'BLUEPRINT ESTRAT√âGICO DE PRODU√á√ÉO'\n",
        "        bp_header.font = Font(bold=True, size=16, color='FFFFFF')\n",
        "        bp_header.fill = PatternFill(start_color='1F4E79', end_color='1F4E79', fill_type='solid')\n",
        "        bp_header.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Receita de sucesso baseada nos top performers\n",
        "        top_performers = df_consolidado[df_consolidado['overall_score'] > df_consolidado['overall_score'].quantile(0.7)]\n",
        "\n",
        "        blueprint_data = [\n",
        "            ['DURA√á√ÉO IDEAL', f\"{top_performers['duracao_segundos'].mean():.1f} segundos (¬±{top_performers['duracao_segundos'].std():.1f}s)\"],\n",
        "            ['RITMO DE EDI√á√ÉO', f\"{top_performers['cortes_por_segundo'].mean():.1f} cortes por segundo\"],\n",
        "            ['FORMATO VENCEDOR', top_performers['formato_detectado'].mode()[0] if not top_performers.empty else 'N/A'],\n",
        "            ['COMPLEXIDADE VISUAL', f\"N√≠vel {top_performers['complexidade_visual_media'].mean():.0f} (escala de est√≠mulo)\"],\n",
        "            ['BPM RECOMENDADO', f\"{top_performers['bpm_audio'].mean():.0f} BPM\" if top_performers['bpm_audio'].notna().any() else 'N/A'],\n",
        "            ['DENSIDADE DE TEXTO', f\"{top_performers['densidade_texto'].mean():.1f} textos por segundo\"],\n",
        "        ]\n",
        "\n",
        "        bp_sub_header = ws_blueprint.cell(row=3, column=1)\n",
        "        bp_sub_header.value = 'F√ìRMULA DE SUCESSO BASEADA EM DADOS'\n",
        "        bp_sub_header.font = Font(bold=True, size=12)\n",
        "\n",
        "        add_data_to_sheet(ws_blueprint, blueprint_data, start_row=4, start_col=1)\n",
        "\n",
        "        # === ABA 5: RECOMENDA√á√ïES ESTRAT√âGICAS ===\n",
        "        log_progress(\"Criando Recomenda√ß√µes Estrat√©gicas...\")\n",
        "        ws_recomendacoes = wb.create_sheet('Recomenda√ß√µes Estrat√©gicas')\n",
        "\n",
        "        rec_header = ws_recomendacoes.cell(row=1, column=1)\n",
        "        rec_header.value = 'RECOMENDA√á√ïES ESTRAT√âGICAS BASEADAS EM IA'\n",
        "        rec_header.font = Font(bold=True, size=16, color='FFFFFF')\n",
        "        rec_header.fill = PatternFill(start_color='C5504B', end_color='C5504B', fill_type='solid')\n",
        "        rec_header.alignment = Alignment(horizontal='center')\n",
        "\n",
        "        # Recomenda√ß√µes inteligentes baseadas nos dados\n",
        "        recommendations = []\n",
        "\n",
        "        # An√°lise de dura√ß√£o\n",
        "        if df_consolidado['duracao_segundos'].mean() > 60:\n",
        "            recommendations.append(['DURA√á√ÉO', 'REDUZA DURA√á√ÉO', 'Seus v√≠deos est√£o longos demais. V√≠deos de 15-30s t√™m melhor performance.', 'ALTA'])\n",
        "        elif df_consolidado['duracao_segundos'].mean() < 15:\n",
        "            recommendations.append(['DURA√á√ÉO', 'AUMENTE DURA√á√ÉO', 'V√≠deos muito curtos podem n√£o transmitir valor suficiente.', 'M√âDIA'])\n",
        "\n",
        "        # An√°lise de ritmo\n",
        "        avg_cuts_per_sec = df_consolidado['cortes_por_segundo'].mean()\n",
        "        if avg_cuts_per_sec < 5:\n",
        "            recommendations.append(['EDI√á√ÉO', 'ACELERE O RITMO', 'Aumente o n√∫mero de cortes para manter aten√ß√£o. Meta: 8-12 cortes/segundo.', 'ALTA'])\n",
        "        elif avg_cuts_per_sec > 20:\n",
        "            recommendations.append(['EDI√á√ÉO', 'DIMINUA CORTES', 'Muitos cortes podem causar fadiga visual. Encontre o equil√≠brio.', 'M√âDIA'])\n",
        "\n",
        "        # An√°lise de formato\n",
        "        formato_dominante = df_consolidado['formato_detectado'].mode()[0] if not df_consolidado['formato_detectado'].empty else 'N/A'\n",
        "        if 'horizontal' in formato_dominante.lower():\n",
        "            recommendations.append(['FORMATO', 'FOQUE EM VERTICAL', 'Formato vertical (9:16) tem melhor performance em redes sociais.', 'ALTA'])\n",
        "\n",
        "        # An√°lise de texto\n",
        "        if df_consolidado['densidade_texto'].mean() < 1:\n",
        "            recommendations.append(['CONTE√öDO', 'ADICIONE MAIS TEXTO', 'Textos na tela aumentam reten√ß√£o e acessibilidade.', 'M√âDIA'])\n",
        "\n",
        "        rec_headers = ['Categoria', 'A√ß√£o', 'Justificativa', 'Prioridade']\n",
        "        add_data_to_sheet(ws_recomendacoes, recommendations, start_row=3, start_col=1, headers=rec_headers)\n",
        "\n",
        "        # Salvar arquivo\n",
        "        log_progress(\"Salvando dashboard...\")\n",
        "        wb.save(output_path)\n",
        "\n",
        "        log_progress(\"DASHBOARD MASTER EXECUTIVO CRIADO COM SUCESSO!\")\n",
        "        log_progress(f\"Arquivo salvo em: {output_path}\")\n",
        "        log_progress(f\"{len(df_consolidado)} v√≠deos analisados\")\n",
        "        log_progress(f\"{len(insights)} insights estrat√©gicos gerados\")\n",
        "        log_progress(f\"{len(recommendations)} recomenda√ß√µes criadas\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        log_progress(f\"ERRO CR√çTICO: {e}\")\n",
        "        log_progress(\"Verifique os arquivos de entrada e tente novamente\")\n",
        "        return False\n",
        "\n",
        "def main():\n",
        "    \"\"\"Fun√ß√£o principal de execu√ß√£o\"\"\"\n",
        "    log_progress(\"INICIANDO SISTEMA DE DASHBOARD INTELIGENTE\")\n",
        "\n",
        "    # Configurar caminhos\n",
        "    BASE_PATH = \"/content/drive/MyDrive/Videos Dona Done/_engenharia_reversa\"\n",
        "    CSV_PATH = os.path.join(BASE_PATH, \"dashboard\", \"dados_consolidados.csv\")\n",
        "    JSON_PATH = os.path.join(BASE_PATH, \"dashboard\", \"dados_detalhados.json\")\n",
        "    OUTPUT_PATH = os.path.join(BASE_PATH, \"dashboard\", \"DASHBOARD_MASTER_EXECUTIVO_INTELIGENTE.xlsx\")\n",
        "\n",
        "    # Verificar se arquivos existem\n",
        "    if not os.path.exists(CSV_PATH):\n",
        "        log_progress(f\"ERRO: Arquivo CSV n√£o encontrado: {CSV_PATH}\")\n",
        "        return False\n",
        "\n",
        "    if not os.path.exists(JSON_PATH):\n",
        "        log_progress(f\"ERRO: Arquivo JSON n√£o encontrado: {JSON_PATH}\")\n",
        "        return False\n",
        "\n",
        "    # Executar cria√ß√£o do dashboard\n",
        "    success = create_enhanced_dashboard_master(CSV_PATH, JSON_PATH, OUTPUT_PATH)\n",
        "\n",
        "    if success:\n",
        "        log_progress(\"PROCESSO CONCLU√çDO COM SUCESSO!\")\n",
        "        log_progress(\"Dashboard inteligente pronto para uso estrat√©gico\")\n",
        "    else:\n",
        "        log_progress(\"PROCESSO FALHOU - Verifique os logs acima\")\n",
        "\n",
        "    return success\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUeniUqRJLuo",
        "outputId": "6a885e66-3bd2-4bb9-cc2b-f584abab91a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[23:42:03] INICIANDO SISTEMA DE DASHBOARD INTELIGENTE\n",
            "[23:42:03] INICIANDO CRIA√á√ÉO DO DASHBOARD MASTER EXECUTIVO INTELIGENTE\n",
            "[23:42:03] Carregando dados consolidados...\n",
            "[23:42:03] Dados carregados: 5 v√≠deos encontrados\n",
            "[23:42:03] Processando intelig√™ncia artificial dos dados...\n",
            "[23:42:03] Calculando scores de performance...\n",
            "[23:42:03] Gerando insights estrat√©gicos...\n",
            "[23:42:03] Criando estrutura do dashboard...\n",
            "[23:42:03] Criando Executive Summary...\n",
            "[23:42:03] Criando An√°lise de Performance...\n",
            "[23:42:03] Criando Intelig√™ncia T√©cnica...\n",
            "[23:42:03] Criando Blueprint de Produ√ß√£o...\n",
            "[23:42:03] Criando Recomenda√ß√µes Estrat√©gicas...\n",
            "[23:42:03] Salvando dashboard...\n",
            "[23:42:03] DASHBOARD MASTER EXECUTIVO CRIADO COM SUCESSO!\n",
            "[23:42:03] Arquivo salvo em: /content/drive/MyDrive/Videos Dona Done/_engenharia_reversa/dashboard/DASHBOARD_MASTER_EXECUTIVO_INTELIGENTE.xlsx\n",
            "[23:42:03] 5 v√≠deos analisados\n",
            "[23:42:03] 6 insights estrat√©gicos gerados\n",
            "[23:42:03] 2 recomenda√ß√µes criadas\n",
            "[23:42:03] PROCESSO CONCLU√çDO COM SUCESSO!\n",
            "[23:42:03] Dashboard inteligente pronto para uso estrat√©gico\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "31zeNKZVLTVP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}